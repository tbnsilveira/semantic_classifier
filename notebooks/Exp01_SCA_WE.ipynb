{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Semantic Similarity - Experiment 01\n",
    "The objective of this trial is to expand the SCA_index (i.e., Semantic Content Analysis Index) to a full word embedding, setting a subjective or objective load for each word."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Introduction"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 100,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-11-12T22:37:14.251208Z",
     "start_time": "2022-11-12T22:37:14.073574Z"
    }
   },
   "outputs": [],
   "source": [
    "## Data analysis packages:\n",
    "import pandas as pd\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 101,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-11-12T22:37:15.470110Z",
     "start_time": "2022-11-12T22:37:14.895488Z"
    }
   },
   "outputs": [],
   "source": [
    "## Visualization packages:\n",
    "# import seaborn as sns\n",
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 102,
   "metadata": {},
   "outputs": [],
   "source": [
    "## MLOps:\n",
    "import pickle"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Definitions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 103,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-04-28T22:15:12.656866Z",
     "start_time": "2022-04-28T22:15:12.612272Z"
    }
   },
   "outputs": [],
   "source": [
    "## Forcing Pandas to display any number of elements\n",
    "pd.set_option('display.max_columns', None)\n",
    "pd.set_option('display.max_rows', 50)\n",
    "pd.set_option('display.max_seq_items', None)\n",
    "pd.set_option('display.width', 2000)\n",
    "pd.set_option('display.max_colwidth', None)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## SpaCy Word Embeddings (WE): \n",
    "Also using Spacy library: https://spacy.io/\n",
    "> !pip install -U spacy  \n",
    "> !python -m spacy download en_core_web_sm  \n",
    "> !python -m spacy download en_core_web_lg\n",
    "\n",
    "Some instructions on how to use it:  \n",
    "https://spacy.io/usage/spacy-101"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 104,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-04-28T22:38:26.997959Z",
     "start_time": "2022-04-28T22:38:25.715707Z"
    }
   },
   "outputs": [],
   "source": [
    "## Importing SpaCy library:\n",
    "import spacy\n",
    "\n",
    "# Load English tokenizer, tagger, parser and NER\n",
    "nlp = spacy.load(\"en_core_web_lg\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 105,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'width': 300,\n",
       " 'vectors': 514157,\n",
       " 'keys': 514157,\n",
       " 'name': 'en_vectors',\n",
       " 'mode': 'default'}"
      ]
     },
     "execution_count": 105,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "## We check below that this model has 514.157 keys and vectors, respectively.\n",
    "nlp.meta['vectors']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 106,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "514157"
      ]
     },
     "execution_count": 106,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "## Again, checking the number of keys.\n",
    "nlp.vocab.vectors.n_keys"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 107,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "4473158105997569131"
      ]
     },
     "execution_count": 107,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "## Finding the SpaCy index for a given word:\n",
    "nlp.vocab.strings['problem']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 108,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Getting the text and vector for a given index:\n",
    "tmp_Idx = 4473158105997569131\n",
    "\n",
    "tmp_vector = nlp.vocab[tmp_Idx].vector\n",
    "tmp_text = nlp.vocab[tmp_Idx].text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 109,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "4473158105997569131: problem:\n",
      "[ 3.3611  4.0891 -2.1247]...\n"
     ]
    }
   ],
   "source": [
    "print(f'{tmp_Idx}: {tmp_text}:\\n{tmp_vector[0:3]}...')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 110,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Defining a method to get vector and text information for a word:\n",
    "def nlp_getVector(word, verbose=False):\n",
    "    '''\n",
    "    Obtains the vector representation of a given word from SpaCy word embedding.\n",
    "    Usage:  nlp_getVector(word)[0] to get the text; nlp_getVector(word)[1] to get the vector.\n",
    "            var_text, var_vector = nlp_getVector(word)\n",
    "    '''\n",
    "    ## Generates the word hash:\n",
    "    hash = nlp.vocab.strings[word]\n",
    "    try:\n",
    "        word_vector = nlp.vocab[hash].vector\n",
    "        word_text = nlp.vocab[hash].text\n",
    "    except:\n",
    "        if verbose:\n",
    "            print('Error: word vector not available.')\n",
    "        return None\n",
    "    return (word_text, word_vector)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 111,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Word: study <-> Vector 3rd elements: [-0.64776 -1.3091   2.0104 ]\n"
     ]
    }
   ],
   "source": [
    "## Testing the method:\n",
    "print(f'Word: {nlp_getVector(\"study\")[0]} <-> Vector 3rd elements: {nlp_getVector(\"study\")[1][:3]}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "## Semantic Content Analysis (SCA)\n",
    "* Read the SCA obtained from Glasgow Norms data;  \n",
    "* Import F_s and F_o from the previous study;  \n",
    "* Generate datasets for training classifiers."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 112,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>words</th>\n",
       "      <th>F_Objectivity</th>\n",
       "      <th>F_Subjectivity</th>\n",
       "      <th>F_Context</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>abattoir</td>\n",
       "      <td>0.512527</td>\n",
       "      <td>0.380603</td>\n",
       "      <td>0.960466</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>abbey</td>\n",
       "      <td>0.714765</td>\n",
       "      <td>0.240456</td>\n",
       "      <td>0.696198</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>abbreviate</td>\n",
       "      <td>0.286952</td>\n",
       "      <td>0.171052</td>\n",
       "      <td>0.767043</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>abdicate</td>\n",
       "      <td>0.144736</td>\n",
       "      <td>0.384300</td>\n",
       "      <td>0.863127</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>abdication</td>\n",
       "      <td>0.167654</td>\n",
       "      <td>0.334086</td>\n",
       "      <td>0.896733</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "        words  F_Objectivity  F_Subjectivity  F_Context\n",
       "0    abattoir       0.512527        0.380603   0.960466\n",
       "1       abbey       0.714765        0.240456   0.696198\n",
       "2  abbreviate       0.286952        0.171052   0.767043\n",
       "3    abdicate       0.144736        0.384300   0.863127\n",
       "4  abdication       0.167654        0.334086   0.896733"
      ]
     },
     "execution_count": 112,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_factors = pd.read_csv('../data/df_factors.csv', sep=';')\n",
    "df_factors.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### De-Duplicating words  \n",
    "There are words in the Glasgow Norms that were differentiated from their homonymous, such as 'case'. In this section, we first select those words and then input a mean value for them."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 113,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>words</th>\n",
       "      <th>F_Objectivity</th>\n",
       "      <th>F_Subjectivity</th>\n",
       "      <th>F_Context</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>533</th>\n",
       "      <td>bookcase</td>\n",
       "      <td>0.926393</td>\n",
       "      <td>0.374441</td>\n",
       "      <td>0.335542</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>756</th>\n",
       "      <td>case</td>\n",
       "      <td>0.715863</td>\n",
       "      <td>0.164100</td>\n",
       "      <td>0.409611</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>757</th>\n",
       "      <td>case (container)</td>\n",
       "      <td>0.821136</td>\n",
       "      <td>0.079956</td>\n",
       "      <td>0.400335</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>758</th>\n",
       "      <td>case (instance)</td>\n",
       "      <td>0.233820</td>\n",
       "      <td>0.213528</td>\n",
       "      <td>0.651273</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>759</th>\n",
       "      <td>case (legal)</td>\n",
       "      <td>0.456260</td>\n",
       "      <td>0.369828</td>\n",
       "      <td>0.733642</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4808</th>\n",
       "      <td>suitcase</td>\n",
       "      <td>0.961068</td>\n",
       "      <td>0.256584</td>\n",
       "      <td>0.356338</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                 words  F_Objectivity  F_Subjectivity  F_Context\n",
       "533           bookcase       0.926393        0.374441   0.335542\n",
       "756               case       0.715863        0.164100   0.409611\n",
       "757   case (container)       0.821136        0.079956   0.400335\n",
       "758    case (instance)       0.233820        0.213528   0.651273\n",
       "759       case (legal)       0.456260        0.369828   0.733642\n",
       "4808          suitcase       0.961068        0.256584   0.356338"
      ]
     },
     "execution_count": 113,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "## Exemplifying the need for de-duplication:\n",
    "df_factors[df_factors['words'].str.contains('case')]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 114,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>words</th>\n",
       "      <th>F_Objectivity</th>\n",
       "      <th>F_Subjectivity</th>\n",
       "      <th>F_Context</th>\n",
       "      <th>word</th>\n",
       "      <th>distinction</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>abattoir</td>\n",
       "      <td>0.512527</td>\n",
       "      <td>0.380603</td>\n",
       "      <td>0.960466</td>\n",
       "      <td>abattoir</td>\n",
       "      <td>None</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>abbey</td>\n",
       "      <td>0.714765</td>\n",
       "      <td>0.240456</td>\n",
       "      <td>0.696198</td>\n",
       "      <td>abbey</td>\n",
       "      <td>None</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>abbreviate</td>\n",
       "      <td>0.286952</td>\n",
       "      <td>0.171052</td>\n",
       "      <td>0.767043</td>\n",
       "      <td>abbreviate</td>\n",
       "      <td>None</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>abdicate</td>\n",
       "      <td>0.144736</td>\n",
       "      <td>0.384300</td>\n",
       "      <td>0.863127</td>\n",
       "      <td>abdicate</td>\n",
       "      <td>None</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>abdication</td>\n",
       "      <td>0.167654</td>\n",
       "      <td>0.334086</td>\n",
       "      <td>0.896733</td>\n",
       "      <td>abdication</td>\n",
       "      <td>None</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "        words  F_Objectivity  F_Subjectivity  F_Context        word distinction\n",
       "0    abattoir       0.512527        0.380603   0.960466    abattoir        None\n",
       "1       abbey       0.714765        0.240456   0.696198       abbey        None\n",
       "2  abbreviate       0.286952        0.171052   0.767043  abbreviate        None\n",
       "3    abdicate       0.144736        0.384300   0.863127    abdicate        None\n",
       "4  abdication       0.167654        0.334086   0.896733  abdication        None"
      ]
     },
     "execution_count": 114,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "## Creating a new dataframe by splitting the 'words' column into two columns:\n",
    "df_homonym = df_factors.copy()\n",
    "df_homonym[['word','distinction']] = df_homonym['words'].str.split('(', expand=True)\n",
    "## Renaming the columns of the new dataframe:\n",
    "# df_homonym.columns = ['word','distinction']\n",
    "\n",
    "# Stripping whitespace from the new columns\n",
    "df_homonym['word'] = df_homonym['word'].str.strip()\n",
    "df_homonym['distinction'] = df_homonym['distinction'].str.strip().str.rstrip(')')\n",
    "\n",
    "## Showing dataframe:\n",
    "df_homonym.head(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 115,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>n_distinction</th>\n",
       "      <th>n_words</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0</td>\n",
       "      <td>4303</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>2</td>\n",
       "      <td>288</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>3</td>\n",
       "      <td>69</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>4</td>\n",
       "      <td>19</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>5</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   n_distinction  n_words\n",
       "0              0     4303\n",
       "1              1        2\n",
       "2              2      288\n",
       "3              3       69\n",
       "4              4       19\n",
       "5              5        2"
      ]
     },
     "execution_count": 115,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Calculating the number of distinct elements in the 'distinction' column for each word in the 'word' column\n",
    "distinct_count = df_homonym.groupby('word')['distinction'].nunique().reset_index()\n",
    "\n",
    "# Counting the frequency of different numbers of distinct elements\n",
    "pivot_table = distinct_count.groupby('distinction')['word'].count().reset_index()\n",
    "\n",
    "# Renaming the columns\n",
    "pivot_table.columns = ['n_distinction', 'n_words']\n",
    "pivot_table"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Analyzing the words with 5 distinct meaning:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 116,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>word</th>\n",
       "      <th>distinction</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>721</th>\n",
       "      <td>charge</td>\n",
       "      <td>5</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>822</th>\n",
       "      <td>club</td>\n",
       "      <td>5</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "       word  distinction\n",
       "721  charge            5\n",
       "822    club            5"
      ]
     },
     "execution_count": 116,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "## Getting the word with the highest number of distinct meanings: \n",
    "distinct_count[distinct_count['distinction'] == 5]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 117,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>words</th>\n",
       "      <th>F_Objectivity</th>\n",
       "      <th>F_Subjectivity</th>\n",
       "      <th>F_Context</th>\n",
       "      <th>word</th>\n",
       "      <th>distinction</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>819</th>\n",
       "      <td>charge</td>\n",
       "      <td>0.340881</td>\n",
       "      <td>0.330534</td>\n",
       "      <td>0.438418</td>\n",
       "      <td>charge</td>\n",
       "      <td>None</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>820</th>\n",
       "      <td>charge (card)</td>\n",
       "      <td>0.477083</td>\n",
       "      <td>0.293352</td>\n",
       "      <td>0.671798</td>\n",
       "      <td>charge</td>\n",
       "      <td>card</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>821</th>\n",
       "      <td>charge (dependent)</td>\n",
       "      <td>0.256759</td>\n",
       "      <td>0.164572</td>\n",
       "      <td>0.791042</td>\n",
       "      <td>charge</td>\n",
       "      <td>dependent</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>822</th>\n",
       "      <td>charge (electric)</td>\n",
       "      <td>0.391504</td>\n",
       "      <td>0.413080</td>\n",
       "      <td>0.697287</td>\n",
       "      <td>charge</td>\n",
       "      <td>electric</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>823</th>\n",
       "      <td>charge (price)</td>\n",
       "      <td>0.428509</td>\n",
       "      <td>0.353942</td>\n",
       "      <td>0.498770</td>\n",
       "      <td>charge</td>\n",
       "      <td>price</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>824</th>\n",
       "      <td>charge (rush)</td>\n",
       "      <td>0.551191</td>\n",
       "      <td>0.557549</td>\n",
       "      <td>0.591914</td>\n",
       "      <td>charge</td>\n",
       "      <td>rush</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>942</th>\n",
       "      <td>club</td>\n",
       "      <td>0.774815</td>\n",
       "      <td>0.476734</td>\n",
       "      <td>0.467275</td>\n",
       "      <td>club</td>\n",
       "      <td>None</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>943</th>\n",
       "      <td>club (card suit)</td>\n",
       "      <td>0.773094</td>\n",
       "      <td>0.187918</td>\n",
       "      <td>0.492067</td>\n",
       "      <td>club</td>\n",
       "      <td>card suit</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>944</th>\n",
       "      <td>club (disco)</td>\n",
       "      <td>0.863845</td>\n",
       "      <td>0.547812</td>\n",
       "      <td>0.658553</td>\n",
       "      <td>club</td>\n",
       "      <td>disco</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>945</th>\n",
       "      <td>club (hit)</td>\n",
       "      <td>0.725571</td>\n",
       "      <td>0.435103</td>\n",
       "      <td>0.607930</td>\n",
       "      <td>club</td>\n",
       "      <td>hit</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>946</th>\n",
       "      <td>club (organisation)</td>\n",
       "      <td>0.699817</td>\n",
       "      <td>0.428406</td>\n",
       "      <td>0.356641</td>\n",
       "      <td>club</td>\n",
       "      <td>organisation</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>947</th>\n",
       "      <td>club (tool)</td>\n",
       "      <td>0.860703</td>\n",
       "      <td>0.289625</td>\n",
       "      <td>0.537132</td>\n",
       "      <td>club</td>\n",
       "      <td>tool</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                   words  F_Objectivity  F_Subjectivity  F_Context    word   distinction\n",
       "819               charge       0.340881        0.330534   0.438418  charge          None\n",
       "820        charge (card)       0.477083        0.293352   0.671798  charge          card\n",
       "821   charge (dependent)       0.256759        0.164572   0.791042  charge     dependent\n",
       "822    charge (electric)       0.391504        0.413080   0.697287  charge      electric\n",
       "823       charge (price)       0.428509        0.353942   0.498770  charge         price\n",
       "824        charge (rush)       0.551191        0.557549   0.591914  charge          rush\n",
       "942                 club       0.774815        0.476734   0.467275    club          None\n",
       "943     club (card suit)       0.773094        0.187918   0.492067    club     card suit\n",
       "944         club (disco)       0.863845        0.547812   0.658553    club         disco\n",
       "945           club (hit)       0.725571        0.435103   0.607930    club           hit\n",
       "946  club (organisation)       0.699817        0.428406   0.356641    club  organisation\n",
       "947          club (tool)       0.860703        0.289625   0.537132    club          tool"
      ]
     },
     "execution_count": 117,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_homonym[(df_homonym['word']=='club') | (df_homonym['word']=='charge')]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Replacing the distinct values for the average (column 'word'):"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 118,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Calculating the mean of F_Objectivity and F_Subjectivity for each group of \"word\"\n",
    "mean_values = df_homonym.groupby('word')[['F_Objectivity', 'F_Subjectivity']].mean().reset_index()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 119,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>word</th>\n",
       "      <th>F_Objectivity</th>\n",
       "      <th>F_Subjectivity</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Apple</td>\n",
       "      <td>0.940620</td>\n",
       "      <td>0.524376</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>Christmas</td>\n",
       "      <td>0.850793</td>\n",
       "      <td>0.833898</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>Dad</td>\n",
       "      <td>0.856533</td>\n",
       "      <td>0.493834</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>Dame</td>\n",
       "      <td>0.626968</td>\n",
       "      <td>0.300580</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>FALSE</td>\n",
       "      <td>0.156905</td>\n",
       "      <td>0.473624</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4678</th>\n",
       "      <td>zeppelin</td>\n",
       "      <td>0.864760</td>\n",
       "      <td>0.396531</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4679</th>\n",
       "      <td>zero</td>\n",
       "      <td>0.379392</td>\n",
       "      <td>0.315118</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4680</th>\n",
       "      <td>zest</td>\n",
       "      <td>0.402894</td>\n",
       "      <td>0.476800</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4681</th>\n",
       "      <td>zoo</td>\n",
       "      <td>0.867152</td>\n",
       "      <td>0.507390</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4682</th>\n",
       "      <td>zoology</td>\n",
       "      <td>0.420591</td>\n",
       "      <td>0.345092</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>4683 rows × 3 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "           word  F_Objectivity  F_Subjectivity\n",
       "0         Apple       0.940620        0.524376\n",
       "1     Christmas       0.850793        0.833898\n",
       "2           Dad       0.856533        0.493834\n",
       "3          Dame       0.626968        0.300580\n",
       "4         FALSE       0.156905        0.473624\n",
       "...         ...            ...             ...\n",
       "4678   zeppelin       0.864760        0.396531\n",
       "4679       zero       0.379392        0.315118\n",
       "4680       zest       0.402894        0.476800\n",
       "4681        zoo       0.867152        0.507390\n",
       "4682    zoology       0.420591        0.345092\n",
       "\n",
       "[4683 rows x 3 columns]"
      ]
     },
     "execution_count": 119,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "mean_values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 120,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>words</th>\n",
       "      <th>F_Objectivity</th>\n",
       "      <th>F_Subjectivity</th>\n",
       "      <th>F_Context</th>\n",
       "      <th>word</th>\n",
       "      <th>distinction</th>\n",
       "      <th>F_Objectivity_mean</th>\n",
       "      <th>F_Subjectivity_mean</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>819</th>\n",
       "      <td>charge</td>\n",
       "      <td>0.340881</td>\n",
       "      <td>0.330534</td>\n",
       "      <td>0.438418</td>\n",
       "      <td>charge</td>\n",
       "      <td>None</td>\n",
       "      <td>0.407655</td>\n",
       "      <td>0.352172</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>820</th>\n",
       "      <td>charge (card)</td>\n",
       "      <td>0.477083</td>\n",
       "      <td>0.293352</td>\n",
       "      <td>0.671798</td>\n",
       "      <td>charge</td>\n",
       "      <td>card</td>\n",
       "      <td>0.407655</td>\n",
       "      <td>0.352172</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>821</th>\n",
       "      <td>charge (dependent)</td>\n",
       "      <td>0.256759</td>\n",
       "      <td>0.164572</td>\n",
       "      <td>0.791042</td>\n",
       "      <td>charge</td>\n",
       "      <td>dependent</td>\n",
       "      <td>0.407655</td>\n",
       "      <td>0.352172</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>822</th>\n",
       "      <td>charge (electric)</td>\n",
       "      <td>0.391504</td>\n",
       "      <td>0.413080</td>\n",
       "      <td>0.697287</td>\n",
       "      <td>charge</td>\n",
       "      <td>electric</td>\n",
       "      <td>0.407655</td>\n",
       "      <td>0.352172</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>823</th>\n",
       "      <td>charge (price)</td>\n",
       "      <td>0.428509</td>\n",
       "      <td>0.353942</td>\n",
       "      <td>0.498770</td>\n",
       "      <td>charge</td>\n",
       "      <td>price</td>\n",
       "      <td>0.407655</td>\n",
       "      <td>0.352172</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>824</th>\n",
       "      <td>charge (rush)</td>\n",
       "      <td>0.551191</td>\n",
       "      <td>0.557549</td>\n",
       "      <td>0.591914</td>\n",
       "      <td>charge</td>\n",
       "      <td>rush</td>\n",
       "      <td>0.407655</td>\n",
       "      <td>0.352172</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                  words  F_Objectivity  F_Subjectivity  F_Context    word distinction  F_Objectivity_mean  F_Subjectivity_mean\n",
       "819              charge       0.340881        0.330534   0.438418  charge        None            0.407655             0.352172\n",
       "820       charge (card)       0.477083        0.293352   0.671798  charge        card            0.407655             0.352172\n",
       "821  charge (dependent)       0.256759        0.164572   0.791042  charge   dependent            0.407655             0.352172\n",
       "822   charge (electric)       0.391504        0.413080   0.697287  charge    electric            0.407655             0.352172\n",
       "823      charge (price)       0.428509        0.353942   0.498770  charge       price            0.407655             0.352172\n",
       "824       charge (rush)       0.551191        0.557549   0.591914  charge        rush            0.407655             0.352172"
      ]
     },
     "execution_count": 120,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "## Merging the mean values back into the original DataFrame\n",
    "df = pd.merge(df_homonym, mean_values, on='word', suffixes=('', '_mean'))\n",
    "\n",
    "## Checking an example (is the \"_mean\" values equal for all instances?): \n",
    "df[df['word'] == 'charge']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 121,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Dropping duplicated words (by column)\n",
    "df.drop_duplicates(subset=['word'], inplace=True)\n",
    "\n",
    "## Selecting only the columns of interest:\n",
    "df = df[['word', 'F_Objectivity_mean', 'F_Subjectivity_mean']]\n",
    "\n",
    "## Renaming the columns to remove the \"_mean\" suffix\n",
    "df.rename(columns={'F_Objectivity_mean': 'F_Objectivity', 'F_Subjectivity_mean': 'F_Subjectivity'}, inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 122,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'pandas.core.frame.DataFrame'>\n",
      "Int64Index: 4683 entries, 0 to 5552\n",
      "Data columns (total 3 columns):\n",
      " #   Column          Non-Null Count  Dtype  \n",
      "---  ------          --------------  -----  \n",
      " 0   word            4683 non-null   object \n",
      " 1   F_Objectivity   4683 non-null   float64\n",
      " 2   F_Subjectivity  4683 non-null   float64\n",
      "dtypes: float64(2), object(1)\n",
      "memory usage: 146.3+ KB\n",
      "None\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>word</th>\n",
       "      <th>F_Objectivity</th>\n",
       "      <th>F_Subjectivity</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>abattoir</td>\n",
       "      <td>0.512527</td>\n",
       "      <td>0.380603</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>abbey</td>\n",
       "      <td>0.714765</td>\n",
       "      <td>0.240456</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>abbreviate</td>\n",
       "      <td>0.286952</td>\n",
       "      <td>0.171052</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>abdicate</td>\n",
       "      <td>0.144736</td>\n",
       "      <td>0.384300</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>abdication</td>\n",
       "      <td>0.167654</td>\n",
       "      <td>0.334086</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "         word  F_Objectivity  F_Subjectivity\n",
       "0    abattoir       0.512527        0.380603\n",
       "1       abbey       0.714765        0.240456\n",
       "2  abbreviate       0.286952        0.171052\n",
       "3    abdicate       0.144736        0.384300\n",
       "4  abdication       0.167654        0.334086"
      ]
     },
     "execution_count": 122,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "## Showing dataframe:\n",
    "print(df.info())\n",
    "df.head(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 123,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Saving the prepared data:\n",
    "# df.to_csv('../data/df_factors_prepared.csv', index=False)\n",
    "\n",
    "## Loading data:\n",
    "df = pd.read_csv('../data/df_factors_prepared.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 124,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ### Selecionando apenas as palavras no df_factors que atendam aos critérios:\n",
    "# df_selected = df.loc[((df['F_Subjectivity'] > 0.75) | (df['F_Subjectivity'] < 0.3)) & ((df['F_Objectivity'] > 0.75) | (df['F_Objectivity'] < 0.3))]\n",
    "# df_factors = df_selected.copy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 125,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "4683"
      ]
     },
     "execution_count": 125,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "## Generating a list of words from SCA for training.\n",
    "SCA_words = [word for word in df.word]\n",
    "len(SCA_words)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 126,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Preparing dataframe for training:\n",
    "# df.set_index('word', inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 127,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>word</th>\n",
       "      <th>F_Objectivity</th>\n",
       "      <th>F_Subjectivity</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>abattoir</td>\n",
       "      <td>0.512527</td>\n",
       "      <td>0.380603</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>abbey</td>\n",
       "      <td>0.714765</td>\n",
       "      <td>0.240456</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>abbreviate</td>\n",
       "      <td>0.286952</td>\n",
       "      <td>0.171052</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>abdicate</td>\n",
       "      <td>0.144736</td>\n",
       "      <td>0.384300</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>abdication</td>\n",
       "      <td>0.167654</td>\n",
       "      <td>0.334086</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4678</th>\n",
       "      <td>zeppelin</td>\n",
       "      <td>0.864760</td>\n",
       "      <td>0.396531</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4679</th>\n",
       "      <td>zero</td>\n",
       "      <td>0.379392</td>\n",
       "      <td>0.315118</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4680</th>\n",
       "      <td>zest</td>\n",
       "      <td>0.402894</td>\n",
       "      <td>0.476800</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4681</th>\n",
       "      <td>zoo</td>\n",
       "      <td>0.867152</td>\n",
       "      <td>0.507390</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4682</th>\n",
       "      <td>zoology</td>\n",
       "      <td>0.420591</td>\n",
       "      <td>0.345092</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>4683 rows × 3 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "            word  F_Objectivity  F_Subjectivity\n",
       "0       abattoir       0.512527        0.380603\n",
       "1          abbey       0.714765        0.240456\n",
       "2     abbreviate       0.286952        0.171052\n",
       "3       abdicate       0.144736        0.384300\n",
       "4     abdication       0.167654        0.334086\n",
       "...          ...            ...             ...\n",
       "4678    zeppelin       0.864760        0.396531\n",
       "4679        zero       0.379392        0.315118\n",
       "4680        zest       0.402894        0.476800\n",
       "4681         zoo       0.867152        0.507390\n",
       "4682     zoology       0.420591        0.345092\n",
       "\n",
       "[4683 rows x 3 columns]"
      ]
     },
     "execution_count": 127,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## DataPrep for machine learning classifiers\n",
    "This section prepares data from Semantic Content Analysis (SCA) and Word Embedding (WE) sources for machine learning classification tasks. Steps include data integration, cleaning, feature engineering, transformation, and splitting into training and test sets for linear and multilabel classification. The goal is to optimize data quality and model performance."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 128,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "# Splitting data in train (75%) and test (25%)\n",
    "train_df, test_df = train_test_split(df, test_size=0.25, random_state=42)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 129,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'pandas.core.frame.DataFrame'>\n",
      "Int64Index: 3512 entries, 208 to 860\n",
      "Data columns (total 3 columns):\n",
      " #   Column          Non-Null Count  Dtype  \n",
      "---  ------          --------------  -----  \n",
      " 0   word            3512 non-null   object \n",
      " 1   F_Objectivity   3512 non-null   float64\n",
      " 2   F_Subjectivity  3512 non-null   float64\n",
      "dtypes: float64(2), object(1)\n",
      "memory usage: 109.8+ KB\n",
      "None\n",
      "<class 'pandas.core.frame.DataFrame'>\n",
      "Int64Index: 1171 entries, 468 to 2900\n",
      "Data columns (total 3 columns):\n",
      " #   Column          Non-Null Count  Dtype  \n",
      "---  ------          --------------  -----  \n",
      " 0   word            1171 non-null   object \n",
      " 1   F_Objectivity   1171 non-null   float64\n",
      " 2   F_Subjectivity  1171 non-null   float64\n",
      "dtypes: float64(2), object(1)\n",
      "memory usage: 36.6+ KB\n",
      "None\n"
     ]
    }
   ],
   "source": [
    "print(train_df.info())\n",
    "print(test_df.info())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 130,
   "metadata": {},
   "outputs": [],
   "source": [
    "def generateData(df_sca:pd.DataFrame):\n",
    "    '''Creates a dataset with explanatory variables from word embedding and dependent variables based on the SCA target data.\n",
    "    Usage example: \"X_train, Y_train = generateData(df)\"\n",
    "    Input: Pandas.DataFrame with SCA words.\n",
    "    Output: X: dataframe with explanatory variables; Y: dataframe with dependent variables.\n",
    "    '''\n",
    "    X = {}\n",
    "    Y = {}\n",
    "    \n",
    "    for i, row in df_sca.iterrows():\n",
    "        word = row['word']\n",
    "        f_objectivity = row['F_Objectivity']\n",
    "        f_subjectivity = row['F_Subjectivity']\n",
    "\n",
    "        try:\n",
    "            X[word] = nlp_getVector(word)[1]  # Stores the word vector\n",
    "            Y[word] = {'F_Objectivity': f_objectivity, 'F_Subjectivity': f_subjectivity}\n",
    "            # print(f'Debug: {word} <=> {nlp_getVector(word)[0]}')\n",
    "        except:\n",
    "            pass\n",
    "\n",
    "    return pd.DataFrame.from_dict(X, orient='index'), pd.DataFrame.from_dict(Y, orient='index')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### A) Regression models: splitting train and test data\n",
    "- Multi output (F_obj, F_sub), with continuous values."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 131,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Generating data:\n",
    "X_train_A, Y_train_A = generateData(train_df)\n",
    "X_test_A, Y_test_A = generateData(test_df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 132,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train data dimension:\n",
      "X_train: (3511, 300)\n",
      "Y_train: (3511, 2)\n",
      "\n",
      "Test data dimension:\n",
      "X_test: (1170, 300)\n",
      "Y_test: (1170, 2)\n"
     ]
    }
   ],
   "source": [
    "## Checking the generated data dimension:\n",
    "print(\"Train data dimension:\")\n",
    "print(\"X_train:\", X_train_A.shape)\n",
    "print(\"Y_train:\", Y_train_A.shape)\n",
    "\n",
    "print(\"\\nTest data dimension:\")\n",
    "print(\"X_test:\", X_test_A.shape)\n",
    "print(\"Y_test:\", Y_test_A.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 133,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>0</th>\n",
       "      <th>1</th>\n",
       "      <th>2</th>\n",
       "      <th>3</th>\n",
       "      <th>4</th>\n",
       "      <th>5</th>\n",
       "      <th>6</th>\n",
       "      <th>7</th>\n",
       "      <th>8</th>\n",
       "      <th>9</th>\n",
       "      <th>10</th>\n",
       "      <th>11</th>\n",
       "      <th>12</th>\n",
       "      <th>13</th>\n",
       "      <th>14</th>\n",
       "      <th>15</th>\n",
       "      <th>16</th>\n",
       "      <th>17</th>\n",
       "      <th>18</th>\n",
       "      <th>19</th>\n",
       "      <th>20</th>\n",
       "      <th>21</th>\n",
       "      <th>22</th>\n",
       "      <th>23</th>\n",
       "      <th>24</th>\n",
       "      <th>25</th>\n",
       "      <th>26</th>\n",
       "      <th>27</th>\n",
       "      <th>28</th>\n",
       "      <th>29</th>\n",
       "      <th>30</th>\n",
       "      <th>31</th>\n",
       "      <th>32</th>\n",
       "      <th>33</th>\n",
       "      <th>34</th>\n",
       "      <th>35</th>\n",
       "      <th>36</th>\n",
       "      <th>37</th>\n",
       "      <th>38</th>\n",
       "      <th>39</th>\n",
       "      <th>40</th>\n",
       "      <th>41</th>\n",
       "      <th>42</th>\n",
       "      <th>43</th>\n",
       "      <th>44</th>\n",
       "      <th>45</th>\n",
       "      <th>46</th>\n",
       "      <th>47</th>\n",
       "      <th>48</th>\n",
       "      <th>49</th>\n",
       "      <th>50</th>\n",
       "      <th>51</th>\n",
       "      <th>52</th>\n",
       "      <th>53</th>\n",
       "      <th>54</th>\n",
       "      <th>55</th>\n",
       "      <th>56</th>\n",
       "      <th>57</th>\n",
       "      <th>58</th>\n",
       "      <th>59</th>\n",
       "      <th>60</th>\n",
       "      <th>61</th>\n",
       "      <th>62</th>\n",
       "      <th>63</th>\n",
       "      <th>64</th>\n",
       "      <th>65</th>\n",
       "      <th>66</th>\n",
       "      <th>67</th>\n",
       "      <th>68</th>\n",
       "      <th>69</th>\n",
       "      <th>70</th>\n",
       "      <th>71</th>\n",
       "      <th>72</th>\n",
       "      <th>73</th>\n",
       "      <th>74</th>\n",
       "      <th>75</th>\n",
       "      <th>76</th>\n",
       "      <th>77</th>\n",
       "      <th>78</th>\n",
       "      <th>79</th>\n",
       "      <th>80</th>\n",
       "      <th>81</th>\n",
       "      <th>82</th>\n",
       "      <th>83</th>\n",
       "      <th>84</th>\n",
       "      <th>85</th>\n",
       "      <th>86</th>\n",
       "      <th>87</th>\n",
       "      <th>88</th>\n",
       "      <th>89</th>\n",
       "      <th>90</th>\n",
       "      <th>91</th>\n",
       "      <th>92</th>\n",
       "      <th>93</th>\n",
       "      <th>94</th>\n",
       "      <th>95</th>\n",
       "      <th>96</th>\n",
       "      <th>97</th>\n",
       "      <th>98</th>\n",
       "      <th>99</th>\n",
       "      <th>100</th>\n",
       "      <th>101</th>\n",
       "      <th>102</th>\n",
       "      <th>103</th>\n",
       "      <th>104</th>\n",
       "      <th>105</th>\n",
       "      <th>106</th>\n",
       "      <th>107</th>\n",
       "      <th>108</th>\n",
       "      <th>109</th>\n",
       "      <th>110</th>\n",
       "      <th>111</th>\n",
       "      <th>112</th>\n",
       "      <th>113</th>\n",
       "      <th>114</th>\n",
       "      <th>115</th>\n",
       "      <th>116</th>\n",
       "      <th>117</th>\n",
       "      <th>118</th>\n",
       "      <th>119</th>\n",
       "      <th>120</th>\n",
       "      <th>121</th>\n",
       "      <th>122</th>\n",
       "      <th>123</th>\n",
       "      <th>124</th>\n",
       "      <th>125</th>\n",
       "      <th>126</th>\n",
       "      <th>127</th>\n",
       "      <th>128</th>\n",
       "      <th>129</th>\n",
       "      <th>130</th>\n",
       "      <th>131</th>\n",
       "      <th>132</th>\n",
       "      <th>133</th>\n",
       "      <th>134</th>\n",
       "      <th>135</th>\n",
       "      <th>136</th>\n",
       "      <th>137</th>\n",
       "      <th>138</th>\n",
       "      <th>139</th>\n",
       "      <th>140</th>\n",
       "      <th>141</th>\n",
       "      <th>142</th>\n",
       "      <th>143</th>\n",
       "      <th>144</th>\n",
       "      <th>145</th>\n",
       "      <th>146</th>\n",
       "      <th>147</th>\n",
       "      <th>148</th>\n",
       "      <th>149</th>\n",
       "      <th>150</th>\n",
       "      <th>151</th>\n",
       "      <th>152</th>\n",
       "      <th>153</th>\n",
       "      <th>154</th>\n",
       "      <th>155</th>\n",
       "      <th>156</th>\n",
       "      <th>157</th>\n",
       "      <th>158</th>\n",
       "      <th>159</th>\n",
       "      <th>160</th>\n",
       "      <th>161</th>\n",
       "      <th>162</th>\n",
       "      <th>163</th>\n",
       "      <th>164</th>\n",
       "      <th>165</th>\n",
       "      <th>166</th>\n",
       "      <th>167</th>\n",
       "      <th>168</th>\n",
       "      <th>169</th>\n",
       "      <th>170</th>\n",
       "      <th>171</th>\n",
       "      <th>172</th>\n",
       "      <th>173</th>\n",
       "      <th>174</th>\n",
       "      <th>175</th>\n",
       "      <th>176</th>\n",
       "      <th>177</th>\n",
       "      <th>178</th>\n",
       "      <th>179</th>\n",
       "      <th>180</th>\n",
       "      <th>181</th>\n",
       "      <th>182</th>\n",
       "      <th>183</th>\n",
       "      <th>184</th>\n",
       "      <th>185</th>\n",
       "      <th>186</th>\n",
       "      <th>187</th>\n",
       "      <th>188</th>\n",
       "      <th>189</th>\n",
       "      <th>190</th>\n",
       "      <th>191</th>\n",
       "      <th>192</th>\n",
       "      <th>193</th>\n",
       "      <th>194</th>\n",
       "      <th>195</th>\n",
       "      <th>196</th>\n",
       "      <th>197</th>\n",
       "      <th>198</th>\n",
       "      <th>199</th>\n",
       "      <th>200</th>\n",
       "      <th>201</th>\n",
       "      <th>202</th>\n",
       "      <th>203</th>\n",
       "      <th>204</th>\n",
       "      <th>205</th>\n",
       "      <th>206</th>\n",
       "      <th>207</th>\n",
       "      <th>208</th>\n",
       "      <th>209</th>\n",
       "      <th>210</th>\n",
       "      <th>211</th>\n",
       "      <th>212</th>\n",
       "      <th>213</th>\n",
       "      <th>214</th>\n",
       "      <th>215</th>\n",
       "      <th>216</th>\n",
       "      <th>217</th>\n",
       "      <th>218</th>\n",
       "      <th>219</th>\n",
       "      <th>220</th>\n",
       "      <th>221</th>\n",
       "      <th>222</th>\n",
       "      <th>223</th>\n",
       "      <th>224</th>\n",
       "      <th>225</th>\n",
       "      <th>226</th>\n",
       "      <th>227</th>\n",
       "      <th>228</th>\n",
       "      <th>229</th>\n",
       "      <th>230</th>\n",
       "      <th>231</th>\n",
       "      <th>232</th>\n",
       "      <th>233</th>\n",
       "      <th>234</th>\n",
       "      <th>235</th>\n",
       "      <th>236</th>\n",
       "      <th>237</th>\n",
       "      <th>238</th>\n",
       "      <th>239</th>\n",
       "      <th>240</th>\n",
       "      <th>241</th>\n",
       "      <th>242</th>\n",
       "      <th>243</th>\n",
       "      <th>244</th>\n",
       "      <th>245</th>\n",
       "      <th>246</th>\n",
       "      <th>247</th>\n",
       "      <th>248</th>\n",
       "      <th>249</th>\n",
       "      <th>250</th>\n",
       "      <th>251</th>\n",
       "      <th>252</th>\n",
       "      <th>253</th>\n",
       "      <th>254</th>\n",
       "      <th>255</th>\n",
       "      <th>256</th>\n",
       "      <th>257</th>\n",
       "      <th>258</th>\n",
       "      <th>259</th>\n",
       "      <th>260</th>\n",
       "      <th>261</th>\n",
       "      <th>262</th>\n",
       "      <th>263</th>\n",
       "      <th>264</th>\n",
       "      <th>265</th>\n",
       "      <th>266</th>\n",
       "      <th>267</th>\n",
       "      <th>268</th>\n",
       "      <th>269</th>\n",
       "      <th>270</th>\n",
       "      <th>271</th>\n",
       "      <th>272</th>\n",
       "      <th>273</th>\n",
       "      <th>274</th>\n",
       "      <th>275</th>\n",
       "      <th>276</th>\n",
       "      <th>277</th>\n",
       "      <th>278</th>\n",
       "      <th>279</th>\n",
       "      <th>280</th>\n",
       "      <th>281</th>\n",
       "      <th>282</th>\n",
       "      <th>283</th>\n",
       "      <th>284</th>\n",
       "      <th>285</th>\n",
       "      <th>286</th>\n",
       "      <th>287</th>\n",
       "      <th>288</th>\n",
       "      <th>289</th>\n",
       "      <th>290</th>\n",
       "      <th>291</th>\n",
       "      <th>292</th>\n",
       "      <th>293</th>\n",
       "      <th>294</th>\n",
       "      <th>295</th>\n",
       "      <th>296</th>\n",
       "      <th>297</th>\n",
       "      <th>298</th>\n",
       "      <th>299</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>argument</th>\n",
       "      <td>-2.39920</td>\n",
       "      <td>-0.31061</td>\n",
       "      <td>0.162480</td>\n",
       "      <td>1.19390</td>\n",
       "      <td>4.55300</td>\n",
       "      <td>2.608000</td>\n",
       "      <td>2.381600</td>\n",
       "      <td>4.37930</td>\n",
       "      <td>-6.4059</td>\n",
       "      <td>-2.898900</td>\n",
       "      <td>6.9065</td>\n",
       "      <td>3.635700</td>\n",
       "      <td>-3.4818</td>\n",
       "      <td>-3.07200</td>\n",
       "      <td>-1.562500</td>\n",
       "      <td>4.45740</td>\n",
       "      <td>2.406200</td>\n",
       "      <td>0.61288</td>\n",
       "      <td>-2.02030</td>\n",
       "      <td>1.85400</td>\n",
       "      <td>-1.28270</td>\n",
       "      <td>-0.866920</td>\n",
       "      <td>-2.234400</td>\n",
       "      <td>-0.202930</td>\n",
       "      <td>-0.98764</td>\n",
       "      <td>-2.57030</td>\n",
       "      <td>-0.42565</td>\n",
       "      <td>-0.67371</td>\n",
       "      <td>-2.95120</td>\n",
       "      <td>2.52310</td>\n",
       "      <td>3.61740</td>\n",
       "      <td>2.046000</td>\n",
       "      <td>0.93659</td>\n",
       "      <td>0.80884</td>\n",
       "      <td>-1.835900</td>\n",
       "      <td>-4.15380</td>\n",
       "      <td>-0.77178</td>\n",
       "      <td>-0.91474</td>\n",
       "      <td>0.357400</td>\n",
       "      <td>2.43110</td>\n",
       "      <td>-2.84270</td>\n",
       "      <td>-4.39410</td>\n",
       "      <td>0.18871</td>\n",
       "      <td>1.04750</td>\n",
       "      <td>-0.79925</td>\n",
       "      <td>-1.58430</td>\n",
       "      <td>3.480400</td>\n",
       "      <td>-2.895800</td>\n",
       "      <td>-1.462500</td>\n",
       "      <td>0.56112</td>\n",
       "      <td>-2.86750</td>\n",
       "      <td>2.770100</td>\n",
       "      <td>2.391500</td>\n",
       "      <td>-1.90630</td>\n",
       "      <td>-2.25350</td>\n",
       "      <td>-0.62433</td>\n",
       "      <td>3.14550</td>\n",
       "      <td>1.222000</td>\n",
       "      <td>2.28540</td>\n",
       "      <td>0.36479</td>\n",
       "      <td>3.091800</td>\n",
       "      <td>1.61080</td>\n",
       "      <td>-1.65840</td>\n",
       "      <td>-1.64510</td>\n",
       "      <td>0.65175</td>\n",
       "      <td>2.85370</td>\n",
       "      <td>-3.29310</td>\n",
       "      <td>-4.5797</td>\n",
       "      <td>1.7337</td>\n",
       "      <td>3.35200</td>\n",
       "      <td>1.768100</td>\n",
       "      <td>1.599000</td>\n",
       "      <td>-5.05160</td>\n",
       "      <td>1.664400</td>\n",
       "      <td>-4.369200</td>\n",
       "      <td>1.14050</td>\n",
       "      <td>-3.99870</td>\n",
       "      <td>5.094700</td>\n",
       "      <td>-0.79874</td>\n",
       "      <td>-0.608100</td>\n",
       "      <td>-3.21580</td>\n",
       "      <td>-0.42495</td>\n",
       "      <td>1.404700</td>\n",
       "      <td>2.54230</td>\n",
       "      <td>0.58327</td>\n",
       "      <td>2.52930</td>\n",
       "      <td>-3.55850</td>\n",
       "      <td>-2.18920</td>\n",
       "      <td>4.41760</td>\n",
       "      <td>1.643900</td>\n",
       "      <td>3.472500</td>\n",
       "      <td>2.23200</td>\n",
       "      <td>5.642900</td>\n",
       "      <td>-2.55720</td>\n",
       "      <td>3.22990</td>\n",
       "      <td>-2.32170</td>\n",
       "      <td>0.044789</td>\n",
       "      <td>3.106200</td>\n",
       "      <td>2.82930</td>\n",
       "      <td>-2.23330</td>\n",
       "      <td>3.69810</td>\n",
       "      <td>3.2175</td>\n",
       "      <td>4.477000</td>\n",
       "      <td>3.126400</td>\n",
       "      <td>-0.69679</td>\n",
       "      <td>3.03720</td>\n",
       "      <td>3.25990</td>\n",
       "      <td>-4.15510</td>\n",
       "      <td>1.04660</td>\n",
       "      <td>-2.30530</td>\n",
       "      <td>0.066955</td>\n",
       "      <td>-0.960050</td>\n",
       "      <td>-0.48711</td>\n",
       "      <td>1.059800</td>\n",
       "      <td>1.91210</td>\n",
       "      <td>2.15400</td>\n",
       "      <td>-4.61480</td>\n",
       "      <td>-0.606290</td>\n",
       "      <td>0.35795</td>\n",
       "      <td>-0.86739</td>\n",
       "      <td>-0.40956</td>\n",
       "      <td>-5.43570</td>\n",
       "      <td>0.59216</td>\n",
       "      <td>-1.75330</td>\n",
       "      <td>-3.36790</td>\n",
       "      <td>0.397490</td>\n",
       "      <td>3.54500</td>\n",
       "      <td>-3.55280</td>\n",
       "      <td>0.711020</td>\n",
       "      <td>-4.57000</td>\n",
       "      <td>-4.58250</td>\n",
       "      <td>-1.846200</td>\n",
       "      <td>3.95070</td>\n",
       "      <td>0.42573</td>\n",
       "      <td>0.61037</td>\n",
       "      <td>0.72052</td>\n",
       "      <td>0.78121</td>\n",
       "      <td>2.548200</td>\n",
       "      <td>0.79837</td>\n",
       "      <td>-1.85340</td>\n",
       "      <td>-1.46980</td>\n",
       "      <td>-0.71325</td>\n",
       "      <td>-0.21927</td>\n",
       "      <td>1.68080</td>\n",
       "      <td>1.2657</td>\n",
       "      <td>-0.95549</td>\n",
       "      <td>-4.65340</td>\n",
       "      <td>-0.422500</td>\n",
       "      <td>-0.655770</td>\n",
       "      <td>3.78360</td>\n",
       "      <td>0.58572</td>\n",
       "      <td>4.75820</td>\n",
       "      <td>-1.281200</td>\n",
       "      <td>1.68730</td>\n",
       "      <td>-1.43430</td>\n",
       "      <td>5.10480</td>\n",
       "      <td>1.84110</td>\n",
       "      <td>-0.33209</td>\n",
       "      <td>-2.84530</td>\n",
       "      <td>-4.33760</td>\n",
       "      <td>2.24460</td>\n",
       "      <td>0.060355</td>\n",
       "      <td>-0.28312</td>\n",
       "      <td>2.110900</td>\n",
       "      <td>-0.214760</td>\n",
       "      <td>-2.08980</td>\n",
       "      <td>-2.99090</td>\n",
       "      <td>1.68460</td>\n",
       "      <td>1.22150</td>\n",
       "      <td>-1.663000</td>\n",
       "      <td>-0.29140</td>\n",
       "      <td>-0.15043</td>\n",
       "      <td>-0.089668</td>\n",
       "      <td>-1.361800</td>\n",
       "      <td>4.252300</td>\n",
       "      <td>1.049900</td>\n",
       "      <td>-0.835560</td>\n",
       "      <td>3.34520</td>\n",
       "      <td>-2.71990</td>\n",
       "      <td>-3.49400</td>\n",
       "      <td>2.819600</td>\n",
       "      <td>3.07220</td>\n",
       "      <td>-0.129980</td>\n",
       "      <td>1.63730</td>\n",
       "      <td>-1.97820</td>\n",
       "      <td>2.92380</td>\n",
       "      <td>-2.132400</td>\n",
       "      <td>-1.18320</td>\n",
       "      <td>3.44950</td>\n",
       "      <td>2.63860</td>\n",
       "      <td>-2.62280</td>\n",
       "      <td>-3.05720</td>\n",
       "      <td>0.773540</td>\n",
       "      <td>1.61730</td>\n",
       "      <td>2.12610</td>\n",
       "      <td>-3.067000</td>\n",
       "      <td>-0.84982</td>\n",
       "      <td>3.68300</td>\n",
       "      <td>1.61980</td>\n",
       "      <td>6.20910</td>\n",
       "      <td>-2.488200</td>\n",
       "      <td>0.70562</td>\n",
       "      <td>0.360680</td>\n",
       "      <td>-0.85727</td>\n",
       "      <td>2.07030</td>\n",
       "      <td>-0.057408</td>\n",
       "      <td>-0.638360</td>\n",
       "      <td>0.628880</td>\n",
       "      <td>-0.26941</td>\n",
       "      <td>-1.863700</td>\n",
       "      <td>-2.42160</td>\n",
       "      <td>-1.494200</td>\n",
       "      <td>-2.671000</td>\n",
       "      <td>4.4052</td>\n",
       "      <td>-0.897830</td>\n",
       "      <td>1.84780</td>\n",
       "      <td>1.427400</td>\n",
       "      <td>1.16620</td>\n",
       "      <td>4.00380</td>\n",
       "      <td>2.206200</td>\n",
       "      <td>-3.77060</td>\n",
       "      <td>-0.94775</td>\n",
       "      <td>0.081936</td>\n",
       "      <td>-0.052859</td>\n",
       "      <td>-1.14660</td>\n",
       "      <td>0.97386</td>\n",
       "      <td>1.65780</td>\n",
       "      <td>5.16590</td>\n",
       "      <td>1.97350</td>\n",
       "      <td>-0.10786</td>\n",
       "      <td>1.467100</td>\n",
       "      <td>-4.17920</td>\n",
       "      <td>1.528100</td>\n",
       "      <td>0.131100</td>\n",
       "      <td>1.19560</td>\n",
       "      <td>0.493110</td>\n",
       "      <td>2.88420</td>\n",
       "      <td>1.03060</td>\n",
       "      <td>2.35040</td>\n",
       "      <td>-2.68750</td>\n",
       "      <td>-1.00970</td>\n",
       "      <td>1.24780</td>\n",
       "      <td>1.294900</td>\n",
       "      <td>4.564800</td>\n",
       "      <td>0.517240</td>\n",
       "      <td>0.524770</td>\n",
       "      <td>-2.016200</td>\n",
       "      <td>2.089300</td>\n",
       "      <td>0.606990</td>\n",
       "      <td>1.592600</td>\n",
       "      <td>1.05390</td>\n",
       "      <td>-4.59500</td>\n",
       "      <td>-3.70100</td>\n",
       "      <td>-0.75894</td>\n",
       "      <td>-0.64485</td>\n",
       "      <td>-4.18290</td>\n",
       "      <td>-1.93710</td>\n",
       "      <td>-0.26863</td>\n",
       "      <td>0.26782</td>\n",
       "      <td>-0.63142</td>\n",
       "      <td>-2.10990</td>\n",
       "      <td>1.153400</td>\n",
       "      <td>1.66280</td>\n",
       "      <td>4.74830</td>\n",
       "      <td>0.868380</td>\n",
       "      <td>0.88791</td>\n",
       "      <td>4.310000</td>\n",
       "      <td>4.37190</td>\n",
       "      <td>-3.07340</td>\n",
       "      <td>-1.11620</td>\n",
       "      <td>-0.311190</td>\n",
       "      <td>-1.498500</td>\n",
       "      <td>-1.209900</td>\n",
       "      <td>0.359000</td>\n",
       "      <td>3.47040</td>\n",
       "      <td>-0.40592</td>\n",
       "      <td>0.012269</td>\n",
       "      <td>-1.62150</td>\n",
       "      <td>-1.38370</td>\n",
       "      <td>4.22990</td>\n",
       "      <td>1.46310</td>\n",
       "      <td>0.48673</td>\n",
       "      <td>1.717800</td>\n",
       "      <td>3.760400</td>\n",
       "      <td>4.84590</td>\n",
       "      <td>1.29130</td>\n",
       "      <td>4.0310</td>\n",
       "      <td>-0.377380</td>\n",
       "      <td>-3.63400</td>\n",
       "      <td>0.84509</td>\n",
       "      <td>3.23500</td>\n",
       "      <td>-1.73110</td>\n",
       "      <td>1.399600</td>\n",
       "      <td>2.05100</td>\n",
       "      <td>-4.25220</td>\n",
       "      <td>1.091100</td>\n",
       "      <td>3.78190</td>\n",
       "      <td>-0.92356</td>\n",
       "      <td>-1.8916</td>\n",
       "      <td>3.350400</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>awful</th>\n",
       "      <td>-0.33203</td>\n",
       "      <td>-0.35220</td>\n",
       "      <td>-0.011708</td>\n",
       "      <td>-0.88964</td>\n",
       "      <td>-0.75291</td>\n",
       "      <td>1.845700</td>\n",
       "      <td>0.623790</td>\n",
       "      <td>1.01190</td>\n",
       "      <td>2.1302</td>\n",
       "      <td>1.779500</td>\n",
       "      <td>1.2335</td>\n",
       "      <td>-1.207700</td>\n",
       "      <td>-1.6876</td>\n",
       "      <td>0.33506</td>\n",
       "      <td>0.376710</td>\n",
       "      <td>0.83856</td>\n",
       "      <td>0.031083</td>\n",
       "      <td>-4.43120</td>\n",
       "      <td>-0.83196</td>\n",
       "      <td>-1.03640</td>\n",
       "      <td>-1.26410</td>\n",
       "      <td>1.848000</td>\n",
       "      <td>-0.288830</td>\n",
       "      <td>-1.527500</td>\n",
       "      <td>0.73359</td>\n",
       "      <td>-1.55750</td>\n",
       "      <td>0.28682</td>\n",
       "      <td>-0.65072</td>\n",
       "      <td>-0.79270</td>\n",
       "      <td>1.72920</td>\n",
       "      <td>0.32438</td>\n",
       "      <td>-0.935490</td>\n",
       "      <td>2.18660</td>\n",
       "      <td>-0.79948</td>\n",
       "      <td>-1.139400</td>\n",
       "      <td>-3.93730</td>\n",
       "      <td>-0.99964</td>\n",
       "      <td>-1.96530</td>\n",
       "      <td>1.945700</td>\n",
       "      <td>4.25720</td>\n",
       "      <td>-0.80120</td>\n",
       "      <td>-0.56891</td>\n",
       "      <td>-1.02410</td>\n",
       "      <td>-4.08480</td>\n",
       "      <td>-5.54380</td>\n",
       "      <td>0.43820</td>\n",
       "      <td>-0.082399</td>\n",
       "      <td>-4.646900</td>\n",
       "      <td>-2.437600</td>\n",
       "      <td>2.14220</td>\n",
       "      <td>-0.80818</td>\n",
       "      <td>-2.262100</td>\n",
       "      <td>3.854500</td>\n",
       "      <td>-2.47080</td>\n",
       "      <td>-1.70140</td>\n",
       "      <td>1.72380</td>\n",
       "      <td>1.50330</td>\n",
       "      <td>0.322180</td>\n",
       "      <td>-1.20730</td>\n",
       "      <td>0.41837</td>\n",
       "      <td>0.112650</td>\n",
       "      <td>0.39928</td>\n",
       "      <td>1.09220</td>\n",
       "      <td>-1.88730</td>\n",
       "      <td>-5.87340</td>\n",
       "      <td>-1.33360</td>\n",
       "      <td>-2.49000</td>\n",
       "      <td>-1.3867</td>\n",
       "      <td>-1.2559</td>\n",
       "      <td>-1.17970</td>\n",
       "      <td>2.555200</td>\n",
       "      <td>1.758700</td>\n",
       "      <td>-1.96580</td>\n",
       "      <td>0.065817</td>\n",
       "      <td>1.182400</td>\n",
       "      <td>0.97211</td>\n",
       "      <td>-0.62050</td>\n",
       "      <td>0.445420</td>\n",
       "      <td>0.92654</td>\n",
       "      <td>1.334700</td>\n",
       "      <td>0.74309</td>\n",
       "      <td>1.55980</td>\n",
       "      <td>1.817000</td>\n",
       "      <td>-1.13250</td>\n",
       "      <td>-2.71450</td>\n",
       "      <td>1.55730</td>\n",
       "      <td>-0.29974</td>\n",
       "      <td>-0.84252</td>\n",
       "      <td>2.15850</td>\n",
       "      <td>-1.295000</td>\n",
       "      <td>-1.229100</td>\n",
       "      <td>-2.28710</td>\n",
       "      <td>-0.085077</td>\n",
       "      <td>-3.63760</td>\n",
       "      <td>-1.69640</td>\n",
       "      <td>-0.69068</td>\n",
       "      <td>0.933470</td>\n",
       "      <td>0.282670</td>\n",
       "      <td>-0.34476</td>\n",
       "      <td>-2.17250</td>\n",
       "      <td>-1.44910</td>\n",
       "      <td>1.3936</td>\n",
       "      <td>0.178300</td>\n",
       "      <td>1.584700</td>\n",
       "      <td>-3.98000</td>\n",
       "      <td>0.54971</td>\n",
       "      <td>-0.37694</td>\n",
       "      <td>1.69870</td>\n",
       "      <td>-1.97800</td>\n",
       "      <td>2.37750</td>\n",
       "      <td>-1.338400</td>\n",
       "      <td>-3.112100</td>\n",
       "      <td>1.79570</td>\n",
       "      <td>-1.136800</td>\n",
       "      <td>-0.49529</td>\n",
       "      <td>1.62900</td>\n",
       "      <td>-2.41880</td>\n",
       "      <td>2.568800</td>\n",
       "      <td>2.28060</td>\n",
       "      <td>0.16880</td>\n",
       "      <td>-0.40327</td>\n",
       "      <td>0.69830</td>\n",
       "      <td>-4.50090</td>\n",
       "      <td>4.18320</td>\n",
       "      <td>-0.55077</td>\n",
       "      <td>0.054487</td>\n",
       "      <td>-0.59604</td>\n",
       "      <td>0.87615</td>\n",
       "      <td>-1.581400</td>\n",
       "      <td>-1.57830</td>\n",
       "      <td>1.68210</td>\n",
       "      <td>4.737100</td>\n",
       "      <td>-1.15540</td>\n",
       "      <td>-2.09060</td>\n",
       "      <td>0.32650</td>\n",
       "      <td>2.58540</td>\n",
       "      <td>1.34470</td>\n",
       "      <td>0.168990</td>\n",
       "      <td>1.90460</td>\n",
       "      <td>-3.21210</td>\n",
       "      <td>-3.23080</td>\n",
       "      <td>-1.03700</td>\n",
       "      <td>0.58301</td>\n",
       "      <td>2.32610</td>\n",
       "      <td>-5.8928</td>\n",
       "      <td>0.38668</td>\n",
       "      <td>0.40396</td>\n",
       "      <td>0.654600</td>\n",
       "      <td>1.161200</td>\n",
       "      <td>-1.41830</td>\n",
       "      <td>0.32217</td>\n",
       "      <td>0.82264</td>\n",
       "      <td>-2.021500</td>\n",
       "      <td>1.36950</td>\n",
       "      <td>-2.71390</td>\n",
       "      <td>2.89760</td>\n",
       "      <td>2.14150</td>\n",
       "      <td>2.38560</td>\n",
       "      <td>-0.97230</td>\n",
       "      <td>0.96908</td>\n",
       "      <td>1.24560</td>\n",
       "      <td>-0.264430</td>\n",
       "      <td>-1.95190</td>\n",
       "      <td>0.051155</td>\n",
       "      <td>-1.987000</td>\n",
       "      <td>0.65262</td>\n",
       "      <td>3.92240</td>\n",
       "      <td>-0.29806</td>\n",
       "      <td>-1.20940</td>\n",
       "      <td>3.451200</td>\n",
       "      <td>1.47300</td>\n",
       "      <td>0.21217</td>\n",
       "      <td>2.308400</td>\n",
       "      <td>-4.005900</td>\n",
       "      <td>-2.395000</td>\n",
       "      <td>-2.805800</td>\n",
       "      <td>-1.657700</td>\n",
       "      <td>2.89430</td>\n",
       "      <td>3.23290</td>\n",
       "      <td>-0.91901</td>\n",
       "      <td>-0.378530</td>\n",
       "      <td>1.12440</td>\n",
       "      <td>2.987100</td>\n",
       "      <td>1.33230</td>\n",
       "      <td>-0.92495</td>\n",
       "      <td>0.23867</td>\n",
       "      <td>-2.978200</td>\n",
       "      <td>0.13664</td>\n",
       "      <td>2.36910</td>\n",
       "      <td>-1.67370</td>\n",
       "      <td>-2.31260</td>\n",
       "      <td>0.50444</td>\n",
       "      <td>-0.865220</td>\n",
       "      <td>-1.83850</td>\n",
       "      <td>-1.97240</td>\n",
       "      <td>-4.246800</td>\n",
       "      <td>-1.98620</td>\n",
       "      <td>-0.92723</td>\n",
       "      <td>-0.16031</td>\n",
       "      <td>0.90947</td>\n",
       "      <td>-0.825260</td>\n",
       "      <td>1.72510</td>\n",
       "      <td>0.051674</td>\n",
       "      <td>-2.06460</td>\n",
       "      <td>-2.95620</td>\n",
       "      <td>1.911900</td>\n",
       "      <td>2.254900</td>\n",
       "      <td>-0.185860</td>\n",
       "      <td>-0.42561</td>\n",
       "      <td>0.403430</td>\n",
       "      <td>-1.33110</td>\n",
       "      <td>0.570660</td>\n",
       "      <td>-0.447660</td>\n",
       "      <td>-1.3727</td>\n",
       "      <td>-1.888100</td>\n",
       "      <td>-1.88290</td>\n",
       "      <td>-2.907800</td>\n",
       "      <td>1.76160</td>\n",
       "      <td>1.54830</td>\n",
       "      <td>-2.220500</td>\n",
       "      <td>-0.28344</td>\n",
       "      <td>-1.33110</td>\n",
       "      <td>-0.059834</td>\n",
       "      <td>3.859300</td>\n",
       "      <td>1.22310</td>\n",
       "      <td>-0.01470</td>\n",
       "      <td>-3.25130</td>\n",
       "      <td>0.46675</td>\n",
       "      <td>-1.01930</td>\n",
       "      <td>0.27312</td>\n",
       "      <td>-2.595400</td>\n",
       "      <td>-1.99740</td>\n",
       "      <td>3.321800</td>\n",
       "      <td>0.074786</td>\n",
       "      <td>-0.78470</td>\n",
       "      <td>0.012676</td>\n",
       "      <td>-0.83699</td>\n",
       "      <td>3.13540</td>\n",
       "      <td>-0.92994</td>\n",
       "      <td>0.17290</td>\n",
       "      <td>0.37295</td>\n",
       "      <td>-1.16190</td>\n",
       "      <td>1.398700</td>\n",
       "      <td>0.621990</td>\n",
       "      <td>-2.901900</td>\n",
       "      <td>-1.428800</td>\n",
       "      <td>-0.070483</td>\n",
       "      <td>-1.022900</td>\n",
       "      <td>-1.262200</td>\n",
       "      <td>1.138900</td>\n",
       "      <td>-1.67180</td>\n",
       "      <td>3.08470</td>\n",
       "      <td>-3.79880</td>\n",
       "      <td>1.90690</td>\n",
       "      <td>0.78550</td>\n",
       "      <td>-3.10370</td>\n",
       "      <td>-1.12060</td>\n",
       "      <td>1.88120</td>\n",
       "      <td>-0.89774</td>\n",
       "      <td>1.78790</td>\n",
       "      <td>3.15630</td>\n",
       "      <td>-0.670440</td>\n",
       "      <td>0.60400</td>\n",
       "      <td>-0.74343</td>\n",
       "      <td>-0.132450</td>\n",
       "      <td>1.98180</td>\n",
       "      <td>1.471800</td>\n",
       "      <td>0.23751</td>\n",
       "      <td>-1.75600</td>\n",
       "      <td>-0.94157</td>\n",
       "      <td>1.382400</td>\n",
       "      <td>-0.013683</td>\n",
       "      <td>-1.588100</td>\n",
       "      <td>1.741800</td>\n",
       "      <td>-2.34360</td>\n",
       "      <td>1.63810</td>\n",
       "      <td>2.527400</td>\n",
       "      <td>-1.56340</td>\n",
       "      <td>2.29810</td>\n",
       "      <td>-1.79800</td>\n",
       "      <td>-1.65460</td>\n",
       "      <td>-2.39590</td>\n",
       "      <td>0.468430</td>\n",
       "      <td>0.998010</td>\n",
       "      <td>1.73970</td>\n",
       "      <td>-1.96550</td>\n",
       "      <td>3.3896</td>\n",
       "      <td>-0.558010</td>\n",
       "      <td>-0.75166</td>\n",
       "      <td>-0.45850</td>\n",
       "      <td>-0.33523</td>\n",
       "      <td>-0.37922</td>\n",
       "      <td>4.542300</td>\n",
       "      <td>-2.32350</td>\n",
       "      <td>-1.34550</td>\n",
       "      <td>-2.070200</td>\n",
       "      <td>0.20349</td>\n",
       "      <td>3.63000</td>\n",
       "      <td>1.0342</td>\n",
       "      <td>1.682900</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>decorator</th>\n",
       "      <td>-4.13860</td>\n",
       "      <td>-3.10150</td>\n",
       "      <td>-2.393800</td>\n",
       "      <td>-1.23100</td>\n",
       "      <td>-0.20878</td>\n",
       "      <td>-3.961600</td>\n",
       "      <td>0.000133</td>\n",
       "      <td>1.21540</td>\n",
       "      <td>-0.7785</td>\n",
       "      <td>4.896700</td>\n",
       "      <td>4.6354</td>\n",
       "      <td>0.250680</td>\n",
       "      <td>-3.1679</td>\n",
       "      <td>1.74190</td>\n",
       "      <td>3.622500</td>\n",
       "      <td>0.49026</td>\n",
       "      <td>0.047730</td>\n",
       "      <td>3.93780</td>\n",
       "      <td>-0.35841</td>\n",
       "      <td>-2.43020</td>\n",
       "      <td>-1.59880</td>\n",
       "      <td>-0.020996</td>\n",
       "      <td>1.055600</td>\n",
       "      <td>3.488800</td>\n",
       "      <td>-1.66060</td>\n",
       "      <td>0.70270</td>\n",
       "      <td>-4.41830</td>\n",
       "      <td>-0.22423</td>\n",
       "      <td>-1.40400</td>\n",
       "      <td>-3.28490</td>\n",
       "      <td>0.30462</td>\n",
       "      <td>-1.996100</td>\n",
       "      <td>2.26480</td>\n",
       "      <td>-0.64048</td>\n",
       "      <td>-1.962800</td>\n",
       "      <td>2.47050</td>\n",
       "      <td>1.26400</td>\n",
       "      <td>0.27926</td>\n",
       "      <td>1.726000</td>\n",
       "      <td>-0.98930</td>\n",
       "      <td>-1.40990</td>\n",
       "      <td>-2.07690</td>\n",
       "      <td>3.54400</td>\n",
       "      <td>-0.39312</td>\n",
       "      <td>-2.45640</td>\n",
       "      <td>0.75642</td>\n",
       "      <td>-1.430800</td>\n",
       "      <td>-1.372300</td>\n",
       "      <td>0.484910</td>\n",
       "      <td>-0.73407</td>\n",
       "      <td>-1.71950</td>\n",
       "      <td>2.165200</td>\n",
       "      <td>1.165000</td>\n",
       "      <td>-1.06380</td>\n",
       "      <td>-2.98700</td>\n",
       "      <td>1.47890</td>\n",
       "      <td>-0.13572</td>\n",
       "      <td>-0.417030</td>\n",
       "      <td>0.25105</td>\n",
       "      <td>-1.45970</td>\n",
       "      <td>-0.014275</td>\n",
       "      <td>-2.86830</td>\n",
       "      <td>-0.30213</td>\n",
       "      <td>-2.49690</td>\n",
       "      <td>-0.70024</td>\n",
       "      <td>-2.53520</td>\n",
       "      <td>-0.99733</td>\n",
       "      <td>-2.2389</td>\n",
       "      <td>-1.1097</td>\n",
       "      <td>0.57388</td>\n",
       "      <td>-2.226800</td>\n",
       "      <td>-0.811130</td>\n",
       "      <td>-2.37140</td>\n",
       "      <td>1.481200</td>\n",
       "      <td>3.125200</td>\n",
       "      <td>-0.31270</td>\n",
       "      <td>0.97177</td>\n",
       "      <td>-0.685180</td>\n",
       "      <td>-0.18344</td>\n",
       "      <td>-1.384500</td>\n",
       "      <td>-0.98578</td>\n",
       "      <td>-1.66480</td>\n",
       "      <td>-0.587210</td>\n",
       "      <td>-1.89180</td>\n",
       "      <td>-0.90744</td>\n",
       "      <td>-0.50375</td>\n",
       "      <td>1.98690</td>\n",
       "      <td>3.06890</td>\n",
       "      <td>-1.61810</td>\n",
       "      <td>-1.163900</td>\n",
       "      <td>-2.776400</td>\n",
       "      <td>1.03950</td>\n",
       "      <td>1.702800</td>\n",
       "      <td>-2.04780</td>\n",
       "      <td>3.11900</td>\n",
       "      <td>-2.65770</td>\n",
       "      <td>4.217000</td>\n",
       "      <td>-0.887980</td>\n",
       "      <td>0.50361</td>\n",
       "      <td>0.24387</td>\n",
       "      <td>-2.49820</td>\n",
       "      <td>-1.1432</td>\n",
       "      <td>0.863590</td>\n",
       "      <td>0.581940</td>\n",
       "      <td>1.05840</td>\n",
       "      <td>1.30550</td>\n",
       "      <td>-0.97641</td>\n",
       "      <td>0.24125</td>\n",
       "      <td>3.99320</td>\n",
       "      <td>-0.97158</td>\n",
       "      <td>1.309100</td>\n",
       "      <td>0.773220</td>\n",
       "      <td>0.42159</td>\n",
       "      <td>0.757590</td>\n",
       "      <td>-2.78450</td>\n",
       "      <td>-2.22850</td>\n",
       "      <td>1.35470</td>\n",
       "      <td>0.159160</td>\n",
       "      <td>1.37000</td>\n",
       "      <td>-2.32700</td>\n",
       "      <td>-1.41640</td>\n",
       "      <td>2.16230</td>\n",
       "      <td>0.47771</td>\n",
       "      <td>1.76670</td>\n",
       "      <td>4.59320</td>\n",
       "      <td>-0.923760</td>\n",
       "      <td>-1.75620</td>\n",
       "      <td>-0.83212</td>\n",
       "      <td>5.286400</td>\n",
       "      <td>-0.77980</td>\n",
       "      <td>-1.91110</td>\n",
       "      <td>1.782100</td>\n",
       "      <td>-0.20666</td>\n",
       "      <td>-4.17980</td>\n",
       "      <td>0.37562</td>\n",
       "      <td>-2.32090</td>\n",
       "      <td>0.69554</td>\n",
       "      <td>-2.982000</td>\n",
       "      <td>2.38170</td>\n",
       "      <td>1.53930</td>\n",
       "      <td>0.42728</td>\n",
       "      <td>2.94390</td>\n",
       "      <td>-1.18590</td>\n",
       "      <td>0.38971</td>\n",
       "      <td>3.1219</td>\n",
       "      <td>-0.89303</td>\n",
       "      <td>-2.56450</td>\n",
       "      <td>-0.035272</td>\n",
       "      <td>1.375600</td>\n",
       "      <td>-1.80250</td>\n",
       "      <td>0.86336</td>\n",
       "      <td>-0.94475</td>\n",
       "      <td>-0.533470</td>\n",
       "      <td>-0.32559</td>\n",
       "      <td>0.94793</td>\n",
       "      <td>-2.72790</td>\n",
       "      <td>0.87363</td>\n",
       "      <td>0.27429</td>\n",
       "      <td>0.68351</td>\n",
       "      <td>-2.76550</td>\n",
       "      <td>2.04130</td>\n",
       "      <td>-1.957400</td>\n",
       "      <td>2.75130</td>\n",
       "      <td>-0.911740</td>\n",
       "      <td>1.290600</td>\n",
       "      <td>1.89180</td>\n",
       "      <td>-4.49850</td>\n",
       "      <td>-2.90070</td>\n",
       "      <td>-1.03190</td>\n",
       "      <td>0.038302</td>\n",
       "      <td>0.62120</td>\n",
       "      <td>-1.66310</td>\n",
       "      <td>0.201810</td>\n",
       "      <td>1.205600</td>\n",
       "      <td>1.642000</td>\n",
       "      <td>0.307360</td>\n",
       "      <td>1.391700</td>\n",
       "      <td>-1.75260</td>\n",
       "      <td>-1.37860</td>\n",
       "      <td>-1.46090</td>\n",
       "      <td>-1.487300</td>\n",
       "      <td>0.95038</td>\n",
       "      <td>-3.100800</td>\n",
       "      <td>-2.85720</td>\n",
       "      <td>1.45410</td>\n",
       "      <td>-0.70981</td>\n",
       "      <td>3.747600</td>\n",
       "      <td>-1.26090</td>\n",
       "      <td>-2.79440</td>\n",
       "      <td>3.44740</td>\n",
       "      <td>0.94558</td>\n",
       "      <td>-0.54897</td>\n",
       "      <td>-1.971400</td>\n",
       "      <td>1.93290</td>\n",
       "      <td>-0.28302</td>\n",
       "      <td>2.727500</td>\n",
       "      <td>1.36420</td>\n",
       "      <td>2.61480</td>\n",
       "      <td>5.45890</td>\n",
       "      <td>-1.36260</td>\n",
       "      <td>3.589800</td>\n",
       "      <td>-2.92200</td>\n",
       "      <td>0.504400</td>\n",
       "      <td>-0.21339</td>\n",
       "      <td>-0.45199</td>\n",
       "      <td>-2.413400</td>\n",
       "      <td>-1.171300</td>\n",
       "      <td>1.025500</td>\n",
       "      <td>1.17010</td>\n",
       "      <td>-1.561200</td>\n",
       "      <td>1.51710</td>\n",
       "      <td>-0.048064</td>\n",
       "      <td>2.645300</td>\n",
       "      <td>3.5460</td>\n",
       "      <td>-0.010302</td>\n",
       "      <td>0.45441</td>\n",
       "      <td>3.383400</td>\n",
       "      <td>-1.70950</td>\n",
       "      <td>-1.43170</td>\n",
       "      <td>1.058400</td>\n",
       "      <td>3.40950</td>\n",
       "      <td>1.31250</td>\n",
       "      <td>-2.143600</td>\n",
       "      <td>2.829800</td>\n",
       "      <td>-0.96006</td>\n",
       "      <td>-1.74710</td>\n",
       "      <td>2.03530</td>\n",
       "      <td>2.69070</td>\n",
       "      <td>1.57160</td>\n",
       "      <td>1.29650</td>\n",
       "      <td>-1.584500</td>\n",
       "      <td>1.27180</td>\n",
       "      <td>-1.046800</td>\n",
       "      <td>1.037600</td>\n",
       "      <td>-4.00340</td>\n",
       "      <td>-1.697000</td>\n",
       "      <td>-2.62320</td>\n",
       "      <td>-1.38540</td>\n",
       "      <td>1.98640</td>\n",
       "      <td>0.70325</td>\n",
       "      <td>-1.24460</td>\n",
       "      <td>2.15750</td>\n",
       "      <td>0.954570</td>\n",
       "      <td>1.792900</td>\n",
       "      <td>2.826900</td>\n",
       "      <td>-3.779100</td>\n",
       "      <td>0.256970</td>\n",
       "      <td>0.070994</td>\n",
       "      <td>-4.422200</td>\n",
       "      <td>2.765800</td>\n",
       "      <td>0.25410</td>\n",
       "      <td>0.13624</td>\n",
       "      <td>-2.57590</td>\n",
       "      <td>-2.75180</td>\n",
       "      <td>-1.28510</td>\n",
       "      <td>1.66780</td>\n",
       "      <td>0.51108</td>\n",
       "      <td>1.22750</td>\n",
       "      <td>-2.18050</td>\n",
       "      <td>-3.17170</td>\n",
       "      <td>-0.35066</td>\n",
       "      <td>2.274800</td>\n",
       "      <td>-2.38250</td>\n",
       "      <td>-1.22730</td>\n",
       "      <td>-0.024748</td>\n",
       "      <td>0.44043</td>\n",
       "      <td>1.319800</td>\n",
       "      <td>1.65810</td>\n",
       "      <td>-3.01110</td>\n",
       "      <td>0.53332</td>\n",
       "      <td>-0.081214</td>\n",
       "      <td>1.185000</td>\n",
       "      <td>-3.559500</td>\n",
       "      <td>-0.493960</td>\n",
       "      <td>-1.60800</td>\n",
       "      <td>-1.68240</td>\n",
       "      <td>-1.651100</td>\n",
       "      <td>2.89460</td>\n",
       "      <td>-1.35680</td>\n",
       "      <td>2.44590</td>\n",
       "      <td>-0.19233</td>\n",
       "      <td>0.14830</td>\n",
       "      <td>-0.146960</td>\n",
       "      <td>2.232800</td>\n",
       "      <td>5.93290</td>\n",
       "      <td>-0.84719</td>\n",
       "      <td>-1.3155</td>\n",
       "      <td>3.271900</td>\n",
       "      <td>-2.61820</td>\n",
       "      <td>1.64680</td>\n",
       "      <td>-0.68108</td>\n",
       "      <td>1.77150</td>\n",
       "      <td>0.459930</td>\n",
       "      <td>1.23450</td>\n",
       "      <td>-2.67310</td>\n",
       "      <td>-0.080917</td>\n",
       "      <td>-0.17837</td>\n",
       "      <td>0.10582</td>\n",
       "      <td>-2.1597</td>\n",
       "      <td>-0.781270</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>misconception</th>\n",
       "      <td>-1.44020</td>\n",
       "      <td>-0.65446</td>\n",
       "      <td>1.421700</td>\n",
       "      <td>-0.39426</td>\n",
       "      <td>2.44130</td>\n",
       "      <td>1.224300</td>\n",
       "      <td>3.486700</td>\n",
       "      <td>3.91120</td>\n",
       "      <td>-1.8492</td>\n",
       "      <td>-0.833800</td>\n",
       "      <td>5.1676</td>\n",
       "      <td>0.869870</td>\n",
       "      <td>-1.9083</td>\n",
       "      <td>-0.31838</td>\n",
       "      <td>0.433870</td>\n",
       "      <td>3.06350</td>\n",
       "      <td>2.563100</td>\n",
       "      <td>1.42770</td>\n",
       "      <td>-2.36530</td>\n",
       "      <td>0.17331</td>\n",
       "      <td>0.11683</td>\n",
       "      <td>2.073600</td>\n",
       "      <td>-3.468900</td>\n",
       "      <td>1.209100</td>\n",
       "      <td>-1.95440</td>\n",
       "      <td>-2.79620</td>\n",
       "      <td>-1.27760</td>\n",
       "      <td>-0.51866</td>\n",
       "      <td>-2.11150</td>\n",
       "      <td>1.32270</td>\n",
       "      <td>1.41510</td>\n",
       "      <td>-0.012075</td>\n",
       "      <td>-1.12090</td>\n",
       "      <td>-1.74920</td>\n",
       "      <td>-2.263800</td>\n",
       "      <td>-0.74907</td>\n",
       "      <td>0.69087</td>\n",
       "      <td>0.06555</td>\n",
       "      <td>0.039321</td>\n",
       "      <td>-0.48229</td>\n",
       "      <td>0.36317</td>\n",
       "      <td>-0.84680</td>\n",
       "      <td>-1.32010</td>\n",
       "      <td>0.32879</td>\n",
       "      <td>-3.16190</td>\n",
       "      <td>0.33472</td>\n",
       "      <td>2.281400</td>\n",
       "      <td>-2.216900</td>\n",
       "      <td>0.884990</td>\n",
       "      <td>1.09090</td>\n",
       "      <td>-1.70110</td>\n",
       "      <td>1.191000</td>\n",
       "      <td>-0.395970</td>\n",
       "      <td>-2.05440</td>\n",
       "      <td>-1.55900</td>\n",
       "      <td>1.19610</td>\n",
       "      <td>0.93411</td>\n",
       "      <td>1.891800</td>\n",
       "      <td>0.77402</td>\n",
       "      <td>1.48950</td>\n",
       "      <td>0.063936</td>\n",
       "      <td>0.77872</td>\n",
       "      <td>-0.73827</td>\n",
       "      <td>-0.33916</td>\n",
       "      <td>0.36916</td>\n",
       "      <td>0.17412</td>\n",
       "      <td>-1.83790</td>\n",
       "      <td>-5.0217</td>\n",
       "      <td>1.6003</td>\n",
       "      <td>3.26650</td>\n",
       "      <td>-0.160080</td>\n",
       "      <td>1.016900</td>\n",
       "      <td>-2.34140</td>\n",
       "      <td>1.077900</td>\n",
       "      <td>-3.283900</td>\n",
       "      <td>1.13220</td>\n",
       "      <td>-2.15710</td>\n",
       "      <td>1.155200</td>\n",
       "      <td>-1.67790</td>\n",
       "      <td>-0.063517</td>\n",
       "      <td>-1.90090</td>\n",
       "      <td>1.26600</td>\n",
       "      <td>1.650600</td>\n",
       "      <td>0.56655</td>\n",
       "      <td>1.62230</td>\n",
       "      <td>0.64850</td>\n",
       "      <td>-4.29680</td>\n",
       "      <td>-2.34700</td>\n",
       "      <td>1.79010</td>\n",
       "      <td>1.047000</td>\n",
       "      <td>1.344900</td>\n",
       "      <td>0.47285</td>\n",
       "      <td>2.663100</td>\n",
       "      <td>-3.58440</td>\n",
       "      <td>0.12740</td>\n",
       "      <td>-0.72851</td>\n",
       "      <td>-0.512600</td>\n",
       "      <td>0.893700</td>\n",
       "      <td>2.00550</td>\n",
       "      <td>-0.12694</td>\n",
       "      <td>1.38400</td>\n",
       "      <td>2.2943</td>\n",
       "      <td>2.528100</td>\n",
       "      <td>1.634900</td>\n",
       "      <td>-0.17525</td>\n",
       "      <td>4.56080</td>\n",
       "      <td>0.28487</td>\n",
       "      <td>-1.18960</td>\n",
       "      <td>0.98215</td>\n",
       "      <td>-2.40600</td>\n",
       "      <td>0.553100</td>\n",
       "      <td>-0.084463</td>\n",
       "      <td>-0.16912</td>\n",
       "      <td>0.654340</td>\n",
       "      <td>0.27161</td>\n",
       "      <td>2.64430</td>\n",
       "      <td>-4.85060</td>\n",
       "      <td>-1.336800</td>\n",
       "      <td>0.15755</td>\n",
       "      <td>-0.63597</td>\n",
       "      <td>-1.25890</td>\n",
       "      <td>-2.00010</td>\n",
       "      <td>-0.17487</td>\n",
       "      <td>0.20887</td>\n",
       "      <td>-1.90630</td>\n",
       "      <td>-2.197000</td>\n",
       "      <td>0.34965</td>\n",
       "      <td>-3.18230</td>\n",
       "      <td>2.224800</td>\n",
       "      <td>-2.99950</td>\n",
       "      <td>-3.96240</td>\n",
       "      <td>0.383090</td>\n",
       "      <td>3.67700</td>\n",
       "      <td>0.93066</td>\n",
       "      <td>-1.88170</td>\n",
       "      <td>-0.28573</td>\n",
       "      <td>-0.22240</td>\n",
       "      <td>-0.452800</td>\n",
       "      <td>1.74900</td>\n",
       "      <td>-1.16860</td>\n",
       "      <td>-0.61897</td>\n",
       "      <td>-1.24770</td>\n",
       "      <td>0.51393</td>\n",
       "      <td>1.52660</td>\n",
       "      <td>1.5007</td>\n",
       "      <td>-0.78604</td>\n",
       "      <td>-5.06570</td>\n",
       "      <td>-1.354700</td>\n",
       "      <td>0.014334</td>\n",
       "      <td>2.57200</td>\n",
       "      <td>1.70360</td>\n",
       "      <td>1.37560</td>\n",
       "      <td>-0.834790</td>\n",
       "      <td>0.49078</td>\n",
       "      <td>-0.94801</td>\n",
       "      <td>1.02110</td>\n",
       "      <td>2.25870</td>\n",
       "      <td>0.41330</td>\n",
       "      <td>0.33318</td>\n",
       "      <td>-2.24850</td>\n",
       "      <td>0.81790</td>\n",
       "      <td>-0.449510</td>\n",
       "      <td>2.39340</td>\n",
       "      <td>1.402700</td>\n",
       "      <td>-0.925540</td>\n",
       "      <td>-2.61830</td>\n",
       "      <td>-2.78740</td>\n",
       "      <td>0.46414</td>\n",
       "      <td>1.56080</td>\n",
       "      <td>-0.996830</td>\n",
       "      <td>-1.98250</td>\n",
       "      <td>-1.40860</td>\n",
       "      <td>1.560800</td>\n",
       "      <td>-0.201570</td>\n",
       "      <td>2.286300</td>\n",
       "      <td>2.374600</td>\n",
       "      <td>0.210200</td>\n",
       "      <td>1.66130</td>\n",
       "      <td>-1.81680</td>\n",
       "      <td>-1.88840</td>\n",
       "      <td>-2.322100</td>\n",
       "      <td>0.33800</td>\n",
       "      <td>3.544200</td>\n",
       "      <td>0.17021</td>\n",
       "      <td>-2.11830</td>\n",
       "      <td>1.24420</td>\n",
       "      <td>-4.505700</td>\n",
       "      <td>-2.29700</td>\n",
       "      <td>2.76450</td>\n",
       "      <td>-0.59000</td>\n",
       "      <td>-2.95320</td>\n",
       "      <td>-0.53757</td>\n",
       "      <td>1.151600</td>\n",
       "      <td>0.22778</td>\n",
       "      <td>1.87150</td>\n",
       "      <td>-0.588330</td>\n",
       "      <td>-3.21310</td>\n",
       "      <td>1.77990</td>\n",
       "      <td>0.23060</td>\n",
       "      <td>1.74870</td>\n",
       "      <td>0.300530</td>\n",
       "      <td>-0.91628</td>\n",
       "      <td>-1.513200</td>\n",
       "      <td>-0.56853</td>\n",
       "      <td>3.41440</td>\n",
       "      <td>0.673730</td>\n",
       "      <td>-2.347100</td>\n",
       "      <td>-0.405600</td>\n",
       "      <td>-0.77719</td>\n",
       "      <td>-2.719700</td>\n",
       "      <td>-0.55854</td>\n",
       "      <td>-0.443050</td>\n",
       "      <td>-2.018000</td>\n",
       "      <td>1.6621</td>\n",
       "      <td>0.316260</td>\n",
       "      <td>2.27980</td>\n",
       "      <td>-0.019118</td>\n",
       "      <td>-3.19110</td>\n",
       "      <td>2.61610</td>\n",
       "      <td>-0.224910</td>\n",
       "      <td>-2.05110</td>\n",
       "      <td>0.10540</td>\n",
       "      <td>0.159160</td>\n",
       "      <td>-0.211710</td>\n",
       "      <td>-3.13590</td>\n",
       "      <td>0.24266</td>\n",
       "      <td>0.63316</td>\n",
       "      <td>3.11420</td>\n",
       "      <td>3.00830</td>\n",
       "      <td>1.03040</td>\n",
       "      <td>3.224900</td>\n",
       "      <td>-2.12900</td>\n",
       "      <td>0.886660</td>\n",
       "      <td>1.320600</td>\n",
       "      <td>1.46330</td>\n",
       "      <td>0.527630</td>\n",
       "      <td>-0.14561</td>\n",
       "      <td>1.04800</td>\n",
       "      <td>1.59470</td>\n",
       "      <td>2.48760</td>\n",
       "      <td>-1.09030</td>\n",
       "      <td>2.05320</td>\n",
       "      <td>2.159600</td>\n",
       "      <td>1.694900</td>\n",
       "      <td>1.021200</td>\n",
       "      <td>-0.024739</td>\n",
       "      <td>-1.293400</td>\n",
       "      <td>0.798630</td>\n",
       "      <td>-2.245600</td>\n",
       "      <td>1.850900</td>\n",
       "      <td>-0.57083</td>\n",
       "      <td>-2.74620</td>\n",
       "      <td>-3.69620</td>\n",
       "      <td>-1.12950</td>\n",
       "      <td>1.54350</td>\n",
       "      <td>-1.16400</td>\n",
       "      <td>0.57704</td>\n",
       "      <td>-0.16744</td>\n",
       "      <td>-1.03510</td>\n",
       "      <td>-0.98163</td>\n",
       "      <td>-0.79970</td>\n",
       "      <td>1.068900</td>\n",
       "      <td>4.23780</td>\n",
       "      <td>3.09940</td>\n",
       "      <td>0.344750</td>\n",
       "      <td>0.84469</td>\n",
       "      <td>-0.054672</td>\n",
       "      <td>3.06860</td>\n",
       "      <td>-2.48950</td>\n",
       "      <td>2.08190</td>\n",
       "      <td>-0.608570</td>\n",
       "      <td>0.267390</td>\n",
       "      <td>2.073200</td>\n",
       "      <td>-1.325900</td>\n",
       "      <td>1.86590</td>\n",
       "      <td>-0.41704</td>\n",
       "      <td>0.652790</td>\n",
       "      <td>-2.40710</td>\n",
       "      <td>-1.32990</td>\n",
       "      <td>2.64320</td>\n",
       "      <td>1.03320</td>\n",
       "      <td>-0.08317</td>\n",
       "      <td>1.558700</td>\n",
       "      <td>-1.642900</td>\n",
       "      <td>1.70270</td>\n",
       "      <td>0.51001</td>\n",
       "      <td>2.0300</td>\n",
       "      <td>0.983990</td>\n",
       "      <td>-0.32521</td>\n",
       "      <td>-0.73525</td>\n",
       "      <td>2.69540</td>\n",
       "      <td>-1.24210</td>\n",
       "      <td>2.630100</td>\n",
       "      <td>-0.89313</td>\n",
       "      <td>-0.80961</td>\n",
       "      <td>0.536610</td>\n",
       "      <td>2.65300</td>\n",
       "      <td>-2.67700</td>\n",
       "      <td>-2.5539</td>\n",
       "      <td>3.299300</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>crab</th>\n",
       "      <td>-0.29427</td>\n",
       "      <td>-3.77220</td>\n",
       "      <td>-0.707150</td>\n",
       "      <td>0.25983</td>\n",
       "      <td>1.18960</td>\n",
       "      <td>-4.619400</td>\n",
       "      <td>-3.174600</td>\n",
       "      <td>3.59190</td>\n",
       "      <td>-2.7857</td>\n",
       "      <td>2.099300</td>\n",
       "      <td>5.1909</td>\n",
       "      <td>-2.951200</td>\n",
       "      <td>4.8228</td>\n",
       "      <td>2.43830</td>\n",
       "      <td>2.764800</td>\n",
       "      <td>-6.81330</td>\n",
       "      <td>1.249800</td>\n",
       "      <td>1.50060</td>\n",
       "      <td>2.30220</td>\n",
       "      <td>-3.04830</td>\n",
       "      <td>0.36441</td>\n",
       "      <td>1.408100</td>\n",
       "      <td>0.207200</td>\n",
       "      <td>-1.062100</td>\n",
       "      <td>3.85600</td>\n",
       "      <td>-1.65500</td>\n",
       "      <td>-2.68290</td>\n",
       "      <td>1.23670</td>\n",
       "      <td>1.14700</td>\n",
       "      <td>-0.44072</td>\n",
       "      <td>-1.89680</td>\n",
       "      <td>-3.589700</td>\n",
       "      <td>2.88470</td>\n",
       "      <td>-1.03540</td>\n",
       "      <td>4.117700</td>\n",
       "      <td>-2.90360</td>\n",
       "      <td>2.12960</td>\n",
       "      <td>-1.46760</td>\n",
       "      <td>1.717600</td>\n",
       "      <td>0.35180</td>\n",
       "      <td>0.94584</td>\n",
       "      <td>-1.83730</td>\n",
       "      <td>1.65680</td>\n",
       "      <td>2.34720</td>\n",
       "      <td>1.33600</td>\n",
       "      <td>0.36764</td>\n",
       "      <td>-0.066217</td>\n",
       "      <td>0.026254</td>\n",
       "      <td>-1.341900</td>\n",
       "      <td>-0.54964</td>\n",
       "      <td>-2.24190</td>\n",
       "      <td>4.537100</td>\n",
       "      <td>1.879500</td>\n",
       "      <td>-1.14570</td>\n",
       "      <td>-0.97683</td>\n",
       "      <td>-3.38040</td>\n",
       "      <td>-2.94590</td>\n",
       "      <td>1.478700</td>\n",
       "      <td>3.63150</td>\n",
       "      <td>-1.43010</td>\n",
       "      <td>2.215300</td>\n",
       "      <td>-2.65400</td>\n",
       "      <td>4.46650</td>\n",
       "      <td>4.65510</td>\n",
       "      <td>-1.09250</td>\n",
       "      <td>-3.48330</td>\n",
       "      <td>-3.30940</td>\n",
       "      <td>1.8197</td>\n",
       "      <td>1.0144</td>\n",
       "      <td>-0.83145</td>\n",
       "      <td>-1.488100</td>\n",
       "      <td>-2.943900</td>\n",
       "      <td>2.05330</td>\n",
       "      <td>2.050800</td>\n",
       "      <td>-0.333460</td>\n",
       "      <td>-1.17890</td>\n",
       "      <td>-3.00110</td>\n",
       "      <td>-5.810900</td>\n",
       "      <td>2.90630</td>\n",
       "      <td>-1.847700</td>\n",
       "      <td>1.83780</td>\n",
       "      <td>2.53050</td>\n",
       "      <td>2.266700</td>\n",
       "      <td>0.56153</td>\n",
       "      <td>-0.74400</td>\n",
       "      <td>-0.93070</td>\n",
       "      <td>6.29680</td>\n",
       "      <td>0.41122</td>\n",
       "      <td>-4.43690</td>\n",
       "      <td>-1.893000</td>\n",
       "      <td>7.908900</td>\n",
       "      <td>-0.12312</td>\n",
       "      <td>1.228400</td>\n",
       "      <td>-2.99290</td>\n",
       "      <td>-0.86969</td>\n",
       "      <td>1.30830</td>\n",
       "      <td>3.628700</td>\n",
       "      <td>-2.369500</td>\n",
       "      <td>-0.57882</td>\n",
       "      <td>-3.64610</td>\n",
       "      <td>-1.03220</td>\n",
       "      <td>-2.6258</td>\n",
       "      <td>0.012072</td>\n",
       "      <td>-4.216900</td>\n",
       "      <td>-2.11010</td>\n",
       "      <td>-0.88852</td>\n",
       "      <td>-5.93000</td>\n",
       "      <td>5.09930</td>\n",
       "      <td>-1.02640</td>\n",
       "      <td>-1.12670</td>\n",
       "      <td>2.017900</td>\n",
       "      <td>2.965200</td>\n",
       "      <td>-0.76725</td>\n",
       "      <td>0.522110</td>\n",
       "      <td>-1.58860</td>\n",
       "      <td>1.94680</td>\n",
       "      <td>3.93830</td>\n",
       "      <td>1.207200</td>\n",
       "      <td>-2.61410</td>\n",
       "      <td>-1.28920</td>\n",
       "      <td>-0.85353</td>\n",
       "      <td>5.07760</td>\n",
       "      <td>-7.48520</td>\n",
       "      <td>-0.99613</td>\n",
       "      <td>-4.88490</td>\n",
       "      <td>-0.119810</td>\n",
       "      <td>-0.43466</td>\n",
       "      <td>-5.60860</td>\n",
       "      <td>2.011500</td>\n",
       "      <td>2.62300</td>\n",
       "      <td>1.09020</td>\n",
       "      <td>-0.068215</td>\n",
       "      <td>3.41990</td>\n",
       "      <td>-1.15150</td>\n",
       "      <td>4.62600</td>\n",
       "      <td>-2.56210</td>\n",
       "      <td>-3.07810</td>\n",
       "      <td>-2.581000</td>\n",
       "      <td>-7.82810</td>\n",
       "      <td>-4.71150</td>\n",
       "      <td>-5.97700</td>\n",
       "      <td>-2.65340</td>\n",
       "      <td>-3.86190</td>\n",
       "      <td>-1.54650</td>\n",
       "      <td>1.2815</td>\n",
       "      <td>-1.45570</td>\n",
       "      <td>-0.57384</td>\n",
       "      <td>2.791600</td>\n",
       "      <td>-4.704000</td>\n",
       "      <td>-5.45030</td>\n",
       "      <td>-0.46227</td>\n",
       "      <td>5.71290</td>\n",
       "      <td>1.221000</td>\n",
       "      <td>-3.08060</td>\n",
       "      <td>3.54630</td>\n",
       "      <td>-3.19900</td>\n",
       "      <td>1.66120</td>\n",
       "      <td>-0.46012</td>\n",
       "      <td>-2.21520</td>\n",
       "      <td>-0.22702</td>\n",
       "      <td>-0.86561</td>\n",
       "      <td>-2.131200</td>\n",
       "      <td>-0.17625</td>\n",
       "      <td>-3.066900</td>\n",
       "      <td>-2.314000</td>\n",
       "      <td>-2.91810</td>\n",
       "      <td>-0.54938</td>\n",
       "      <td>1.93340</td>\n",
       "      <td>2.52950</td>\n",
       "      <td>1.829700</td>\n",
       "      <td>2.29010</td>\n",
       "      <td>0.73415</td>\n",
       "      <td>0.081001</td>\n",
       "      <td>-1.224200</td>\n",
       "      <td>-4.276700</td>\n",
       "      <td>0.761780</td>\n",
       "      <td>-0.061992</td>\n",
       "      <td>-4.37250</td>\n",
       "      <td>2.72370</td>\n",
       "      <td>-0.17234</td>\n",
       "      <td>0.307930</td>\n",
       "      <td>-0.28302</td>\n",
       "      <td>1.218200</td>\n",
       "      <td>2.78480</td>\n",
       "      <td>-0.65654</td>\n",
       "      <td>0.28948</td>\n",
       "      <td>1.641700</td>\n",
       "      <td>-2.67990</td>\n",
       "      <td>-1.33830</td>\n",
       "      <td>0.13474</td>\n",
       "      <td>-2.31100</td>\n",
       "      <td>0.55091</td>\n",
       "      <td>0.296710</td>\n",
       "      <td>-1.23900</td>\n",
       "      <td>0.37136</td>\n",
       "      <td>-1.317100</td>\n",
       "      <td>-1.04930</td>\n",
       "      <td>0.99578</td>\n",
       "      <td>0.63019</td>\n",
       "      <td>-2.42330</td>\n",
       "      <td>-0.031176</td>\n",
       "      <td>0.99886</td>\n",
       "      <td>1.983800</td>\n",
       "      <td>3.15850</td>\n",
       "      <td>0.64745</td>\n",
       "      <td>-2.374500</td>\n",
       "      <td>0.064163</td>\n",
       "      <td>-1.466000</td>\n",
       "      <td>1.11430</td>\n",
       "      <td>-0.345950</td>\n",
       "      <td>3.85840</td>\n",
       "      <td>5.071100</td>\n",
       "      <td>-0.055536</td>\n",
       "      <td>-2.4047</td>\n",
       "      <td>1.348400</td>\n",
       "      <td>-0.22253</td>\n",
       "      <td>4.494800</td>\n",
       "      <td>1.04910</td>\n",
       "      <td>-1.16750</td>\n",
       "      <td>-3.155200</td>\n",
       "      <td>5.73740</td>\n",
       "      <td>-3.42350</td>\n",
       "      <td>1.718600</td>\n",
       "      <td>2.886400</td>\n",
       "      <td>-0.40758</td>\n",
       "      <td>-6.42380</td>\n",
       "      <td>-2.59500</td>\n",
       "      <td>-2.30090</td>\n",
       "      <td>1.62540</td>\n",
       "      <td>3.10150</td>\n",
       "      <td>2.735800</td>\n",
       "      <td>-1.53540</td>\n",
       "      <td>3.023500</td>\n",
       "      <td>-0.834300</td>\n",
       "      <td>-3.46940</td>\n",
       "      <td>-0.312980</td>\n",
       "      <td>-1.15280</td>\n",
       "      <td>0.83032</td>\n",
       "      <td>-2.08590</td>\n",
       "      <td>-1.82890</td>\n",
       "      <td>-1.71760</td>\n",
       "      <td>-4.34410</td>\n",
       "      <td>1.527500</td>\n",
       "      <td>5.958800</td>\n",
       "      <td>-3.111300</td>\n",
       "      <td>0.425880</td>\n",
       "      <td>2.792900</td>\n",
       "      <td>-1.149900</td>\n",
       "      <td>-1.547800</td>\n",
       "      <td>-1.793800</td>\n",
       "      <td>-1.87730</td>\n",
       "      <td>-0.66235</td>\n",
       "      <td>0.52911</td>\n",
       "      <td>-0.17560</td>\n",
       "      <td>-1.51630</td>\n",
       "      <td>-2.88540</td>\n",
       "      <td>4.74170</td>\n",
       "      <td>9.27220</td>\n",
       "      <td>-1.85590</td>\n",
       "      <td>2.90540</td>\n",
       "      <td>1.97350</td>\n",
       "      <td>-0.069534</td>\n",
       "      <td>0.75262</td>\n",
       "      <td>0.87348</td>\n",
       "      <td>-5.207600</td>\n",
       "      <td>-0.63884</td>\n",
       "      <td>-1.302700</td>\n",
       "      <td>-3.73270</td>\n",
       "      <td>-4.39430</td>\n",
       "      <td>4.93160</td>\n",
       "      <td>3.559100</td>\n",
       "      <td>-0.115260</td>\n",
       "      <td>1.041200</td>\n",
       "      <td>4.139300</td>\n",
       "      <td>0.55631</td>\n",
       "      <td>-4.32760</td>\n",
       "      <td>3.124000</td>\n",
       "      <td>3.06510</td>\n",
       "      <td>1.53070</td>\n",
       "      <td>0.52090</td>\n",
       "      <td>0.91262</td>\n",
       "      <td>-0.33345</td>\n",
       "      <td>-3.418300</td>\n",
       "      <td>-0.089182</td>\n",
       "      <td>0.59738</td>\n",
       "      <td>1.85210</td>\n",
       "      <td>-1.3994</td>\n",
       "      <td>-2.982200</td>\n",
       "      <td>-2.78770</td>\n",
       "      <td>1.50800</td>\n",
       "      <td>3.99800</td>\n",
       "      <td>-2.59870</td>\n",
       "      <td>7.186000</td>\n",
       "      <td>-0.18771</td>\n",
       "      <td>-1.66890</td>\n",
       "      <td>1.588400</td>\n",
       "      <td>0.81274</td>\n",
       "      <td>1.51700</td>\n",
       "      <td>-2.0713</td>\n",
       "      <td>2.980400</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>vaccinate</th>\n",
       "      <td>0.26977</td>\n",
       "      <td>3.14150</td>\n",
       "      <td>-2.580500</td>\n",
       "      <td>2.16510</td>\n",
       "      <td>1.75510</td>\n",
       "      <td>0.902260</td>\n",
       "      <td>0.206410</td>\n",
       "      <td>3.53160</td>\n",
       "      <td>-1.2185</td>\n",
       "      <td>3.915300</td>\n",
       "      <td>2.0898</td>\n",
       "      <td>2.449300</td>\n",
       "      <td>-6.4308</td>\n",
       "      <td>0.51798</td>\n",
       "      <td>-0.017812</td>\n",
       "      <td>-1.01860</td>\n",
       "      <td>-0.817590</td>\n",
       "      <td>-2.12740</td>\n",
       "      <td>-2.55710</td>\n",
       "      <td>-1.24400</td>\n",
       "      <td>-2.03330</td>\n",
       "      <td>-1.801100</td>\n",
       "      <td>-3.576000</td>\n",
       "      <td>-2.004600</td>\n",
       "      <td>-2.38120</td>\n",
       "      <td>0.23166</td>\n",
       "      <td>-0.29845</td>\n",
       "      <td>3.79450</td>\n",
       "      <td>-2.25420</td>\n",
       "      <td>0.27720</td>\n",
       "      <td>4.39550</td>\n",
       "      <td>-0.327550</td>\n",
       "      <td>-1.07670</td>\n",
       "      <td>-3.44250</td>\n",
       "      <td>-1.051400</td>\n",
       "      <td>-3.06890</td>\n",
       "      <td>2.25370</td>\n",
       "      <td>0.39276</td>\n",
       "      <td>1.079000</td>\n",
       "      <td>0.29247</td>\n",
       "      <td>-3.34410</td>\n",
       "      <td>2.40720</td>\n",
       "      <td>-0.88180</td>\n",
       "      <td>3.39740</td>\n",
       "      <td>-3.74330</td>\n",
       "      <td>0.16462</td>\n",
       "      <td>4.559300</td>\n",
       "      <td>-5.435500</td>\n",
       "      <td>-2.510000</td>\n",
       "      <td>2.09480</td>\n",
       "      <td>0.41158</td>\n",
       "      <td>1.357000</td>\n",
       "      <td>1.633400</td>\n",
       "      <td>-3.67600</td>\n",
       "      <td>-2.55740</td>\n",
       "      <td>0.19681</td>\n",
       "      <td>-1.17610</td>\n",
       "      <td>-0.889060</td>\n",
       "      <td>1.07560</td>\n",
       "      <td>0.75712</td>\n",
       "      <td>-2.074700</td>\n",
       "      <td>-1.96230</td>\n",
       "      <td>0.29657</td>\n",
       "      <td>-2.47170</td>\n",
       "      <td>4.38570</td>\n",
       "      <td>3.18270</td>\n",
       "      <td>-1.17960</td>\n",
       "      <td>3.0087</td>\n",
       "      <td>2.5556</td>\n",
       "      <td>3.01570</td>\n",
       "      <td>0.571730</td>\n",
       "      <td>-0.068611</td>\n",
       "      <td>-3.35580</td>\n",
       "      <td>1.659700</td>\n",
       "      <td>-1.606800</td>\n",
       "      <td>0.58696</td>\n",
       "      <td>-1.79070</td>\n",
       "      <td>-4.188000</td>\n",
       "      <td>-3.66680</td>\n",
       "      <td>0.392170</td>\n",
       "      <td>-5.73340</td>\n",
       "      <td>1.64190</td>\n",
       "      <td>0.233090</td>\n",
       "      <td>2.46940</td>\n",
       "      <td>1.07510</td>\n",
       "      <td>-2.50540</td>\n",
       "      <td>-0.70993</td>\n",
       "      <td>-2.63680</td>\n",
       "      <td>3.56010</td>\n",
       "      <td>0.451730</td>\n",
       "      <td>-3.951400</td>\n",
       "      <td>0.69130</td>\n",
       "      <td>-2.810800</td>\n",
       "      <td>-0.97022</td>\n",
       "      <td>0.73158</td>\n",
       "      <td>-1.60140</td>\n",
       "      <td>-1.033700</td>\n",
       "      <td>-2.130900</td>\n",
       "      <td>-0.35910</td>\n",
       "      <td>3.49160</td>\n",
       "      <td>4.24490</td>\n",
       "      <td>1.0448</td>\n",
       "      <td>-0.947880</td>\n",
       "      <td>0.021135</td>\n",
       "      <td>-3.66050</td>\n",
       "      <td>1.30450</td>\n",
       "      <td>-1.98540</td>\n",
       "      <td>0.34249</td>\n",
       "      <td>-0.36231</td>\n",
       "      <td>-3.40710</td>\n",
       "      <td>1.056700</td>\n",
       "      <td>-1.607200</td>\n",
       "      <td>-1.31940</td>\n",
       "      <td>-1.656100</td>\n",
       "      <td>-0.12402</td>\n",
       "      <td>5.32950</td>\n",
       "      <td>-6.35620</td>\n",
       "      <td>-2.267300</td>\n",
       "      <td>-2.82070</td>\n",
       "      <td>-2.15110</td>\n",
       "      <td>-4.11550</td>\n",
       "      <td>0.13449</td>\n",
       "      <td>-3.86090</td>\n",
       "      <td>-2.25690</td>\n",
       "      <td>-4.24510</td>\n",
       "      <td>-2.028700</td>\n",
       "      <td>0.59665</td>\n",
       "      <td>0.43709</td>\n",
       "      <td>0.099868</td>\n",
       "      <td>-0.87762</td>\n",
       "      <td>0.44413</td>\n",
       "      <td>-2.840300</td>\n",
       "      <td>0.40321</td>\n",
       "      <td>1.21180</td>\n",
       "      <td>2.79890</td>\n",
       "      <td>0.28185</td>\n",
       "      <td>-3.06000</td>\n",
       "      <td>0.486680</td>\n",
       "      <td>2.62330</td>\n",
       "      <td>-4.10200</td>\n",
       "      <td>0.56496</td>\n",
       "      <td>-0.65196</td>\n",
       "      <td>-1.51670</td>\n",
       "      <td>-2.66470</td>\n",
       "      <td>-1.3859</td>\n",
       "      <td>-0.18319</td>\n",
       "      <td>-1.38200</td>\n",
       "      <td>-3.401200</td>\n",
       "      <td>0.628200</td>\n",
       "      <td>-0.70373</td>\n",
       "      <td>0.15838</td>\n",
       "      <td>-1.43580</td>\n",
       "      <td>1.019500</td>\n",
       "      <td>-0.58029</td>\n",
       "      <td>-0.58021</td>\n",
       "      <td>-0.80893</td>\n",
       "      <td>3.22150</td>\n",
       "      <td>-1.07960</td>\n",
       "      <td>0.58521</td>\n",
       "      <td>-1.05340</td>\n",
       "      <td>-0.66281</td>\n",
       "      <td>-0.716090</td>\n",
       "      <td>-1.81220</td>\n",
       "      <td>2.166700</td>\n",
       "      <td>-1.082500</td>\n",
       "      <td>-4.07670</td>\n",
       "      <td>-3.48420</td>\n",
       "      <td>-2.29270</td>\n",
       "      <td>5.34660</td>\n",
       "      <td>-2.576900</td>\n",
       "      <td>0.98606</td>\n",
       "      <td>3.54770</td>\n",
       "      <td>3.360800</td>\n",
       "      <td>0.019701</td>\n",
       "      <td>2.274800</td>\n",
       "      <td>0.759890</td>\n",
       "      <td>-0.937240</td>\n",
       "      <td>3.51350</td>\n",
       "      <td>-3.37670</td>\n",
       "      <td>0.68563</td>\n",
       "      <td>-0.685130</td>\n",
       "      <td>-0.12965</td>\n",
       "      <td>-0.935630</td>\n",
       "      <td>-1.57750</td>\n",
       "      <td>-0.43092</td>\n",
       "      <td>-0.36830</td>\n",
       "      <td>2.168400</td>\n",
       "      <td>0.26855</td>\n",
       "      <td>6.50050</td>\n",
       "      <td>2.76950</td>\n",
       "      <td>-0.03352</td>\n",
       "      <td>0.95000</td>\n",
       "      <td>-5.731400</td>\n",
       "      <td>-0.59452</td>\n",
       "      <td>-5.43230</td>\n",
       "      <td>-0.658170</td>\n",
       "      <td>0.61197</td>\n",
       "      <td>1.30270</td>\n",
       "      <td>-3.93070</td>\n",
       "      <td>2.79820</td>\n",
       "      <td>-4.061400</td>\n",
       "      <td>1.38160</td>\n",
       "      <td>-1.454700</td>\n",
       "      <td>1.00920</td>\n",
       "      <td>1.02070</td>\n",
       "      <td>6.075100</td>\n",
       "      <td>-2.108400</td>\n",
       "      <td>1.352100</td>\n",
       "      <td>-2.42820</td>\n",
       "      <td>4.327700</td>\n",
       "      <td>3.32510</td>\n",
       "      <td>0.122290</td>\n",
       "      <td>-2.083900</td>\n",
       "      <td>-2.8886</td>\n",
       "      <td>1.312100</td>\n",
       "      <td>-1.83270</td>\n",
       "      <td>2.821600</td>\n",
       "      <td>-0.86470</td>\n",
       "      <td>-2.36690</td>\n",
       "      <td>0.913070</td>\n",
       "      <td>-2.53410</td>\n",
       "      <td>-2.12640</td>\n",
       "      <td>-1.472300</td>\n",
       "      <td>-2.764700</td>\n",
       "      <td>0.49121</td>\n",
       "      <td>0.21180</td>\n",
       "      <td>-4.94940</td>\n",
       "      <td>1.60280</td>\n",
       "      <td>-0.63591</td>\n",
       "      <td>1.43280</td>\n",
       "      <td>-0.052498</td>\n",
       "      <td>-2.33480</td>\n",
       "      <td>-4.262300</td>\n",
       "      <td>0.728640</td>\n",
       "      <td>-0.95491</td>\n",
       "      <td>-3.274200</td>\n",
       "      <td>-0.73952</td>\n",
       "      <td>3.08630</td>\n",
       "      <td>1.36060</td>\n",
       "      <td>2.89490</td>\n",
       "      <td>-1.61040</td>\n",
       "      <td>1.03930</td>\n",
       "      <td>-0.403410</td>\n",
       "      <td>-1.298400</td>\n",
       "      <td>-0.942330</td>\n",
       "      <td>4.850000</td>\n",
       "      <td>-0.572850</td>\n",
       "      <td>0.595640</td>\n",
       "      <td>3.462700</td>\n",
       "      <td>1.456700</td>\n",
       "      <td>0.44842</td>\n",
       "      <td>-1.12320</td>\n",
       "      <td>-5.39100</td>\n",
       "      <td>-3.25190</td>\n",
       "      <td>1.28320</td>\n",
       "      <td>-3.54020</td>\n",
       "      <td>2.52720</td>\n",
       "      <td>0.65217</td>\n",
       "      <td>0.30093</td>\n",
       "      <td>3.86440</td>\n",
       "      <td>-3.24370</td>\n",
       "      <td>1.444300</td>\n",
       "      <td>1.98240</td>\n",
       "      <td>0.44735</td>\n",
       "      <td>2.016600</td>\n",
       "      <td>-2.24120</td>\n",
       "      <td>-2.563000</td>\n",
       "      <td>0.23768</td>\n",
       "      <td>-4.52500</td>\n",
       "      <td>-3.96800</td>\n",
       "      <td>-1.727100</td>\n",
       "      <td>1.855300</td>\n",
       "      <td>0.080492</td>\n",
       "      <td>-5.815800</td>\n",
       "      <td>-0.24918</td>\n",
       "      <td>-0.46305</td>\n",
       "      <td>-5.298600</td>\n",
       "      <td>-0.12286</td>\n",
       "      <td>-0.44774</td>\n",
       "      <td>2.08160</td>\n",
       "      <td>0.98389</td>\n",
       "      <td>0.26566</td>\n",
       "      <td>1.349700</td>\n",
       "      <td>-3.135400</td>\n",
       "      <td>2.14610</td>\n",
       "      <td>-1.67230</td>\n",
       "      <td>4.5805</td>\n",
       "      <td>2.143900</td>\n",
       "      <td>-1.38470</td>\n",
       "      <td>-1.13120</td>\n",
       "      <td>2.56490</td>\n",
       "      <td>0.73630</td>\n",
       "      <td>-2.244500</td>\n",
       "      <td>0.61483</td>\n",
       "      <td>-0.17209</td>\n",
       "      <td>-0.582790</td>\n",
       "      <td>-0.41249</td>\n",
       "      <td>1.38020</td>\n",
       "      <td>-1.5483</td>\n",
       "      <td>4.379500</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>boastful</th>\n",
       "      <td>-0.64374</td>\n",
       "      <td>0.29787</td>\n",
       "      <td>0.752070</td>\n",
       "      <td>-1.06370</td>\n",
       "      <td>0.55945</td>\n",
       "      <td>1.679200</td>\n",
       "      <td>-0.100350</td>\n",
       "      <td>0.82323</td>\n",
       "      <td>-0.7660</td>\n",
       "      <td>-0.412910</td>\n",
       "      <td>1.1909</td>\n",
       "      <td>-1.431300</td>\n",
       "      <td>-0.4628</td>\n",
       "      <td>-1.92730</td>\n",
       "      <td>1.663300</td>\n",
       "      <td>1.03090</td>\n",
       "      <td>-1.780400</td>\n",
       "      <td>-0.96916</td>\n",
       "      <td>1.59300</td>\n",
       "      <td>1.90480</td>\n",
       "      <td>0.98672</td>\n",
       "      <td>1.198000</td>\n",
       "      <td>-1.703500</td>\n",
       "      <td>0.046899</td>\n",
       "      <td>0.68360</td>\n",
       "      <td>-0.44087</td>\n",
       "      <td>-0.39414</td>\n",
       "      <td>-1.11860</td>\n",
       "      <td>0.91179</td>\n",
       "      <td>0.36917</td>\n",
       "      <td>0.19616</td>\n",
       "      <td>-0.914720</td>\n",
       "      <td>0.67642</td>\n",
       "      <td>-2.11920</td>\n",
       "      <td>0.194090</td>\n",
       "      <td>2.15560</td>\n",
       "      <td>-0.38760</td>\n",
       "      <td>2.25630</td>\n",
       "      <td>2.501400</td>\n",
       "      <td>1.04100</td>\n",
       "      <td>-1.23900</td>\n",
       "      <td>1.06510</td>\n",
       "      <td>-0.49369</td>\n",
       "      <td>-1.96000</td>\n",
       "      <td>-1.86890</td>\n",
       "      <td>-1.13320</td>\n",
       "      <td>-1.904200</td>\n",
       "      <td>-0.921120</td>\n",
       "      <td>0.060186</td>\n",
       "      <td>2.14350</td>\n",
       "      <td>1.21580</td>\n",
       "      <td>0.025998</td>\n",
       "      <td>-0.016047</td>\n",
       "      <td>-0.60961</td>\n",
       "      <td>1.16490</td>\n",
       "      <td>-1.35660</td>\n",
       "      <td>2.03010</td>\n",
       "      <td>-0.085189</td>\n",
       "      <td>-0.53251</td>\n",
       "      <td>0.15678</td>\n",
       "      <td>-0.085620</td>\n",
       "      <td>0.29298</td>\n",
       "      <td>1.04070</td>\n",
       "      <td>-1.63120</td>\n",
       "      <td>-0.68777</td>\n",
       "      <td>0.39717</td>\n",
       "      <td>0.19293</td>\n",
       "      <td>-0.2917</td>\n",
       "      <td>1.4363</td>\n",
       "      <td>-1.32810</td>\n",
       "      <td>-0.058618</td>\n",
       "      <td>0.488930</td>\n",
       "      <td>-0.52708</td>\n",
       "      <td>0.484440</td>\n",
       "      <td>0.719880</td>\n",
       "      <td>-1.36150</td>\n",
       "      <td>0.72896</td>\n",
       "      <td>2.880800</td>\n",
       "      <td>1.27880</td>\n",
       "      <td>2.461900</td>\n",
       "      <td>-1.50350</td>\n",
       "      <td>1.42860</td>\n",
       "      <td>0.070308</td>\n",
       "      <td>-2.62890</td>\n",
       "      <td>-0.86384</td>\n",
       "      <td>-0.14705</td>\n",
       "      <td>-0.24122</td>\n",
       "      <td>0.22958</td>\n",
       "      <td>-1.27410</td>\n",
       "      <td>-0.068621</td>\n",
       "      <td>-0.431590</td>\n",
       "      <td>-1.12390</td>\n",
       "      <td>1.130700</td>\n",
       "      <td>-1.35440</td>\n",
       "      <td>0.37984</td>\n",
       "      <td>-1.23670</td>\n",
       "      <td>1.050800</td>\n",
       "      <td>-1.162400</td>\n",
       "      <td>-0.75523</td>\n",
       "      <td>-2.17010</td>\n",
       "      <td>-1.78150</td>\n",
       "      <td>0.4644</td>\n",
       "      <td>0.347620</td>\n",
       "      <td>3.311200</td>\n",
       "      <td>-0.54717</td>\n",
       "      <td>-1.08290</td>\n",
       "      <td>-0.38567</td>\n",
       "      <td>-2.12180</td>\n",
       "      <td>-1.35770</td>\n",
       "      <td>1.21880</td>\n",
       "      <td>0.017559</td>\n",
       "      <td>0.382600</td>\n",
       "      <td>-1.70140</td>\n",
       "      <td>0.025059</td>\n",
       "      <td>-0.96561</td>\n",
       "      <td>-0.18866</td>\n",
       "      <td>-0.36405</td>\n",
       "      <td>0.890350</td>\n",
       "      <td>-2.07950</td>\n",
       "      <td>0.90127</td>\n",
       "      <td>1.66280</td>\n",
       "      <td>-1.68040</td>\n",
       "      <td>1.05420</td>\n",
       "      <td>1.71250</td>\n",
       "      <td>0.26156</td>\n",
       "      <td>0.078374</td>\n",
       "      <td>-0.40963</td>\n",
       "      <td>-0.77176</td>\n",
       "      <td>1.478600</td>\n",
       "      <td>-0.13770</td>\n",
       "      <td>0.69175</td>\n",
       "      <td>-1.080100</td>\n",
       "      <td>1.33990</td>\n",
       "      <td>-0.31152</td>\n",
       "      <td>0.05605</td>\n",
       "      <td>1.59190</td>\n",
       "      <td>-1.36860</td>\n",
       "      <td>0.015767</td>\n",
       "      <td>0.77700</td>\n",
       "      <td>0.58118</td>\n",
       "      <td>0.86163</td>\n",
       "      <td>-0.57362</td>\n",
       "      <td>1.91570</td>\n",
       "      <td>1.23490</td>\n",
       "      <td>-2.0593</td>\n",
       "      <td>-0.85008</td>\n",
       "      <td>-0.43776</td>\n",
       "      <td>2.751200</td>\n",
       "      <td>0.930160</td>\n",
       "      <td>1.13530</td>\n",
       "      <td>1.55300</td>\n",
       "      <td>-0.42481</td>\n",
       "      <td>1.610000</td>\n",
       "      <td>1.23660</td>\n",
       "      <td>-1.30670</td>\n",
       "      <td>1.38260</td>\n",
       "      <td>1.15300</td>\n",
       "      <td>1.06490</td>\n",
       "      <td>-0.62674</td>\n",
       "      <td>1.07510</td>\n",
       "      <td>1.71940</td>\n",
       "      <td>-1.512200</td>\n",
       "      <td>-1.47450</td>\n",
       "      <td>0.404720</td>\n",
       "      <td>-0.078352</td>\n",
       "      <td>0.44541</td>\n",
       "      <td>0.80621</td>\n",
       "      <td>-1.86190</td>\n",
       "      <td>-0.28317</td>\n",
       "      <td>-0.287990</td>\n",
       "      <td>2.37780</td>\n",
       "      <td>0.44314</td>\n",
       "      <td>1.103500</td>\n",
       "      <td>-2.771800</td>\n",
       "      <td>-0.049913</td>\n",
       "      <td>1.009000</td>\n",
       "      <td>0.478840</td>\n",
       "      <td>2.13450</td>\n",
       "      <td>0.99308</td>\n",
       "      <td>-1.66640</td>\n",
       "      <td>1.383300</td>\n",
       "      <td>0.99228</td>\n",
       "      <td>1.332200</td>\n",
       "      <td>-0.41434</td>\n",
       "      <td>0.19128</td>\n",
       "      <td>1.25520</td>\n",
       "      <td>-0.063287</td>\n",
       "      <td>-0.55261</td>\n",
       "      <td>-0.88812</td>\n",
       "      <td>0.94904</td>\n",
       "      <td>0.53548</td>\n",
       "      <td>0.72654</td>\n",
       "      <td>-0.591190</td>\n",
       "      <td>-0.31803</td>\n",
       "      <td>-0.54894</td>\n",
       "      <td>0.336040</td>\n",
       "      <td>-1.19460</td>\n",
       "      <td>-1.47730</td>\n",
       "      <td>0.79351</td>\n",
       "      <td>0.70217</td>\n",
       "      <td>2.155500</td>\n",
       "      <td>1.53310</td>\n",
       "      <td>-0.527130</td>\n",
       "      <td>1.93340</td>\n",
       "      <td>-1.48970</td>\n",
       "      <td>0.862440</td>\n",
       "      <td>0.719120</td>\n",
       "      <td>-1.126200</td>\n",
       "      <td>1.72440</td>\n",
       "      <td>0.056383</td>\n",
       "      <td>-0.67042</td>\n",
       "      <td>0.850890</td>\n",
       "      <td>0.940950</td>\n",
       "      <td>-1.0217</td>\n",
       "      <td>0.284350</td>\n",
       "      <td>2.21610</td>\n",
       "      <td>0.687610</td>\n",
       "      <td>0.22706</td>\n",
       "      <td>-2.16310</td>\n",
       "      <td>-0.071359</td>\n",
       "      <td>0.83324</td>\n",
       "      <td>-0.35323</td>\n",
       "      <td>1.447700</td>\n",
       "      <td>1.027600</td>\n",
       "      <td>-0.91213</td>\n",
       "      <td>2.09180</td>\n",
       "      <td>0.67396</td>\n",
       "      <td>-0.42748</td>\n",
       "      <td>2.28300</td>\n",
       "      <td>-0.64679</td>\n",
       "      <td>-0.032428</td>\n",
       "      <td>0.90743</td>\n",
       "      <td>-0.843530</td>\n",
       "      <td>-0.116310</td>\n",
       "      <td>-1.33620</td>\n",
       "      <td>0.537520</td>\n",
       "      <td>-0.11961</td>\n",
       "      <td>-1.33170</td>\n",
       "      <td>-0.52374</td>\n",
       "      <td>1.82830</td>\n",
       "      <td>1.73760</td>\n",
       "      <td>0.20633</td>\n",
       "      <td>0.637220</td>\n",
       "      <td>-0.265840</td>\n",
       "      <td>-0.021253</td>\n",
       "      <td>-2.348400</td>\n",
       "      <td>0.100370</td>\n",
       "      <td>-2.786100</td>\n",
       "      <td>-1.044600</td>\n",
       "      <td>1.294900</td>\n",
       "      <td>0.41091</td>\n",
       "      <td>1.18700</td>\n",
       "      <td>-0.98475</td>\n",
       "      <td>0.94293</td>\n",
       "      <td>-0.64350</td>\n",
       "      <td>-1.56340</td>\n",
       "      <td>-0.13868</td>\n",
       "      <td>1.93950</td>\n",
       "      <td>-0.79538</td>\n",
       "      <td>0.12822</td>\n",
       "      <td>0.77388</td>\n",
       "      <td>-2.523100</td>\n",
       "      <td>-0.63884</td>\n",
       "      <td>0.28427</td>\n",
       "      <td>0.559130</td>\n",
       "      <td>2.07090</td>\n",
       "      <td>0.056340</td>\n",
       "      <td>-0.53789</td>\n",
       "      <td>-1.09510</td>\n",
       "      <td>-1.97810</td>\n",
       "      <td>1.981200</td>\n",
       "      <td>-0.296360</td>\n",
       "      <td>-0.925080</td>\n",
       "      <td>-0.258420</td>\n",
       "      <td>-0.82158</td>\n",
       "      <td>0.10705</td>\n",
       "      <td>1.488300</td>\n",
       "      <td>-1.37520</td>\n",
       "      <td>0.45689</td>\n",
       "      <td>0.81544</td>\n",
       "      <td>1.25860</td>\n",
       "      <td>-0.75812</td>\n",
       "      <td>-0.098145</td>\n",
       "      <td>-1.418200</td>\n",
       "      <td>-0.61353</td>\n",
       "      <td>-1.06770</td>\n",
       "      <td>1.9770</td>\n",
       "      <td>-1.299600</td>\n",
       "      <td>1.99420</td>\n",
       "      <td>1.65910</td>\n",
       "      <td>0.21427</td>\n",
       "      <td>0.26348</td>\n",
       "      <td>0.001225</td>\n",
       "      <td>-0.93475</td>\n",
       "      <td>1.77620</td>\n",
       "      <td>-0.303800</td>\n",
       "      <td>-1.63480</td>\n",
       "      <td>2.33610</td>\n",
       "      <td>-2.6880</td>\n",
       "      <td>1.166900</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>phrase</th>\n",
       "      <td>1.90980</td>\n",
       "      <td>1.81190</td>\n",
       "      <td>6.846300</td>\n",
       "      <td>-1.89180</td>\n",
       "      <td>1.64390</td>\n",
       "      <td>0.012368</td>\n",
       "      <td>0.868690</td>\n",
       "      <td>-2.57400</td>\n",
       "      <td>-5.9958</td>\n",
       "      <td>0.056522</td>\n",
       "      <td>5.4924</td>\n",
       "      <td>-0.026202</td>\n",
       "      <td>-1.3393</td>\n",
       "      <td>-0.26483</td>\n",
       "      <td>-0.080494</td>\n",
       "      <td>-0.37879</td>\n",
       "      <td>0.413080</td>\n",
       "      <td>1.04440</td>\n",
       "      <td>1.62010</td>\n",
       "      <td>4.09840</td>\n",
       "      <td>-1.63390</td>\n",
       "      <td>-0.351710</td>\n",
       "      <td>-0.098921</td>\n",
       "      <td>0.248200</td>\n",
       "      <td>-1.31840</td>\n",
       "      <td>-0.52174</td>\n",
       "      <td>0.27486</td>\n",
       "      <td>-0.88031</td>\n",
       "      <td>-1.33450</td>\n",
       "      <td>2.28100</td>\n",
       "      <td>-2.73120</td>\n",
       "      <td>1.394900</td>\n",
       "      <td>-3.54490</td>\n",
       "      <td>0.40907</td>\n",
       "      <td>0.018951</td>\n",
       "      <td>-3.00710</td>\n",
       "      <td>-0.70820</td>\n",
       "      <td>-0.55946</td>\n",
       "      <td>3.119700</td>\n",
       "      <td>2.09890</td>\n",
       "      <td>-1.56880</td>\n",
       "      <td>-1.14060</td>\n",
       "      <td>-0.30611</td>\n",
       "      <td>0.75219</td>\n",
       "      <td>-2.91770</td>\n",
       "      <td>1.78770</td>\n",
       "      <td>-0.718460</td>\n",
       "      <td>-2.479500</td>\n",
       "      <td>0.780140</td>\n",
       "      <td>-2.04310</td>\n",
       "      <td>-4.63470</td>\n",
       "      <td>-1.284300</td>\n",
       "      <td>3.730000</td>\n",
       "      <td>0.48098</td>\n",
       "      <td>-1.93570</td>\n",
       "      <td>1.64160</td>\n",
       "      <td>3.35750</td>\n",
       "      <td>-0.547100</td>\n",
       "      <td>0.62148</td>\n",
       "      <td>-0.19466</td>\n",
       "      <td>3.355800</td>\n",
       "      <td>-0.55743</td>\n",
       "      <td>1.98030</td>\n",
       "      <td>-0.34399</td>\n",
       "      <td>-1.33710</td>\n",
       "      <td>1.75380</td>\n",
       "      <td>-1.01500</td>\n",
       "      <td>-6.5020</td>\n",
       "      <td>-3.3270</td>\n",
       "      <td>3.51330</td>\n",
       "      <td>-0.466630</td>\n",
       "      <td>1.281600</td>\n",
       "      <td>0.93278</td>\n",
       "      <td>-0.601670</td>\n",
       "      <td>0.053144</td>\n",
       "      <td>-0.18861</td>\n",
       "      <td>-3.70440</td>\n",
       "      <td>0.079878</td>\n",
       "      <td>-4.38510</td>\n",
       "      <td>0.383640</td>\n",
       "      <td>-2.14290</td>\n",
       "      <td>-0.42491</td>\n",
       "      <td>1.200300</td>\n",
       "      <td>1.63080</td>\n",
       "      <td>2.36380</td>\n",
       "      <td>-1.36050</td>\n",
       "      <td>-1.29360</td>\n",
       "      <td>2.83080</td>\n",
       "      <td>-0.53375</td>\n",
       "      <td>-0.056111</td>\n",
       "      <td>2.731700</td>\n",
       "      <td>1.29990</td>\n",
       "      <td>4.334000</td>\n",
       "      <td>-1.61420</td>\n",
       "      <td>0.23669</td>\n",
       "      <td>-3.10660</td>\n",
       "      <td>-1.626800</td>\n",
       "      <td>0.305590</td>\n",
       "      <td>-1.30930</td>\n",
       "      <td>-3.74580</td>\n",
       "      <td>-0.78939</td>\n",
       "      <td>-1.3251</td>\n",
       "      <td>2.849700</td>\n",
       "      <td>0.355450</td>\n",
       "      <td>-1.26800</td>\n",
       "      <td>2.56390</td>\n",
       "      <td>4.09510</td>\n",
       "      <td>-1.72470</td>\n",
       "      <td>-1.16550</td>\n",
       "      <td>-2.16040</td>\n",
       "      <td>0.336930</td>\n",
       "      <td>-2.044500</td>\n",
       "      <td>0.53786</td>\n",
       "      <td>0.822130</td>\n",
       "      <td>-0.61540</td>\n",
       "      <td>0.67781</td>\n",
       "      <td>-2.93170</td>\n",
       "      <td>-2.199100</td>\n",
       "      <td>-1.34660</td>\n",
       "      <td>-0.43242</td>\n",
       "      <td>-0.75780</td>\n",
       "      <td>-1.10700</td>\n",
       "      <td>-0.36677</td>\n",
       "      <td>3.88030</td>\n",
       "      <td>-1.26090</td>\n",
       "      <td>0.504550</td>\n",
       "      <td>0.19001</td>\n",
       "      <td>-4.27850</td>\n",
       "      <td>0.128850</td>\n",
       "      <td>-0.21011</td>\n",
       "      <td>-3.12990</td>\n",
       "      <td>-1.267000</td>\n",
       "      <td>-0.60806</td>\n",
       "      <td>-0.26853</td>\n",
       "      <td>1.03280</td>\n",
       "      <td>1.05090</td>\n",
       "      <td>-2.54130</td>\n",
       "      <td>-0.534860</td>\n",
       "      <td>1.18150</td>\n",
       "      <td>-2.62710</td>\n",
       "      <td>-2.42250</td>\n",
       "      <td>-1.00900</td>\n",
       "      <td>2.22810</td>\n",
       "      <td>-1.31130</td>\n",
       "      <td>-3.2605</td>\n",
       "      <td>-2.17160</td>\n",
       "      <td>-1.02320</td>\n",
       "      <td>2.562200</td>\n",
       "      <td>-3.325600</td>\n",
       "      <td>2.60640</td>\n",
       "      <td>-1.87190</td>\n",
       "      <td>5.30890</td>\n",
       "      <td>-0.052055</td>\n",
       "      <td>0.11815</td>\n",
       "      <td>-1.14210</td>\n",
       "      <td>1.76300</td>\n",
       "      <td>1.70210</td>\n",
       "      <td>3.74770</td>\n",
       "      <td>-0.69020</td>\n",
       "      <td>0.21864</td>\n",
       "      <td>4.16030</td>\n",
       "      <td>2.264200</td>\n",
       "      <td>-0.31813</td>\n",
       "      <td>-0.927690</td>\n",
       "      <td>0.521580</td>\n",
       "      <td>-6.85050</td>\n",
       "      <td>-1.61680</td>\n",
       "      <td>0.42942</td>\n",
       "      <td>-0.91831</td>\n",
       "      <td>-1.051700</td>\n",
       "      <td>1.92290</td>\n",
       "      <td>0.28125</td>\n",
       "      <td>0.400890</td>\n",
       "      <td>1.731100</td>\n",
       "      <td>1.923600</td>\n",
       "      <td>-1.762200</td>\n",
       "      <td>0.405290</td>\n",
       "      <td>0.95263</td>\n",
       "      <td>0.31829</td>\n",
       "      <td>-1.47090</td>\n",
       "      <td>-0.041693</td>\n",
       "      <td>-3.10430</td>\n",
       "      <td>0.202310</td>\n",
       "      <td>0.85276</td>\n",
       "      <td>2.33350</td>\n",
       "      <td>0.73921</td>\n",
       "      <td>-0.236930</td>\n",
       "      <td>-5.81200</td>\n",
       "      <td>1.89110</td>\n",
       "      <td>-1.25280</td>\n",
       "      <td>-5.35320</td>\n",
       "      <td>1.55580</td>\n",
       "      <td>0.719910</td>\n",
       "      <td>3.12340</td>\n",
       "      <td>3.28570</td>\n",
       "      <td>-1.066500</td>\n",
       "      <td>0.83074</td>\n",
       "      <td>1.02060</td>\n",
       "      <td>1.63980</td>\n",
       "      <td>5.82020</td>\n",
       "      <td>-2.373900</td>\n",
       "      <td>0.52736</td>\n",
       "      <td>-0.510420</td>\n",
       "      <td>-1.12720</td>\n",
       "      <td>-2.52650</td>\n",
       "      <td>-0.267610</td>\n",
       "      <td>-1.830900</td>\n",
       "      <td>-4.061800</td>\n",
       "      <td>2.22760</td>\n",
       "      <td>-0.065473</td>\n",
       "      <td>0.74785</td>\n",
       "      <td>0.021271</td>\n",
       "      <td>1.055400</td>\n",
       "      <td>3.4777</td>\n",
       "      <td>2.177300</td>\n",
       "      <td>0.59564</td>\n",
       "      <td>0.656110</td>\n",
       "      <td>0.19841</td>\n",
       "      <td>-0.64718</td>\n",
       "      <td>0.542730</td>\n",
       "      <td>-0.95929</td>\n",
       "      <td>-1.33110</td>\n",
       "      <td>2.861200</td>\n",
       "      <td>0.597610</td>\n",
       "      <td>-7.59000</td>\n",
       "      <td>-0.42122</td>\n",
       "      <td>0.43525</td>\n",
       "      <td>1.47540</td>\n",
       "      <td>6.00280</td>\n",
       "      <td>-2.77720</td>\n",
       "      <td>-3.559700</td>\n",
       "      <td>-0.50006</td>\n",
       "      <td>0.012587</td>\n",
       "      <td>-2.432700</td>\n",
       "      <td>-2.01460</td>\n",
       "      <td>1.121800</td>\n",
       "      <td>0.51161</td>\n",
       "      <td>3.56260</td>\n",
       "      <td>0.88015</td>\n",
       "      <td>-1.91900</td>\n",
       "      <td>0.13394</td>\n",
       "      <td>1.88700</td>\n",
       "      <td>2.628200</td>\n",
       "      <td>0.038495</td>\n",
       "      <td>1.245400</td>\n",
       "      <td>-1.296500</td>\n",
       "      <td>0.219900</td>\n",
       "      <td>1.896600</td>\n",
       "      <td>-1.949800</td>\n",
       "      <td>-0.044596</td>\n",
       "      <td>1.43860</td>\n",
       "      <td>-6.38570</td>\n",
       "      <td>-1.10110</td>\n",
       "      <td>0.30281</td>\n",
       "      <td>-2.01700</td>\n",
       "      <td>-1.41300</td>\n",
       "      <td>-2.00630</td>\n",
       "      <td>2.39210</td>\n",
       "      <td>1.04490</td>\n",
       "      <td>-0.59200</td>\n",
       "      <td>2.25480</td>\n",
       "      <td>2.089400</td>\n",
       "      <td>-1.03140</td>\n",
       "      <td>-0.71747</td>\n",
       "      <td>0.784110</td>\n",
       "      <td>2.56510</td>\n",
       "      <td>-2.553000</td>\n",
       "      <td>2.13470</td>\n",
       "      <td>-0.66752</td>\n",
       "      <td>-2.84100</td>\n",
       "      <td>0.353430</td>\n",
       "      <td>-0.947280</td>\n",
       "      <td>-1.459200</td>\n",
       "      <td>1.205400</td>\n",
       "      <td>0.57659</td>\n",
       "      <td>1.34080</td>\n",
       "      <td>0.237130</td>\n",
       "      <td>2.29840</td>\n",
       "      <td>-2.11830</td>\n",
       "      <td>2.09240</td>\n",
       "      <td>-2.81410</td>\n",
       "      <td>-2.15360</td>\n",
       "      <td>0.256310</td>\n",
       "      <td>4.198000</td>\n",
       "      <td>3.84430</td>\n",
       "      <td>0.26602</td>\n",
       "      <td>1.9790</td>\n",
       "      <td>-2.296000</td>\n",
       "      <td>1.64780</td>\n",
       "      <td>1.90050</td>\n",
       "      <td>4.49740</td>\n",
       "      <td>-1.89430</td>\n",
       "      <td>4.200100</td>\n",
       "      <td>-1.14030</td>\n",
       "      <td>1.27300</td>\n",
       "      <td>1.737200</td>\n",
       "      <td>4.91640</td>\n",
       "      <td>-0.50940</td>\n",
       "      <td>-1.2522</td>\n",
       "      <td>0.074529</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>ski</th>\n",
       "      <td>2.98080</td>\n",
       "      <td>10.30700</td>\n",
       "      <td>-0.143810</td>\n",
       "      <td>-8.48640</td>\n",
       "      <td>10.91400</td>\n",
       "      <td>7.630000</td>\n",
       "      <td>-4.985500</td>\n",
       "      <td>8.12060</td>\n",
       "      <td>6.3504</td>\n",
       "      <td>-2.288800</td>\n",
       "      <td>-2.0521</td>\n",
       "      <td>-1.112400</td>\n",
       "      <td>-4.3927</td>\n",
       "      <td>1.92840</td>\n",
       "      <td>4.367200</td>\n",
       "      <td>-1.61950</td>\n",
       "      <td>-5.694200</td>\n",
       "      <td>-2.21270</td>\n",
       "      <td>-1.13000</td>\n",
       "      <td>-1.02360</td>\n",
       "      <td>1.56940</td>\n",
       "      <td>4.118700</td>\n",
       "      <td>4.194100</td>\n",
       "      <td>-1.142700</td>\n",
       "      <td>1.77080</td>\n",
       "      <td>9.82530</td>\n",
       "      <td>-10.33500</td>\n",
       "      <td>-6.96280</td>\n",
       "      <td>2.19810</td>\n",
       "      <td>4.81370</td>\n",
       "      <td>0.28503</td>\n",
       "      <td>4.649800</td>\n",
       "      <td>3.68580</td>\n",
       "      <td>-0.35060</td>\n",
       "      <td>6.388500</td>\n",
       "      <td>3.50230</td>\n",
       "      <td>-5.53560</td>\n",
       "      <td>8.15740</td>\n",
       "      <td>-4.272200</td>\n",
       "      <td>-5.98230</td>\n",
       "      <td>-1.25010</td>\n",
       "      <td>4.85890</td>\n",
       "      <td>6.42290</td>\n",
       "      <td>-3.87160</td>\n",
       "      <td>1.63390</td>\n",
       "      <td>-8.21740</td>\n",
       "      <td>1.625900</td>\n",
       "      <td>1.687000</td>\n",
       "      <td>1.532100</td>\n",
       "      <td>2.44460</td>\n",
       "      <td>7.61380</td>\n",
       "      <td>4.230500</td>\n",
       "      <td>-5.355900</td>\n",
       "      <td>0.94129</td>\n",
       "      <td>9.83830</td>\n",
       "      <td>-6.51060</td>\n",
       "      <td>3.01970</td>\n",
       "      <td>-2.092200</td>\n",
       "      <td>-4.69150</td>\n",
       "      <td>1.67960</td>\n",
       "      <td>-4.093100</td>\n",
       "      <td>-2.41250</td>\n",
       "      <td>-4.20310</td>\n",
       "      <td>5.58470</td>\n",
       "      <td>1.49650</td>\n",
       "      <td>-1.26020</td>\n",
       "      <td>-3.07870</td>\n",
       "      <td>1.7105</td>\n",
       "      <td>-0.1317</td>\n",
       "      <td>2.12420</td>\n",
       "      <td>-7.859000</td>\n",
       "      <td>1.046200</td>\n",
       "      <td>-0.66114</td>\n",
       "      <td>1.696700</td>\n",
       "      <td>3.391400</td>\n",
       "      <td>1.76680</td>\n",
       "      <td>-0.38296</td>\n",
       "      <td>-1.759600</td>\n",
       "      <td>0.88219</td>\n",
       "      <td>-0.612040</td>\n",
       "      <td>-3.45490</td>\n",
       "      <td>3.69810</td>\n",
       "      <td>-0.395080</td>\n",
       "      <td>-10.42700</td>\n",
       "      <td>-7.01470</td>\n",
       "      <td>5.78440</td>\n",
       "      <td>7.51540</td>\n",
       "      <td>-1.27760</td>\n",
       "      <td>-2.08340</td>\n",
       "      <td>-3.076900</td>\n",
       "      <td>2.208900</td>\n",
       "      <td>-3.33630</td>\n",
       "      <td>-0.463400</td>\n",
       "      <td>2.08540</td>\n",
       "      <td>-1.97810</td>\n",
       "      <td>5.03610</td>\n",
       "      <td>1.160900</td>\n",
       "      <td>-0.015598</td>\n",
       "      <td>1.96290</td>\n",
       "      <td>2.28800</td>\n",
       "      <td>-4.67160</td>\n",
       "      <td>-3.1697</td>\n",
       "      <td>3.640300</td>\n",
       "      <td>7.733100</td>\n",
       "      <td>4.97370</td>\n",
       "      <td>-4.39850</td>\n",
       "      <td>1.94820</td>\n",
       "      <td>6.05260</td>\n",
       "      <td>-2.12980</td>\n",
       "      <td>-3.13140</td>\n",
       "      <td>-0.692390</td>\n",
       "      <td>-6.058100</td>\n",
       "      <td>5.81440</td>\n",
       "      <td>7.916000</td>\n",
       "      <td>-5.21250</td>\n",
       "      <td>3.58080</td>\n",
       "      <td>4.67770</td>\n",
       "      <td>0.077567</td>\n",
       "      <td>5.00710</td>\n",
       "      <td>-5.93610</td>\n",
       "      <td>-4.44720</td>\n",
       "      <td>4.03170</td>\n",
       "      <td>-10.68000</td>\n",
       "      <td>4.81280</td>\n",
       "      <td>2.60130</td>\n",
       "      <td>-3.431300</td>\n",
       "      <td>-4.52070</td>\n",
       "      <td>-7.01210</td>\n",
       "      <td>2.613500</td>\n",
       "      <td>-0.13274</td>\n",
       "      <td>5.24840</td>\n",
       "      <td>-1.757500</td>\n",
       "      <td>-2.01200</td>\n",
       "      <td>-0.49666</td>\n",
       "      <td>0.12080</td>\n",
       "      <td>-6.35580</td>\n",
       "      <td>-4.89980</td>\n",
       "      <td>7.957600</td>\n",
       "      <td>2.25840</td>\n",
       "      <td>-0.63749</td>\n",
       "      <td>1.43060</td>\n",
       "      <td>-4.46580</td>\n",
       "      <td>5.36180</td>\n",
       "      <td>-1.63170</td>\n",
       "      <td>1.9955</td>\n",
       "      <td>7.47590</td>\n",
       "      <td>5.54130</td>\n",
       "      <td>-1.362900</td>\n",
       "      <td>-9.061200</td>\n",
       "      <td>-0.61989</td>\n",
       "      <td>-0.15132</td>\n",
       "      <td>-2.07130</td>\n",
       "      <td>-1.543100</td>\n",
       "      <td>-4.65730</td>\n",
       "      <td>6.09780</td>\n",
       "      <td>-5.64670</td>\n",
       "      <td>6.54960</td>\n",
       "      <td>0.97285</td>\n",
       "      <td>12.12600</td>\n",
       "      <td>8.24390</td>\n",
       "      <td>-1.72780</td>\n",
       "      <td>0.652040</td>\n",
       "      <td>-1.28160</td>\n",
       "      <td>-2.946000</td>\n",
       "      <td>3.729700</td>\n",
       "      <td>5.10500</td>\n",
       "      <td>-6.33930</td>\n",
       "      <td>-7.97800</td>\n",
       "      <td>-4.73550</td>\n",
       "      <td>4.052400</td>\n",
       "      <td>-3.90740</td>\n",
       "      <td>3.48360</td>\n",
       "      <td>-2.297000</td>\n",
       "      <td>3.512600</td>\n",
       "      <td>0.438070</td>\n",
       "      <td>-0.035033</td>\n",
       "      <td>-1.504500</td>\n",
       "      <td>-7.21640</td>\n",
       "      <td>-6.59120</td>\n",
       "      <td>-5.87210</td>\n",
       "      <td>-2.539900</td>\n",
       "      <td>5.02010</td>\n",
       "      <td>-0.042889</td>\n",
       "      <td>-3.76320</td>\n",
       "      <td>-2.72320</td>\n",
       "      <td>5.64850</td>\n",
       "      <td>8.990300</td>\n",
       "      <td>-3.44530</td>\n",
       "      <td>-6.06020</td>\n",
       "      <td>5.10750</td>\n",
       "      <td>-6.03090</td>\n",
       "      <td>1.38820</td>\n",
       "      <td>-2.458200</td>\n",
       "      <td>-1.28910</td>\n",
       "      <td>7.89950</td>\n",
       "      <td>0.038937</td>\n",
       "      <td>-6.01760</td>\n",
       "      <td>0.81326</td>\n",
       "      <td>3.02260</td>\n",
       "      <td>4.93930</td>\n",
       "      <td>4.519300</td>\n",
       "      <td>4.36140</td>\n",
       "      <td>1.933000</td>\n",
       "      <td>-0.74458</td>\n",
       "      <td>-3.76650</td>\n",
       "      <td>6.008000</td>\n",
       "      <td>-8.702000</td>\n",
       "      <td>-0.308610</td>\n",
       "      <td>-2.62190</td>\n",
       "      <td>5.138400</td>\n",
       "      <td>-2.95640</td>\n",
       "      <td>0.722520</td>\n",
       "      <td>-0.258650</td>\n",
       "      <td>-6.8622</td>\n",
       "      <td>-2.153100</td>\n",
       "      <td>-2.97740</td>\n",
       "      <td>7.517200</td>\n",
       "      <td>-8.15670</td>\n",
       "      <td>-3.92100</td>\n",
       "      <td>-5.460900</td>\n",
       "      <td>-1.41160</td>\n",
       "      <td>3.01930</td>\n",
       "      <td>-2.237400</td>\n",
       "      <td>2.511600</td>\n",
       "      <td>0.95671</td>\n",
       "      <td>-2.63050</td>\n",
       "      <td>-0.47477</td>\n",
       "      <td>6.28210</td>\n",
       "      <td>-0.36173</td>\n",
       "      <td>5.43230</td>\n",
       "      <td>2.092000</td>\n",
       "      <td>-0.43824</td>\n",
       "      <td>-3.224900</td>\n",
       "      <td>3.899600</td>\n",
       "      <td>-5.35810</td>\n",
       "      <td>-2.186200</td>\n",
       "      <td>3.44090</td>\n",
       "      <td>1.61110</td>\n",
       "      <td>-3.38270</td>\n",
       "      <td>3.56410</td>\n",
       "      <td>-4.84760</td>\n",
       "      <td>-6.88430</td>\n",
       "      <td>0.788540</td>\n",
       "      <td>-1.404600</td>\n",
       "      <td>7.863400</td>\n",
       "      <td>-0.596120</td>\n",
       "      <td>-6.729600</td>\n",
       "      <td>0.520990</td>\n",
       "      <td>-6.615000</td>\n",
       "      <td>-1.442000</td>\n",
       "      <td>-2.20560</td>\n",
       "      <td>-2.90950</td>\n",
       "      <td>-11.04000</td>\n",
       "      <td>0.75927</td>\n",
       "      <td>-4.95950</td>\n",
       "      <td>0.59751</td>\n",
       "      <td>1.24690</td>\n",
       "      <td>-7.42060</td>\n",
       "      <td>0.25332</td>\n",
       "      <td>-2.18350</td>\n",
       "      <td>1.16500</td>\n",
       "      <td>0.524600</td>\n",
       "      <td>-5.02260</td>\n",
       "      <td>5.77140</td>\n",
       "      <td>4.121500</td>\n",
       "      <td>-5.36600</td>\n",
       "      <td>-4.513500</td>\n",
       "      <td>-2.37540</td>\n",
       "      <td>-5.18000</td>\n",
       "      <td>1.02000</td>\n",
       "      <td>-5.764900</td>\n",
       "      <td>-12.020000</td>\n",
       "      <td>5.292500</td>\n",
       "      <td>3.104200</td>\n",
       "      <td>-3.05580</td>\n",
       "      <td>-0.36082</td>\n",
       "      <td>-5.798400</td>\n",
       "      <td>1.85570</td>\n",
       "      <td>-4.78430</td>\n",
       "      <td>-3.44740</td>\n",
       "      <td>0.89605</td>\n",
       "      <td>5.59090</td>\n",
       "      <td>2.089300</td>\n",
       "      <td>-4.443100</td>\n",
       "      <td>-0.53930</td>\n",
       "      <td>5.38030</td>\n",
       "      <td>-5.3462</td>\n",
       "      <td>-1.469100</td>\n",
       "      <td>-1.57080</td>\n",
       "      <td>-4.85010</td>\n",
       "      <td>-2.82460</td>\n",
       "      <td>0.22498</td>\n",
       "      <td>4.944100</td>\n",
       "      <td>3.23380</td>\n",
       "      <td>-2.98880</td>\n",
       "      <td>4.551300</td>\n",
       "      <td>-0.99296</td>\n",
       "      <td>9.45230</td>\n",
       "      <td>1.8164</td>\n",
       "      <td>-1.226400</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>commanding</th>\n",
       "      <td>-1.59280</td>\n",
       "      <td>0.40658</td>\n",
       "      <td>-0.189960</td>\n",
       "      <td>2.13110</td>\n",
       "      <td>0.21254</td>\n",
       "      <td>1.550900</td>\n",
       "      <td>-0.638750</td>\n",
       "      <td>1.61440</td>\n",
       "      <td>1.5201</td>\n",
       "      <td>-0.984460</td>\n",
       "      <td>5.2634</td>\n",
       "      <td>3.197500</td>\n",
       "      <td>-3.0917</td>\n",
       "      <td>3.10210</td>\n",
       "      <td>2.844400</td>\n",
       "      <td>3.09070</td>\n",
       "      <td>1.542200</td>\n",
       "      <td>4.22190</td>\n",
       "      <td>1.07730</td>\n",
       "      <td>2.43810</td>\n",
       "      <td>0.14950</td>\n",
       "      <td>-1.125100</td>\n",
       "      <td>-1.458600</td>\n",
       "      <td>2.382200</td>\n",
       "      <td>4.00470</td>\n",
       "      <td>-1.18650</td>\n",
       "      <td>-3.33620</td>\n",
       "      <td>-2.35150</td>\n",
       "      <td>-0.59568</td>\n",
       "      <td>2.18910</td>\n",
       "      <td>1.11900</td>\n",
       "      <td>-0.700080</td>\n",
       "      <td>0.83824</td>\n",
       "      <td>-1.92810</td>\n",
       "      <td>-2.209300</td>\n",
       "      <td>-1.86570</td>\n",
       "      <td>-1.36610</td>\n",
       "      <td>4.33010</td>\n",
       "      <td>-0.309060</td>\n",
       "      <td>0.20639</td>\n",
       "      <td>-0.59435</td>\n",
       "      <td>-2.87530</td>\n",
       "      <td>0.13022</td>\n",
       "      <td>-0.22724</td>\n",
       "      <td>-1.46660</td>\n",
       "      <td>0.48309</td>\n",
       "      <td>-0.372330</td>\n",
       "      <td>-2.598800</td>\n",
       "      <td>1.920900</td>\n",
       "      <td>0.74662</td>\n",
       "      <td>-1.23260</td>\n",
       "      <td>2.295000</td>\n",
       "      <td>1.622400</td>\n",
       "      <td>-1.05030</td>\n",
       "      <td>-0.23773</td>\n",
       "      <td>-0.80576</td>\n",
       "      <td>-0.87834</td>\n",
       "      <td>-1.279600</td>\n",
       "      <td>-0.70184</td>\n",
       "      <td>-4.48980</td>\n",
       "      <td>-1.506000</td>\n",
       "      <td>-0.90765</td>\n",
       "      <td>-1.39330</td>\n",
       "      <td>-3.15470</td>\n",
       "      <td>2.24820</td>\n",
       "      <td>1.12010</td>\n",
       "      <td>-0.46624</td>\n",
       "      <td>-2.8329</td>\n",
       "      <td>3.2271</td>\n",
       "      <td>3.40410</td>\n",
       "      <td>1.167100</td>\n",
       "      <td>-1.385100</td>\n",
       "      <td>-1.37940</td>\n",
       "      <td>0.844130</td>\n",
       "      <td>-1.996100</td>\n",
       "      <td>-0.60021</td>\n",
       "      <td>-1.92280</td>\n",
       "      <td>3.672200</td>\n",
       "      <td>-1.75190</td>\n",
       "      <td>-0.522440</td>\n",
       "      <td>-6.07810</td>\n",
       "      <td>-3.39830</td>\n",
       "      <td>-3.574700</td>\n",
       "      <td>4.04230</td>\n",
       "      <td>0.58130</td>\n",
       "      <td>0.47143</td>\n",
       "      <td>-1.74080</td>\n",
       "      <td>-2.07660</td>\n",
       "      <td>-0.45620</td>\n",
       "      <td>0.036687</td>\n",
       "      <td>-0.096209</td>\n",
       "      <td>2.31830</td>\n",
       "      <td>0.964400</td>\n",
       "      <td>-1.87050</td>\n",
       "      <td>2.45880</td>\n",
       "      <td>-2.74540</td>\n",
       "      <td>-0.668050</td>\n",
       "      <td>-0.018221</td>\n",
       "      <td>0.80530</td>\n",
       "      <td>2.19470</td>\n",
       "      <td>4.11040</td>\n",
       "      <td>1.4484</td>\n",
       "      <td>3.247800</td>\n",
       "      <td>-0.239520</td>\n",
       "      <td>-2.79790</td>\n",
       "      <td>4.44860</td>\n",
       "      <td>1.40580</td>\n",
       "      <td>-0.48561</td>\n",
       "      <td>-0.94428</td>\n",
       "      <td>-3.23370</td>\n",
       "      <td>3.106900</td>\n",
       "      <td>-1.920500</td>\n",
       "      <td>-2.43690</td>\n",
       "      <td>0.218110</td>\n",
       "      <td>-0.52357</td>\n",
       "      <td>-1.18370</td>\n",
       "      <td>-2.55880</td>\n",
       "      <td>1.313200</td>\n",
       "      <td>-2.30070</td>\n",
       "      <td>-2.12190</td>\n",
       "      <td>0.16552</td>\n",
       "      <td>-5.12860</td>\n",
       "      <td>1.70800</td>\n",
       "      <td>0.66239</td>\n",
       "      <td>-0.20327</td>\n",
       "      <td>-4.396500</td>\n",
       "      <td>3.54050</td>\n",
       "      <td>-6.11900</td>\n",
       "      <td>5.231900</td>\n",
       "      <td>-1.51650</td>\n",
       "      <td>-4.28600</td>\n",
       "      <td>0.720430</td>\n",
       "      <td>5.99650</td>\n",
       "      <td>1.29710</td>\n",
       "      <td>-1.11650</td>\n",
       "      <td>-1.06120</td>\n",
       "      <td>-3.64180</td>\n",
       "      <td>1.048300</td>\n",
       "      <td>1.08230</td>\n",
       "      <td>-1.19370</td>\n",
       "      <td>-0.62115</td>\n",
       "      <td>-1.53380</td>\n",
       "      <td>-3.18110</td>\n",
       "      <td>4.78890</td>\n",
       "      <td>2.7876</td>\n",
       "      <td>-0.51639</td>\n",
       "      <td>-6.02690</td>\n",
       "      <td>0.220850</td>\n",
       "      <td>0.155940</td>\n",
       "      <td>1.73650</td>\n",
       "      <td>1.51050</td>\n",
       "      <td>-1.56630</td>\n",
       "      <td>-0.717120</td>\n",
       "      <td>4.93590</td>\n",
       "      <td>2.39010</td>\n",
       "      <td>-0.85229</td>\n",
       "      <td>2.63470</td>\n",
       "      <td>-1.39630</td>\n",
       "      <td>-2.79040</td>\n",
       "      <td>-4.00330</td>\n",
       "      <td>-1.63610</td>\n",
       "      <td>-3.611900</td>\n",
       "      <td>0.90931</td>\n",
       "      <td>1.178500</td>\n",
       "      <td>1.287600</td>\n",
       "      <td>-1.87920</td>\n",
       "      <td>-3.88590</td>\n",
       "      <td>1.24370</td>\n",
       "      <td>1.20270</td>\n",
       "      <td>-0.611850</td>\n",
       "      <td>0.41406</td>\n",
       "      <td>-1.10760</td>\n",
       "      <td>0.783070</td>\n",
       "      <td>-2.311700</td>\n",
       "      <td>1.582000</td>\n",
       "      <td>3.407900</td>\n",
       "      <td>2.433000</td>\n",
       "      <td>-0.45974</td>\n",
       "      <td>-3.17050</td>\n",
       "      <td>-1.65090</td>\n",
       "      <td>-0.274040</td>\n",
       "      <td>-0.79963</td>\n",
       "      <td>0.257710</td>\n",
       "      <td>0.02978</td>\n",
       "      <td>0.91945</td>\n",
       "      <td>3.44640</td>\n",
       "      <td>-1.803700</td>\n",
       "      <td>-0.34887</td>\n",
       "      <td>-2.67680</td>\n",
       "      <td>-0.26284</td>\n",
       "      <td>5.61010</td>\n",
       "      <td>-2.15700</td>\n",
       "      <td>-0.058571</td>\n",
       "      <td>0.73875</td>\n",
       "      <td>-0.67325</td>\n",
       "      <td>0.424640</td>\n",
       "      <td>-3.84930</td>\n",
       "      <td>-1.56960</td>\n",
       "      <td>1.76160</td>\n",
       "      <td>1.19630</td>\n",
       "      <td>0.300910</td>\n",
       "      <td>0.25813</td>\n",
       "      <td>-1.880400</td>\n",
       "      <td>-3.58870</td>\n",
       "      <td>3.57120</td>\n",
       "      <td>0.125750</td>\n",
       "      <td>-0.184290</td>\n",
       "      <td>0.049307</td>\n",
       "      <td>0.93837</td>\n",
       "      <td>5.039600</td>\n",
       "      <td>-1.58540</td>\n",
       "      <td>1.545600</td>\n",
       "      <td>-3.065900</td>\n",
       "      <td>2.1012</td>\n",
       "      <td>-0.771930</td>\n",
       "      <td>1.25030</td>\n",
       "      <td>2.831400</td>\n",
       "      <td>-1.03990</td>\n",
       "      <td>2.53140</td>\n",
       "      <td>-0.194810</td>\n",
       "      <td>-3.07620</td>\n",
       "      <td>4.52930</td>\n",
       "      <td>-0.373110</td>\n",
       "      <td>0.698800</td>\n",
       "      <td>-0.59152</td>\n",
       "      <td>0.29593</td>\n",
       "      <td>-1.04760</td>\n",
       "      <td>2.82370</td>\n",
       "      <td>-0.22383</td>\n",
       "      <td>-2.87970</td>\n",
       "      <td>4.733000</td>\n",
       "      <td>-1.30650</td>\n",
       "      <td>-1.127100</td>\n",
       "      <td>2.048100</td>\n",
       "      <td>2.93710</td>\n",
       "      <td>1.182000</td>\n",
       "      <td>-1.87960</td>\n",
       "      <td>-0.86135</td>\n",
       "      <td>-3.23100</td>\n",
       "      <td>0.83011</td>\n",
       "      <td>3.48760</td>\n",
       "      <td>-0.15529</td>\n",
       "      <td>-0.087641</td>\n",
       "      <td>-1.196500</td>\n",
       "      <td>2.856600</td>\n",
       "      <td>-0.611970</td>\n",
       "      <td>0.116750</td>\n",
       "      <td>1.377800</td>\n",
       "      <td>0.001078</td>\n",
       "      <td>2.999300</td>\n",
       "      <td>2.12430</td>\n",
       "      <td>-4.27170</td>\n",
       "      <td>-4.73210</td>\n",
       "      <td>-1.81830</td>\n",
       "      <td>-1.43200</td>\n",
       "      <td>-4.31060</td>\n",
       "      <td>1.60090</td>\n",
       "      <td>-2.41790</td>\n",
       "      <td>-1.12020</td>\n",
       "      <td>1.20090</td>\n",
       "      <td>1.68740</td>\n",
       "      <td>3.998700</td>\n",
       "      <td>3.57460</td>\n",
       "      <td>2.54020</td>\n",
       "      <td>2.950300</td>\n",
       "      <td>-1.89720</td>\n",
       "      <td>0.398170</td>\n",
       "      <td>0.82580</td>\n",
       "      <td>-0.27822</td>\n",
       "      <td>1.07930</td>\n",
       "      <td>1.480500</td>\n",
       "      <td>0.226500</td>\n",
       "      <td>0.024602</td>\n",
       "      <td>-0.032531</td>\n",
       "      <td>-0.39685</td>\n",
       "      <td>-1.13450</td>\n",
       "      <td>1.074500</td>\n",
       "      <td>-5.53470</td>\n",
       "      <td>2.25110</td>\n",
       "      <td>4.91880</td>\n",
       "      <td>3.64720</td>\n",
       "      <td>0.13404</td>\n",
       "      <td>1.913200</td>\n",
       "      <td>1.327300</td>\n",
       "      <td>2.81720</td>\n",
       "      <td>-0.56290</td>\n",
       "      <td>-1.1239</td>\n",
       "      <td>0.068115</td>\n",
       "      <td>-3.13650</td>\n",
       "      <td>0.68217</td>\n",
       "      <td>1.08590</td>\n",
       "      <td>2.16180</td>\n",
       "      <td>-2.638700</td>\n",
       "      <td>1.62060</td>\n",
       "      <td>-1.91880</td>\n",
       "      <td>2.043900</td>\n",
       "      <td>2.12860</td>\n",
       "      <td>-3.98320</td>\n",
       "      <td>-2.3288</td>\n",
       "      <td>2.544700</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>3511 rows × 300 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                   0         1         2        3         4         5         6        7       8         9       10        11      12       13        14       15        16       17       18       19       20        21        22        23       24       25        26       27       28       29       30        31       32       33        34       35       36       37        38       39       40       41       42       43       44       45        46        47        48       49       50        51        52       53       54       55       56        57       58       59        60       61       62       63       64       65       66      67      68       69        70        71       72        73        74       75       76        77       78        79       80       81        82        83       84       85       86       87       88        89        90       91        92       93       94       95        96        97       98       99       100     101       102       103      104      105      106      107      108      109       110       111      112       113      114      115      116       117      118      119      120      121       122      123      124       125      126      127       128      129      130       131      132      133      134      135      136       137      138      139      140      141      142      143     144      145      146       147       148      149      150      151       152      153      154      155      156      157       158      159      160       161      162       163       164      165      166      167      168       169      170      171       172       173       174       175       176      177      178      179       180      181       182      183      184      185       186      187      188      189      190      191       192      193      194       195      196      197      198      199       200      201       202      203      204       205       206       207      208       209      210       211       212  \\\n",
       "argument      -2.39920  -0.31061  0.162480  1.19390   4.55300  2.608000  2.381600  4.37930 -6.4059 -2.898900  6.9065  3.635700 -3.4818 -3.07200 -1.562500  4.45740  2.406200  0.61288 -2.02030  1.85400 -1.28270 -0.866920 -2.234400 -0.202930 -0.98764 -2.57030  -0.42565 -0.67371 -2.95120  2.52310  3.61740  2.046000  0.93659  0.80884 -1.835900 -4.15380 -0.77178 -0.91474  0.357400  2.43110 -2.84270 -4.39410  0.18871  1.04750 -0.79925 -1.58430  3.480400 -2.895800 -1.462500  0.56112 -2.86750  2.770100  2.391500 -1.90630 -2.25350 -0.62433  3.14550  1.222000  2.28540  0.36479  3.091800  1.61080 -1.65840 -1.64510  0.65175  2.85370 -3.29310 -4.5797  1.7337  3.35200  1.768100  1.599000 -5.05160  1.664400 -4.369200  1.14050 -3.99870  5.094700 -0.79874 -0.608100 -3.21580 -0.42495  1.404700   2.54230  0.58327  2.52930 -3.55850 -2.18920  4.41760  1.643900  3.472500  2.23200  5.642900 -2.55720  3.22990 -2.32170  0.044789  3.106200  2.82930 -2.23330  3.69810  3.2175  4.477000  3.126400 -0.69679  3.03720  3.25990 -4.15510  1.04660 -2.30530  0.066955 -0.960050 -0.48711  1.059800  1.91210  2.15400 -4.61480 -0.606290  0.35795 -0.86739 -0.40956 -5.43570   0.59216 -1.75330 -3.36790  0.397490  3.54500 -3.55280  0.711020 -4.57000 -4.58250 -1.846200  3.95070  0.42573  0.61037  0.72052  0.78121  2.548200  0.79837 -1.85340 -1.46980 -0.71325 -0.21927  1.68080  1.2657 -0.95549 -4.65340 -0.422500 -0.655770  3.78360  0.58572  4.75820 -1.281200  1.68730 -1.43430  5.10480  1.84110 -0.33209  -2.84530 -4.33760  2.24460  0.060355 -0.28312  2.110900 -0.214760 -2.08980 -2.99090  1.68460  1.22150 -1.663000 -0.29140 -0.15043 -0.089668 -1.361800  4.252300  1.049900 -0.835560  3.34520 -2.71990 -3.49400  2.819600  3.07220 -0.129980  1.63730 -1.97820  2.92380 -2.132400 -1.18320  3.44950  2.63860 -2.62280 -3.05720  0.773540  1.61730  2.12610 -3.067000 -0.84982  3.68300  1.61980  6.20910 -2.488200  0.70562  0.360680 -0.85727  2.07030 -0.057408 -0.638360  0.628880 -0.26941 -1.863700 -2.42160 -1.494200 -2.671000   \n",
       "awful         -0.33203  -0.35220 -0.011708 -0.88964  -0.75291  1.845700  0.623790  1.01190  2.1302  1.779500  1.2335 -1.207700 -1.6876  0.33506  0.376710  0.83856  0.031083 -4.43120 -0.83196 -1.03640 -1.26410  1.848000 -0.288830 -1.527500  0.73359 -1.55750   0.28682 -0.65072 -0.79270  1.72920  0.32438 -0.935490  2.18660 -0.79948 -1.139400 -3.93730 -0.99964 -1.96530  1.945700  4.25720 -0.80120 -0.56891 -1.02410 -4.08480 -5.54380  0.43820 -0.082399 -4.646900 -2.437600  2.14220 -0.80818 -2.262100  3.854500 -2.47080 -1.70140  1.72380  1.50330  0.322180 -1.20730  0.41837  0.112650  0.39928  1.09220 -1.88730 -5.87340 -1.33360 -2.49000 -1.3867 -1.2559 -1.17970  2.555200  1.758700 -1.96580  0.065817  1.182400  0.97211 -0.62050  0.445420  0.92654  1.334700  0.74309  1.55980  1.817000  -1.13250 -2.71450  1.55730 -0.29974 -0.84252  2.15850 -1.295000 -1.229100 -2.28710 -0.085077 -3.63760 -1.69640 -0.69068  0.933470  0.282670 -0.34476 -2.17250 -1.44910  1.3936  0.178300  1.584700 -3.98000  0.54971 -0.37694  1.69870 -1.97800  2.37750 -1.338400 -3.112100  1.79570 -1.136800 -0.49529  1.62900 -2.41880  2.568800  2.28060  0.16880 -0.40327  0.69830  -4.50090  4.18320 -0.55077  0.054487 -0.59604  0.87615 -1.581400 -1.57830  1.68210  4.737100 -1.15540 -2.09060  0.32650  2.58540  1.34470  0.168990  1.90460 -3.21210 -3.23080 -1.03700  0.58301  2.32610 -5.8928  0.38668  0.40396  0.654600  1.161200 -1.41830  0.32217  0.82264 -2.021500  1.36950 -2.71390  2.89760  2.14150  2.38560  -0.97230  0.96908  1.24560 -0.264430 -1.95190  0.051155 -1.987000  0.65262  3.92240 -0.29806 -1.20940  3.451200  1.47300  0.21217  2.308400 -4.005900 -2.395000 -2.805800 -1.657700  2.89430  3.23290 -0.91901 -0.378530  1.12440  2.987100  1.33230 -0.92495  0.23867 -2.978200  0.13664  2.36910 -1.67370 -2.31260  0.50444 -0.865220 -1.83850 -1.97240 -4.246800 -1.98620 -0.92723 -0.16031  0.90947 -0.825260  1.72510  0.051674 -2.06460 -2.95620  1.911900  2.254900 -0.185860 -0.42561  0.403430 -1.33110  0.570660 -0.447660   \n",
       "decorator     -4.13860  -3.10150 -2.393800 -1.23100  -0.20878 -3.961600  0.000133  1.21540 -0.7785  4.896700  4.6354  0.250680 -3.1679  1.74190  3.622500  0.49026  0.047730  3.93780 -0.35841 -2.43020 -1.59880 -0.020996  1.055600  3.488800 -1.66060  0.70270  -4.41830 -0.22423 -1.40400 -3.28490  0.30462 -1.996100  2.26480 -0.64048 -1.962800  2.47050  1.26400  0.27926  1.726000 -0.98930 -1.40990 -2.07690  3.54400 -0.39312 -2.45640  0.75642 -1.430800 -1.372300  0.484910 -0.73407 -1.71950  2.165200  1.165000 -1.06380 -2.98700  1.47890 -0.13572 -0.417030  0.25105 -1.45970 -0.014275 -2.86830 -0.30213 -2.49690 -0.70024 -2.53520 -0.99733 -2.2389 -1.1097  0.57388 -2.226800 -0.811130 -2.37140  1.481200  3.125200 -0.31270  0.97177 -0.685180 -0.18344 -1.384500 -0.98578 -1.66480 -0.587210  -1.89180 -0.90744 -0.50375  1.98690  3.06890 -1.61810 -1.163900 -2.776400  1.03950  1.702800 -2.04780  3.11900 -2.65770  4.217000 -0.887980  0.50361  0.24387 -2.49820 -1.1432  0.863590  0.581940  1.05840  1.30550 -0.97641  0.24125  3.99320 -0.97158  1.309100  0.773220  0.42159  0.757590 -2.78450 -2.22850  1.35470  0.159160  1.37000 -2.32700 -1.41640  2.16230   0.47771  1.76670  4.59320 -0.923760 -1.75620 -0.83212  5.286400 -0.77980 -1.91110  1.782100 -0.20666 -4.17980  0.37562 -2.32090  0.69554 -2.982000  2.38170  1.53930  0.42728  2.94390 -1.18590  0.38971  3.1219 -0.89303 -2.56450 -0.035272  1.375600 -1.80250  0.86336 -0.94475 -0.533470 -0.32559  0.94793 -2.72790  0.87363  0.27429   0.68351 -2.76550  2.04130 -1.957400  2.75130 -0.911740  1.290600  1.89180 -4.49850 -2.90070 -1.03190  0.038302  0.62120 -1.66310  0.201810  1.205600  1.642000  0.307360  1.391700 -1.75260 -1.37860 -1.46090 -1.487300  0.95038 -3.100800 -2.85720  1.45410 -0.70981  3.747600 -1.26090 -2.79440  3.44740  0.94558 -0.54897 -1.971400  1.93290 -0.28302  2.727500  1.36420  2.61480  5.45890 -1.36260  3.589800 -2.92200  0.504400 -0.21339 -0.45199 -2.413400 -1.171300  1.025500  1.17010 -1.561200  1.51710 -0.048064  2.645300   \n",
       "misconception -1.44020  -0.65446  1.421700 -0.39426   2.44130  1.224300  3.486700  3.91120 -1.8492 -0.833800  5.1676  0.869870 -1.9083 -0.31838  0.433870  3.06350  2.563100  1.42770 -2.36530  0.17331  0.11683  2.073600 -3.468900  1.209100 -1.95440 -2.79620  -1.27760 -0.51866 -2.11150  1.32270  1.41510 -0.012075 -1.12090 -1.74920 -2.263800 -0.74907  0.69087  0.06555  0.039321 -0.48229  0.36317 -0.84680 -1.32010  0.32879 -3.16190  0.33472  2.281400 -2.216900  0.884990  1.09090 -1.70110  1.191000 -0.395970 -2.05440 -1.55900  1.19610  0.93411  1.891800  0.77402  1.48950  0.063936  0.77872 -0.73827 -0.33916  0.36916  0.17412 -1.83790 -5.0217  1.6003  3.26650 -0.160080  1.016900 -2.34140  1.077900 -3.283900  1.13220 -2.15710  1.155200 -1.67790 -0.063517 -1.90090  1.26600  1.650600   0.56655  1.62230  0.64850 -4.29680 -2.34700  1.79010  1.047000  1.344900  0.47285  2.663100 -3.58440  0.12740 -0.72851 -0.512600  0.893700  2.00550 -0.12694  1.38400  2.2943  2.528100  1.634900 -0.17525  4.56080  0.28487 -1.18960  0.98215 -2.40600  0.553100 -0.084463 -0.16912  0.654340  0.27161  2.64430 -4.85060 -1.336800  0.15755 -0.63597 -1.25890 -2.00010  -0.17487  0.20887 -1.90630 -2.197000  0.34965 -3.18230  2.224800 -2.99950 -3.96240  0.383090  3.67700  0.93066 -1.88170 -0.28573 -0.22240 -0.452800  1.74900 -1.16860 -0.61897 -1.24770  0.51393  1.52660  1.5007 -0.78604 -5.06570 -1.354700  0.014334  2.57200  1.70360  1.37560 -0.834790  0.49078 -0.94801  1.02110  2.25870  0.41330   0.33318 -2.24850  0.81790 -0.449510  2.39340  1.402700 -0.925540 -2.61830 -2.78740  0.46414  1.56080 -0.996830 -1.98250 -1.40860  1.560800 -0.201570  2.286300  2.374600  0.210200  1.66130 -1.81680 -1.88840 -2.322100  0.33800  3.544200  0.17021 -2.11830  1.24420 -4.505700 -2.29700  2.76450 -0.59000 -2.95320 -0.53757  1.151600  0.22778  1.87150 -0.588330 -3.21310  1.77990  0.23060  1.74870  0.300530 -0.91628 -1.513200 -0.56853  3.41440  0.673730 -2.347100 -0.405600 -0.77719 -2.719700 -0.55854 -0.443050 -2.018000   \n",
       "crab          -0.29427  -3.77220 -0.707150  0.25983   1.18960 -4.619400 -3.174600  3.59190 -2.7857  2.099300  5.1909 -2.951200  4.8228  2.43830  2.764800 -6.81330  1.249800  1.50060  2.30220 -3.04830  0.36441  1.408100  0.207200 -1.062100  3.85600 -1.65500  -2.68290  1.23670  1.14700 -0.44072 -1.89680 -3.589700  2.88470 -1.03540  4.117700 -2.90360  2.12960 -1.46760  1.717600  0.35180  0.94584 -1.83730  1.65680  2.34720  1.33600  0.36764 -0.066217  0.026254 -1.341900 -0.54964 -2.24190  4.537100  1.879500 -1.14570 -0.97683 -3.38040 -2.94590  1.478700  3.63150 -1.43010  2.215300 -2.65400  4.46650  4.65510 -1.09250 -3.48330 -3.30940  1.8197  1.0144 -0.83145 -1.488100 -2.943900  2.05330  2.050800 -0.333460 -1.17890 -3.00110 -5.810900  2.90630 -1.847700  1.83780  2.53050  2.266700   0.56153 -0.74400 -0.93070  6.29680  0.41122 -4.43690 -1.893000  7.908900 -0.12312  1.228400 -2.99290 -0.86969  1.30830  3.628700 -2.369500 -0.57882 -3.64610 -1.03220 -2.6258  0.012072 -4.216900 -2.11010 -0.88852 -5.93000  5.09930 -1.02640 -1.12670  2.017900  2.965200 -0.76725  0.522110 -1.58860  1.94680  3.93830  1.207200 -2.61410 -1.28920 -0.85353  5.07760  -7.48520 -0.99613 -4.88490 -0.119810 -0.43466 -5.60860  2.011500  2.62300  1.09020 -0.068215  3.41990 -1.15150  4.62600 -2.56210 -3.07810 -2.581000 -7.82810 -4.71150 -5.97700 -2.65340 -3.86190 -1.54650  1.2815 -1.45570 -0.57384  2.791600 -4.704000 -5.45030 -0.46227  5.71290  1.221000 -3.08060  3.54630 -3.19900  1.66120 -0.46012  -2.21520 -0.22702 -0.86561 -2.131200 -0.17625 -3.066900 -2.314000 -2.91810 -0.54938  1.93340  2.52950  1.829700  2.29010  0.73415  0.081001 -1.224200 -4.276700  0.761780 -0.061992 -4.37250  2.72370 -0.17234  0.307930 -0.28302  1.218200  2.78480 -0.65654  0.28948  1.641700 -2.67990 -1.33830  0.13474 -2.31100  0.55091  0.296710 -1.23900  0.37136 -1.317100 -1.04930  0.99578  0.63019 -2.42330 -0.031176  0.99886  1.983800  3.15850  0.64745 -2.374500  0.064163 -1.466000  1.11430 -0.345950  3.85840  5.071100 -0.055536   \n",
       "...                ...       ...       ...      ...       ...       ...       ...      ...     ...       ...     ...       ...     ...      ...       ...      ...       ...      ...      ...      ...      ...       ...       ...       ...      ...      ...       ...      ...      ...      ...      ...       ...      ...      ...       ...      ...      ...      ...       ...      ...      ...      ...      ...      ...      ...      ...       ...       ...       ...      ...      ...       ...       ...      ...      ...      ...      ...       ...      ...      ...       ...      ...      ...      ...      ...      ...      ...     ...     ...      ...       ...       ...      ...       ...       ...      ...      ...       ...      ...       ...      ...      ...       ...       ...      ...      ...      ...      ...      ...       ...       ...      ...       ...      ...      ...      ...       ...       ...      ...      ...      ...     ...       ...       ...      ...      ...      ...      ...      ...      ...       ...       ...      ...       ...      ...      ...      ...       ...      ...      ...      ...      ...       ...      ...      ...       ...      ...      ...       ...      ...      ...       ...      ...      ...      ...      ...      ...       ...      ...      ...      ...      ...      ...      ...     ...      ...      ...       ...       ...      ...      ...      ...       ...      ...      ...      ...      ...      ...       ...      ...      ...       ...      ...       ...       ...      ...      ...      ...      ...       ...      ...      ...       ...       ...       ...       ...       ...      ...      ...      ...       ...      ...       ...      ...      ...      ...       ...      ...      ...      ...      ...      ...       ...      ...      ...       ...      ...      ...      ...      ...       ...      ...       ...      ...      ...       ...       ...       ...      ...       ...      ...       ...       ...   \n",
       "vaccinate      0.26977   3.14150 -2.580500  2.16510   1.75510  0.902260  0.206410  3.53160 -1.2185  3.915300  2.0898  2.449300 -6.4308  0.51798 -0.017812 -1.01860 -0.817590 -2.12740 -2.55710 -1.24400 -2.03330 -1.801100 -3.576000 -2.004600 -2.38120  0.23166  -0.29845  3.79450 -2.25420  0.27720  4.39550 -0.327550 -1.07670 -3.44250 -1.051400 -3.06890  2.25370  0.39276  1.079000  0.29247 -3.34410  2.40720 -0.88180  3.39740 -3.74330  0.16462  4.559300 -5.435500 -2.510000  2.09480  0.41158  1.357000  1.633400 -3.67600 -2.55740  0.19681 -1.17610 -0.889060  1.07560  0.75712 -2.074700 -1.96230  0.29657 -2.47170  4.38570  3.18270 -1.17960  3.0087  2.5556  3.01570  0.571730 -0.068611 -3.35580  1.659700 -1.606800  0.58696 -1.79070 -4.188000 -3.66680  0.392170 -5.73340  1.64190  0.233090   2.46940  1.07510 -2.50540 -0.70993 -2.63680  3.56010  0.451730 -3.951400  0.69130 -2.810800 -0.97022  0.73158 -1.60140 -1.033700 -2.130900 -0.35910  3.49160  4.24490  1.0448 -0.947880  0.021135 -3.66050  1.30450 -1.98540  0.34249 -0.36231 -3.40710  1.056700 -1.607200 -1.31940 -1.656100 -0.12402  5.32950 -6.35620 -2.267300 -2.82070 -2.15110 -4.11550  0.13449  -3.86090 -2.25690 -4.24510 -2.028700  0.59665  0.43709  0.099868 -0.87762  0.44413 -2.840300  0.40321  1.21180  2.79890  0.28185 -3.06000  0.486680  2.62330 -4.10200  0.56496 -0.65196 -1.51670 -2.66470 -1.3859 -0.18319 -1.38200 -3.401200  0.628200 -0.70373  0.15838 -1.43580  1.019500 -0.58029 -0.58021 -0.80893  3.22150 -1.07960   0.58521 -1.05340 -0.66281 -0.716090 -1.81220  2.166700 -1.082500 -4.07670 -3.48420 -2.29270  5.34660 -2.576900  0.98606  3.54770  3.360800  0.019701  2.274800  0.759890 -0.937240  3.51350 -3.37670  0.68563 -0.685130 -0.12965 -0.935630 -1.57750 -0.43092 -0.36830  2.168400  0.26855  6.50050  2.76950 -0.03352  0.95000 -5.731400 -0.59452 -5.43230 -0.658170  0.61197  1.30270 -3.93070  2.79820 -4.061400  1.38160 -1.454700  1.00920  1.02070  6.075100 -2.108400  1.352100 -2.42820  4.327700  3.32510  0.122290 -2.083900   \n",
       "boastful      -0.64374   0.29787  0.752070 -1.06370   0.55945  1.679200 -0.100350  0.82323 -0.7660 -0.412910  1.1909 -1.431300 -0.4628 -1.92730  1.663300  1.03090 -1.780400 -0.96916  1.59300  1.90480  0.98672  1.198000 -1.703500  0.046899  0.68360 -0.44087  -0.39414 -1.11860  0.91179  0.36917  0.19616 -0.914720  0.67642 -2.11920  0.194090  2.15560 -0.38760  2.25630  2.501400  1.04100 -1.23900  1.06510 -0.49369 -1.96000 -1.86890 -1.13320 -1.904200 -0.921120  0.060186  2.14350  1.21580  0.025998 -0.016047 -0.60961  1.16490 -1.35660  2.03010 -0.085189 -0.53251  0.15678 -0.085620  0.29298  1.04070 -1.63120 -0.68777  0.39717  0.19293 -0.2917  1.4363 -1.32810 -0.058618  0.488930 -0.52708  0.484440  0.719880 -1.36150  0.72896  2.880800  1.27880  2.461900 -1.50350  1.42860  0.070308  -2.62890 -0.86384 -0.14705 -0.24122  0.22958 -1.27410 -0.068621 -0.431590 -1.12390  1.130700 -1.35440  0.37984 -1.23670  1.050800 -1.162400 -0.75523 -2.17010 -1.78150  0.4644  0.347620  3.311200 -0.54717 -1.08290 -0.38567 -2.12180 -1.35770  1.21880  0.017559  0.382600 -1.70140  0.025059 -0.96561 -0.18866 -0.36405  0.890350 -2.07950  0.90127  1.66280 -1.68040   1.05420  1.71250  0.26156  0.078374 -0.40963 -0.77176  1.478600 -0.13770  0.69175 -1.080100  1.33990 -0.31152  0.05605  1.59190 -1.36860  0.015767  0.77700  0.58118  0.86163 -0.57362  1.91570  1.23490 -2.0593 -0.85008 -0.43776  2.751200  0.930160  1.13530  1.55300 -0.42481  1.610000  1.23660 -1.30670  1.38260  1.15300  1.06490  -0.62674  1.07510  1.71940 -1.512200 -1.47450  0.404720 -0.078352  0.44541  0.80621 -1.86190 -0.28317 -0.287990  2.37780  0.44314  1.103500 -2.771800 -0.049913  1.009000  0.478840  2.13450  0.99308 -1.66640  1.383300  0.99228  1.332200 -0.41434  0.19128  1.25520 -0.063287 -0.55261 -0.88812  0.94904  0.53548  0.72654 -0.591190 -0.31803 -0.54894  0.336040 -1.19460 -1.47730  0.79351  0.70217  2.155500  1.53310 -0.527130  1.93340 -1.48970  0.862440  0.719120 -1.126200  1.72440  0.056383 -0.67042  0.850890  0.940950   \n",
       "phrase         1.90980   1.81190  6.846300 -1.89180   1.64390  0.012368  0.868690 -2.57400 -5.9958  0.056522  5.4924 -0.026202 -1.3393 -0.26483 -0.080494 -0.37879  0.413080  1.04440  1.62010  4.09840 -1.63390 -0.351710 -0.098921  0.248200 -1.31840 -0.52174   0.27486 -0.88031 -1.33450  2.28100 -2.73120  1.394900 -3.54490  0.40907  0.018951 -3.00710 -0.70820 -0.55946  3.119700  2.09890 -1.56880 -1.14060 -0.30611  0.75219 -2.91770  1.78770 -0.718460 -2.479500  0.780140 -2.04310 -4.63470 -1.284300  3.730000  0.48098 -1.93570  1.64160  3.35750 -0.547100  0.62148 -0.19466  3.355800 -0.55743  1.98030 -0.34399 -1.33710  1.75380 -1.01500 -6.5020 -3.3270  3.51330 -0.466630  1.281600  0.93278 -0.601670  0.053144 -0.18861 -3.70440  0.079878 -4.38510  0.383640 -2.14290 -0.42491  1.200300   1.63080  2.36380 -1.36050 -1.29360  2.83080 -0.53375 -0.056111  2.731700  1.29990  4.334000 -1.61420  0.23669 -3.10660 -1.626800  0.305590 -1.30930 -3.74580 -0.78939 -1.3251  2.849700  0.355450 -1.26800  2.56390  4.09510 -1.72470 -1.16550 -2.16040  0.336930 -2.044500  0.53786  0.822130 -0.61540  0.67781 -2.93170 -2.199100 -1.34660 -0.43242 -0.75780 -1.10700  -0.36677  3.88030 -1.26090  0.504550  0.19001 -4.27850  0.128850 -0.21011 -3.12990 -1.267000 -0.60806 -0.26853  1.03280  1.05090 -2.54130 -0.534860  1.18150 -2.62710 -2.42250 -1.00900  2.22810 -1.31130 -3.2605 -2.17160 -1.02320  2.562200 -3.325600  2.60640 -1.87190  5.30890 -0.052055  0.11815 -1.14210  1.76300  1.70210  3.74770  -0.69020  0.21864  4.16030  2.264200 -0.31813 -0.927690  0.521580 -6.85050 -1.61680  0.42942 -0.91831 -1.051700  1.92290  0.28125  0.400890  1.731100  1.923600 -1.762200  0.405290  0.95263  0.31829 -1.47090 -0.041693 -3.10430  0.202310  0.85276  2.33350  0.73921 -0.236930 -5.81200  1.89110 -1.25280 -5.35320  1.55580  0.719910  3.12340  3.28570 -1.066500  0.83074  1.02060  1.63980  5.82020 -2.373900  0.52736 -0.510420 -1.12720 -2.52650 -0.267610 -1.830900 -4.061800  2.22760 -0.065473  0.74785  0.021271  1.055400   \n",
       "ski            2.98080  10.30700 -0.143810 -8.48640  10.91400  7.630000 -4.985500  8.12060  6.3504 -2.288800 -2.0521 -1.112400 -4.3927  1.92840  4.367200 -1.61950 -5.694200 -2.21270 -1.13000 -1.02360  1.56940  4.118700  4.194100 -1.142700  1.77080  9.82530 -10.33500 -6.96280  2.19810  4.81370  0.28503  4.649800  3.68580 -0.35060  6.388500  3.50230 -5.53560  8.15740 -4.272200 -5.98230 -1.25010  4.85890  6.42290 -3.87160  1.63390 -8.21740  1.625900  1.687000  1.532100  2.44460  7.61380  4.230500 -5.355900  0.94129  9.83830 -6.51060  3.01970 -2.092200 -4.69150  1.67960 -4.093100 -2.41250 -4.20310  5.58470  1.49650 -1.26020 -3.07870  1.7105 -0.1317  2.12420 -7.859000  1.046200 -0.66114  1.696700  3.391400  1.76680 -0.38296 -1.759600  0.88219 -0.612040 -3.45490  3.69810 -0.395080 -10.42700 -7.01470  5.78440  7.51540 -1.27760 -2.08340 -3.076900  2.208900 -3.33630 -0.463400  2.08540 -1.97810  5.03610  1.160900 -0.015598  1.96290  2.28800 -4.67160 -3.1697  3.640300  7.733100  4.97370 -4.39850  1.94820  6.05260 -2.12980 -3.13140 -0.692390 -6.058100  5.81440  7.916000 -5.21250  3.58080  4.67770  0.077567  5.00710 -5.93610 -4.44720  4.03170 -10.68000  4.81280  2.60130 -3.431300 -4.52070 -7.01210  2.613500 -0.13274  5.24840 -1.757500 -2.01200 -0.49666  0.12080 -6.35580 -4.89980  7.957600  2.25840 -0.63749  1.43060 -4.46580  5.36180 -1.63170  1.9955  7.47590  5.54130 -1.362900 -9.061200 -0.61989 -0.15132 -2.07130 -1.543100 -4.65730  6.09780 -5.64670  6.54960  0.97285  12.12600  8.24390 -1.72780  0.652040 -1.28160 -2.946000  3.729700  5.10500 -6.33930 -7.97800 -4.73550  4.052400 -3.90740  3.48360 -2.297000  3.512600  0.438070 -0.035033 -1.504500 -7.21640 -6.59120 -5.87210 -2.539900  5.02010 -0.042889 -3.76320 -2.72320  5.64850  8.990300 -3.44530 -6.06020  5.10750 -6.03090  1.38820 -2.458200 -1.28910  7.89950  0.038937 -6.01760  0.81326  3.02260  4.93930  4.519300  4.36140  1.933000 -0.74458 -3.76650  6.008000 -8.702000 -0.308610 -2.62190  5.138400 -2.95640  0.722520 -0.258650   \n",
       "commanding    -1.59280   0.40658 -0.189960  2.13110   0.21254  1.550900 -0.638750  1.61440  1.5201 -0.984460  5.2634  3.197500 -3.0917  3.10210  2.844400  3.09070  1.542200  4.22190  1.07730  2.43810  0.14950 -1.125100 -1.458600  2.382200  4.00470 -1.18650  -3.33620 -2.35150 -0.59568  2.18910  1.11900 -0.700080  0.83824 -1.92810 -2.209300 -1.86570 -1.36610  4.33010 -0.309060  0.20639 -0.59435 -2.87530  0.13022 -0.22724 -1.46660  0.48309 -0.372330 -2.598800  1.920900  0.74662 -1.23260  2.295000  1.622400 -1.05030 -0.23773 -0.80576 -0.87834 -1.279600 -0.70184 -4.48980 -1.506000 -0.90765 -1.39330 -3.15470  2.24820  1.12010 -0.46624 -2.8329  3.2271  3.40410  1.167100 -1.385100 -1.37940  0.844130 -1.996100 -0.60021 -1.92280  3.672200 -1.75190 -0.522440 -6.07810 -3.39830 -3.574700   4.04230  0.58130  0.47143 -1.74080 -2.07660 -0.45620  0.036687 -0.096209  2.31830  0.964400 -1.87050  2.45880 -2.74540 -0.668050 -0.018221  0.80530  2.19470  4.11040  1.4484  3.247800 -0.239520 -2.79790  4.44860  1.40580 -0.48561 -0.94428 -3.23370  3.106900 -1.920500 -2.43690  0.218110 -0.52357 -1.18370 -2.55880  1.313200 -2.30070 -2.12190  0.16552 -5.12860   1.70800  0.66239 -0.20327 -4.396500  3.54050 -6.11900  5.231900 -1.51650 -4.28600  0.720430  5.99650  1.29710 -1.11650 -1.06120 -3.64180  1.048300  1.08230 -1.19370 -0.62115 -1.53380 -3.18110  4.78890  2.7876 -0.51639 -6.02690  0.220850  0.155940  1.73650  1.51050 -1.56630 -0.717120  4.93590  2.39010 -0.85229  2.63470 -1.39630  -2.79040 -4.00330 -1.63610 -3.611900  0.90931  1.178500  1.287600 -1.87920 -3.88590  1.24370  1.20270 -0.611850  0.41406 -1.10760  0.783070 -2.311700  1.582000  3.407900  2.433000 -0.45974 -3.17050 -1.65090 -0.274040 -0.79963  0.257710  0.02978  0.91945  3.44640 -1.803700 -0.34887 -2.67680 -0.26284  5.61010 -2.15700 -0.058571  0.73875 -0.67325  0.424640 -3.84930 -1.56960  1.76160  1.19630  0.300910  0.25813 -1.880400 -3.58870  3.57120  0.125750 -0.184290  0.049307  0.93837  5.039600 -1.58540  1.545600 -3.065900   \n",
       "\n",
       "                  213       214      215       216      217      218       219      220      221       222       223      224      225      226      227      228      229       230      231       232       233      234       235      236      237      238      239      240      241       242       243       244       245       246       247       248       249      250      251       252      253      254      255      256      257      258      259      260       261      262      263       264      265       266      267      268      269       270        271       272       273      274      275       276      277      278      279      280      281       282       283      284      285     286       287      288      289      290      291       292      293      294       295      296      297     298       299  \n",
       "argument       4.4052 -0.897830  1.84780  1.427400  1.16620  4.00380  2.206200 -3.77060 -0.94775  0.081936 -0.052859 -1.14660  0.97386  1.65780  5.16590  1.97350 -0.10786  1.467100 -4.17920  1.528100  0.131100  1.19560  0.493110  2.88420  1.03060  2.35040 -2.68750 -1.00970  1.24780  1.294900  4.564800  0.517240  0.524770 -2.016200  2.089300  0.606990  1.592600  1.05390 -4.59500  -3.70100 -0.75894 -0.64485 -4.18290 -1.93710 -0.26863  0.26782 -0.63142 -2.10990  1.153400  1.66280  4.74830  0.868380  0.88791  4.310000  4.37190 -3.07340 -1.11620 -0.311190  -1.498500 -1.209900  0.359000  3.47040 -0.40592  0.012269 -1.62150 -1.38370  4.22990  1.46310  0.48673  1.717800  3.760400  4.84590  1.29130  4.0310 -0.377380 -3.63400  0.84509  3.23500 -1.73110  1.399600  2.05100 -4.25220  1.091100  3.78190 -0.92356 -1.8916  3.350400  \n",
       "awful         -1.3727 -1.888100 -1.88290 -2.907800  1.76160  1.54830 -2.220500 -0.28344 -1.33110 -0.059834  3.859300  1.22310 -0.01470 -3.25130  0.46675 -1.01930  0.27312 -2.595400 -1.99740  3.321800  0.074786 -0.78470  0.012676 -0.83699  3.13540 -0.92994  0.17290  0.37295 -1.16190  1.398700  0.621990 -2.901900 -1.428800 -0.070483 -1.022900 -1.262200  1.138900 -1.67180  3.08470  -3.79880  1.90690  0.78550 -3.10370 -1.12060  1.88120 -0.89774  1.78790  3.15630 -0.670440  0.60400 -0.74343 -0.132450  1.98180  1.471800  0.23751 -1.75600 -0.94157  1.382400  -0.013683 -1.588100  1.741800 -2.34360  1.63810  2.527400 -1.56340  2.29810 -1.79800 -1.65460 -2.39590  0.468430  0.998010  1.73970 -1.96550  3.3896 -0.558010 -0.75166 -0.45850 -0.33523 -0.37922  4.542300 -2.32350 -1.34550 -2.070200  0.20349  3.63000  1.0342  1.682900  \n",
       "decorator      3.5460 -0.010302  0.45441  3.383400 -1.70950 -1.43170  1.058400  3.40950  1.31250 -2.143600  2.829800 -0.96006 -1.74710  2.03530  2.69070  1.57160  1.29650 -1.584500  1.27180 -1.046800  1.037600 -4.00340 -1.697000 -2.62320 -1.38540  1.98640  0.70325 -1.24460  2.15750  0.954570  1.792900  2.826900 -3.779100  0.256970  0.070994 -4.422200  2.765800  0.25410  0.13624  -2.57590 -2.75180 -1.28510  1.66780  0.51108  1.22750 -2.18050 -3.17170 -0.35066  2.274800 -2.38250 -1.22730 -0.024748  0.44043  1.319800  1.65810 -3.01110  0.53332 -0.081214   1.185000 -3.559500 -0.493960 -1.60800 -1.68240 -1.651100  2.89460 -1.35680  2.44590 -0.19233  0.14830 -0.146960  2.232800  5.93290 -0.84719 -1.3155  3.271900 -2.61820  1.64680 -0.68108  1.77150  0.459930  1.23450 -2.67310 -0.080917 -0.17837  0.10582 -2.1597 -0.781270  \n",
       "misconception  1.6621  0.316260  2.27980 -0.019118 -3.19110  2.61610 -0.224910 -2.05110  0.10540  0.159160 -0.211710 -3.13590  0.24266  0.63316  3.11420  3.00830  1.03040  3.224900 -2.12900  0.886660  1.320600  1.46330  0.527630 -0.14561  1.04800  1.59470  2.48760 -1.09030  2.05320  2.159600  1.694900  1.021200 -0.024739 -1.293400  0.798630 -2.245600  1.850900 -0.57083 -2.74620  -3.69620 -1.12950  1.54350 -1.16400  0.57704 -0.16744 -1.03510 -0.98163 -0.79970  1.068900  4.23780  3.09940  0.344750  0.84469 -0.054672  3.06860 -2.48950  2.08190 -0.608570   0.267390  2.073200 -1.325900  1.86590 -0.41704  0.652790 -2.40710 -1.32990  2.64320  1.03320 -0.08317  1.558700 -1.642900  1.70270  0.51001  2.0300  0.983990 -0.32521 -0.73525  2.69540 -1.24210  2.630100 -0.89313 -0.80961  0.536610  2.65300 -2.67700 -2.5539  3.299300  \n",
       "crab          -2.4047  1.348400 -0.22253  4.494800  1.04910 -1.16750 -3.155200  5.73740 -3.42350  1.718600  2.886400 -0.40758 -6.42380 -2.59500 -2.30090  1.62540  3.10150  2.735800 -1.53540  3.023500 -0.834300 -3.46940 -0.312980 -1.15280  0.83032 -2.08590 -1.82890 -1.71760 -4.34410  1.527500  5.958800 -3.111300  0.425880  2.792900 -1.149900 -1.547800 -1.793800 -1.87730 -0.66235   0.52911 -0.17560 -1.51630 -2.88540  4.74170  9.27220 -1.85590  2.90540  1.97350 -0.069534  0.75262  0.87348 -5.207600 -0.63884 -1.302700 -3.73270 -4.39430  4.93160  3.559100  -0.115260  1.041200  4.139300  0.55631 -4.32760  3.124000  3.06510  1.53070  0.52090  0.91262 -0.33345 -3.418300 -0.089182  0.59738  1.85210 -1.3994 -2.982200 -2.78770  1.50800  3.99800 -2.59870  7.186000 -0.18771 -1.66890  1.588400  0.81274  1.51700 -2.0713  2.980400  \n",
       "...               ...       ...      ...       ...      ...      ...       ...      ...      ...       ...       ...      ...      ...      ...      ...      ...      ...       ...      ...       ...       ...      ...       ...      ...      ...      ...      ...      ...      ...       ...       ...       ...       ...       ...       ...       ...       ...      ...      ...       ...      ...      ...      ...      ...      ...      ...      ...      ...       ...      ...      ...       ...      ...       ...      ...      ...      ...       ...        ...       ...       ...      ...      ...       ...      ...      ...      ...      ...      ...       ...       ...      ...      ...     ...       ...      ...      ...      ...      ...       ...      ...      ...       ...      ...      ...     ...       ...  \n",
       "vaccinate     -2.8886  1.312100 -1.83270  2.821600 -0.86470 -2.36690  0.913070 -2.53410 -2.12640 -1.472300 -2.764700  0.49121  0.21180 -4.94940  1.60280 -0.63591  1.43280 -0.052498 -2.33480 -4.262300  0.728640 -0.95491 -3.274200 -0.73952  3.08630  1.36060  2.89490 -1.61040  1.03930 -0.403410 -1.298400 -0.942330  4.850000 -0.572850  0.595640  3.462700  1.456700  0.44842 -1.12320  -5.39100 -3.25190  1.28320 -3.54020  2.52720  0.65217  0.30093  3.86440 -3.24370  1.444300  1.98240  0.44735  2.016600 -2.24120 -2.563000  0.23768 -4.52500 -3.96800 -1.727100   1.855300  0.080492 -5.815800 -0.24918 -0.46305 -5.298600 -0.12286 -0.44774  2.08160  0.98389  0.26566  1.349700 -3.135400  2.14610 -1.67230  4.5805  2.143900 -1.38470 -1.13120  2.56490  0.73630 -2.244500  0.61483 -0.17209 -0.582790 -0.41249  1.38020 -1.5483  4.379500  \n",
       "boastful      -1.0217  0.284350  2.21610  0.687610  0.22706 -2.16310 -0.071359  0.83324 -0.35323  1.447700  1.027600 -0.91213  2.09180  0.67396 -0.42748  2.28300 -0.64679 -0.032428  0.90743 -0.843530 -0.116310 -1.33620  0.537520 -0.11961 -1.33170 -0.52374  1.82830  1.73760  0.20633  0.637220 -0.265840 -0.021253 -2.348400  0.100370 -2.786100 -1.044600  1.294900  0.41091  1.18700  -0.98475  0.94293 -0.64350 -1.56340 -0.13868  1.93950 -0.79538  0.12822  0.77388 -2.523100 -0.63884  0.28427  0.559130  2.07090  0.056340 -0.53789 -1.09510 -1.97810  1.981200  -0.296360 -0.925080 -0.258420 -0.82158  0.10705  1.488300 -1.37520  0.45689  0.81544  1.25860 -0.75812 -0.098145 -1.418200 -0.61353 -1.06770  1.9770 -1.299600  1.99420  1.65910  0.21427  0.26348  0.001225 -0.93475  1.77620 -0.303800 -1.63480  2.33610 -2.6880  1.166900  \n",
       "phrase         3.4777  2.177300  0.59564  0.656110  0.19841 -0.64718  0.542730 -0.95929 -1.33110  2.861200  0.597610 -7.59000 -0.42122  0.43525  1.47540  6.00280 -2.77720 -3.559700 -0.50006  0.012587 -2.432700 -2.01460  1.121800  0.51161  3.56260  0.88015 -1.91900  0.13394  1.88700  2.628200  0.038495  1.245400 -1.296500  0.219900  1.896600 -1.949800 -0.044596  1.43860 -6.38570  -1.10110  0.30281 -2.01700 -1.41300 -2.00630  2.39210  1.04490 -0.59200  2.25480  2.089400 -1.03140 -0.71747  0.784110  2.56510 -2.553000  2.13470 -0.66752 -2.84100  0.353430  -0.947280 -1.459200  1.205400  0.57659  1.34080  0.237130  2.29840 -2.11830  2.09240 -2.81410 -2.15360  0.256310  4.198000  3.84430  0.26602  1.9790 -2.296000  1.64780  1.90050  4.49740 -1.89430  4.200100 -1.14030  1.27300  1.737200  4.91640 -0.50940 -1.2522  0.074529  \n",
       "ski           -6.8622 -2.153100 -2.97740  7.517200 -8.15670 -3.92100 -5.460900 -1.41160  3.01930 -2.237400  2.511600  0.95671 -2.63050 -0.47477  6.28210 -0.36173  5.43230  2.092000 -0.43824 -3.224900  3.899600 -5.35810 -2.186200  3.44090  1.61110 -3.38270  3.56410 -4.84760 -6.88430  0.788540 -1.404600  7.863400 -0.596120 -6.729600  0.520990 -6.615000 -1.442000 -2.20560 -2.90950 -11.04000  0.75927 -4.95950  0.59751  1.24690 -7.42060  0.25332 -2.18350  1.16500  0.524600 -5.02260  5.77140  4.121500 -5.36600 -4.513500 -2.37540 -5.18000  1.02000 -5.764900 -12.020000  5.292500  3.104200 -3.05580 -0.36082 -5.798400  1.85570 -4.78430 -3.44740  0.89605  5.59090  2.089300 -4.443100 -0.53930  5.38030 -5.3462 -1.469100 -1.57080 -4.85010 -2.82460  0.22498  4.944100  3.23380 -2.98880  4.551300 -0.99296  9.45230  1.8164 -1.226400  \n",
       "commanding     2.1012 -0.771930  1.25030  2.831400 -1.03990  2.53140 -0.194810 -3.07620  4.52930 -0.373110  0.698800 -0.59152  0.29593 -1.04760  2.82370 -0.22383 -2.87970  4.733000 -1.30650 -1.127100  2.048100  2.93710  1.182000 -1.87960 -0.86135 -3.23100  0.83011  3.48760 -0.15529 -0.087641 -1.196500  2.856600 -0.611970  0.116750  1.377800  0.001078  2.999300  2.12430 -4.27170  -4.73210 -1.81830 -1.43200 -4.31060  1.60090 -2.41790 -1.12020  1.20090  1.68740  3.998700  3.57460  2.54020  2.950300 -1.89720  0.398170  0.82580 -0.27822  1.07930  1.480500   0.226500  0.024602 -0.032531 -0.39685 -1.13450  1.074500 -5.53470  2.25110  4.91880  3.64720  0.13404  1.913200  1.327300  2.81720 -0.56290 -1.1239  0.068115 -3.13650  0.68217  1.08590  2.16180 -2.638700  1.62060 -1.91880  2.043900  2.12860 -3.98320 -2.3288  2.544700  \n",
       "\n",
       "[3511 rows x 300 columns]"
      ]
     },
     "execution_count": 133,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X_train_A"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 134,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>F_Objectivity</th>\n",
       "      <th>F_Subjectivity</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>argument</th>\n",
       "      <td>0.365722</td>\n",
       "      <td>0.647198</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>awful</th>\n",
       "      <td>0.201122</td>\n",
       "      <td>0.507171</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>decorator</th>\n",
       "      <td>0.774472</td>\n",
       "      <td>0.291107</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>misconception</th>\n",
       "      <td>0.013464</td>\n",
       "      <td>0.474901</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>crab</th>\n",
       "      <td>0.816438</td>\n",
       "      <td>0.227631</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>vaccinate</th>\n",
       "      <td>0.678125</td>\n",
       "      <td>0.443751</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>boastful</th>\n",
       "      <td>0.437815</td>\n",
       "      <td>0.575248</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>phrase</th>\n",
       "      <td>0.265757</td>\n",
       "      <td>0.144886</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>ski</th>\n",
       "      <td>0.900524</td>\n",
       "      <td>0.370531</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>commanding</th>\n",
       "      <td>0.316342</td>\n",
       "      <td>0.538581</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>3511 rows × 2 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "               F_Objectivity  F_Subjectivity\n",
       "argument            0.365722        0.647198\n",
       "awful               0.201122        0.507171\n",
       "decorator           0.774472        0.291107\n",
       "misconception       0.013464        0.474901\n",
       "crab                0.816438        0.227631\n",
       "...                      ...             ...\n",
       "vaccinate           0.678125        0.443751\n",
       "boastful            0.437815        0.575248\n",
       "phrase              0.265757        0.144886\n",
       "ski                 0.900524        0.370531\n",
       "commanding          0.316342        0.538581\n",
       "\n",
       "[3511 rows x 2 columns]"
      ]
     },
     "execution_count": 134,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "Y_train_A"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### B) Multiclass classification: splitting train and test data\n",
    "- The model predicts between more than two mutually exclusive classes.  \n",
    "- In this case, it will predict between its semantic content: latent, manifest, contextual, or perceptual.  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 135,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Generating data (the same process for dataset A):\n",
    "X_train_B, Y_train_B = generateData(train_df)\n",
    "X_test_B, Y_test_B = generateData(test_df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 136,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # Categorical labeling through list comprehension accordingly to SCA median criteria:\n",
    "# Y_train_B['F_Objectivity'] = ['high' if f_objectivity >= 0.565 else 'low' for f_objectivity in Y_train_B['F_Objectivity']]\n",
    "# Y_train_B['F_Subjectivity'] = ['high' if f_subjectivity >= 0.392 else 'low' for f_subjectivity in Y_train_B['F_Subjectivity']]\n",
    "\n",
    "# Y_test_B['F_Objectivity'] = ['high' if f_objectivity >= 0.565 else 'low' for f_objectivity in Y_test_B['F_Objectivity']]\n",
    "# Y_test_B['F_Subjectivity'] = ['high' if f_subjectivity >= 0.392 else 'low' for f_subjectivity in Y_test_B['F_Subjectivity']]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 137,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Categorical binary labeling through list comprehension accordingly to SCA median criteria:\n",
    "Y_train_B['F_Objectivity'] = [1 if f_objectivity >= 0.565 else 0 for f_objectivity in Y_train_B['F_Objectivity']]\n",
    "Y_train_B['F_Subjectivity'] = [1 if f_subjectivity >= 0.392 else 0 for f_subjectivity in Y_train_B['F_Subjectivity']]\n",
    "\n",
    "Y_test_B['F_Objectivity'] = [1 if f_objectivity >= 0.565 else 0 for f_objectivity in Y_test_B['F_Objectivity']]\n",
    "Y_test_B['F_Subjectivity'] = [1 if f_subjectivity >= 0.392 else 0 for f_subjectivity in Y_test_B['F_Subjectivity']]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 138,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train data dimension:\n",
      "X_train: (3511, 300)\n",
      "Y_train: (3511, 2)\n",
      "\n",
      "Test data dimension:\n",
      "X_test: (1170, 300)\n",
      "Y_test: (1170, 2)\n"
     ]
    }
   ],
   "source": [
    "## Checking the generated data dimension:\n",
    "print(\"Train data dimension:\")\n",
    "print(\"X_train:\", X_train_B.shape)\n",
    "print(\"Y_train:\", Y_train_B.shape)\n",
    "\n",
    "print(\"\\nTest data dimension:\")\n",
    "print(\"X_test:\", X_test_B.shape)\n",
    "print(\"Y_test:\", Y_test_B.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 139,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>F_Objectivity</th>\n",
       "      <th>F_Subjectivity</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>brunch</th>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>handle</th>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>refugee</th>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>draw</th>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>vary</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "         F_Objectivity  F_Subjectivity\n",
       "brunch               1               0\n",
       "handle               1               0\n",
       "refugee              0               1\n",
       "draw                 0               1\n",
       "vary                 0               0"
      ]
     },
     "execution_count": 139,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "Y_train_B.sample(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 140,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Defines a method to map values from columns to the respective labels according to SCA analysis.\n",
    "def map_labels(row):\n",
    "    if row['F_Objectivity'] == 0 and row['F_Subjectivity'] == 1:\n",
    "        return 'Latent'\n",
    "    elif row['F_Objectivity'] == 0 and row['F_Subjectivity'] == 0:\n",
    "        return 'Contextual'\n",
    "    elif row['F_Objectivity'] == 1 and row['F_Subjectivity'] == 0:\n",
    "        return 'Manifest'\n",
    "    elif row['F_Objectivity'] == 1 and row['F_Subjectivity'] == 1:\n",
    "        return 'Perceptual'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 141,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Maps Y_train_B, Y_test_B values into a target column:\n",
    "Y_train_B['target'] = Y_train_B.apply(map_labels, axis=1)\n",
    "Y_test_B['target'] = Y_test_B.apply(map_labels, axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 142,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Dropping unnecessariy columns:\n",
    "Y_train_B.drop(['F_Objectivity','F_Subjectivity'], axis=1, inplace=True)\n",
    "Y_test_B.drop(['F_Objectivity','F_Subjectivity'], axis=1, inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 143,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>target</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>weave</th>\n",
       "      <td>Manifest</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>scorn</th>\n",
       "      <td>Latent</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>wholesaler</th>\n",
       "      <td>Manifest</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>healthy</th>\n",
       "      <td>Latent</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>emotional</th>\n",
       "      <td>Latent</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "              target\n",
       "weave       Manifest\n",
       "scorn         Latent\n",
       "wholesaler  Manifest\n",
       "healthy       Latent\n",
       "emotional     Latent"
      ]
     },
     "execution_count": 143,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "Y_train_B.sample(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 144,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 144,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "## All training data, except for train_D are equals:\n",
    "X_train_A.equals(X_train_B)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### C) MAD selection data\n",
    "Since SCA uses the median as a measure of centrality instead of the mean, we will consider the median absolute deviation (MAD) as an equivalent to standard deviation. MAD is a robust measure of variability that is less sensitive to outliers compared to the standard deviation.  \n",
    "\n",
    "The purpose of using MAD is to focus on the distant data points rather than the central tendency of the data. The central data points may potentially confuse the algorithm, especially if they represent a region of overlap or ambiguity. By considering the median absolute deviation instead of the mean and standard deviation, we aim to identify data points that are far from the central measurement, thereby capturing well-separated regions of the data. This approach helps ensure that our classifier is trained on data points that are more distinct and representative of different classes or categories, leading to more effective classification performance.  \n",
    "\n",
    "$\\text{MAD} = \\text{median}(|x_i - \\text{median}(X)|)$, where $x_i$ represents each data point in the dataset X."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 145,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Median of F_Obj: 0.5649972252148387\n",
      "Median of F_Subj: 0.3925501677449325\n"
     ]
    }
   ],
   "source": [
    "med_fObj = df_factors['F_Objectivity'].median()\n",
    "med_fSubj = df_factors['F_Subjectivity'].median()\n",
    "print(f\"Median of F_Obj: {med_fObj}\")\n",
    "print(f\"Median of F_Subj: {med_fSubj}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 146,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "MAD for F_Obj: 0.24804732650935357\n",
      "MAD for F_Subj: 0.1399289370570712\n"
     ]
    }
   ],
   "source": [
    "# Calculating MAD\n",
    "mad_fObj = np.median(np.abs(df_factors['F_Objectivity'] - med_fObj))\n",
    "mad_fSubj = np.median(np.abs(df_factors['F_Subjectivity'] - med_fSubj))\n",
    "print(f\"MAD for F_Obj: {mad_fObj}\")\n",
    "print(f\"MAD for F_Subj: {mad_fSubj}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 147,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Defining thresholds as MAD:\n",
    "threshold_fObj = mad_fObj\n",
    "threshold_fSubj = mad_fSubj\n",
    "\n",
    "# ## Defining thresholds as TWICE MAD:\n",
    "# threshold_fObj = 2*mad_fObj\n",
    "# threshold_fSubj = 2*mad_fSubj"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 148,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Selecting from the original dataframe (df: SCA) only those words distant from the median (see threshold above):\n",
    "df_MAD = df[\n",
    "    (abs(df['F_Objectivity'] - med_fObj) > threshold_fObj) |\n",
    "    (abs(df['F_Subjectivity'] - med_fSubj) > threshold_fSubj)\n",
    "]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 149,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'pandas.core.frame.DataFrame'>\n",
      "Int64Index: 3484 entries, 1 to 4681\n",
      "Data columns (total 3 columns):\n",
      " #   Column          Non-Null Count  Dtype  \n",
      "---  ------          --------------  -----  \n",
      " 0   word            3484 non-null   object \n",
      " 1   F_Objectivity   3484 non-null   float64\n",
      " 2   F_Subjectivity  3484 non-null   float64\n",
      "dtypes: float64(2), object(1)\n",
      "memory usage: 108.9+ KB\n",
      "None\n",
      "\n",
      "          word  F_Objectivity  F_Subjectivity\n",
      "1146    demon       0.710539        0.766884\n",
      "1775  gallery       0.820264        0.387011\n",
      "3307   punish       0.367482        0.655155\n"
     ]
    }
   ],
   "source": [
    "print(df_MAD.info())\n",
    "print('\\n',df_MAD.sample(3))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 150,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<matplotlib.collections.PathCollection at 0x173325d2990>"
      ]
     },
     "execution_count": 150,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 640x480 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "plt.scatter(df_MAD['F_Objectivity'], df_MAD['F_Subjectivity'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 151,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Repeating the splitting data in train (75%) and test (25%)\n",
    "train_MAD, test_MAD = train_test_split(df_MAD, test_size=0.25, random_state=42)\n",
    "\n",
    "## Generating data based on MAD selected points:\n",
    "X_train_C, Y_train_C = generateData(train_MAD)\n",
    "X_test_C, Y_test_C = generateData(test_MAD)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### D) MAD data for Multiclass\n",
    "This data puts together the manipulation applied for (B) and (C) datasets."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 152,
   "metadata": {},
   "outputs": [],
   "source": [
    "## It makes use of Y_train_C and Y_test_C data.\n",
    "Y_train_D = Y_train_C.copy()\n",
    "Y_test_D = Y_test_C.copy()\n",
    "\n",
    "## It also makes a copy of X datasets:\n",
    "X_train_D = X_train_C.copy()\n",
    "X_test_D = X_test_C.copy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 153,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Categorical binary labeling through list comprehension accordingly to SCA median criteria:\n",
    "Y_train_D['F_Objectivity'] = [1 if f_objectivity >= 0.565 else 0 for f_objectivity in Y_train_D['F_Objectivity']]\n",
    "Y_train_D['F_Subjectivity'] = [1 if f_subjectivity >= 0.392 else 0 for f_subjectivity in Y_train_D['F_Subjectivity']]\n",
    "\n",
    "Y_test_D['F_Objectivity'] = [1 if f_objectivity >= 0.565 else 0 for f_objectivity in Y_test_D['F_Objectivity']]\n",
    "Y_test_D['F_Subjectivity'] = [1 if f_subjectivity >= 0.392 else 0 for f_subjectivity in Y_test_D['F_Subjectivity']]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 154,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Maps Y_train_D, Y_test_D values into a target column:\n",
    "Y_train_D['target'] = Y_train_D.apply(map_labels, axis=1)\n",
    "Y_test_D['target'] = Y_test_D.apply(map_labels, axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 155,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Dropping unnecessariy columns:\n",
    "Y_train_D.drop(['F_Objectivity','F_Subjectivity'], axis=1, inplace=True)\n",
    "Y_test_D.drop(['F_Objectivity','F_Subjectivity'], axis=1, inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 156,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>target</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>obsessive</th>\n",
       "      <td>Latent</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>affect</th>\n",
       "      <td>Contextual</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>subject</th>\n",
       "      <td>Contextual</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>special</th>\n",
       "      <td>Latent</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>voyage</th>\n",
       "      <td>Perceptual</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "               target\n",
       "obsessive      Latent\n",
       "affect     Contextual\n",
       "subject    Contextual\n",
       "special        Latent\n",
       "voyage     Perceptual"
      ]
     },
     "execution_count": 156,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "Y_train_D.sample(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "## Model 01: MLP Classifier for word semantic content\n",
    "- Multilayer perceptron with two continuous outputs between 0 and 1 (sigmoid)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 157,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from tensorflow.keras.models import Sequential\n",
    "from tensorflow.keras.layers import Dense"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Training with data (A)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 158,
   "metadata": {},
   "outputs": [],
   "source": [
    "### Model 01: MLP architecture for continuous output:\n",
    "model_01_A = Sequential([\n",
    "    Dense(256, activation='relu', input_shape=(300,)),\n",
    "    Dense(128, activation='relu'),\n",
    "    Dense(64, activation='relu'),\n",
    "    Dense(32, activation='relu'),\n",
    "    Dense(2, activation='sigmoid')  # 2 neurons for continuous output between 0 and 1\n",
    "], name='Model_01_A')\n",
    "\n",
    "# Compile the model\n",
    "model_01_A.compile(optimizer='adam',\n",
    "              loss='mean_squared_error',\n",
    "              metrics=['accuracy'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 159,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"Model_01_A\"\n",
      "_________________________________________________________________\n",
      " Layer (type)                Output Shape              Param #   \n",
      "=================================================================\n",
      " dense_5 (Dense)             (None, 256)               77056     \n",
      "                                                                 \n",
      " dense_6 (Dense)             (None, 128)               32896     \n",
      "                                                                 \n",
      " dense_7 (Dense)             (None, 64)                8256      \n",
      "                                                                 \n",
      " dense_8 (Dense)             (None, 32)                2080      \n",
      "                                                                 \n",
      " dense_9 (Dense)             (None, 2)                 66        \n",
      "                                                                 \n",
      "=================================================================\n",
      "Total params: 120354 (470.13 KB)\n",
      "Trainable params: 120354 (470.13 KB)\n",
      "Non-trainable params: 0 (0.00 Byte)\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "# Print model summary\n",
    "model_01_A.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[0.36572243 0.64719766]\n",
      " [0.20112169 0.50717064]\n",
      " [0.77447234 0.29110657]\n",
      " [0.01346386 0.47490132]\n",
      " [0.81643835 0.2276313 ]]\n"
     ]
    }
   ],
   "source": [
    "## Transforming data into numpy array:\n",
    "Y_train_A_array = Y_train_A.to_numpy()\n",
    "Y_test_A_array = Y_test_A.to_numpy()\n",
    "\n",
    "X_train_A_array = X_train_A.to_numpy()\n",
    "X_test_A_array = X_test_A.to_numpy()\n",
    "\n",
    "# Print the first few elements to verify\n",
    "print(Y_train_A_array[:5])  # Print the first 5 elements\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/50\n",
      "  1/176 [..............................] - ETA: 0s - loss: 5.0952e-04 - accuracy: 1.0000"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "176/176 [==============================] - 0s 2ms/step - loss: 7.3620e-04 - accuracy: 0.9790 - val_loss: 0.0143 - val_accuracy: 0.8706\n",
      "Epoch 2/50\n",
      "176/176 [==============================] - 0s 2ms/step - loss: 8.3364e-04 - accuracy: 0.9747 - val_loss: 0.0144 - val_accuracy: 0.8578\n",
      "Epoch 3/50\n",
      "176/176 [==============================] - 0s 3ms/step - loss: 8.7437e-04 - accuracy: 0.9751 - val_loss: 0.0140 - val_accuracy: 0.8620\n",
      "Epoch 4/50\n",
      "176/176 [==============================] - 0s 2ms/step - loss: 8.4182e-04 - accuracy: 0.9786 - val_loss: 0.0143 - val_accuracy: 0.8578\n",
      "Epoch 5/50\n",
      "176/176 [==============================] - 0s 2ms/step - loss: 8.4285e-04 - accuracy: 0.9779 - val_loss: 0.0143 - val_accuracy: 0.8663\n",
      "Epoch 6/50\n",
      "176/176 [==============================] - 0s 2ms/step - loss: 9.0011e-04 - accuracy: 0.9704 - val_loss: 0.0141 - val_accuracy: 0.8720\n",
      "Epoch 7/50\n",
      "176/176 [==============================] - 0s 2ms/step - loss: 8.9188e-04 - accuracy: 0.9726 - val_loss: 0.0141 - val_accuracy: 0.8706\n",
      "Epoch 8/50\n",
      "176/176 [==============================] - 0s 2ms/step - loss: 8.4099e-04 - accuracy: 0.9719 - val_loss: 0.0142 - val_accuracy: 0.8691\n",
      "Epoch 9/50\n",
      "176/176 [==============================] - 0s 2ms/step - loss: 7.5118e-04 - accuracy: 0.9776 - val_loss: 0.0140 - val_accuracy: 0.8691\n",
      "Epoch 10/50\n",
      "176/176 [==============================] - 0s 2ms/step - loss: 7.0109e-04 - accuracy: 0.9797 - val_loss: 0.0137 - val_accuracy: 0.8649\n",
      "Epoch 11/50\n",
      "176/176 [==============================] - 0s 3ms/step - loss: 6.1215e-04 - accuracy: 0.9793 - val_loss: 0.0140 - val_accuracy: 0.8734\n",
      "Epoch 12/50\n",
      "176/176 [==============================] - 0s 2ms/step - loss: 5.3903e-04 - accuracy: 0.9822 - val_loss: 0.0139 - val_accuracy: 0.8706\n",
      "Epoch 13/50\n",
      "176/176 [==============================] - 0s 2ms/step - loss: 5.4983e-04 - accuracy: 0.9850 - val_loss: 0.0139 - val_accuracy: 0.8734\n",
      "Epoch 14/50\n",
      "176/176 [==============================] - 0s 2ms/step - loss: 5.1563e-04 - accuracy: 0.9818 - val_loss: 0.0138 - val_accuracy: 0.8706\n",
      "Epoch 15/50\n",
      "176/176 [==============================] - 0s 2ms/step - loss: 5.1109e-04 - accuracy: 0.9811 - val_loss: 0.0139 - val_accuracy: 0.8720\n",
      "Epoch 16/50\n",
      "176/176 [==============================] - 0s 2ms/step - loss: 5.2951e-04 - accuracy: 0.9815 - val_loss: 0.0136 - val_accuracy: 0.8706\n",
      "Epoch 17/50\n",
      "176/176 [==============================] - 0s 2ms/step - loss: 5.9829e-04 - accuracy: 0.9829 - val_loss: 0.0135 - val_accuracy: 0.8762\n",
      "Epoch 18/50\n",
      "176/176 [==============================] - 0s 2ms/step - loss: 6.9078e-04 - accuracy: 0.9783 - val_loss: 0.0139 - val_accuracy: 0.8762\n",
      "Epoch 19/50\n",
      "176/176 [==============================] - 0s 2ms/step - loss: 8.1412e-04 - accuracy: 0.9747 - val_loss: 0.0137 - val_accuracy: 0.8677\n",
      "Epoch 20/50\n",
      "176/176 [==============================] - 0s 2ms/step - loss: 7.6964e-04 - accuracy: 0.9793 - val_loss: 0.0137 - val_accuracy: 0.8706\n",
      "Epoch 21/50\n",
      "176/176 [==============================] - 0s 2ms/step - loss: 7.3665e-04 - accuracy: 0.9786 - val_loss: 0.0139 - val_accuracy: 0.8762\n",
      "Epoch 22/50\n",
      "176/176 [==============================] - 0s 2ms/step - loss: 7.6952e-04 - accuracy: 0.9740 - val_loss: 0.0139 - val_accuracy: 0.8720\n",
      "Epoch 23/50\n",
      "176/176 [==============================] - 0s 2ms/step - loss: 6.1151e-04 - accuracy: 0.9815 - val_loss: 0.0137 - val_accuracy: 0.8677\n",
      "Epoch 24/50\n",
      "176/176 [==============================] - 0s 2ms/step - loss: 5.2129e-04 - accuracy: 0.9811 - val_loss: 0.0136 - val_accuracy: 0.8720\n",
      "Epoch 25/50\n",
      "176/176 [==============================] - 0s 2ms/step - loss: 4.4982e-04 - accuracy: 0.9804 - val_loss: 0.0140 - val_accuracy: 0.8762\n",
      "Epoch 26/50\n",
      "176/176 [==============================] - 0s 2ms/step - loss: 4.7346e-04 - accuracy: 0.9829 - val_loss: 0.0134 - val_accuracy: 0.8691\n",
      "Epoch 27/50\n",
      "176/176 [==============================] - 0s 2ms/step - loss: 4.5257e-04 - accuracy: 0.9808 - val_loss: 0.0139 - val_accuracy: 0.8762\n",
      "Epoch 28/50\n",
      "176/176 [==============================] - 0s 2ms/step - loss: 4.5395e-04 - accuracy: 0.9843 - val_loss: 0.0135 - val_accuracy: 0.8720\n",
      "Epoch 29/50\n",
      "176/176 [==============================] - 0s 2ms/step - loss: 4.3983e-04 - accuracy: 0.9793 - val_loss: 0.0137 - val_accuracy: 0.8691\n",
      "Epoch 30/50\n",
      "176/176 [==============================] - 0s 2ms/step - loss: 4.3103e-04 - accuracy: 0.9840 - val_loss: 0.0135 - val_accuracy: 0.8691\n",
      "Epoch 31/50\n",
      "176/176 [==============================] - 0s 2ms/step - loss: 4.1980e-04 - accuracy: 0.9854 - val_loss: 0.0135 - val_accuracy: 0.8691\n",
      "Epoch 32/50\n",
      "176/176 [==============================] - 0s 2ms/step - loss: 4.6750e-04 - accuracy: 0.9815 - val_loss: 0.0135 - val_accuracy: 0.8606\n",
      "Epoch 33/50\n",
      "176/176 [==============================] - 0s 2ms/step - loss: 5.0642e-04 - accuracy: 0.9790 - val_loss: 0.0134 - val_accuracy: 0.8592\n",
      "Epoch 34/50\n",
      "176/176 [==============================] - 0s 2ms/step - loss: 6.1837e-04 - accuracy: 0.9786 - val_loss: 0.0133 - val_accuracy: 0.8634\n",
      "Epoch 35/50\n",
      "176/176 [==============================] - 0s 2ms/step - loss: 7.8956e-04 - accuracy: 0.9712 - val_loss: 0.0142 - val_accuracy: 0.8606\n",
      "Epoch 36/50\n",
      "176/176 [==============================] - 0s 2ms/step - loss: 7.8370e-04 - accuracy: 0.9779 - val_loss: 0.0138 - val_accuracy: 0.8677\n",
      "Epoch 37/50\n",
      "176/176 [==============================] - 0s 2ms/step - loss: 6.6881e-04 - accuracy: 0.9801 - val_loss: 0.0138 - val_accuracy: 0.8777\n",
      "Epoch 38/50\n",
      "176/176 [==============================] - 0s 2ms/step - loss: 5.0200e-04 - accuracy: 0.9769 - val_loss: 0.0136 - val_accuracy: 0.8677\n",
      "Epoch 39/50\n",
      "176/176 [==============================] - 0s 2ms/step - loss: 3.8353e-04 - accuracy: 0.9843 - val_loss: 0.0134 - val_accuracy: 0.8734\n",
      "Epoch 40/50\n",
      "176/176 [==============================] - 0s 2ms/step - loss: 3.1629e-04 - accuracy: 0.9875 - val_loss: 0.0135 - val_accuracy: 0.8649\n",
      "Epoch 41/50\n",
      "176/176 [==============================] - 0s 2ms/step - loss: 2.9760e-04 - accuracy: 0.9897 - val_loss: 0.0134 - val_accuracy: 0.8620\n",
      "Epoch 42/50\n",
      "176/176 [==============================] - 0s 2ms/step - loss: 3.1430e-04 - accuracy: 0.9843 - val_loss: 0.0133 - val_accuracy: 0.8748\n",
      "Epoch 43/50\n",
      "176/176 [==============================] - 0s 2ms/step - loss: 3.0374e-04 - accuracy: 0.9843 - val_loss: 0.0132 - val_accuracy: 0.8720\n",
      "Epoch 44/50\n",
      "176/176 [==============================] - 0s 2ms/step - loss: 3.3499e-04 - accuracy: 0.9868 - val_loss: 0.0133 - val_accuracy: 0.8691\n",
      "Epoch 45/50\n",
      "176/176 [==============================] - 0s 2ms/step - loss: 3.5298e-04 - accuracy: 0.9861 - val_loss: 0.0131 - val_accuracy: 0.8706\n",
      "Epoch 46/50\n",
      "176/176 [==============================] - 0s 2ms/step - loss: 3.8091e-04 - accuracy: 0.9858 - val_loss: 0.0131 - val_accuracy: 0.8706\n",
      "Epoch 47/50\n",
      "176/176 [==============================] - 0s 2ms/step - loss: 4.0229e-04 - accuracy: 0.9850 - val_loss: 0.0133 - val_accuracy: 0.8748\n",
      "Epoch 48/50\n",
      "176/176 [==============================] - 0s 2ms/step - loss: 4.8326e-04 - accuracy: 0.9836 - val_loss: 0.0135 - val_accuracy: 0.8677\n",
      "Epoch 49/50\n",
      "176/176 [==============================] - 0s 2ms/step - loss: 5.0223e-04 - accuracy: 0.9850 - val_loss: 0.0137 - val_accuracy: 0.8649\n",
      "Epoch 50/50\n",
      "176/176 [==============================] - 0s 2ms/step - loss: 5.3725e-04 - accuracy: 0.9822 - val_loss: 0.0134 - val_accuracy: 0.8734\n"
     ]
    }
   ],
   "source": [
    "# Train the model\n",
    "history_01_A = model_01_A.fit(X_train_A_array, Y_train_A_array, epochs=50, batch_size=16, validation_split=0.2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " 1/37 [..............................] - ETA: 0s - loss: 0.0170 - accuracy: 0.8125"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "37/37 [==============================] - 0s 1ms/step - loss: 0.0146 - accuracy: 0.8838\n",
      "Model_01_A: Test Accuracy: 88.38%\n"
     ]
    }
   ],
   "source": [
    "# Evaluate the model\n",
    "loss_01_A, accuracy_01_A = model_01_A.evaluate(X_test_A_array, Y_test_A_array)\n",
    "print(f\"{model_01_A.name}: Test Accuracy: {accuracy_01_A * 100:.2f}%\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Applying the trained model to predict SCA for a distinct word:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Choosing an arbitrary word:\n",
    "entry = 'monster'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 79,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Getting its vector representation (from word embedding) and preparing it to the model input format:\n",
    "new_entry = nlp_getVector(entry)[1]\n",
    "new_entry = np.expand_dims(new_entry, axis=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 80,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[ 1.1305  , -1.5563  , -2.4944  , -2.024   , -1.5499  ,  2.395   ,\n",
       "         0.91659 ,  1.5593  , -0.32409 ,  3.3341  ,  2.9497  , -1.1545  ,\n",
       "         0.41695 ,  4.3341  ,  2.2508  , -0.35554 ,  4.7371  , -1.1894  ,\n",
       "         1.1579  ,  0.65003 ,  1.4585  ,  2.6705  , -0.27433 , -3.0269  ,\n",
       "         1.3221  ,  0.39813 , -2.5172  , -3.6463  ,  1.5057  ,  4.1713  ,\n",
       "        -1.1484  , -0.57279 ,  3.2361  , -0.98623 , -1.0243  , -1.5263  ,\n",
       "         1.067   ,  1.5488  ,  2.2951  ,  1.5047  , -0.72364 , -0.62793 ,\n",
       "         1.0891  ,  2.0448  , -0.78437 ,  0.063764, -2.2023  , -2.2536  ,\n",
       "         0.92767 ,  2.184   ,  0.35328 ,  2.0611  ,  2.7751  , -0.53082 ,\n",
       "        -0.069744,  0.10063 ,  0.75115 ,  0.10459 ,  0.75907 ,  2.3196  ,\n",
       "         1.7684  , -0.10156 , -2.3969  , -0.51817 ,  0.83881 ,  1.4263  ,\n",
       "        -4.3255  , -3.3691  ,  0.64677 , -1.3543  ,  0.4567  ,  2.7448  ,\n",
       "         0.10841 ,  0.99697 , -0.8093  , -1.0998  , -0.25594 ,  2.2797  ,\n",
       "         1.1398  , -0.62884 , -2.5255  , -2.6905  ,  0.36304 ,  2.2072  ,\n",
       "        -0.29771 , -3.9942  ,  3.3569  ,  1.1281  , -2.7332  , -0.32186 ,\n",
       "        -1.545   , -1.0737  ,  4.0648  , -1.2537  , -0.44874 ,  0.082098,\n",
       "         0.066379,  0.8873  ,  0.45497 , -1.369   ,  0.68211 ,  1.9595  ,\n",
       "         1.2504  ,  0.72418 , -2.4315  , -0.36966 , -1.0748  ,  2.6735  ,\n",
       "        -0.28612 , -0.97961 , -3.2151  ,  2.31    ,  0.22973 , -0.37868 ,\n",
       "        -0.67255 , -0.011135, -1.1273  , -0.96775 ,  2.5767  ,  1.0886  ,\n",
       "        -2.8112  , -0.57315 , -0.94201 ,  0.070089, -1.5448  , -1.2671  ,\n",
       "         1.8494  , -0.075647,  0.51177 ,  1.7711  , -0.44207 ,  0.89171 ,\n",
       "         0.16086 , -1.3117  , -1.7678  , -0.58707 , -0.28207 , -2.7216  ,\n",
       "        -0.1644  , -3.3008  , -3.8791  , -0.77481 , -0.64495 ,  0.1699  ,\n",
       "         2.8083  ,  1.2432  , -4.3077  ,  2.9253  , -0.58623 ,  0.83495 ,\n",
       "         2.197   ,  1.5698  , -0.87068 , -0.75874 , -2.9566  ,  1.0316  ,\n",
       "         4.547   ,  1.4039  ,  0.18047 ,  2.2761  , -0.23308 , -3.2644  ,\n",
       "        -1.8115  , -0.91208 , -3.3944  , -3.3258  , -0.99416 , -1.5298  ,\n",
       "         0.045789, -0.058987, -0.080757,  0.072613,  0.87138 , -1.0231  ,\n",
       "         1.1491  , -1.0426  , -2.0457  ,  1.8157  ,  0.25984 , -1.8087  ,\n",
       "        -1.1602  , -2.4917  ,  1.2827  ,  0.3939  , -2.4034  ,  0.79455 ,\n",
       "         0.82779 , -0.5307  ,  2.1268  , -3.6032  ,  1.5437  ,  1.4383  ,\n",
       "         0.94343 ,  0.58339 , -0.72413 , -0.81486 , -1.6317  ,  0.87058 ,\n",
       "        -1.4303  ,  1.8455  ,  1.4209  ,  0.51556 ,  0.69301 ,  2.1575  ,\n",
       "         0.93253 ,  0.31772 ,  0.94772 ,  1.0823  , -1.0416  , -1.6206  ,\n",
       "         1.7147  ,  0.90982 ,  0.35616 ,  0.069524, -0.1249  ,  1.4281  ,\n",
       "         1.1987  , -0.69507 , -1.0894  , -3.1731  ,  2.1274  , -1.2724  ,\n",
       "        -0.57508 ,  2.7835  , -3.6419  , -0.27661 ,  1.6699  ,  0.63669 ,\n",
       "         0.2546  ,  0.2383  ,  0.42105 , -2.8707  , -0.61649 ,  2.3545  ,\n",
       "        -0.53447 , -0.027006, -0.34414 ,  1.3818  , -1.7727  ,  0.58262 ,\n",
       "         0.72634 , -0.35486 ,  0.12951 ,  1.3553  , -1.6891  , -0.91424 ,\n",
       "        -0.55332 ,  1.034   ,  0.46562 ,  1.9132  ,  2.6207  , -0.8626  ,\n",
       "        -2.6333  ,  1.3578  , -2.9262  , -1.3675  ,  2.6229  ,  4.5005  ,\n",
       "        -0.39742 ,  1.3122  ,  1.3972  , -0.88239 ,  3.6921  , -0.73468 ,\n",
       "        -2.1917  , -0.48474 , -1.6221  , -1.9219  , -5.7649  ,  0.83702 ,\n",
       "         1.9281  , -1.3715  , -0.13242 ,  1.6834  ,  2.4138  , -0.54738 ,\n",
       "         1.4467  ,  0.45566 , -1.97    ,  3.7824  , -3.6635  , -2.9138  ,\n",
       "         1.6246  ,  1.1304  ,  4.4507  , -1.6033  , -3.9024  ,  0.32859 ,\n",
       "        -1.2553  ,  1.818   , -0.81985 , -2.5751  ,  3.258   , -0.2291  ,\n",
       "        -1.1937  ,  0.72715 , -0.49191 ,  3.2366  , -2.5627  , -0.14486 ]],\n",
       "      dtype=float32)"
      ]
     },
     "execution_count": 80,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "## Showing how this entry is laid out:\n",
    "new_entry"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 82,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1/1 [==============================] - 0s 27ms/step\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "array([[0.7135444, 0.708372 ]], dtype=float32)"
      ]
     },
     "execution_count": 82,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "## Predicting the objective and subjective loads for the 'entry' word:\n",
    "result = model_01_A.predict(new_entry)\n",
    "result"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 171,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Generalizing the process above for any word through a python method:\n",
    "def wordClassifier_regression(word, model=model_01_A):\n",
    "    '''Given a word vector, shows the probability for objective and subjective semantic content, respectively.'''\n",
    "    new_entry = nlp_getVector(word)\n",
    "    if new_entry:\n",
    "        vector = nlp_getVector(word)[1]\n",
    "        vector = np.expand_dims(vector, axis=0)\n",
    "        result = model.predict(vector)\n",
    "    else:\n",
    "        print('Word not existent in database.')\n",
    "        return\n",
    "    print(f'--- {word}:\\n{result[0][0]*100:.2f} of objectivity\\n{result[0][1]*100:.2f} of subjectivity')\n",
    "    print(f'Model used: {model.name}')  # Print the name of the model\n",
    "    return\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 84,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1/1 [==============================] - 0s 25ms/step\n",
      "--- hippopotamus:\n",
      "96.00 of objectivity\n",
      "34.07 of subjectivity\n",
      "Model used: Model_01_A\n"
     ]
    }
   ],
   "source": [
    "## Experimenting with a trial word:\n",
    "trial_word = 'hippopotamus'\n",
    "wordClassifier_regression(trial_word)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 85,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>words</th>\n",
       "      <th>F_Objectivity</th>\n",
       "      <th>F_Subjectivity</th>\n",
       "      <th>F_Context</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>2387</th>\n",
       "      <td>hippopotamus</td>\n",
       "      <td>0.973593</td>\n",
       "      <td>0.377959</td>\n",
       "      <td>0.309328</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "             words  F_Objectivity  F_Subjectivity  F_Context\n",
       "2387  hippopotamus       0.973593        0.377959   0.309328"
      ]
     },
     "execution_count": 85,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "## Checking the same trial word in the original SCA dataset (if available)\n",
    "df_factors[df_factors['words']==trial_word]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Training with data (C)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 160,
   "metadata": {},
   "outputs": [],
   "source": [
    "### Model 01: MLP architecture for continuous output:\n",
    "model_01_C = Sequential([\n",
    "    Dense(256, activation='relu', input_shape=(300,)),\n",
    "    Dense(128, activation='relu'),\n",
    "    Dense(64, activation='relu'),\n",
    "    Dense(32, activation='relu'),\n",
    "    # Dense(2, activation='linear')  # 2 neurons for continuous output between 0 and 1\n",
    "    Dense(2, activation='sigmoid')  # 2 neurons for continuous output between 0 and 1\n",
    "], name='Model_01_C')\n",
    "\n",
    "# Compile the model\n",
    "model_01_C.compile(optimizer='adam',\n",
    "              loss='mean_squared_error',\n",
    "              metrics=['accuracy'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 164,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"Model_01_C\"\n",
      "_________________________________________________________________\n",
      " Layer (type)                Output Shape              Param #   \n",
      "=================================================================\n",
      " dense_10 (Dense)            (None, 256)               77056     \n",
      "                                                                 \n",
      " dense_11 (Dense)            (None, 128)               32896     \n",
      "                                                                 \n",
      " dense_12 (Dense)            (None, 64)                8256      \n",
      "                                                                 \n",
      " dense_13 (Dense)            (None, 32)                2080      \n",
      "                                                                 \n",
      " dense_14 (Dense)            (None, 2)                 66        \n",
      "                                                                 \n",
      "=================================================================\n",
      "Total params: 120354 (470.13 KB)\n",
      "Trainable params: 120354 (470.13 KB)\n",
      "Non-trainable params: 0 (0.00 Byte)\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "# Print model summary\n",
    "model_01_C.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 165,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[0.15859087 0.58544867]\n",
      " [0.14240247 0.50945274]\n",
      " [0.95035619 0.17403995]\n",
      " [0.21539557 0.31870239]\n",
      " [0.25452163 0.66739412]]\n"
     ]
    }
   ],
   "source": [
    "## Transforming data into numpy array:\n",
    "Y_train_C_array = Y_train_C.to_numpy()\n",
    "Y_test_C_array = Y_test_C.to_numpy()\n",
    "\n",
    "X_train_C_array = X_train_C.to_numpy()\n",
    "X_test_C_array = X_test_C.to_numpy()\n",
    "\n",
    "# Print the first few elements to verify\n",
    "print(Y_train_C_array[:5])  # Print the first 5 elements\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 168,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/50\n",
      "  1/131 [..............................] - ETA: 0s - loss: 3.9342e-04 - accuracy: 1.0000"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "131/131 [==============================] - 0s 3ms/step - loss: 8.1216e-04 - accuracy: 0.9813 - val_loss: 0.0152 - val_accuracy: 0.9006\n",
      "Epoch 2/50\n",
      "131/131 [==============================] - 0s 2ms/step - loss: 8.0864e-04 - accuracy: 0.9837 - val_loss: 0.0154 - val_accuracy: 0.9101\n",
      "Epoch 3/50\n",
      "131/131 [==============================] - 0s 2ms/step - loss: 9.1933e-04 - accuracy: 0.9794 - val_loss: 0.0154 - val_accuracy: 0.9063\n",
      "Epoch 4/50\n",
      "131/131 [==============================] - 0s 2ms/step - loss: 9.3127e-04 - accuracy: 0.9813 - val_loss: 0.0152 - val_accuracy: 0.9044\n",
      "Epoch 5/50\n",
      "131/131 [==============================] - 0s 2ms/step - loss: 9.1377e-04 - accuracy: 0.9799 - val_loss: 0.0149 - val_accuracy: 0.9082\n",
      "Epoch 6/50\n",
      "131/131 [==============================] - 0s 2ms/step - loss: 0.0011 - accuracy: 0.9799 - val_loss: 0.0157 - val_accuracy: 0.9101\n",
      "Epoch 7/50\n",
      "131/131 [==============================] - 0s 2ms/step - loss: 9.6522e-04 - accuracy: 0.9847 - val_loss: 0.0156 - val_accuracy: 0.9101\n",
      "Epoch 8/50\n",
      "131/131 [==============================] - 0s 2ms/step - loss: 9.7821e-04 - accuracy: 0.9813 - val_loss: 0.0156 - val_accuracy: 0.9025\n",
      "Epoch 9/50\n",
      "131/131 [==============================] - 0s 2ms/step - loss: 9.1848e-04 - accuracy: 0.9823 - val_loss: 0.0152 - val_accuracy: 0.9063\n",
      "Epoch 10/50\n",
      "131/131 [==============================] - 0s 2ms/step - loss: 9.0768e-04 - accuracy: 0.9804 - val_loss: 0.0156 - val_accuracy: 0.9044\n",
      "Epoch 11/50\n",
      "131/131 [==============================] - 0s 2ms/step - loss: 7.5717e-04 - accuracy: 0.9818 - val_loss: 0.0153 - val_accuracy: 0.9025\n",
      "Epoch 12/50\n",
      "131/131 [==============================] - 0s 2ms/step - loss: 7.1458e-04 - accuracy: 0.9799 - val_loss: 0.0155 - val_accuracy: 0.9120\n",
      "Epoch 13/50\n",
      "131/131 [==============================] - 0s 2ms/step - loss: 6.2008e-04 - accuracy: 0.9818 - val_loss: 0.0151 - val_accuracy: 0.9044\n",
      "Epoch 14/50\n",
      "131/131 [==============================] - 0s 2ms/step - loss: 5.4307e-04 - accuracy: 0.9880 - val_loss: 0.0151 - val_accuracy: 0.9120\n",
      "Epoch 15/50\n",
      "131/131 [==============================] - 0s 2ms/step - loss: 5.3478e-04 - accuracy: 0.9861 - val_loss: 0.0150 - val_accuracy: 0.9063\n",
      "Epoch 16/50\n",
      "131/131 [==============================] - 0s 2ms/step - loss: 4.4289e-04 - accuracy: 0.9909 - val_loss: 0.0152 - val_accuracy: 0.9063\n",
      "Epoch 17/50\n",
      "131/131 [==============================] - 0s 2ms/step - loss: 4.0925e-04 - accuracy: 0.9866 - val_loss: 0.0150 - val_accuracy: 0.9082\n",
      "Epoch 18/50\n",
      "131/131 [==============================] - 0s 2ms/step - loss: 3.8983e-04 - accuracy: 0.9871 - val_loss: 0.0151 - val_accuracy: 0.8987\n",
      "Epoch 19/50\n",
      "131/131 [==============================] - 0s 2ms/step - loss: 4.0568e-04 - accuracy: 0.9904 - val_loss: 0.0152 - val_accuracy: 0.8987\n",
      "Epoch 20/50\n",
      "131/131 [==============================] - 0s 2ms/step - loss: 4.2408e-04 - accuracy: 0.9880 - val_loss: 0.0147 - val_accuracy: 0.9120\n",
      "Epoch 21/50\n",
      "131/131 [==============================] - 0s 2ms/step - loss: 4.5249e-04 - accuracy: 0.9880 - val_loss: 0.0155 - val_accuracy: 0.9063\n",
      "Epoch 22/50\n",
      "131/131 [==============================] - 0s 2ms/step - loss: 5.4145e-04 - accuracy: 0.9847 - val_loss: 0.0153 - val_accuracy: 0.9025\n",
      "Epoch 23/50\n",
      "131/131 [==============================] - 0s 2ms/step - loss: 6.2928e-04 - accuracy: 0.9832 - val_loss: 0.0160 - val_accuracy: 0.9101\n",
      "Epoch 24/50\n",
      "131/131 [==============================] - 0s 2ms/step - loss: 7.7555e-04 - accuracy: 0.9847 - val_loss: 0.0151 - val_accuracy: 0.9101\n",
      "Epoch 25/50\n",
      "131/131 [==============================] - 0s 2ms/step - loss: 9.6572e-04 - accuracy: 0.9808 - val_loss: 0.0157 - val_accuracy: 0.9025\n",
      "Epoch 26/50\n",
      "131/131 [==============================] - 0s 2ms/step - loss: 0.0012 - accuracy: 0.9751 - val_loss: 0.0155 - val_accuracy: 0.8967\n",
      "Epoch 27/50\n",
      "131/131 [==============================] - 0s 2ms/step - loss: 0.0012 - accuracy: 0.9789 - val_loss: 0.0156 - val_accuracy: 0.9025\n",
      "Epoch 28/50\n",
      "131/131 [==============================] - 0s 2ms/step - loss: 9.6077e-04 - accuracy: 0.9775 - val_loss: 0.0158 - val_accuracy: 0.9006\n",
      "Epoch 29/50\n",
      "131/131 [==============================] - 0s 2ms/step - loss: 7.6085e-04 - accuracy: 0.9823 - val_loss: 0.0150 - val_accuracy: 0.9025\n",
      "Epoch 30/50\n",
      "131/131 [==============================] - 0s 2ms/step - loss: 5.5002e-04 - accuracy: 0.9866 - val_loss: 0.0151 - val_accuracy: 0.9063\n",
      "Epoch 31/50\n",
      "131/131 [==============================] - 0s 2ms/step - loss: 4.0195e-04 - accuracy: 0.9856 - val_loss: 0.0150 - val_accuracy: 0.9006\n",
      "Epoch 32/50\n",
      "131/131 [==============================] - 0s 2ms/step - loss: 3.2041e-04 - accuracy: 0.9909 - val_loss: 0.0153 - val_accuracy: 0.9025\n",
      "Epoch 33/50\n",
      "131/131 [==============================] - 0s 2ms/step - loss: 2.5464e-04 - accuracy: 0.9899 - val_loss: 0.0149 - val_accuracy: 0.9063\n",
      "Epoch 34/50\n",
      "131/131 [==============================] - 0s 2ms/step - loss: 2.2912e-04 - accuracy: 0.9909 - val_loss: 0.0150 - val_accuracy: 0.9063\n",
      "Epoch 35/50\n",
      "131/131 [==============================] - 0s 2ms/step - loss: 2.4406e-04 - accuracy: 0.9943 - val_loss: 0.0151 - val_accuracy: 0.9101\n",
      "Epoch 36/50\n",
      "131/131 [==============================] - 0s 2ms/step - loss: 2.5510e-04 - accuracy: 0.9919 - val_loss: 0.0149 - val_accuracy: 0.9082\n",
      "Epoch 37/50\n",
      "131/131 [==============================] - 0s 2ms/step - loss: 2.5759e-04 - accuracy: 0.9890 - val_loss: 0.0149 - val_accuracy: 0.9025\n",
      "Epoch 38/50\n",
      "131/131 [==============================] - 0s 2ms/step - loss: 2.7480e-04 - accuracy: 0.9914 - val_loss: 0.0151 - val_accuracy: 0.9044\n",
      "Epoch 39/50\n",
      "131/131 [==============================] - 0s 2ms/step - loss: 2.8838e-04 - accuracy: 0.9923 - val_loss: 0.0148 - val_accuracy: 0.9006\n",
      "Epoch 40/50\n",
      "131/131 [==============================] - 0s 2ms/step - loss: 3.3806e-04 - accuracy: 0.9890 - val_loss: 0.0153 - val_accuracy: 0.9006\n",
      "Epoch 41/50\n",
      "131/131 [==============================] - 0s 2ms/step - loss: 4.7232e-04 - accuracy: 0.9847 - val_loss: 0.0152 - val_accuracy: 0.9159\n",
      "Epoch 42/50\n",
      "131/131 [==============================] - 0s 2ms/step - loss: 6.6299e-04 - accuracy: 0.9813 - val_loss: 0.0157 - val_accuracy: 0.9006\n",
      "Epoch 43/50\n",
      "131/131 [==============================] - 0s 2ms/step - loss: 9.0304e-04 - accuracy: 0.9813 - val_loss: 0.0162 - val_accuracy: 0.8967\n",
      "Epoch 44/50\n",
      "131/131 [==============================] - 0s 2ms/step - loss: 0.0010 - accuracy: 0.9775 - val_loss: 0.0156 - val_accuracy: 0.9025\n",
      "Epoch 45/50\n",
      "131/131 [==============================] - 0s 2ms/step - loss: 0.0011 - accuracy: 0.9828 - val_loss: 0.0152 - val_accuracy: 0.9120\n",
      "Epoch 46/50\n",
      "131/131 [==============================] - 0s 2ms/step - loss: 8.5454e-04 - accuracy: 0.9804 - val_loss: 0.0155 - val_accuracy: 0.9006\n",
      "Epoch 47/50\n",
      "131/131 [==============================] - 0s 2ms/step - loss: 5.7442e-04 - accuracy: 0.9866 - val_loss: 0.0151 - val_accuracy: 0.9044\n",
      "Epoch 48/50\n",
      "131/131 [==============================] - 0s 2ms/step - loss: 4.0028e-04 - accuracy: 0.9847 - val_loss: 0.0149 - val_accuracy: 0.9044\n",
      "Epoch 49/50\n",
      "131/131 [==============================] - 0s 2ms/step - loss: 3.1788e-04 - accuracy: 0.9914 - val_loss: 0.0154 - val_accuracy: 0.9063\n",
      "Epoch 50/50\n",
      "131/131 [==============================] - 0s 2ms/step - loss: 2.5897e-04 - accuracy: 0.9885 - val_loss: 0.0149 - val_accuracy: 0.9101\n"
     ]
    }
   ],
   "source": [
    "# Train the model\n",
    "history_01_C = model_01_C.fit(X_train_C_array, Y_train_C_array, epochs=50, batch_size=16, validation_split=0.2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 169,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " 1/28 [>.............................] - ETA: 0s - loss: 0.0161 - accuracy: 0.9375"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "28/28 [==============================] - 0s 1ms/step - loss: 0.0152 - accuracy: 0.9024\n",
      "Model_01_C: Test Accuracy: 90.24%\n"
     ]
    }
   ],
   "source": [
    "# Evaluate the model\n",
    "loss_01_C, accuracy_01_C = model_01_C.evaluate(X_test_C_array, Y_test_C_array)\n",
    "print(f\"{model_01_C.name}: Test Accuracy: {accuracy_01_C * 100:.2f}%\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Applying Model_01_C to predict SCA for a distinct word:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 172,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1/1 [==============================] - 0s 117ms/step\n",
      "--- child:\n",
      "43.83 of objectivity\n",
      "59.77 of subjectivity\n",
      "Model used: Model_01_A\n",
      "1/1 [==============================] - 0s 63ms/step\n",
      "--- child:\n",
      "90.79 of objectivity\n",
      "37.91 of subjectivity\n",
      "Model used: Model_01_C\n"
     ]
    }
   ],
   "source": [
    "## Experimenting with a trial word:\n",
    "trial_word = 'child'\n",
    "wordClassifier_regression(trial_word, model=model_01_A)\n",
    "wordClassifier_regression(trial_word, model=model_01_C)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 173,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>words</th>\n",
       "      <th>F_Objectivity</th>\n",
       "      <th>F_Subjectivity</th>\n",
       "      <th>F_Context</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>859</th>\n",
       "      <td>child</td>\n",
       "      <td>0.90202</td>\n",
       "      <td>0.380464</td>\n",
       "      <td>0.233721</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "     words  F_Objectivity  F_Subjectivity  F_Context\n",
       "859  child        0.90202        0.380464   0.233721"
      ]
     },
     "execution_count": 173,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "## Checking the same trial word in the original SCA dataset (if available)\n",
    "df_factors[df_factors['words']==trial_word]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "----\n",
    "## Model 02: MLP for multilabel classification:\n",
    "- Multilayer perceptron with a single multilabel output (softmax activation) based on SCA\n",
    "- Output: Objective (0) or Subjective (1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 174,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.preprocessing import OneHotEncoder"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Training with data (B)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 175,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Preparing data:\n",
    "encoder_oneHot_B = OneHotEncoder()\n",
    "Y_train_B_encoded = encoder_oneHot_B.fit_transform(Y_train_B[['target']]).toarray()\n",
    "Y_test_B_encoded = encoder_oneHot_B.fit_transform(Y_test_B[['target']]).toarray()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 98,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Transforming data into numpy array -- Y data was transformed above:\n",
    "X_train_B_array = X_train_B.to_numpy()\n",
    "X_test_B_array = X_test_B.to_numpy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 100,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_02_B = Sequential([\n",
    "    Dense(300, activation='relu', input_shape=(X_train_B.shape[1],)), ## Equivalent to input_shape=(300,)\n",
    "    Dense(150, activation='relu'),\n",
    "    Dense(100, activation='relu'),\n",
    "    Dense(64, activation='relu'),\n",
    "    Dense(32, activation='relu'),\n",
    "    Dense(Y_train_B_encoded.shape[1], activation='softmax')  # Output layer with softmax activation for multiclass classification\n",
    "], name='Model_02_B')\n",
    "\n",
    "# Compile the model\n",
    "model_02_B.compile(optimizer='adam',\n",
    "                 loss='categorical_crossentropy', \n",
    "                 metrics=['accuracy'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 101,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"Model_02_B\"\n",
      "_________________________________________________________________\n",
      " Layer (type)                Output Shape              Param #   \n",
      "=================================================================\n",
      " dense_20 (Dense)            (None, 300)               90300     \n",
      "                                                                 \n",
      " dense_21 (Dense)            (None, 150)               45150     \n",
      "                                                                 \n",
      " dense_22 (Dense)            (None, 100)               15100     \n",
      "                                                                 \n",
      " dense_23 (Dense)            (None, 64)                6464      \n",
      "                                                                 \n",
      " dense_24 (Dense)            (None, 32)                2080      \n",
      "                                                                 \n",
      " dense_25 (Dense)            (None, 4)                 132       \n",
      "                                                                 \n",
      "=================================================================\n",
      "Total params: 159226 (621.98 KB)\n",
      "Trainable params: 159226 (621.98 KB)\n",
      "Non-trainable params: 0 (0.00 Byte)\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "model_02_B.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 102,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/50\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "176/176 [==============================] - 2s 4ms/step - loss: 0.9746 - accuracy: 0.6136 - val_loss: 0.7849 - val_accuracy: 0.6899\n",
      "Epoch 2/50\n",
      "176/176 [==============================] - 0s 2ms/step - loss: 0.6570 - accuracy: 0.7411 - val_loss: 0.7562 - val_accuracy: 0.7255\n",
      "Epoch 3/50\n",
      "176/176 [==============================] - 0s 2ms/step - loss: 0.5152 - accuracy: 0.8006 - val_loss: 0.8036 - val_accuracy: 0.6984\n",
      "Epoch 4/50\n",
      "176/176 [==============================] - 1s 3ms/step - loss: 0.3889 - accuracy: 0.8554 - val_loss: 0.8191 - val_accuracy: 0.6842\n",
      "Epoch 5/50\n",
      "176/176 [==============================] - 0s 2ms/step - loss: 0.2957 - accuracy: 0.8928 - val_loss: 0.9064 - val_accuracy: 0.7240\n",
      "Epoch 6/50\n",
      "176/176 [==============================] - 0s 2ms/step - loss: 0.2074 - accuracy: 0.9220 - val_loss: 1.1871 - val_accuracy: 0.7013\n",
      "Epoch 7/50\n",
      "176/176 [==============================] - 0s 2ms/step - loss: 0.1972 - accuracy: 0.9341 - val_loss: 1.1690 - val_accuracy: 0.7013\n",
      "Epoch 8/50\n",
      "176/176 [==============================] - 0s 2ms/step - loss: 0.1343 - accuracy: 0.9566 - val_loss: 1.3812 - val_accuracy: 0.6515\n",
      "Epoch 9/50\n",
      "176/176 [==============================] - 0s 2ms/step - loss: 0.1659 - accuracy: 0.9473 - val_loss: 1.3691 - val_accuracy: 0.6785\n",
      "Epoch 10/50\n",
      "176/176 [==============================] - 1s 3ms/step - loss: 0.1054 - accuracy: 0.9676 - val_loss: 1.6635 - val_accuracy: 0.6842\n",
      "Epoch 11/50\n",
      "176/176 [==============================] - 0s 2ms/step - loss: 0.1162 - accuracy: 0.9630 - val_loss: 1.4590 - val_accuracy: 0.6942\n",
      "Epoch 12/50\n",
      "176/176 [==============================] - 0s 2ms/step - loss: 0.0669 - accuracy: 0.9776 - val_loss: 1.7211 - val_accuracy: 0.7055\n",
      "Epoch 13/50\n",
      "176/176 [==============================] - 0s 3ms/step - loss: 0.0302 - accuracy: 0.9922 - val_loss: 2.0410 - val_accuracy: 0.6956\n",
      "Epoch 14/50\n",
      "176/176 [==============================] - 1s 3ms/step - loss: 0.0874 - accuracy: 0.9772 - val_loss: 1.4803 - val_accuracy: 0.6728\n",
      "Epoch 15/50\n",
      "176/176 [==============================] - 0s 3ms/step - loss: 0.0813 - accuracy: 0.9754 - val_loss: 1.5654 - val_accuracy: 0.6984\n",
      "Epoch 16/50\n",
      "176/176 [==============================] - 0s 2ms/step - loss: 0.0658 - accuracy: 0.9811 - val_loss: 1.8172 - val_accuracy: 0.7112\n",
      "Epoch 17/50\n",
      "176/176 [==============================] - 0s 3ms/step - loss: 0.0754 - accuracy: 0.9779 - val_loss: 1.4903 - val_accuracy: 0.7055\n",
      "Epoch 18/50\n",
      "176/176 [==============================] - 0s 3ms/step - loss: 0.0428 - accuracy: 0.9893 - val_loss: 1.8251 - val_accuracy: 0.6942\n",
      "Epoch 19/50\n",
      "176/176 [==============================] - 1s 3ms/step - loss: 0.0553 - accuracy: 0.9850 - val_loss: 1.8943 - val_accuracy: 0.6913\n",
      "Epoch 20/50\n",
      "176/176 [==============================] - 0s 3ms/step - loss: 0.0981 - accuracy: 0.9744 - val_loss: 1.5845 - val_accuracy: 0.6999\n",
      "Epoch 21/50\n",
      "176/176 [==============================] - 0s 2ms/step - loss: 0.0344 - accuracy: 0.9904 - val_loss: 1.9574 - val_accuracy: 0.7055\n",
      "Epoch 22/50\n",
      "176/176 [==============================] - 0s 3ms/step - loss: 0.0177 - accuracy: 0.9954 - val_loss: 1.9718 - val_accuracy: 0.6942\n",
      "Epoch 23/50\n",
      "176/176 [==============================] - 0s 3ms/step - loss: 0.0540 - accuracy: 0.9829 - val_loss: 1.8805 - val_accuracy: 0.6543\n",
      "Epoch 24/50\n",
      "176/176 [==============================] - 1s 3ms/step - loss: 0.0765 - accuracy: 0.9751 - val_loss: 1.7082 - val_accuracy: 0.6871\n",
      "Epoch 25/50\n",
      "176/176 [==============================] - 0s 3ms/step - loss: 0.0443 - accuracy: 0.9858 - val_loss: 2.0249 - val_accuracy: 0.6785\n",
      "Epoch 26/50\n",
      "176/176 [==============================] - 0s 2ms/step - loss: 0.0387 - accuracy: 0.9915 - val_loss: 2.0525 - val_accuracy: 0.6970\n",
      "Epoch 27/50\n",
      "176/176 [==============================] - 1s 3ms/step - loss: 0.0466 - accuracy: 0.9879 - val_loss: 1.8579 - val_accuracy: 0.7070\n",
      "Epoch 28/50\n",
      "176/176 [==============================] - 0s 3ms/step - loss: 0.0565 - accuracy: 0.9858 - val_loss: 1.7324 - val_accuracy: 0.6942\n",
      "Epoch 29/50\n",
      "176/176 [==============================] - 0s 2ms/step - loss: 0.0214 - accuracy: 0.9925 - val_loss: 2.2455 - val_accuracy: 0.6842\n",
      "Epoch 30/50\n",
      "176/176 [==============================] - 0s 2ms/step - loss: 0.0351 - accuracy: 0.9900 - val_loss: 2.1618 - val_accuracy: 0.6899\n",
      "Epoch 31/50\n",
      "176/176 [==============================] - 0s 3ms/step - loss: 0.0652 - accuracy: 0.9815 - val_loss: 2.1820 - val_accuracy: 0.6856\n",
      "Epoch 32/50\n",
      "176/176 [==============================] - 0s 2ms/step - loss: 0.0247 - accuracy: 0.9932 - val_loss: 2.2655 - val_accuracy: 0.6956\n",
      "Epoch 33/50\n",
      "176/176 [==============================] - 0s 3ms/step - loss: 0.0098 - accuracy: 0.9964 - val_loss: 2.6560 - val_accuracy: 0.6842\n",
      "Epoch 34/50\n",
      "176/176 [==============================] - 0s 2ms/step - loss: 0.0426 - accuracy: 0.9900 - val_loss: 2.1601 - val_accuracy: 0.6970\n",
      "Epoch 35/50\n",
      "176/176 [==============================] - 0s 2ms/step - loss: 0.0669 - accuracy: 0.9804 - val_loss: 2.0704 - val_accuracy: 0.6657\n",
      "Epoch 36/50\n",
      "176/176 [==============================] - 0s 2ms/step - loss: 0.0704 - accuracy: 0.9797 - val_loss: 1.7852 - val_accuracy: 0.6885\n",
      "Epoch 37/50\n",
      "176/176 [==============================] - 0s 3ms/step - loss: 0.0239 - accuracy: 0.9936 - val_loss: 2.1783 - val_accuracy: 0.6899\n",
      "Epoch 38/50\n",
      "176/176 [==============================] - 0s 2ms/step - loss: 0.0098 - accuracy: 0.9979 - val_loss: 2.2458 - val_accuracy: 0.6927\n",
      "Epoch 39/50\n",
      "176/176 [==============================] - 0s 2ms/step - loss: 0.0040 - accuracy: 0.9993 - val_loss: 2.5197 - val_accuracy: 0.7070\n",
      "Epoch 40/50\n",
      "176/176 [==============================] - 0s 2ms/step - loss: 0.0242 - accuracy: 0.9939 - val_loss: 2.4161 - val_accuracy: 0.6842\n",
      "Epoch 41/50\n",
      "176/176 [==============================] - 0s 3ms/step - loss: 0.0330 - accuracy: 0.9918 - val_loss: 2.4492 - val_accuracy: 0.6999\n",
      "Epoch 42/50\n",
      "176/176 [==============================] - 0s 3ms/step - loss: 0.0658 - accuracy: 0.9840 - val_loss: 1.8501 - val_accuracy: 0.7070\n",
      "Epoch 43/50\n",
      "176/176 [==============================] - 0s 2ms/step - loss: 0.0402 - accuracy: 0.9886 - val_loss: 2.0219 - val_accuracy: 0.6970\n",
      "Epoch 44/50\n",
      "176/176 [==============================] - 0s 3ms/step - loss: 0.0411 - accuracy: 0.9875 - val_loss: 1.9785 - val_accuracy: 0.6942\n",
      "Epoch 45/50\n",
      "176/176 [==============================] - 0s 3ms/step - loss: 0.0428 - accuracy: 0.9900 - val_loss: 1.9494 - val_accuracy: 0.6970\n",
      "Epoch 46/50\n",
      "176/176 [==============================] - 0s 2ms/step - loss: 0.0193 - accuracy: 0.9968 - val_loss: 2.0828 - val_accuracy: 0.7027\n",
      "Epoch 47/50\n",
      "176/176 [==============================] - 0s 2ms/step - loss: 0.0078 - accuracy: 0.9989 - val_loss: 2.3737 - val_accuracy: 0.7070\n",
      "Epoch 48/50\n",
      "176/176 [==============================] - 0s 3ms/step - loss: 6.2974e-04 - accuracy: 1.0000 - val_loss: 2.5293 - val_accuracy: 0.7098\n",
      "Epoch 49/50\n",
      "176/176 [==============================] - 0s 3ms/step - loss: 2.1994e-04 - accuracy: 1.0000 - val_loss: 2.6487 - val_accuracy: 0.7084\n",
      "Epoch 50/50\n",
      "176/176 [==============================] - 0s 2ms/step - loss: 1.3899e-04 - accuracy: 1.0000 - val_loss: 2.7428 - val_accuracy: 0.7070\n"
     ]
    }
   ],
   "source": [
    "# Train the model\n",
    "history_02_B = model_02_B.fit(X_train_B_array, Y_train_B_encoded, epochs=50, batch_size=16, validation_split=0.2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 103,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " 1/37 [..............................] - ETA: 0s - loss: 4.2685 - accuracy: 0.6875"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "37/37 [==============================] - 0s 2ms/step - loss: 3.1055 - accuracy: 0.7009\n",
      "Model_02_B: Test Accuracy: 70.09%\n"
     ]
    }
   ],
   "source": [
    "# Evaluate the model\n",
    "loss_02_B, accuracy_02_B = model_02_B.evaluate(X_test_B_array, Y_test_B_encoded)\n",
    "print(f\"{model_02_B.name}: Test Accuracy: {accuracy_02_B * 100:.2f}%\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Training with data (D)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 104,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Preparing data:\n",
    "encoder_oneHot_D = OneHotEncoder()\n",
    "Y_train_D_encoded = encoder_oneHot_D.fit_transform(Y_train_D[['target']]).toarray()\n",
    "Y_test_D_encoded = encoder_oneHot_D.fit_transform(Y_test_D[['target']]).toarray()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 105,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Transforming data into numpy array -- Y data was transformed above:\n",
    "X_train_D_array = X_train_D.to_numpy()\n",
    "X_test_D_array = X_test_D.to_numpy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 106,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"Model_02_D\"\n",
      "_________________________________________________________________\n",
      " Layer (type)                Output Shape              Param #   \n",
      "=================================================================\n",
      " dense_26 (Dense)            (None, 300)               90300     \n",
      "                                                                 \n",
      " dense_27 (Dense)            (None, 150)               45150     \n",
      "                                                                 \n",
      " dense_28 (Dense)            (None, 100)               15100     \n",
      "                                                                 \n",
      " dense_29 (Dense)            (None, 64)                6464      \n",
      "                                                                 \n",
      " dense_30 (Dense)            (None, 32)                2080      \n",
      "                                                                 \n",
      " dense_31 (Dense)            (None, 4)                 132       \n",
      "                                                                 \n",
      "=================================================================\n",
      "Total params: 159226 (621.98 KB)\n",
      "Trainable params: 159226 (621.98 KB)\n",
      "Non-trainable params: 0 (0.00 Byte)\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "model_02_D = Sequential([\n",
    "    Dense(300, activation='relu', input_shape=(X_train_D.shape[1],)), ## Equivalent to input_shape=(300,)\n",
    "    Dense(150, activation='relu'),\n",
    "    Dense(100, activation='relu'),\n",
    "    Dense(64, activation='relu'),\n",
    "    Dense(32, activation='relu'),\n",
    "    Dense(Y_train_D_encoded.shape[1], activation='softmax')  # Output layer with softmax activation for multiclass classification\n",
    "], name='Model_02_D')\n",
    "\n",
    "# Compile the model\n",
    "model_02_D.compile(optimizer='adam',\n",
    "                 loss='categorical_crossentropy', \n",
    "                 metrics=['accuracy'])\n",
    "\n",
    "model_02_D.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 114,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/50\n",
      "  1/131 [..............................] - ETA: 0s - loss: 0.0000e+00 - accuracy: 1.0000"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "131/131 [==============================] - 0s 3ms/step - loss: 1.5415e-09 - accuracy: 1.0000 - val_loss: 4.0527 - val_accuracy: 0.7667\n",
      "Epoch 2/50\n",
      "131/131 [==============================] - 0s 3ms/step - loss: 1.5415e-09 - accuracy: 1.0000 - val_loss: 4.0534 - val_accuracy: 0.7667\n",
      "Epoch 3/50\n",
      "131/131 [==============================] - 0s 3ms/step - loss: 1.3702e-09 - accuracy: 1.0000 - val_loss: 4.0580 - val_accuracy: 0.7667\n",
      "Epoch 4/50\n",
      "131/131 [==============================] - 0s 2ms/step - loss: 1.2560e-09 - accuracy: 1.0000 - val_loss: 4.0643 - val_accuracy: 0.7667\n",
      "Epoch 5/50\n",
      "131/131 [==============================] - 0s 3ms/step - loss: 1.1419e-09 - accuracy: 1.0000 - val_loss: 4.0661 - val_accuracy: 0.7667\n",
      "Epoch 6/50\n",
      "131/131 [==============================] - 0s 3ms/step - loss: 1.1419e-09 - accuracy: 1.0000 - val_loss: 4.0653 - val_accuracy: 0.7667\n",
      "Epoch 7/50\n",
      "131/131 [==============================] - 0s 3ms/step - loss: 1.0277e-09 - accuracy: 1.0000 - val_loss: 4.0688 - val_accuracy: 0.7667\n",
      "Epoch 8/50\n",
      "131/131 [==============================] - 0s 2ms/step - loss: 1.0277e-09 - accuracy: 1.0000 - val_loss: 4.0738 - val_accuracy: 0.7667\n",
      "Epoch 9/50\n",
      "131/131 [==============================] - 0s 3ms/step - loss: 9.7057e-10 - accuracy: 1.0000 - val_loss: 4.0790 - val_accuracy: 0.7667\n",
      "Epoch 10/50\n",
      "131/131 [==============================] - 0s 2ms/step - loss: 7.4220e-10 - accuracy: 1.0000 - val_loss: 4.0820 - val_accuracy: 0.7667\n",
      "Epoch 11/50\n",
      "131/131 [==============================] - 0s 2ms/step - loss: 7.9930e-10 - accuracy: 1.0000 - val_loss: 4.0894 - val_accuracy: 0.7667\n",
      "Epoch 12/50\n",
      "131/131 [==============================] - 0s 3ms/step - loss: 6.2802e-10 - accuracy: 1.0000 - val_loss: 4.0897 - val_accuracy: 0.7667\n",
      "Epoch 13/50\n",
      "131/131 [==============================] - 0s 2ms/step - loss: 7.4220e-10 - accuracy: 1.0000 - val_loss: 4.0913 - val_accuracy: 0.7667\n",
      "Epoch 14/50\n",
      "131/131 [==============================] - 0s 3ms/step - loss: 7.4220e-10 - accuracy: 1.0000 - val_loss: 4.1001 - val_accuracy: 0.7686\n",
      "Epoch 15/50\n",
      "131/131 [==============================] - 0s 2ms/step - loss: 6.8511e-10 - accuracy: 1.0000 - val_loss: 4.1026 - val_accuracy: 0.7667\n",
      "Epoch 16/50\n",
      "131/131 [==============================] - 0s 2ms/step - loss: 6.2802e-10 - accuracy: 1.0000 - val_loss: 4.1083 - val_accuracy: 0.7686\n",
      "Epoch 17/50\n",
      "131/131 [==============================] - 0s 2ms/step - loss: 6.2802e-10 - accuracy: 1.0000 - val_loss: 4.1157 - val_accuracy: 0.7686\n",
      "Epoch 18/50\n",
      "131/131 [==============================] - 0s 2ms/step - loss: 4.5674e-10 - accuracy: 1.0000 - val_loss: 4.1252 - val_accuracy: 0.7686\n",
      "Epoch 19/50\n",
      "131/131 [==============================] - 0s 2ms/step - loss: 4.5674e-10 - accuracy: 1.0000 - val_loss: 4.1271 - val_accuracy: 0.7686\n",
      "Epoch 20/50\n",
      "131/131 [==============================] - 0s 2ms/step - loss: 4.5674e-10 - accuracy: 1.0000 - val_loss: 4.1355 - val_accuracy: 0.7686\n",
      "Epoch 21/50\n",
      "131/131 [==============================] - 0s 2ms/step - loss: 4.5674e-10 - accuracy: 1.0000 - val_loss: 4.1312 - val_accuracy: 0.7686\n",
      "Epoch 22/50\n",
      "131/131 [==============================] - 0s 3ms/step - loss: 3.4256e-10 - accuracy: 1.0000 - val_loss: 4.1373 - val_accuracy: 0.7686\n",
      "Epoch 23/50\n",
      "131/131 [==============================] - 0s 3ms/step - loss: 2.8546e-10 - accuracy: 1.0000 - val_loss: 4.1408 - val_accuracy: 0.7667\n",
      "Epoch 24/50\n",
      "131/131 [==============================] - 0s 3ms/step - loss: 3.4256e-10 - accuracy: 1.0000 - val_loss: 4.1447 - val_accuracy: 0.7667\n",
      "Epoch 25/50\n",
      "131/131 [==============================] - 0s 3ms/step - loss: 2.2837e-10 - accuracy: 1.0000 - val_loss: 4.1421 - val_accuracy: 0.7648\n",
      "Epoch 26/50\n",
      "131/131 [==============================] - 0s 3ms/step - loss: 2.8546e-10 - accuracy: 1.0000 - val_loss: 4.1492 - val_accuracy: 0.7667\n",
      "Epoch 27/50\n",
      "131/131 [==============================] - 0s 2ms/step - loss: 3.4256e-10 - accuracy: 1.0000 - val_loss: 4.1542 - val_accuracy: 0.7686\n",
      "Epoch 28/50\n",
      "131/131 [==============================] - 0s 2ms/step - loss: 1.7128e-10 - accuracy: 1.0000 - val_loss: 4.1531 - val_accuracy: 0.7686\n",
      "Epoch 29/50\n",
      "131/131 [==============================] - 0s 2ms/step - loss: 1.7128e-10 - accuracy: 1.0000 - val_loss: 4.1499 - val_accuracy: 0.7648\n",
      "Epoch 30/50\n",
      "131/131 [==============================] - 0s 3ms/step - loss: 2.8546e-10 - accuracy: 1.0000 - val_loss: 4.1536 - val_accuracy: 0.7667\n",
      "Epoch 31/50\n",
      "131/131 [==============================] - 0s 3ms/step - loss: 1.7128e-10 - accuracy: 1.0000 - val_loss: 4.1544 - val_accuracy: 0.7648\n",
      "Epoch 32/50\n",
      "131/131 [==============================] - 0s 3ms/step - loss: 1.7128e-10 - accuracy: 1.0000 - val_loss: 4.1587 - val_accuracy: 0.7667\n",
      "Epoch 33/50\n",
      "131/131 [==============================] - 0s 2ms/step - loss: 1.7128e-10 - accuracy: 1.0000 - val_loss: 4.1698 - val_accuracy: 0.7686\n",
      "Epoch 34/50\n",
      "131/131 [==============================] - 0s 3ms/step - loss: 2.8546e-10 - accuracy: 1.0000 - val_loss: 4.1688 - val_accuracy: 0.7648\n",
      "Epoch 35/50\n",
      "131/131 [==============================] - 0s 3ms/step - loss: 1.1419e-10 - accuracy: 1.0000 - val_loss: 4.1778 - val_accuracy: 0.7667\n",
      "Epoch 36/50\n",
      "131/131 [==============================] - 0s 3ms/step - loss: 1.1419e-10 - accuracy: 1.0000 - val_loss: 4.1847 - val_accuracy: 0.7667\n",
      "Epoch 37/50\n",
      "131/131 [==============================] - 0s 3ms/step - loss: 1.1419e-10 - accuracy: 1.0000 - val_loss: 4.1922 - val_accuracy: 0.7686\n",
      "Epoch 38/50\n",
      "131/131 [==============================] - 0s 3ms/step - loss: 1.1419e-10 - accuracy: 1.0000 - val_loss: 4.2008 - val_accuracy: 0.7686\n",
      "Epoch 39/50\n",
      "131/131 [==============================] - 0s 3ms/step - loss: 5.7093e-11 - accuracy: 1.0000 - val_loss: 4.2062 - val_accuracy: 0.7706\n",
      "Epoch 40/50\n",
      "131/131 [==============================] - 0s 3ms/step - loss: 1.1419e-10 - accuracy: 1.0000 - val_loss: 4.2117 - val_accuracy: 0.7706\n",
      "Epoch 41/50\n",
      "131/131 [==============================] - 0s 3ms/step - loss: 1.1419e-10 - accuracy: 1.0000 - val_loss: 4.2189 - val_accuracy: 0.7706\n",
      "Epoch 42/50\n",
      "131/131 [==============================] - 0s 3ms/step - loss: 5.7093e-11 - accuracy: 1.0000 - val_loss: 4.2278 - val_accuracy: 0.7725\n",
      "Epoch 43/50\n",
      "131/131 [==============================] - 0s 3ms/step - loss: 1.1419e-10 - accuracy: 1.0000 - val_loss: 4.2336 - val_accuracy: 0.7725\n",
      "Epoch 44/50\n",
      "131/131 [==============================] - 0s 2ms/step - loss: 1.7128e-10 - accuracy: 1.0000 - val_loss: 4.2400 - val_accuracy: 0.7706\n",
      "Epoch 45/50\n",
      "131/131 [==============================] - 0s 2ms/step - loss: 5.7093e-11 - accuracy: 1.0000 - val_loss: 4.2524 - val_accuracy: 0.7744\n",
      "Epoch 46/50\n",
      "131/131 [==============================] - 0s 3ms/step - loss: 1.1419e-10 - accuracy: 1.0000 - val_loss: 4.2561 - val_accuracy: 0.7706\n",
      "Epoch 47/50\n",
      "131/131 [==============================] - 0s 2ms/step - loss: 5.7093e-11 - accuracy: 1.0000 - val_loss: 4.2652 - val_accuracy: 0.7725\n",
      "Epoch 48/50\n",
      "131/131 [==============================] - 0s 2ms/step - loss: 1.7128e-10 - accuracy: 1.0000 - val_loss: 4.2864 - val_accuracy: 0.7725\n",
      "Epoch 49/50\n",
      "131/131 [==============================] - 0s 2ms/step - loss: 1.7128e-10 - accuracy: 1.0000 - val_loss: 4.3156 - val_accuracy: 0.7725\n",
      "Epoch 50/50\n",
      "131/131 [==============================] - 0s 3ms/step - loss: 1.7128e-10 - accuracy: 1.0000 - val_loss: 4.3178 - val_accuracy: 0.7725\n"
     ]
    }
   ],
   "source": [
    "# Train the model\n",
    "history_02_D = model_02_D.fit(X_train_D_array, Y_train_D_encoded, epochs=50, batch_size=16, validation_split=0.2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 115,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " 1/28 [>.............................] - ETA: 0s - loss: 7.1323 - accuracy: 0.6562"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "28/28 [==============================] - 0s 1ms/step - loss: 5.5330 - accuracy: 0.7520\n",
      "Model_02_D: Test Accuracy: 75.20%\n"
     ]
    }
   ],
   "source": [
    "# Evaluate the model\n",
    "loss_02_D, accuracy_02_D = model_02_D.evaluate(X_test_D_array, Y_test_D_encoded)\n",
    "print(f\"{model_02_D.name}: Test Accuracy: {accuracy_02_D * 100:.2f}%\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Exploring Model 02 results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 116,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Generalizing the process above for any word through a python method:\n",
    "def wordClassifier_multiclass(word, model, encoder):\n",
    "    '''Given a word vector, classifies its content as perceptual, manifest, contextual, or latent, according to SCA.\n",
    "    Usage example: wordClassifier_multiclass(word='trial', model=model_02_C, encoder=encoder_oneHot_C).\n",
    "    '''\n",
    "    new_entry = nlp_getVector(word)\n",
    "    if new_entry:\n",
    "        vector = nlp_getVector(word)[1]\n",
    "        vector = np.expand_dims(vector, axis=0)\n",
    "        result = model.predict(vector)\n",
    "        decoded_result = encoder.inverse_transform(result)\n",
    "    else:\n",
    "        print('Word not existent in database.')\n",
    "        return\n",
    "    print(f'--- SCA: \"{word}\" has {decoded_result[0][0]} content.')\n",
    "    print(f'Model used: {model.name}')  # Print the name of the model\n",
    "    return\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 117,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1/1 [==============================] - 0s 80ms/step\n",
      "--- SCA: \"happyness\" has Latent content.\n",
      "Model used: Model_02_D\n",
      "1/1 [==============================] - 0s 74ms/step\n",
      "--- SCA: \"happyness\" has Latent content.\n",
      "Model used: Model_02_B\n"
     ]
    }
   ],
   "source": [
    "## Experiment with a given word:\n",
    "wordClassifier_multiclass('happyness',model=model_02_D, encoder=encoder_oneHot_D)\n",
    "wordClassifier_multiclass('happyness',model=model_02_B, encoder=encoder_oneHot_B)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 118,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1/1 [==============================] - 0s 24ms/step\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "--- SCA: \"imagination\" has Latent content.\n",
      "Model used: Model_02_D\n",
      "1/1 [==============================] - 0s 22ms/step\n",
      "--- SCA: \"imagination\" has Latent content.\n",
      "Model used: Model_02_B\n",
      "\n",
      "-------------------------\n",
      "1/1 [==============================] - 0s 31ms/step\n",
      "--- SCA: \"care\" has Latent content.\n",
      "Model used: Model_02_D\n",
      "1/1 [==============================] - 0s 22ms/step\n",
      "--- SCA: \"care\" has Latent content.\n",
      "Model used: Model_02_B\n",
      "\n",
      "-------------------------\n",
      "1/1 [==============================] - 0s 21ms/step\n",
      "--- SCA: \"fear\" has Latent content.\n",
      "Model used: Model_02_D\n",
      "1/1 [==============================] - 0s 21ms/step\n",
      "--- SCA: \"fear\" has Latent content.\n",
      "Model used: Model_02_B\n",
      "\n",
      "-------------------------\n",
      "1/1 [==============================] - 0s 50ms/step\n",
      "--- SCA: \"play\" has Perceptual content.\n",
      "Model used: Model_02_D\n",
      "1/1 [==============================] - 0s 23ms/step\n",
      "--- SCA: \"play\" has Perceptual content.\n",
      "Model used: Model_02_B\n",
      "\n",
      "-------------------------\n",
      "1/1 [==============================] - 0s 20ms/step\n",
      "--- SCA: \"protection\" has Latent content.\n",
      "Model used: Model_02_D\n",
      "1/1 [==============================] - 0s 21ms/step\n",
      "--- SCA: \"protection\" has Latent content.\n",
      "Model used: Model_02_B\n",
      "\n",
      "-------------------------\n",
      "1/1 [==============================] - 0s 24ms/step\n",
      "--- SCA: \"monster\" has Perceptual content.\n",
      "Model used: Model_02_D\n",
      "1/1 [==============================] - 0s 23ms/step\n",
      "--- SCA: \"monster\" has Perceptual content.\n",
      "Model used: Model_02_B\n",
      "\n",
      "-------------------------\n",
      "1/1 [==============================] - 0s 21ms/step\n",
      "--- SCA: \"Mom\" has Perceptual content.\n",
      "Model used: Model_02_D\n",
      "1/1 [==============================] - 0s 22ms/step\n",
      "--- SCA: \"Mom\" has Perceptual content.\n",
      "Model used: Model_02_B\n",
      "\n",
      "-------------------------\n",
      "1/1 [==============================] - 0s 21ms/step\n",
      "--- SCA: \"Dad\" has Perceptual content.\n",
      "Model used: Model_02_D\n",
      "1/1 [==============================] - 0s 21ms/step\n",
      "--- SCA: \"Dad\" has Perceptual content.\n",
      "Model used: Model_02_B\n",
      "\n",
      "-------------------------\n",
      "1/1 [==============================] - 0s 21ms/step\n",
      "--- SCA: \"toy\" has Perceptual content.\n",
      "Model used: Model_02_D\n",
      "1/1 [==============================] - 0s 21ms/step\n",
      "--- SCA: \"toy\" has Perceptual content.\n",
      "Model used: Model_02_B\n",
      "\n",
      "-------------------------\n",
      "1/1 [==============================] - 0s 20ms/step\n",
      "--- SCA: \"child\" has Manifest content.\n",
      "Model used: Model_02_D\n",
      "1/1 [==============================] - 0s 30ms/step\n",
      "--- SCA: \"child\" has Manifest content.\n",
      "Model used: Model_02_B\n",
      "\n",
      "-------------------------\n",
      "1/1 [==============================] - 0s 27ms/step\n",
      "--- SCA: \"doll\" has Manifest content.\n",
      "Model used: Model_02_D\n",
      "1/1 [==============================] - 0s 50ms/step\n",
      "--- SCA: \"doll\" has Manifest content.\n",
      "Model used: Model_02_B\n",
      "\n",
      "-------------------------\n"
     ]
    }
   ],
   "source": [
    "## Checking words classification based on the sequence explored in the PhD Thesis.\n",
    "seq_words_A = ['imagination','care','fear','play','protection','monster','Mom','Dad','toy','child','doll']\n",
    "\n",
    "for word in seq_words_A:\n",
    "    wordClassifier_multiclass(word,model=model_02_D, encoder=encoder_oneHot_D)\n",
    "    # print('\\n')\n",
    "    wordClassifier_multiclass(word,model=model_02_B, encoder=encoder_oneHot_B)\n",
    "    print(f'\\n{25*\"-\"}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 119,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1/1 [==============================] - 0s 19ms/step\n",
      "--- SCA: \"pleasure\" has Latent content.\n",
      "Model used: Model_02_D\n",
      "1/1 [==============================] - 0s 20ms/step\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "--- SCA: \"pleasure\" has Latent content.\n",
      "Model used: Model_02_B\n",
      "\n",
      "-------------------------\n",
      "1/1 [==============================] - 0s 23ms/step\n",
      "--- SCA: \"love\" has Latent content.\n",
      "Model used: Model_02_D\n",
      "1/1 [==============================] - 0s 22ms/step\n",
      "--- SCA: \"love\" has Latent content.\n",
      "Model used: Model_02_B\n",
      "\n",
      "-------------------------\n",
      "1/1 [==============================] - 0s 24ms/step\n",
      "--- SCA: \"anxiety\" has Latent content.\n",
      "Model used: Model_02_D\n",
      "1/1 [==============================] - 0s 24ms/step\n",
      "--- SCA: \"anxiety\" has Latent content.\n",
      "Model used: Model_02_B\n",
      "\n",
      "-------------------------\n",
      "1/1 [==============================] - 0s 21ms/step\n",
      "--- SCA: \"suffering\" has Latent content.\n",
      "Model used: Model_02_D\n",
      "1/1 [==============================] - 0s 25ms/step\n",
      "--- SCA: \"suffering\" has Latent content.\n",
      "Model used: Model_02_B\n",
      "\n",
      "-------------------------\n",
      "1/1 [==============================] - 0s 25ms/step\n",
      "--- SCA: \"touch\" has Latent content.\n",
      "Model used: Model_02_D\n",
      "1/1 [==============================] - 0s 21ms/step\n",
      "--- SCA: \"touch\" has Perceptual content.\n",
      "Model used: Model_02_B\n",
      "\n",
      "-------------------------\n",
      "1/1 [==============================] - 0s 28ms/step\n",
      "--- SCA: \"hug\" has Perceptual content.\n",
      "Model used: Model_02_D\n",
      "1/1 [==============================] - 0s 24ms/step\n",
      "--- SCA: \"hug\" has Perceptual content.\n",
      "Model used: Model_02_B\n",
      "\n",
      "-------------------------\n",
      "1/1 [==============================] - 0s 24ms/step\n",
      "--- SCA: \"cuddle\" has Perceptual content.\n",
      "Model used: Model_02_D\n",
      "1/1 [==============================] - 0s 25ms/step\n",
      "--- SCA: \"cuddle\" has Perceptual content.\n",
      "Model used: Model_02_B\n",
      "\n",
      "-------------------------\n",
      "1/1 [==============================] - 0s 23ms/step\n",
      "--- SCA: \"wound\" has Perceptual content.\n",
      "Model used: Model_02_D\n",
      "1/1 [==============================] - 0s 22ms/step\n",
      "--- SCA: \"wound\" has Perceptual content.\n",
      "Model used: Model_02_B\n",
      "\n",
      "-------------------------\n",
      "1/1 [==============================] - 0s 25ms/step\n",
      "--- SCA: \"arms\" has Perceptual content.\n",
      "Model used: Model_02_D\n",
      "1/1 [==============================] - 0s 35ms/step\n",
      "--- SCA: \"arms\" has Perceptual content.\n",
      "Model used: Model_02_B\n",
      "\n",
      "-------------------------\n",
      "1/1 [==============================] - 0s 21ms/step\n",
      "--- SCA: \"hand\" has Manifest content.\n",
      "Model used: Model_02_D\n",
      "1/1 [==============================] - 0s 30ms/step\n",
      "--- SCA: \"hand\" has Manifest content.\n",
      "Model used: Model_02_B\n",
      "\n",
      "-------------------------\n"
     ]
    }
   ],
   "source": [
    "## Checking words classification based on the sequence explored in the PhD Thesis.\n",
    "seq_words_B = ['pleasure','love','anxiety','suffering','touch','hug','cuddle','wound','arms','hand']\n",
    "\n",
    "for word in seq_words_B:\n",
    "    wordClassifier_multiclass(word,model=model_02_D, encoder=encoder_oneHot_D)\n",
    "    # print('\\n')\n",
    "    wordClassifier_multiclass(word,model=model_02_B, encoder=encoder_oneHot_B)\n",
    "    print(f'\\n{25*\"-\"}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "----\n",
    "## Saving and loading Keras trained models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 120,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\tiago\\OneDrive - UNIVALI\\PhD\\atividades de pesquisa\\semantic_similarity\\.venv\\Lib\\site-packages\\keras\\src\\engine\\training.py:3103: UserWarning: You are saving your model as an HDF5 file via `model.save()`. This file format is considered legacy. We recommend using instead the native Keras format, e.g. `model.save('my_model.keras')`.\n",
      "  saving_api.save_model(\n"
     ]
    }
   ],
   "source": [
    "## Saving Keras models for posterior use:\n",
    "model_01_A.save('../models/model_01_A.h5')\n",
    "model_01_C.save('../models/model_01_C.h5')\n",
    "model_02_B.save('../models/model_02_B.h5')\n",
    "model_02_D.save('../models/model_02_D.h5')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 123,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Saving Scklearn models (e.g., OneHot Encoders)\n",
    "with open('../models/encoder_oneHot_B.pickle', 'wb') as f:\n",
    "    pickle.dump(encoder_oneHot_B, f)\n",
    "\n",
    "with open('../models/encoder_oneHot_D.pickle', 'wb') as f:\n",
    "    pickle.dump(encoder_oneHot_D, f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 124,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Loading saved models:\n",
    "from keras.models import load_model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 125,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = load_model('../models/model_02_B.h5')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 126,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Loading saved encoder:\n",
    "with open('../models/encoder_oneHot_B.pickle', 'rb') as f:\n",
    "    encoder = pickle.load(f)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Trying loaded model and encoder:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 127,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1/1 [==============================] - 0s 76ms/step\n",
      "--- SCA: \"debug\" has Contextual content.\n",
      "Model used: Model_02_B\n"
     ]
    }
   ],
   "source": [
    "wordClassifier_multiclass('debug',model=model, encoder=encoder)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "## Pycaret: exploring outperforming models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 302,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pycaret.classification"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 303,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>0</th>\n",
       "      <th>1</th>\n",
       "      <th>2</th>\n",
       "      <th>3</th>\n",
       "      <th>4</th>\n",
       "      <th>5</th>\n",
       "      <th>6</th>\n",
       "      <th>7</th>\n",
       "      <th>8</th>\n",
       "      <th>9</th>\n",
       "      <th>...</th>\n",
       "      <th>291</th>\n",
       "      <th>292</th>\n",
       "      <th>293</th>\n",
       "      <th>294</th>\n",
       "      <th>295</th>\n",
       "      <th>296</th>\n",
       "      <th>297</th>\n",
       "      <th>298</th>\n",
       "      <th>299</th>\n",
       "      <th>target</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>garment</th>\n",
       "      <td>-2.44960</td>\n",
       "      <td>-0.709460</td>\n",
       "      <td>-1.4459</td>\n",
       "      <td>2.7438</td>\n",
       "      <td>2.19640</td>\n",
       "      <td>0.62562</td>\n",
       "      <td>1.3016</td>\n",
       "      <td>2.0838</td>\n",
       "      <td>-2.2201</td>\n",
       "      <td>-2.2038</td>\n",
       "      <td>...</td>\n",
       "      <td>3.413000</td>\n",
       "      <td>0.53852</td>\n",
       "      <td>-0.15337</td>\n",
       "      <td>-4.65310</td>\n",
       "      <td>0.54157</td>\n",
       "      <td>1.41030</td>\n",
       "      <td>-4.592600</td>\n",
       "      <td>0.43055</td>\n",
       "      <td>0.35458</td>\n",
       "      <td>Manifest</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>oven</th>\n",
       "      <td>-2.32870</td>\n",
       "      <td>1.487700</td>\n",
       "      <td>-1.1173</td>\n",
       "      <td>4.4829</td>\n",
       "      <td>1.85580</td>\n",
       "      <td>1.05840</td>\n",
       "      <td>1.8389</td>\n",
       "      <td>-2.3667</td>\n",
       "      <td>-6.2502</td>\n",
       "      <td>3.5786</td>\n",
       "      <td>...</td>\n",
       "      <td>0.578500</td>\n",
       "      <td>-2.85220</td>\n",
       "      <td>-4.87860</td>\n",
       "      <td>-0.81042</td>\n",
       "      <td>-1.06000</td>\n",
       "      <td>1.10440</td>\n",
       "      <td>0.489910</td>\n",
       "      <td>2.73810</td>\n",
       "      <td>3.74540</td>\n",
       "      <td>Manifest</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>landscape</th>\n",
       "      <td>-0.63003</td>\n",
       "      <td>0.041465</td>\n",
       "      <td>-2.3347</td>\n",
       "      <td>-4.7172</td>\n",
       "      <td>0.10905</td>\n",
       "      <td>-1.18150</td>\n",
       "      <td>3.3926</td>\n",
       "      <td>3.1072</td>\n",
       "      <td>-2.1309</td>\n",
       "      <td>1.3538</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.072589</td>\n",
       "      <td>1.17210</td>\n",
       "      <td>0.39403</td>\n",
       "      <td>0.82975</td>\n",
       "      <td>-0.17167</td>\n",
       "      <td>-0.54614</td>\n",
       "      <td>0.039231</td>\n",
       "      <td>-4.14750</td>\n",
       "      <td>0.87988</td>\n",
       "      <td>Perceptual</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>3 rows × 301 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                 0         1       2       3        4        5       6       7       8       9  ...       291      292      293      294      295      296       297      298      299      target\n",
       "garment   -2.44960 -0.709460 -1.4459  2.7438  2.19640  0.62562  1.3016  2.0838 -2.2201 -2.2038  ...  3.413000  0.53852 -0.15337 -4.65310  0.54157  1.41030 -4.592600  0.43055  0.35458    Manifest\n",
       "oven      -2.32870  1.487700 -1.1173  4.4829  1.85580  1.05840  1.8389 -2.3667 -6.2502  3.5786  ...  0.578500 -2.85220 -4.87860 -0.81042 -1.06000  1.10440  0.489910  2.73810  3.74540    Manifest\n",
       "landscape -0.63003  0.041465 -2.3347 -4.7172  0.10905 -1.18150  3.3926  3.1072 -2.1309  1.3538  ... -0.072589  1.17210  0.39403  0.82975 -0.17167 -0.54614  0.039231 -4.14750  0.87988  Perceptual\n",
       "\n",
       "[3 rows x 301 columns]"
      ]
     },
     "execution_count": 303,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_pycaret = pd.concat([X_train_B, Y_train_B], axis=1)\n",
    "df_pycaret.sample(3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 304,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<style type=\"text/css\">\n",
       "#T_f2a58_row9_col1 {\n",
       "  background-color: lightgreen;\n",
       "}\n",
       "</style>\n",
       "<table id=\"T_f2a58\">\n",
       "  <thead>\n",
       "    <tr>\n",
       "      <th class=\"blank level0\" >&nbsp;</th>\n",
       "      <th id=\"T_f2a58_level0_col0\" class=\"col_heading level0 col0\" >Description</th>\n",
       "      <th id=\"T_f2a58_level0_col1\" class=\"col_heading level0 col1\" >Value</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th id=\"T_f2a58_level0_row0\" class=\"row_heading level0 row0\" >0</th>\n",
       "      <td id=\"T_f2a58_row0_col0\" class=\"data row0 col0\" >Session id</td>\n",
       "      <td id=\"T_f2a58_row0_col1\" class=\"data row0 col1\" >9088</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th id=\"T_f2a58_level0_row1\" class=\"row_heading level0 row1\" >1</th>\n",
       "      <td id=\"T_f2a58_row1_col0\" class=\"data row1 col0\" >Target</td>\n",
       "      <td id=\"T_f2a58_row1_col1\" class=\"data row1 col1\" >target</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th id=\"T_f2a58_level0_row2\" class=\"row_heading level0 row2\" >2</th>\n",
       "      <td id=\"T_f2a58_row2_col0\" class=\"data row2 col0\" >Target type</td>\n",
       "      <td id=\"T_f2a58_row2_col1\" class=\"data row2 col1\" >Multiclass</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th id=\"T_f2a58_level0_row3\" class=\"row_heading level0 row3\" >3</th>\n",
       "      <td id=\"T_f2a58_row3_col0\" class=\"data row3 col0\" >Target mapping</td>\n",
       "      <td id=\"T_f2a58_row3_col1\" class=\"data row3 col1\" >Contextual: 0, Latent: 1, Manifest: 2, Perceptual: 3</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th id=\"T_f2a58_level0_row4\" class=\"row_heading level0 row4\" >4</th>\n",
       "      <td id=\"T_f2a58_row4_col0\" class=\"data row4 col0\" >Original data shape</td>\n",
       "      <td id=\"T_f2a58_row4_col1\" class=\"data row4 col1\" >(3511, 301)</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th id=\"T_f2a58_level0_row5\" class=\"row_heading level0 row5\" >5</th>\n",
       "      <td id=\"T_f2a58_row5_col0\" class=\"data row5 col0\" >Transformed data shape</td>\n",
       "      <td id=\"T_f2a58_row5_col1\" class=\"data row5 col1\" >(3511, 301)</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th id=\"T_f2a58_level0_row6\" class=\"row_heading level0 row6\" >6</th>\n",
       "      <td id=\"T_f2a58_row6_col0\" class=\"data row6 col0\" >Transformed train set shape</td>\n",
       "      <td id=\"T_f2a58_row6_col1\" class=\"data row6 col1\" >(2457, 301)</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th id=\"T_f2a58_level0_row7\" class=\"row_heading level0 row7\" >7</th>\n",
       "      <td id=\"T_f2a58_row7_col0\" class=\"data row7 col0\" >Transformed test set shape</td>\n",
       "      <td id=\"T_f2a58_row7_col1\" class=\"data row7 col1\" >(1054, 301)</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th id=\"T_f2a58_level0_row8\" class=\"row_heading level0 row8\" >8</th>\n",
       "      <td id=\"T_f2a58_row8_col0\" class=\"data row8 col0\" >Numeric features</td>\n",
       "      <td id=\"T_f2a58_row8_col1\" class=\"data row8 col1\" >300</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th id=\"T_f2a58_level0_row9\" class=\"row_heading level0 row9\" >9</th>\n",
       "      <td id=\"T_f2a58_row9_col0\" class=\"data row9 col0\" >Preprocess</td>\n",
       "      <td id=\"T_f2a58_row9_col1\" class=\"data row9 col1\" >True</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th id=\"T_f2a58_level0_row10\" class=\"row_heading level0 row10\" >10</th>\n",
       "      <td id=\"T_f2a58_row10_col0\" class=\"data row10 col0\" >Imputation type</td>\n",
       "      <td id=\"T_f2a58_row10_col1\" class=\"data row10 col1\" >simple</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th id=\"T_f2a58_level0_row11\" class=\"row_heading level0 row11\" >11</th>\n",
       "      <td id=\"T_f2a58_row11_col0\" class=\"data row11 col0\" >Numeric imputation</td>\n",
       "      <td id=\"T_f2a58_row11_col1\" class=\"data row11 col1\" >mean</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th id=\"T_f2a58_level0_row12\" class=\"row_heading level0 row12\" >12</th>\n",
       "      <td id=\"T_f2a58_row12_col0\" class=\"data row12 col0\" >Categorical imputation</td>\n",
       "      <td id=\"T_f2a58_row12_col1\" class=\"data row12 col1\" >mode</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th id=\"T_f2a58_level0_row13\" class=\"row_heading level0 row13\" >13</th>\n",
       "      <td id=\"T_f2a58_row13_col0\" class=\"data row13 col0\" >Fold Generator</td>\n",
       "      <td id=\"T_f2a58_row13_col1\" class=\"data row13 col1\" >StratifiedKFold</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th id=\"T_f2a58_level0_row14\" class=\"row_heading level0 row14\" >14</th>\n",
       "      <td id=\"T_f2a58_row14_col0\" class=\"data row14 col0\" >Fold Number</td>\n",
       "      <td id=\"T_f2a58_row14_col1\" class=\"data row14 col1\" >10</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th id=\"T_f2a58_level0_row15\" class=\"row_heading level0 row15\" >15</th>\n",
       "      <td id=\"T_f2a58_row15_col0\" class=\"data row15 col0\" >CPU Jobs</td>\n",
       "      <td id=\"T_f2a58_row15_col1\" class=\"data row15 col1\" >-1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th id=\"T_f2a58_level0_row16\" class=\"row_heading level0 row16\" >16</th>\n",
       "      <td id=\"T_f2a58_row16_col0\" class=\"data row16 col0\" >Use GPU</td>\n",
       "      <td id=\"T_f2a58_row16_col1\" class=\"data row16 col1\" >False</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th id=\"T_f2a58_level0_row17\" class=\"row_heading level0 row17\" >17</th>\n",
       "      <td id=\"T_f2a58_row17_col0\" class=\"data row17 col0\" >Log Experiment</td>\n",
       "      <td id=\"T_f2a58_row17_col1\" class=\"data row17 col1\" >False</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th id=\"T_f2a58_level0_row18\" class=\"row_heading level0 row18\" >18</th>\n",
       "      <td id=\"T_f2a58_row18_col0\" class=\"data row18 col0\" >Experiment Name</td>\n",
       "      <td id=\"T_f2a58_row18_col1\" class=\"data row18 col1\" >clf-default-name</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th id=\"T_f2a58_level0_row19\" class=\"row_heading level0 row19\" >19</th>\n",
       "      <td id=\"T_f2a58_row19_col0\" class=\"data row19 col0\" >USI</td>\n",
       "      <td id=\"T_f2a58_row19_col1\" class=\"data row19 col1\" >ae3f</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n"
      ],
      "text/plain": [
       "<pandas.io.formats.style.Styler at 0x20e4a392a90>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "## Fazendo um experimento:\n",
    "exp_class = pycaret.classification.setup(df_pycaret, target='target', session_id=9088)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 305,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>Initiated</th>\n",
       "      <td>. . . . . . . . . . . . . . . . . .</td>\n",
       "      <td>21:55:05</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Status</th>\n",
       "      <td>. . . . . . . . . . . . . . . . . .</td>\n",
       "      <td>Fitting 10 Folds</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Estimator</th>\n",
       "      <td>. . . . . . . . . . . . . . . . . .</td>\n",
       "      <td>Logistic Regression</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                                   \n",
       "                                                                   \n",
       "Initiated  . . . . . . . . . . . . . . . . . .             21:55:05\n",
       "Status     . . . . . . . . . . . . . . . . . .     Fitting 10 Folds\n",
       "Estimator  . . . . . . . . . . . . . . . . . .  Logistic Regression"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<style type=\"text/css\">\n",
       "#T_e8219 th {\n",
       "  text-align: left;\n",
       "}\n",
       "#T_e8219_row0_col0, #T_e8219_row0_col2, #T_e8219_row0_col4, #T_e8219_row0_col5, #T_e8219_row0_col6, #T_e8219_row0_col7, #T_e8219_row1_col0, #T_e8219_row1_col1, #T_e8219_row1_col2, #T_e8219_row1_col3, #T_e8219_row2_col0, #T_e8219_row2_col1, #T_e8219_row2_col3, #T_e8219_row2_col4, #T_e8219_row2_col5, #T_e8219_row2_col6, #T_e8219_row2_col7, #T_e8219_row3_col0, #T_e8219_row3_col1, #T_e8219_row3_col2, #T_e8219_row3_col3, #T_e8219_row3_col4, #T_e8219_row3_col5, #T_e8219_row3_col6, #T_e8219_row3_col7, #T_e8219_row4_col0, #T_e8219_row4_col1, #T_e8219_row4_col2, #T_e8219_row4_col3, #T_e8219_row4_col4, #T_e8219_row4_col5, #T_e8219_row4_col6, #T_e8219_row4_col7, #T_e8219_row5_col0, #T_e8219_row5_col1, #T_e8219_row5_col2, #T_e8219_row5_col3, #T_e8219_row5_col4, #T_e8219_row5_col5, #T_e8219_row5_col6, #T_e8219_row5_col7, #T_e8219_row6_col0, #T_e8219_row6_col1, #T_e8219_row6_col2, #T_e8219_row6_col3, #T_e8219_row6_col4, #T_e8219_row6_col5, #T_e8219_row6_col6, #T_e8219_row6_col7, #T_e8219_row7_col0, #T_e8219_row7_col1, #T_e8219_row7_col2, #T_e8219_row7_col3, #T_e8219_row7_col4, #T_e8219_row7_col5, #T_e8219_row7_col6, #T_e8219_row7_col7, #T_e8219_row8_col0, #T_e8219_row8_col1, #T_e8219_row8_col2, #T_e8219_row8_col3, #T_e8219_row8_col4, #T_e8219_row8_col5, #T_e8219_row8_col6, #T_e8219_row8_col7, #T_e8219_row9_col0, #T_e8219_row9_col1, #T_e8219_row9_col2, #T_e8219_row9_col3, #T_e8219_row9_col4, #T_e8219_row9_col5, #T_e8219_row9_col6, #T_e8219_row9_col7, #T_e8219_row10_col0, #T_e8219_row10_col1, #T_e8219_row10_col2, #T_e8219_row10_col3, #T_e8219_row10_col4, #T_e8219_row10_col5, #T_e8219_row10_col6, #T_e8219_row10_col7, #T_e8219_row11_col0, #T_e8219_row11_col1, #T_e8219_row11_col2, #T_e8219_row11_col3, #T_e8219_row11_col4, #T_e8219_row11_col5, #T_e8219_row11_col6, #T_e8219_row11_col7, #T_e8219_row12_col0, #T_e8219_row12_col1, #T_e8219_row12_col2, #T_e8219_row12_col3, #T_e8219_row12_col4, #T_e8219_row12_col5, #T_e8219_row12_col6, #T_e8219_row12_col7, #T_e8219_row13_col0, #T_e8219_row13_col1, #T_e8219_row13_col2, #T_e8219_row13_col3, #T_e8219_row13_col4, #T_e8219_row13_col5, #T_e8219_row13_col6, #T_e8219_row13_col7 {\n",
       "  text-align: left;\n",
       "}\n",
       "#T_e8219_row0_col1, #T_e8219_row0_col3, #T_e8219_row1_col4, #T_e8219_row1_col5, #T_e8219_row1_col6, #T_e8219_row1_col7, #T_e8219_row2_col2 {\n",
       "  text-align: left;\n",
       "  background-color: yellow;\n",
       "}\n",
       "#T_e8219_row0_col8, #T_e8219_row13_col8 {\n",
       "  text-align: left;\n",
       "  background-color: yellow;\n",
       "  background-color: lightgrey;\n",
       "}\n",
       "#T_e8219_row1_col8, #T_e8219_row2_col8, #T_e8219_row3_col8, #T_e8219_row4_col8, #T_e8219_row5_col8, #T_e8219_row6_col8, #T_e8219_row7_col8, #T_e8219_row8_col8, #T_e8219_row9_col8, #T_e8219_row10_col8, #T_e8219_row11_col8, #T_e8219_row12_col8 {\n",
       "  text-align: left;\n",
       "  background-color: lightgrey;\n",
       "}\n",
       "</style>\n",
       "<table id=\"T_e8219\">\n",
       "  <thead>\n",
       "    <tr>\n",
       "      <th class=\"blank level0\" >&nbsp;</th>\n",
       "      <th id=\"T_e8219_level0_col0\" class=\"col_heading level0 col0\" >Model</th>\n",
       "      <th id=\"T_e8219_level0_col1\" class=\"col_heading level0 col1\" >Accuracy</th>\n",
       "      <th id=\"T_e8219_level0_col2\" class=\"col_heading level0 col2\" >AUC</th>\n",
       "      <th id=\"T_e8219_level0_col3\" class=\"col_heading level0 col3\" >Recall</th>\n",
       "      <th id=\"T_e8219_level0_col4\" class=\"col_heading level0 col4\" >Prec.</th>\n",
       "      <th id=\"T_e8219_level0_col5\" class=\"col_heading level0 col5\" >F1</th>\n",
       "      <th id=\"T_e8219_level0_col6\" class=\"col_heading level0 col6\" >Kappa</th>\n",
       "      <th id=\"T_e8219_level0_col7\" class=\"col_heading level0 col7\" >MCC</th>\n",
       "      <th id=\"T_e8219_level0_col8\" class=\"col_heading level0 col8\" >TT (Sec)</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th id=\"T_e8219_level0_row0\" class=\"row_heading level0 row0\" >ridge</th>\n",
       "      <td id=\"T_e8219_row0_col0\" class=\"data row0 col0\" >Ridge Classifier</td>\n",
       "      <td id=\"T_e8219_row0_col1\" class=\"data row0 col1\" >0.6789</td>\n",
       "      <td id=\"T_e8219_row0_col2\" class=\"data row0 col2\" >0.0000</td>\n",
       "      <td id=\"T_e8219_row0_col3\" class=\"data row0 col3\" >0.6789</td>\n",
       "      <td id=\"T_e8219_row0_col4\" class=\"data row0 col4\" >0.6689</td>\n",
       "      <td id=\"T_e8219_row0_col5\" class=\"data row0 col5\" >0.6675</td>\n",
       "      <td id=\"T_e8219_row0_col6\" class=\"data row0 col6\" >0.5516</td>\n",
       "      <td id=\"T_e8219_row0_col7\" class=\"data row0 col7\" >0.5552</td>\n",
       "      <td id=\"T_e8219_row0_col8\" class=\"data row0 col8\" >0.0220</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th id=\"T_e8219_level0_row1\" class=\"row_heading level0 row1\" >lda</th>\n",
       "      <td id=\"T_e8219_row1_col0\" class=\"data row1 col0\" >Linear Discriminant Analysis</td>\n",
       "      <td id=\"T_e8219_row1_col1\" class=\"data row1 col1\" >0.6777</td>\n",
       "      <td id=\"T_e8219_row1_col2\" class=\"data row1 col2\" >0.8846</td>\n",
       "      <td id=\"T_e8219_row1_col3\" class=\"data row1 col3\" >0.6777</td>\n",
       "      <td id=\"T_e8219_row1_col4\" class=\"data row1 col4\" >0.6770</td>\n",
       "      <td id=\"T_e8219_row1_col5\" class=\"data row1 col5\" >0.6758</td>\n",
       "      <td id=\"T_e8219_row1_col6\" class=\"data row1 col6\" >0.5564</td>\n",
       "      <td id=\"T_e8219_row1_col7\" class=\"data row1 col7\" >0.5573</td>\n",
       "      <td id=\"T_e8219_row1_col8\" class=\"data row1 col8\" >0.0560</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th id=\"T_e8219_level0_row2\" class=\"row_heading level0 row2\" >lightgbm</th>\n",
       "      <td id=\"T_e8219_row2_col0\" class=\"data row2 col0\" >Light Gradient Boosting Machine</td>\n",
       "      <td id=\"T_e8219_row2_col1\" class=\"data row2 col1\" >0.6712</td>\n",
       "      <td id=\"T_e8219_row2_col2\" class=\"data row2 col2\" >0.8857</td>\n",
       "      <td id=\"T_e8219_row2_col3\" class=\"data row2 col3\" >0.6712</td>\n",
       "      <td id=\"T_e8219_row2_col4\" class=\"data row2 col4\" >0.6594</td>\n",
       "      <td id=\"T_e8219_row2_col5\" class=\"data row2 col5\" >0.6541</td>\n",
       "      <td id=\"T_e8219_row2_col6\" class=\"data row2 col6\" >0.5381</td>\n",
       "      <td id=\"T_e8219_row2_col7\" class=\"data row2 col7\" >0.5441</td>\n",
       "      <td id=\"T_e8219_row2_col8\" class=\"data row2 col8\" >2.0130</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th id=\"T_e8219_level0_row3\" class=\"row_heading level0 row3\" >gbc</th>\n",
       "      <td id=\"T_e8219_row3_col0\" class=\"data row3 col0\" >Gradient Boosting Classifier</td>\n",
       "      <td id=\"T_e8219_row3_col1\" class=\"data row3 col1\" >0.6590</td>\n",
       "      <td id=\"T_e8219_row3_col2\" class=\"data row3 col2\" >0.8830</td>\n",
       "      <td id=\"T_e8219_row3_col3\" class=\"data row3 col3\" >0.6590</td>\n",
       "      <td id=\"T_e8219_row3_col4\" class=\"data row3 col4\" >0.6475</td>\n",
       "      <td id=\"T_e8219_row3_col5\" class=\"data row3 col5\" >0.6451</td>\n",
       "      <td id=\"T_e8219_row3_col6\" class=\"data row3 col6\" >0.5227</td>\n",
       "      <td id=\"T_e8219_row3_col7\" class=\"data row3 col7\" >0.5270</td>\n",
       "      <td id=\"T_e8219_row3_col8\" class=\"data row3 col8\" >13.9600</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th id=\"T_e8219_level0_row4\" class=\"row_heading level0 row4\" >lr</th>\n",
       "      <td id=\"T_e8219_row4_col0\" class=\"data row4 col0\" >Logistic Regression</td>\n",
       "      <td id=\"T_e8219_row4_col1\" class=\"data row4 col1\" >0.6443</td>\n",
       "      <td id=\"T_e8219_row4_col2\" class=\"data row4 col2\" >0.8568</td>\n",
       "      <td id=\"T_e8219_row4_col3\" class=\"data row4 col3\" >0.6443</td>\n",
       "      <td id=\"T_e8219_row4_col4\" class=\"data row4 col4\" >0.6458</td>\n",
       "      <td id=\"T_e8219_row4_col5\" class=\"data row4 col5\" >0.6440</td>\n",
       "      <td id=\"T_e8219_row4_col6\" class=\"data row4 col6\" >0.5121</td>\n",
       "      <td id=\"T_e8219_row4_col7\" class=\"data row4 col7\" >0.5128</td>\n",
       "      <td id=\"T_e8219_row4_col8\" class=\"data row4 col8\" >0.6520</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th id=\"T_e8219_level0_row5\" class=\"row_heading level0 row5\" >svm</th>\n",
       "      <td id=\"T_e8219_row5_col0\" class=\"data row5 col0\" >SVM - Linear Kernel</td>\n",
       "      <td id=\"T_e8219_row5_col1\" class=\"data row5 col1\" >0.6325</td>\n",
       "      <td id=\"T_e8219_row5_col2\" class=\"data row5 col2\" >0.0000</td>\n",
       "      <td id=\"T_e8219_row5_col3\" class=\"data row5 col3\" >0.6325</td>\n",
       "      <td id=\"T_e8219_row5_col4\" class=\"data row5 col4\" >0.6294</td>\n",
       "      <td id=\"T_e8219_row5_col5\" class=\"data row5 col5\" >0.6254</td>\n",
       "      <td id=\"T_e8219_row5_col6\" class=\"data row5 col6\" >0.4924</td>\n",
       "      <td id=\"T_e8219_row5_col7\" class=\"data row5 col7\" >0.4956</td>\n",
       "      <td id=\"T_e8219_row5_col8\" class=\"data row5 col8\" >0.0560</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th id=\"T_e8219_level0_row6\" class=\"row_heading level0 row6\" >rf</th>\n",
       "      <td id=\"T_e8219_row6_col0\" class=\"data row6 col0\" >Random Forest Classifier</td>\n",
       "      <td id=\"T_e8219_row6_col1\" class=\"data row6 col1\" >0.6305</td>\n",
       "      <td id=\"T_e8219_row6_col2\" class=\"data row6 col2\" >0.8571</td>\n",
       "      <td id=\"T_e8219_row6_col3\" class=\"data row6 col3\" >0.6305</td>\n",
       "      <td id=\"T_e8219_row6_col4\" class=\"data row6 col4\" >0.6097</td>\n",
       "      <td id=\"T_e8219_row6_col5\" class=\"data row6 col5\" >0.5810</td>\n",
       "      <td id=\"T_e8219_row6_col6\" class=\"data row6 col6\" >0.4691</td>\n",
       "      <td id=\"T_e8219_row6_col7\" class=\"data row6 col7\" >0.4888</td>\n",
       "      <td id=\"T_e8219_row6_col8\" class=\"data row6 col8\" >0.3810</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th id=\"T_e8219_level0_row7\" class=\"row_heading level0 row7\" >et</th>\n",
       "      <td id=\"T_e8219_row7_col0\" class=\"data row7 col0\" >Extra Trees Classifier</td>\n",
       "      <td id=\"T_e8219_row7_col1\" class=\"data row7 col1\" >0.6260</td>\n",
       "      <td id=\"T_e8219_row7_col2\" class=\"data row7 col2\" >0.8647</td>\n",
       "      <td id=\"T_e8219_row7_col3\" class=\"data row7 col3\" >0.6260</td>\n",
       "      <td id=\"T_e8219_row7_col4\" class=\"data row7 col4\" >0.6127</td>\n",
       "      <td id=\"T_e8219_row7_col5\" class=\"data row7 col5\" >0.5636</td>\n",
       "      <td id=\"T_e8219_row7_col6\" class=\"data row7 col6\" >0.4588</td>\n",
       "      <td id=\"T_e8219_row7_col7\" class=\"data row7 col7\" >0.4857</td>\n",
       "      <td id=\"T_e8219_row7_col8\" class=\"data row7 col8\" >0.1060</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th id=\"T_e8219_level0_row8\" class=\"row_heading level0 row8\" >knn</th>\n",
       "      <td id=\"T_e8219_row8_col0\" class=\"data row8 col0\" >K Neighbors Classifier</td>\n",
       "      <td id=\"T_e8219_row8_col1\" class=\"data row8 col1\" >0.6215</td>\n",
       "      <td id=\"T_e8219_row8_col2\" class=\"data row8 col2\" >0.8314</td>\n",
       "      <td id=\"T_e8219_row8_col3\" class=\"data row8 col3\" >0.6215</td>\n",
       "      <td id=\"T_e8219_row8_col4\" class=\"data row8 col4\" >0.6134</td>\n",
       "      <td id=\"T_e8219_row8_col5\" class=\"data row8 col5\" >0.6043</td>\n",
       "      <td id=\"T_e8219_row8_col6\" class=\"data row8 col6\" >0.4684</td>\n",
       "      <td id=\"T_e8219_row8_col7\" class=\"data row8 col7\" >0.4747</td>\n",
       "      <td id=\"T_e8219_row8_col8\" class=\"data row8 col8\" >0.2140</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th id=\"T_e8219_level0_row9\" class=\"row_heading level0 row9\" >qda</th>\n",
       "      <td id=\"T_e8219_row9_col0\" class=\"data row9 col0\" >Quadratic Discriminant Analysis</td>\n",
       "      <td id=\"T_e8219_row9_col1\" class=\"data row9 col1\" >0.6125</td>\n",
       "      <td id=\"T_e8219_row9_col2\" class=\"data row9 col2\" >0.7806</td>\n",
       "      <td id=\"T_e8219_row9_col3\" class=\"data row9 col3\" >0.6125</td>\n",
       "      <td id=\"T_e8219_row9_col4\" class=\"data row9 col4\" >0.5969</td>\n",
       "      <td id=\"T_e8219_row9_col5\" class=\"data row9 col5\" >0.4932</td>\n",
       "      <td id=\"T_e8219_row9_col6\" class=\"data row9 col6\" >0.4298</td>\n",
       "      <td id=\"T_e8219_row9_col7\" class=\"data row9 col7\" >0.4790</td>\n",
       "      <td id=\"T_e8219_row9_col8\" class=\"data row9 col8\" >0.0360</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th id=\"T_e8219_level0_row10\" class=\"row_heading level0 row10\" >ada</th>\n",
       "      <td id=\"T_e8219_row10_col0\" class=\"data row10 col0\" >Ada Boost Classifier</td>\n",
       "      <td id=\"T_e8219_row10_col1\" class=\"data row10 col1\" >0.5967</td>\n",
       "      <td id=\"T_e8219_row10_col2\" class=\"data row10 col2\" >0.8112</td>\n",
       "      <td id=\"T_e8219_row10_col3\" class=\"data row10 col3\" >0.5967</td>\n",
       "      <td id=\"T_e8219_row10_col4\" class=\"data row10 col4\" >0.5852</td>\n",
       "      <td id=\"T_e8219_row10_col5\" class=\"data row10 col5\" >0.5890</td>\n",
       "      <td id=\"T_e8219_row10_col6\" class=\"data row10 col6\" >0.4413</td>\n",
       "      <td id=\"T_e8219_row10_col7\" class=\"data row10 col7\" >0.4425</td>\n",
       "      <td id=\"T_e8219_row10_col8\" class=\"data row10 col8\" >0.6600</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th id=\"T_e8219_level0_row11\" class=\"row_heading level0 row11\" >nb</th>\n",
       "      <td id=\"T_e8219_row11_col0\" class=\"data row11 col0\" >Naive Bayes</td>\n",
       "      <td id=\"T_e8219_row11_col1\" class=\"data row11 col1\" >0.5934</td>\n",
       "      <td id=\"T_e8219_row11_col2\" class=\"data row11 col2\" >0.8249</td>\n",
       "      <td id=\"T_e8219_row11_col3\" class=\"data row11 col3\" >0.5934</td>\n",
       "      <td id=\"T_e8219_row11_col4\" class=\"data row11 col4\" >0.5949</td>\n",
       "      <td id=\"T_e8219_row11_col5\" class=\"data row11 col5\" >0.5933</td>\n",
       "      <td id=\"T_e8219_row11_col6\" class=\"data row11 col6\" >0.4423</td>\n",
       "      <td id=\"T_e8219_row11_col7\" class=\"data row11 col7\" >0.4428</td>\n",
       "      <td id=\"T_e8219_row11_col8\" class=\"data row11 col8\" >0.0270</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th id=\"T_e8219_level0_row12\" class=\"row_heading level0 row12\" >dt</th>\n",
       "      <td id=\"T_e8219_row12_col0\" class=\"data row12 col0\" >Decision Tree Classifier</td>\n",
       "      <td id=\"T_e8219_row12_col1\" class=\"data row12 col1\" >0.4587</td>\n",
       "      <td id=\"T_e8219_row12_col2\" class=\"data row12 col2\" >0.6353</td>\n",
       "      <td id=\"T_e8219_row12_col3\" class=\"data row12 col3\" >0.4587</td>\n",
       "      <td id=\"T_e8219_row12_col4\" class=\"data row12 col4\" >0.4639</td>\n",
       "      <td id=\"T_e8219_row12_col5\" class=\"data row12 col5\" >0.4602</td>\n",
       "      <td id=\"T_e8219_row12_col6\" class=\"data row12 col6\" >0.2590</td>\n",
       "      <td id=\"T_e8219_row12_col7\" class=\"data row12 col7\" >0.2595</td>\n",
       "      <td id=\"T_e8219_row12_col8\" class=\"data row12 col8\" >0.1000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th id=\"T_e8219_level0_row13\" class=\"row_heading level0 row13\" >dummy</th>\n",
       "      <td id=\"T_e8219_row13_col0\" class=\"data row13 col0\" >Dummy Classifier</td>\n",
       "      <td id=\"T_e8219_row13_col1\" class=\"data row13 col1\" >0.3407</td>\n",
       "      <td id=\"T_e8219_row13_col2\" class=\"data row13 col2\" >0.5000</td>\n",
       "      <td id=\"T_e8219_row13_col3\" class=\"data row13 col3\" >0.3407</td>\n",
       "      <td id=\"T_e8219_row13_col4\" class=\"data row13 col4\" >0.1161</td>\n",
       "      <td id=\"T_e8219_row13_col5\" class=\"data row13 col5\" >0.1731</td>\n",
       "      <td id=\"T_e8219_row13_col6\" class=\"data row13 col6\" >0.0000</td>\n",
       "      <td id=\"T_e8219_row13_col7\" class=\"data row13 col7\" >0.0000</td>\n",
       "      <td id=\"T_e8219_row13_col8\" class=\"data row13 col8\" >0.0220</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n"
      ],
      "text/plain": [
       "<pandas.io.formats.style.Styler at 0x20e3fb57b10>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<style>#sk-container-id-1 {color: black;background-color: white;}#sk-container-id-1 pre{padding: 0;}#sk-container-id-1 div.sk-toggleable {background-color: white;}#sk-container-id-1 label.sk-toggleable__label {cursor: pointer;display: block;width: 100%;margin-bottom: 0;padding: 0.3em;box-sizing: border-box;text-align: center;}#sk-container-id-1 label.sk-toggleable__label-arrow:before {content: \"▸\";float: left;margin-right: 0.25em;color: #696969;}#sk-container-id-1 label.sk-toggleable__label-arrow:hover:before {color: black;}#sk-container-id-1 div.sk-estimator:hover label.sk-toggleable__label-arrow:before {color: black;}#sk-container-id-1 div.sk-toggleable__content {max-height: 0;max-width: 0;overflow: hidden;text-align: left;background-color: #f0f8ff;}#sk-container-id-1 div.sk-toggleable__content pre {margin: 0.2em;color: black;border-radius: 0.25em;background-color: #f0f8ff;}#sk-container-id-1 input.sk-toggleable__control:checked~div.sk-toggleable__content {max-height: 200px;max-width: 100%;overflow: auto;}#sk-container-id-1 input.sk-toggleable__control:checked~label.sk-toggleable__label-arrow:before {content: \"▾\";}#sk-container-id-1 div.sk-estimator input.sk-toggleable__control:checked~label.sk-toggleable__label {background-color: #d4ebff;}#sk-container-id-1 div.sk-label input.sk-toggleable__control:checked~label.sk-toggleable__label {background-color: #d4ebff;}#sk-container-id-1 input.sk-hidden--visually {border: 0;clip: rect(1px 1px 1px 1px);clip: rect(1px, 1px, 1px, 1px);height: 1px;margin: -1px;overflow: hidden;padding: 0;position: absolute;width: 1px;}#sk-container-id-1 div.sk-estimator {font-family: monospace;background-color: #f0f8ff;border: 1px dotted black;border-radius: 0.25em;box-sizing: border-box;margin-bottom: 0.5em;}#sk-container-id-1 div.sk-estimator:hover {background-color: #d4ebff;}#sk-container-id-1 div.sk-parallel-item::after {content: \"\";width: 100%;border-bottom: 1px solid gray;flex-grow: 1;}#sk-container-id-1 div.sk-label:hover label.sk-toggleable__label {background-color: #d4ebff;}#sk-container-id-1 div.sk-serial::before {content: \"\";position: absolute;border-left: 1px solid gray;box-sizing: border-box;top: 0;bottom: 0;left: 50%;z-index: 0;}#sk-container-id-1 div.sk-serial {display: flex;flex-direction: column;align-items: center;background-color: white;padding-right: 0.2em;padding-left: 0.2em;position: relative;}#sk-container-id-1 div.sk-item {position: relative;z-index: 1;}#sk-container-id-1 div.sk-parallel {display: flex;align-items: stretch;justify-content: center;background-color: white;position: relative;}#sk-container-id-1 div.sk-item::before, #sk-container-id-1 div.sk-parallel-item::before {content: \"\";position: absolute;border-left: 1px solid gray;box-sizing: border-box;top: 0;bottom: 0;left: 50%;z-index: -1;}#sk-container-id-1 div.sk-parallel-item {display: flex;flex-direction: column;z-index: 1;position: relative;background-color: white;}#sk-container-id-1 div.sk-parallel-item:first-child::after {align-self: flex-end;width: 50%;}#sk-container-id-1 div.sk-parallel-item:last-child::after {align-self: flex-start;width: 50%;}#sk-container-id-1 div.sk-parallel-item:only-child::after {width: 0;}#sk-container-id-1 div.sk-dashed-wrapped {border: 1px dashed gray;margin: 0 0.4em 0.5em 0.4em;box-sizing: border-box;padding-bottom: 0.4em;background-color: white;}#sk-container-id-1 div.sk-label label {font-family: monospace;font-weight: bold;display: inline-block;line-height: 1.2em;}#sk-container-id-1 div.sk-label-container {text-align: center;}#sk-container-id-1 div.sk-container {/* jupyter's `normalize.less` sets `[hidden] { display: none; }` but bootstrap.min.css set `[hidden] { display: none !important; }` so we also need the `!important` here to be able to override the default hidden behavior on the sphinx rendered scikit-learn.org. See: https://github.com/scikit-learn/scikit-learn/issues/21755 */display: inline-block !important;position: relative;}#sk-container-id-1 div.sk-text-repr-fallback {display: none;}</style><div id=\"sk-container-id-1\" class=\"sk-top-container\"><div class=\"sk-text-repr-fallback\"><pre>RidgeClassifier(alpha=1.0, class_weight=None, copy_X=True, fit_intercept=True,\n",
       "                max_iter=None, positive=False, random_state=9088, solver=&#x27;auto&#x27;,\n",
       "                tol=0.0001)</pre><b>In a Jupyter environment, please rerun this cell to show the HTML representation or trust the notebook. <br />On GitHub, the HTML representation is unable to render, please try loading this page with nbviewer.org.</b></div><div class=\"sk-container\" hidden><div class=\"sk-item\"><div class=\"sk-estimator sk-toggleable\"><input class=\"sk-toggleable__control sk-hidden--visually\" id=\"sk-estimator-id-1\" type=\"checkbox\" checked><label for=\"sk-estimator-id-1\" class=\"sk-toggleable__label sk-toggleable__label-arrow\">RidgeClassifier</label><div class=\"sk-toggleable__content\"><pre>RidgeClassifier(alpha=1.0, class_weight=None, copy_X=True, fit_intercept=True,\n",
       "                max_iter=None, positive=False, random_state=9088, solver=&#x27;auto&#x27;,\n",
       "                tol=0.0001)</pre></div></div></div></div></div>"
      ],
      "text/plain": [
       "RidgeClassifier(alpha=1.0, class_weight=None, copy_X=True, fit_intercept=True,\n",
       "                max_iter=None, positive=False, random_state=9088, solver='auto',\n",
       "                tol=0.0001)"
      ]
     },
     "execution_count": 305,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "exp_class.compare_models()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "hide_input": false,
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.8"
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {},
   "number_sections": true,
   "sideBar": true,
   "skip_h1_title": true,
   "title_cell": "Summary",
   "title_sidebar": "Contents",
   "toc_cell": true,
   "toc_position": {
    "height": "calc(100% - 180px)",
    "left": "10px",
    "top": "150px",
    "width": "300.15px"
   },
   "toc_section_display": true,
   "toc_window_display": false
  },
  "varInspector": {
   "cols": {
    "lenName": 16,
    "lenType": 16,
    "lenVar": 40
   },
   "kernels_config": {
    "python": {
     "delete_cmd_postfix": "",
     "delete_cmd_prefix": "del ",
     "library": "var_list.py",
     "varRefreshCmd": "print(var_dic_list())"
    },
    "r": {
     "delete_cmd_postfix": ") ",
     "delete_cmd_prefix": "rm(",
     "library": "var_list.r",
     "varRefreshCmd": "cat(var_dic_list()) "
    }
   },
   "position": {
    "height": "498.85px",
    "left": "651.8px",
    "right": "20px",
    "top": "56px",
    "width": "715px"
   },
   "types_to_exclude": [
    "module",
    "function",
    "builtin_function_or_method",
    "instance",
    "_Feature"
   ],
   "window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
