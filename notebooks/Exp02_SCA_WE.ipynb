{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Semantic Similarity - Experiment 02\n",
    "The objective of this trial is to expand the SCA_index (i.e., Semantic Content Analysis Index) to a full word embedding, setting a subjective or objective load for each word.\n",
    "\n",
    "version 2: using SpaCy Universal Sentence Encoder, which means vectors with dimension 512 instead of 300."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Introduction"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-11-12T22:37:14.251208Z",
     "start_time": "2022-11-12T22:37:14.073574Z"
    }
   },
   "outputs": [],
   "source": [
    "## Data analysis packages:\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "#from math import isnan  #Verifies if a given value is numerical.\n",
    "#import re  # Regular Expression"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-11-12T22:37:15.470110Z",
     "start_time": "2022-11-12T22:37:14.895488Z"
    }
   },
   "outputs": [],
   "source": [
    "## Visualization packages:\n",
    "# import seaborn as sns\n",
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "## MLOps:\n",
    "import pickle"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Definitions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-04-28T22:15:12.656866Z",
     "start_time": "2022-04-28T22:15:12.612272Z"
    }
   },
   "outputs": [],
   "source": [
    "## Forcing Pandas to display any number of elements\n",
    "pd.set_option('display.max_columns', None)\n",
    "pd.set_option('display.max_rows', 50)\n",
    "pd.set_option('display.max_seq_items', None)\n",
    "pd.set_option('display.width', 2000)\n",
    "pd.set_option('display.max_colwidth', None)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## SpaCy Word Embeddings (WE): \n",
    "Also using Spacy library: https://spacy.io/\n",
    "> !pip install -U spacy  \n",
    "> !python -m spacy download en_core_web_sm  \n",
    "> !python -m spacy download en_core_web_lg\n",
    "\n",
    "Some instructions on how to use it:  \n",
    "https://spacy.io/usage/spacy-101  \n",
    "\n",
    "\n",
    "> Using the Universal Sentence Encoder: https://github.com/MartinoMensio/spacy-universal-sentence-encoder"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-04-28T22:38:26.997959Z",
     "start_time": "2022-04-28T22:38:25.715707Z"
    }
   },
   "outputs": [],
   "source": [
    "## Importing SpaCy library:\n",
    "import spacy\n",
    "\n",
    "# Load English tokenizer, tagger, parser and NER\n",
    "nlp = spacy.load(\"en_core_web_lg\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'width': 300,\n",
       " 'vectors': 514157,\n",
       " 'keys': 514157,\n",
       " 'name': 'en_vectors',\n",
       " 'mode': 'default'}"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "## We check below that this model has 514.157 keys and vectors, respectively.\n",
    "nlp.meta['vectors']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "514157"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "## Again, checking the number of keys.\n",
    "nlp.vocab.vectors.n_keys"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "4473158105997569131"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "## Finding the SpaCy index for a given word:\n",
    "nlp.vocab.strings['problem']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Getting the text and vector for a given index:\n",
    "tmp_Idx = 4473158105997569131\n",
    "\n",
    "tmp_vector = nlp.vocab[tmp_Idx].vector\n",
    "tmp_text = nlp.vocab[tmp_Idx].text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "4473158105997569131: problem:\n",
      "[ 3.3611  4.0891 -2.1247]...\n"
     ]
    }
   ],
   "source": [
    "print(f'{tmp_Idx}: {tmp_text}:\\n{tmp_vector[0:3]}...')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "import spacy_universal_sentence_encoder"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 197,
   "metadata": {},
   "outputs": [],
   "source": [
    "# nlp = spacy_universal_sentence_encoder.load_model('en_use_md')\n",
    "nlp = spacy_universal_sentence_encoder.load_model('en_use_lg')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 198,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Defining a method to get vector and text information for a word:\n",
    "def nlp_getVector(word, verbose=False):\n",
    "    '''\n",
    "    Obtains the vector representation of a given word from SpaCy word embedding.\n",
    "    Usage:  nlp_getVector(word)[0] to get the text; nlp_getVector(word)[1] to get the vector.\n",
    "            var_text, var_vector = nlp_getVector(word)\n",
    "    '''\n",
    "    ## Generates the word hash:\n",
    "    # hash = nlp.vocab.strings[word]\n",
    "    try:\n",
    "        word_vector = nlp(word).vector\n",
    "        word_text = nlp(word).text\n",
    "    except:\n",
    "        if verbose:\n",
    "            print('Error: word vector not available.')\n",
    "        return None\n",
    "    return (word_text, word_vector)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 199,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Word: study <-> Vector 3rd elements: [ 0.02729203 -0.0518213   0.03944567]\n"
     ]
    }
   ],
   "source": [
    "## Testing the method:\n",
    "print(f'Word: {nlp_getVector(\"study\")[0]} <-> Vector 3rd elements: {nlp_getVector(\"study\")[1][:3]}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "## Semantic Content Analysis (SCA)\n",
    "* Read the SCA obtained from Glasgow Norms data;  \n",
    "* Import F_s and F_o from the previous study;  \n",
    "* Generate datasets for training classifiers."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>words</th>\n",
       "      <th>F_Objectivity</th>\n",
       "      <th>F_Subjectivity</th>\n",
       "      <th>F_Context</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>abattoir</td>\n",
       "      <td>0.512527</td>\n",
       "      <td>0.380603</td>\n",
       "      <td>0.960466</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>abbey</td>\n",
       "      <td>0.714765</td>\n",
       "      <td>0.240456</td>\n",
       "      <td>0.696198</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>abbreviate</td>\n",
       "      <td>0.286952</td>\n",
       "      <td>0.171052</td>\n",
       "      <td>0.767043</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>abdicate</td>\n",
       "      <td>0.144736</td>\n",
       "      <td>0.384300</td>\n",
       "      <td>0.863127</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>abdication</td>\n",
       "      <td>0.167654</td>\n",
       "      <td>0.334086</td>\n",
       "      <td>0.896733</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "        words  F_Objectivity  F_Subjectivity  F_Context\n",
       "0    abattoir       0.512527        0.380603   0.960466\n",
       "1       abbey       0.714765        0.240456   0.696198\n",
       "2  abbreviate       0.286952        0.171052   0.767043\n",
       "3    abdicate       0.144736        0.384300   0.863127\n",
       "4  abdication       0.167654        0.334086   0.896733"
      ]
     },
     "execution_count": 27,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_factors = pd.read_csv('../data/df_factors.csv', sep=';')\n",
    "df_factors.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### De-Duplicating words  \n",
    "There are words in the Glasgow Norms that were differentiated from their homonymous, such as 'case'. In this section, we first select those words and then input a mean value for them."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>words</th>\n",
       "      <th>F_Objectivity</th>\n",
       "      <th>F_Subjectivity</th>\n",
       "      <th>F_Context</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>533</th>\n",
       "      <td>bookcase</td>\n",
       "      <td>0.926393</td>\n",
       "      <td>0.374441</td>\n",
       "      <td>0.335542</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>756</th>\n",
       "      <td>case</td>\n",
       "      <td>0.715863</td>\n",
       "      <td>0.164100</td>\n",
       "      <td>0.409611</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>757</th>\n",
       "      <td>case (container)</td>\n",
       "      <td>0.821136</td>\n",
       "      <td>0.079956</td>\n",
       "      <td>0.400335</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>758</th>\n",
       "      <td>case (instance)</td>\n",
       "      <td>0.233820</td>\n",
       "      <td>0.213528</td>\n",
       "      <td>0.651273</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>759</th>\n",
       "      <td>case (legal)</td>\n",
       "      <td>0.456260</td>\n",
       "      <td>0.369828</td>\n",
       "      <td>0.733642</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4808</th>\n",
       "      <td>suitcase</td>\n",
       "      <td>0.961068</td>\n",
       "      <td>0.256584</td>\n",
       "      <td>0.356338</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                 words  F_Objectivity  F_Subjectivity  F_Context\n",
       "533           bookcase       0.926393        0.374441   0.335542\n",
       "756               case       0.715863        0.164100   0.409611\n",
       "757   case (container)       0.821136        0.079956   0.400335\n",
       "758    case (instance)       0.233820        0.213528   0.651273\n",
       "759       case (legal)       0.456260        0.369828   0.733642\n",
       "4808          suitcase       0.961068        0.256584   0.356338"
      ]
     },
     "execution_count": 28,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "## Exemplifying the need for de-duplication:\n",
    "df_factors[df_factors['words'].str.contains('case')]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>words</th>\n",
       "      <th>F_Objectivity</th>\n",
       "      <th>F_Subjectivity</th>\n",
       "      <th>F_Context</th>\n",
       "      <th>word</th>\n",
       "      <th>distinction</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>abattoir</td>\n",
       "      <td>0.512527</td>\n",
       "      <td>0.380603</td>\n",
       "      <td>0.960466</td>\n",
       "      <td>abattoir</td>\n",
       "      <td>None</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>abbey</td>\n",
       "      <td>0.714765</td>\n",
       "      <td>0.240456</td>\n",
       "      <td>0.696198</td>\n",
       "      <td>abbey</td>\n",
       "      <td>None</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>abbreviate</td>\n",
       "      <td>0.286952</td>\n",
       "      <td>0.171052</td>\n",
       "      <td>0.767043</td>\n",
       "      <td>abbreviate</td>\n",
       "      <td>None</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>abdicate</td>\n",
       "      <td>0.144736</td>\n",
       "      <td>0.384300</td>\n",
       "      <td>0.863127</td>\n",
       "      <td>abdicate</td>\n",
       "      <td>None</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>abdication</td>\n",
       "      <td>0.167654</td>\n",
       "      <td>0.334086</td>\n",
       "      <td>0.896733</td>\n",
       "      <td>abdication</td>\n",
       "      <td>None</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "        words  F_Objectivity  F_Subjectivity  F_Context        word distinction\n",
       "0    abattoir       0.512527        0.380603   0.960466    abattoir        None\n",
       "1       abbey       0.714765        0.240456   0.696198       abbey        None\n",
       "2  abbreviate       0.286952        0.171052   0.767043  abbreviate        None\n",
       "3    abdicate       0.144736        0.384300   0.863127    abdicate        None\n",
       "4  abdication       0.167654        0.334086   0.896733  abdication        None"
      ]
     },
     "execution_count": 29,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "## Creating a new dataframe by splitting the 'words' column into two columns:\n",
    "df_homonym = df_factors.copy()\n",
    "df_homonym[['word','distinction']] = df_homonym['words'].str.split('(', expand=True)\n",
    "## Renaming the columns of the new dataframe:\n",
    "# df_homonym.columns = ['word','distinction']\n",
    "\n",
    "# Stripping whitespace from the new columns\n",
    "df_homonym['word'] = df_homonym['word'].str.strip()\n",
    "df_homonym['distinction'] = df_homonym['distinction'].str.strip().str.rstrip(')')\n",
    "\n",
    "## Showing dataframe:\n",
    "df_homonym.head(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>n_distinction</th>\n",
       "      <th>n_words</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0</td>\n",
       "      <td>4303</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>2</td>\n",
       "      <td>288</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>3</td>\n",
       "      <td>69</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>4</td>\n",
       "      <td>19</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>5</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   n_distinction  n_words\n",
       "0              0     4303\n",
       "1              1        2\n",
       "2              2      288\n",
       "3              3       69\n",
       "4              4       19\n",
       "5              5        2"
      ]
     },
     "execution_count": 30,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Calculating the number of distinct elements in the 'distinction' column for each word in the 'word' column\n",
    "distinct_count = df_homonym.groupby('word')['distinction'].nunique().reset_index()\n",
    "\n",
    "# Counting the frequency of different numbers of distinct elements\n",
    "pivot_table = distinct_count.groupby('distinction')['word'].count().reset_index()\n",
    "\n",
    "# Renaming the columns\n",
    "pivot_table.columns = ['n_distinction', 'n_words']\n",
    "pivot_table"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Analyzing the words with 5 distinct meaning:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>word</th>\n",
       "      <th>distinction</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>721</th>\n",
       "      <td>charge</td>\n",
       "      <td>5</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>822</th>\n",
       "      <td>club</td>\n",
       "      <td>5</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "       word  distinction\n",
       "721  charge            5\n",
       "822    club            5"
      ]
     },
     "execution_count": 31,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "## Getting the word with the highest number of distinct meanings: \n",
    "distinct_count[distinct_count['distinction'] == 5]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>words</th>\n",
       "      <th>F_Objectivity</th>\n",
       "      <th>F_Subjectivity</th>\n",
       "      <th>F_Context</th>\n",
       "      <th>word</th>\n",
       "      <th>distinction</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>819</th>\n",
       "      <td>charge</td>\n",
       "      <td>0.340881</td>\n",
       "      <td>0.330534</td>\n",
       "      <td>0.438418</td>\n",
       "      <td>charge</td>\n",
       "      <td>None</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>820</th>\n",
       "      <td>charge (card)</td>\n",
       "      <td>0.477083</td>\n",
       "      <td>0.293352</td>\n",
       "      <td>0.671798</td>\n",
       "      <td>charge</td>\n",
       "      <td>card</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>821</th>\n",
       "      <td>charge (dependent)</td>\n",
       "      <td>0.256759</td>\n",
       "      <td>0.164572</td>\n",
       "      <td>0.791042</td>\n",
       "      <td>charge</td>\n",
       "      <td>dependent</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>822</th>\n",
       "      <td>charge (electric)</td>\n",
       "      <td>0.391504</td>\n",
       "      <td>0.413080</td>\n",
       "      <td>0.697287</td>\n",
       "      <td>charge</td>\n",
       "      <td>electric</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>823</th>\n",
       "      <td>charge (price)</td>\n",
       "      <td>0.428509</td>\n",
       "      <td>0.353942</td>\n",
       "      <td>0.498770</td>\n",
       "      <td>charge</td>\n",
       "      <td>price</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>824</th>\n",
       "      <td>charge (rush)</td>\n",
       "      <td>0.551191</td>\n",
       "      <td>0.557549</td>\n",
       "      <td>0.591914</td>\n",
       "      <td>charge</td>\n",
       "      <td>rush</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>942</th>\n",
       "      <td>club</td>\n",
       "      <td>0.774815</td>\n",
       "      <td>0.476734</td>\n",
       "      <td>0.467275</td>\n",
       "      <td>club</td>\n",
       "      <td>None</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>943</th>\n",
       "      <td>club (card suit)</td>\n",
       "      <td>0.773094</td>\n",
       "      <td>0.187918</td>\n",
       "      <td>0.492067</td>\n",
       "      <td>club</td>\n",
       "      <td>card suit</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>944</th>\n",
       "      <td>club (disco)</td>\n",
       "      <td>0.863845</td>\n",
       "      <td>0.547812</td>\n",
       "      <td>0.658553</td>\n",
       "      <td>club</td>\n",
       "      <td>disco</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>945</th>\n",
       "      <td>club (hit)</td>\n",
       "      <td>0.725571</td>\n",
       "      <td>0.435103</td>\n",
       "      <td>0.607930</td>\n",
       "      <td>club</td>\n",
       "      <td>hit</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>946</th>\n",
       "      <td>club (organisation)</td>\n",
       "      <td>0.699817</td>\n",
       "      <td>0.428406</td>\n",
       "      <td>0.356641</td>\n",
       "      <td>club</td>\n",
       "      <td>organisation</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>947</th>\n",
       "      <td>club (tool)</td>\n",
       "      <td>0.860703</td>\n",
       "      <td>0.289625</td>\n",
       "      <td>0.537132</td>\n",
       "      <td>club</td>\n",
       "      <td>tool</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                   words  F_Objectivity  F_Subjectivity  F_Context    word   distinction\n",
       "819               charge       0.340881        0.330534   0.438418  charge          None\n",
       "820        charge (card)       0.477083        0.293352   0.671798  charge          card\n",
       "821   charge (dependent)       0.256759        0.164572   0.791042  charge     dependent\n",
       "822    charge (electric)       0.391504        0.413080   0.697287  charge      electric\n",
       "823       charge (price)       0.428509        0.353942   0.498770  charge         price\n",
       "824        charge (rush)       0.551191        0.557549   0.591914  charge          rush\n",
       "942                 club       0.774815        0.476734   0.467275    club          None\n",
       "943     club (card suit)       0.773094        0.187918   0.492067    club     card suit\n",
       "944         club (disco)       0.863845        0.547812   0.658553    club         disco\n",
       "945           club (hit)       0.725571        0.435103   0.607930    club           hit\n",
       "946  club (organisation)       0.699817        0.428406   0.356641    club  organisation\n",
       "947          club (tool)       0.860703        0.289625   0.537132    club          tool"
      ]
     },
     "execution_count": 32,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_homonym[(df_homonym['word']=='club') | (df_homonym['word']=='charge')]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Replacing the distinct values for the average (column 'word'):"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Calculating the mean of F_Objectivity and F_Subjectivity for each group of \"word\"\n",
    "mean_values = df_homonym.groupby('word')[['F_Objectivity', 'F_Subjectivity']].mean().reset_index()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>word</th>\n",
       "      <th>F_Objectivity</th>\n",
       "      <th>F_Subjectivity</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Apple</td>\n",
       "      <td>0.940620</td>\n",
       "      <td>0.524376</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>Christmas</td>\n",
       "      <td>0.850793</td>\n",
       "      <td>0.833898</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>Dad</td>\n",
       "      <td>0.856533</td>\n",
       "      <td>0.493834</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>Dame</td>\n",
       "      <td>0.626968</td>\n",
       "      <td>0.300580</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>FALSE</td>\n",
       "      <td>0.156905</td>\n",
       "      <td>0.473624</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4678</th>\n",
       "      <td>zeppelin</td>\n",
       "      <td>0.864760</td>\n",
       "      <td>0.396531</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4679</th>\n",
       "      <td>zero</td>\n",
       "      <td>0.379392</td>\n",
       "      <td>0.315118</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4680</th>\n",
       "      <td>zest</td>\n",
       "      <td>0.402894</td>\n",
       "      <td>0.476800</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4681</th>\n",
       "      <td>zoo</td>\n",
       "      <td>0.867152</td>\n",
       "      <td>0.507390</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4682</th>\n",
       "      <td>zoology</td>\n",
       "      <td>0.420591</td>\n",
       "      <td>0.345092</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>4683 rows × 3 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "           word  F_Objectivity  F_Subjectivity\n",
       "0         Apple       0.940620        0.524376\n",
       "1     Christmas       0.850793        0.833898\n",
       "2           Dad       0.856533        0.493834\n",
       "3          Dame       0.626968        0.300580\n",
       "4         FALSE       0.156905        0.473624\n",
       "...         ...            ...             ...\n",
       "4678   zeppelin       0.864760        0.396531\n",
       "4679       zero       0.379392        0.315118\n",
       "4680       zest       0.402894        0.476800\n",
       "4681        zoo       0.867152        0.507390\n",
       "4682    zoology       0.420591        0.345092\n",
       "\n",
       "[4683 rows x 3 columns]"
      ]
     },
     "execution_count": 34,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "mean_values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>words</th>\n",
       "      <th>F_Objectivity</th>\n",
       "      <th>F_Subjectivity</th>\n",
       "      <th>F_Context</th>\n",
       "      <th>word</th>\n",
       "      <th>distinction</th>\n",
       "      <th>F_Objectivity_mean</th>\n",
       "      <th>F_Subjectivity_mean</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>819</th>\n",
       "      <td>charge</td>\n",
       "      <td>0.340881</td>\n",
       "      <td>0.330534</td>\n",
       "      <td>0.438418</td>\n",
       "      <td>charge</td>\n",
       "      <td>None</td>\n",
       "      <td>0.407655</td>\n",
       "      <td>0.352172</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>820</th>\n",
       "      <td>charge (card)</td>\n",
       "      <td>0.477083</td>\n",
       "      <td>0.293352</td>\n",
       "      <td>0.671798</td>\n",
       "      <td>charge</td>\n",
       "      <td>card</td>\n",
       "      <td>0.407655</td>\n",
       "      <td>0.352172</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>821</th>\n",
       "      <td>charge (dependent)</td>\n",
       "      <td>0.256759</td>\n",
       "      <td>0.164572</td>\n",
       "      <td>0.791042</td>\n",
       "      <td>charge</td>\n",
       "      <td>dependent</td>\n",
       "      <td>0.407655</td>\n",
       "      <td>0.352172</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>822</th>\n",
       "      <td>charge (electric)</td>\n",
       "      <td>0.391504</td>\n",
       "      <td>0.413080</td>\n",
       "      <td>0.697287</td>\n",
       "      <td>charge</td>\n",
       "      <td>electric</td>\n",
       "      <td>0.407655</td>\n",
       "      <td>0.352172</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>823</th>\n",
       "      <td>charge (price)</td>\n",
       "      <td>0.428509</td>\n",
       "      <td>0.353942</td>\n",
       "      <td>0.498770</td>\n",
       "      <td>charge</td>\n",
       "      <td>price</td>\n",
       "      <td>0.407655</td>\n",
       "      <td>0.352172</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>824</th>\n",
       "      <td>charge (rush)</td>\n",
       "      <td>0.551191</td>\n",
       "      <td>0.557549</td>\n",
       "      <td>0.591914</td>\n",
       "      <td>charge</td>\n",
       "      <td>rush</td>\n",
       "      <td>0.407655</td>\n",
       "      <td>0.352172</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                  words  F_Objectivity  F_Subjectivity  F_Context    word distinction  F_Objectivity_mean  F_Subjectivity_mean\n",
       "819              charge       0.340881        0.330534   0.438418  charge        None            0.407655             0.352172\n",
       "820       charge (card)       0.477083        0.293352   0.671798  charge        card            0.407655             0.352172\n",
       "821  charge (dependent)       0.256759        0.164572   0.791042  charge   dependent            0.407655             0.352172\n",
       "822   charge (electric)       0.391504        0.413080   0.697287  charge    electric            0.407655             0.352172\n",
       "823      charge (price)       0.428509        0.353942   0.498770  charge       price            0.407655             0.352172\n",
       "824       charge (rush)       0.551191        0.557549   0.591914  charge        rush            0.407655             0.352172"
      ]
     },
     "execution_count": 35,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "## Merging the mean values back into the original DataFrame\n",
    "df = pd.merge(df_homonym, mean_values, on='word', suffixes=('', '_mean'))\n",
    "\n",
    "## Checking an example (is the \"_mean\" values equal for all instances?): \n",
    "df[df['word'] == 'charge']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Dropping duplicated words (by column)\n",
    "df.drop_duplicates(subset=['word'], inplace=True)\n",
    "\n",
    "## Selecting only the columns of interest:\n",
    "df = df[['word', 'F_Objectivity_mean', 'F_Subjectivity_mean']]\n",
    "\n",
    "## Renaming the columns to remove the \"_mean\" suffix\n",
    "df.rename(columns={'F_Objectivity_mean': 'F_Objectivity', 'F_Subjectivity_mean': 'F_Subjectivity'}, inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'pandas.core.frame.DataFrame'>\n",
      "Int64Index: 4683 entries, 0 to 5552\n",
      "Data columns (total 3 columns):\n",
      " #   Column          Non-Null Count  Dtype  \n",
      "---  ------          --------------  -----  \n",
      " 0   word            4683 non-null   object \n",
      " 1   F_Objectivity   4683 non-null   float64\n",
      " 2   F_Subjectivity  4683 non-null   float64\n",
      "dtypes: float64(2), object(1)\n",
      "memory usage: 146.3+ KB\n",
      "None\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>word</th>\n",
       "      <th>F_Objectivity</th>\n",
       "      <th>F_Subjectivity</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>abattoir</td>\n",
       "      <td>0.512527</td>\n",
       "      <td>0.380603</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>abbey</td>\n",
       "      <td>0.714765</td>\n",
       "      <td>0.240456</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>abbreviate</td>\n",
       "      <td>0.286952</td>\n",
       "      <td>0.171052</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>abdicate</td>\n",
       "      <td>0.144736</td>\n",
       "      <td>0.384300</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>abdication</td>\n",
       "      <td>0.167654</td>\n",
       "      <td>0.334086</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "         word  F_Objectivity  F_Subjectivity\n",
       "0    abattoir       0.512527        0.380603\n",
       "1       abbey       0.714765        0.240456\n",
       "2  abbreviate       0.286952        0.171052\n",
       "3    abdicate       0.144736        0.384300\n",
       "4  abdication       0.167654        0.334086"
      ]
     },
     "execution_count": 37,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "## Showing dataframe:\n",
    "print(df.info())\n",
    "df.head(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Saving the prepared data:\n",
    "# df.to_csv('../data/df_factors_prepared.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ### Selecionando apenas as palavras no df_factors que atendam aos critérios:\n",
    "# df_selected = df.loc[((df['F_Subjectivity'] > 0.75) | (df['F_Subjectivity'] < 0.3)) & ((df['F_Objectivity'] > 0.75) | (df['F_Objectivity'] < 0.3))]\n",
    "# df_factors = df_selected.copy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "4683"
      ]
     },
     "execution_count": 40,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "## Generating a list of words from SCA for training.\n",
    "SCA_words = [word for word in df.word]\n",
    "len(SCA_words)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Preparing dataframe for training:\n",
    "# df.set_index('word', inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>word</th>\n",
       "      <th>F_Objectivity</th>\n",
       "      <th>F_Subjectivity</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>abattoir</td>\n",
       "      <td>0.512527</td>\n",
       "      <td>0.380603</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>abbey</td>\n",
       "      <td>0.714765</td>\n",
       "      <td>0.240456</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>abbreviate</td>\n",
       "      <td>0.286952</td>\n",
       "      <td>0.171052</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>abdicate</td>\n",
       "      <td>0.144736</td>\n",
       "      <td>0.384300</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>abdication</td>\n",
       "      <td>0.167654</td>\n",
       "      <td>0.334086</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5548</th>\n",
       "      <td>zeppelin</td>\n",
       "      <td>0.864760</td>\n",
       "      <td>0.396531</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5549</th>\n",
       "      <td>zero</td>\n",
       "      <td>0.379392</td>\n",
       "      <td>0.315118</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5550</th>\n",
       "      <td>zest</td>\n",
       "      <td>0.402894</td>\n",
       "      <td>0.476800</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5551</th>\n",
       "      <td>zoo</td>\n",
       "      <td>0.867152</td>\n",
       "      <td>0.507390</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5552</th>\n",
       "      <td>zoology</td>\n",
       "      <td>0.420591</td>\n",
       "      <td>0.345092</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>4683 rows × 3 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "            word  F_Objectivity  F_Subjectivity\n",
       "0       abattoir       0.512527        0.380603\n",
       "1          abbey       0.714765        0.240456\n",
       "2     abbreviate       0.286952        0.171052\n",
       "3       abdicate       0.144736        0.384300\n",
       "4     abdication       0.167654        0.334086\n",
       "...          ...            ...             ...\n",
       "5548    zeppelin       0.864760        0.396531\n",
       "5549        zero       0.379392        0.315118\n",
       "5550        zest       0.402894        0.476800\n",
       "5551         zoo       0.867152        0.507390\n",
       "5552     zoology       0.420591        0.345092\n",
       "\n",
       "[4683 rows x 3 columns]"
      ]
     },
     "execution_count": 42,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## DataPrep for machine learning classifiers\n",
    "This section prepares data from Semantic Content Analysis (SCA) and Word Embedding (WE) sources for machine learning classification tasks. Steps include data integration, cleaning, feature engineering, transformation, and splitting into training and test sets for linear and multilabel classification. The goal is to optimize data quality and model performance."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 200,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "# Splitting data in train (75%) and test (25%)\n",
    "train_df, test_df = train_test_split(df, test_size=0.25, random_state=42)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 201,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'pandas.core.frame.DataFrame'>\n",
      "Int64Index: 3512 entries, 213 to 1012\n",
      "Data columns (total 3 columns):\n",
      " #   Column          Non-Null Count  Dtype  \n",
      "---  ------          --------------  -----  \n",
      " 0   word            3512 non-null   object \n",
      " 1   F_Objectivity   3512 non-null   float64\n",
      " 2   F_Subjectivity  3512 non-null   float64\n",
      "dtypes: float64(2), object(1)\n",
      "memory usage: 109.8+ KB\n",
      "None\n",
      "<class 'pandas.core.frame.DataFrame'>\n",
      "Int64Index: 1171 entries, 513 to 3371\n",
      "Data columns (total 3 columns):\n",
      " #   Column          Non-Null Count  Dtype  \n",
      "---  ------          --------------  -----  \n",
      " 0   word            1171 non-null   object \n",
      " 1   F_Objectivity   1171 non-null   float64\n",
      " 2   F_Subjectivity  1171 non-null   float64\n",
      "dtypes: float64(2), object(1)\n",
      "memory usage: 36.6+ KB\n",
      "None\n"
     ]
    }
   ],
   "source": [
    "print(train_df.info())\n",
    "print(test_df.info())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 202,
   "metadata": {},
   "outputs": [],
   "source": [
    "def generateData(df_sca:pd.DataFrame):\n",
    "    '''Creates a dataset with explanatory variables from word embedding and dependent variables based on the SCA target data.\n",
    "    Usage example: \"X_train, Y_train = generateData(df)\"\n",
    "    Input: Pandas.DataFrame with SCA words.\n",
    "    Output: X: dataframe with explanatory variables; Y: dataframe with dependent variables.\n",
    "    '''\n",
    "    X = {}\n",
    "    Y = {}\n",
    "    \n",
    "    for i, row in df_sca.iterrows():\n",
    "        word = row['word']\n",
    "        f_objectivity = row['F_Objectivity']\n",
    "        f_subjectivity = row['F_Subjectivity']\n",
    "\n",
    "        try:\n",
    "            X[word] = nlp_getVector(word)[1]  # Stores the word vector\n",
    "            Y[word] = {'F_Objectivity': f_objectivity, 'F_Subjectivity': f_subjectivity}\n",
    "            # print(f'Debug: {word} <=> {nlp_getVector(word)[0]}')\n",
    "        except:\n",
    "            pass\n",
    "\n",
    "    return pd.DataFrame.from_dict(X, orient='index'), pd.DataFrame.from_dict(Y, orient='index')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### A) Regression models: splitting train and test data\n",
    "- Multi output (F_obj, F_sub), with continuous values."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 203,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Generating data:\n",
    "X_train_A, Y_train_A = generateData(train_df)\n",
    "X_test_A, Y_test_A = generateData(test_df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 204,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train data dimension:\n",
      "X_train: (3512, 512)\n",
      "Y_train: (3512, 2)\n",
      "\n",
      "Test data dimension:\n",
      "X_test: (1171, 512)\n",
      "Y_test: (1171, 2)\n"
     ]
    }
   ],
   "source": [
    "## Checking the generated data dimension:\n",
    "print(\"Train data dimension:\")\n",
    "print(\"X_train:\", X_train_A.shape)\n",
    "print(\"Y_train:\", Y_train_A.shape)\n",
    "\n",
    "print(\"\\nTest data dimension:\")\n",
    "print(\"X_test:\", X_test_A.shape)\n",
    "print(\"Y_test:\", Y_test_A.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 207,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>0</th>\n",
       "      <th>1</th>\n",
       "      <th>2</th>\n",
       "      <th>3</th>\n",
       "      <th>4</th>\n",
       "      <th>5</th>\n",
       "      <th>6</th>\n",
       "      <th>7</th>\n",
       "      <th>8</th>\n",
       "      <th>9</th>\n",
       "      <th>10</th>\n",
       "      <th>11</th>\n",
       "      <th>12</th>\n",
       "      <th>13</th>\n",
       "      <th>14</th>\n",
       "      <th>15</th>\n",
       "      <th>16</th>\n",
       "      <th>17</th>\n",
       "      <th>18</th>\n",
       "      <th>19</th>\n",
       "      <th>20</th>\n",
       "      <th>21</th>\n",
       "      <th>22</th>\n",
       "      <th>23</th>\n",
       "      <th>24</th>\n",
       "      <th>25</th>\n",
       "      <th>26</th>\n",
       "      <th>27</th>\n",
       "      <th>28</th>\n",
       "      <th>29</th>\n",
       "      <th>30</th>\n",
       "      <th>31</th>\n",
       "      <th>32</th>\n",
       "      <th>33</th>\n",
       "      <th>34</th>\n",
       "      <th>35</th>\n",
       "      <th>36</th>\n",
       "      <th>37</th>\n",
       "      <th>38</th>\n",
       "      <th>39</th>\n",
       "      <th>40</th>\n",
       "      <th>41</th>\n",
       "      <th>42</th>\n",
       "      <th>43</th>\n",
       "      <th>44</th>\n",
       "      <th>45</th>\n",
       "      <th>46</th>\n",
       "      <th>47</th>\n",
       "      <th>48</th>\n",
       "      <th>49</th>\n",
       "      <th>50</th>\n",
       "      <th>51</th>\n",
       "      <th>52</th>\n",
       "      <th>53</th>\n",
       "      <th>54</th>\n",
       "      <th>55</th>\n",
       "      <th>56</th>\n",
       "      <th>57</th>\n",
       "      <th>58</th>\n",
       "      <th>59</th>\n",
       "      <th>60</th>\n",
       "      <th>61</th>\n",
       "      <th>62</th>\n",
       "      <th>63</th>\n",
       "      <th>64</th>\n",
       "      <th>65</th>\n",
       "      <th>66</th>\n",
       "      <th>67</th>\n",
       "      <th>68</th>\n",
       "      <th>69</th>\n",
       "      <th>70</th>\n",
       "      <th>71</th>\n",
       "      <th>72</th>\n",
       "      <th>73</th>\n",
       "      <th>74</th>\n",
       "      <th>75</th>\n",
       "      <th>76</th>\n",
       "      <th>77</th>\n",
       "      <th>78</th>\n",
       "      <th>79</th>\n",
       "      <th>80</th>\n",
       "      <th>81</th>\n",
       "      <th>82</th>\n",
       "      <th>83</th>\n",
       "      <th>84</th>\n",
       "      <th>85</th>\n",
       "      <th>86</th>\n",
       "      <th>87</th>\n",
       "      <th>88</th>\n",
       "      <th>89</th>\n",
       "      <th>90</th>\n",
       "      <th>91</th>\n",
       "      <th>92</th>\n",
       "      <th>93</th>\n",
       "      <th>94</th>\n",
       "      <th>95</th>\n",
       "      <th>96</th>\n",
       "      <th>97</th>\n",
       "      <th>98</th>\n",
       "      <th>99</th>\n",
       "      <th>100</th>\n",
       "      <th>101</th>\n",
       "      <th>102</th>\n",
       "      <th>103</th>\n",
       "      <th>104</th>\n",
       "      <th>105</th>\n",
       "      <th>106</th>\n",
       "      <th>107</th>\n",
       "      <th>108</th>\n",
       "      <th>109</th>\n",
       "      <th>110</th>\n",
       "      <th>111</th>\n",
       "      <th>112</th>\n",
       "      <th>113</th>\n",
       "      <th>114</th>\n",
       "      <th>115</th>\n",
       "      <th>116</th>\n",
       "      <th>117</th>\n",
       "      <th>118</th>\n",
       "      <th>119</th>\n",
       "      <th>120</th>\n",
       "      <th>121</th>\n",
       "      <th>122</th>\n",
       "      <th>123</th>\n",
       "      <th>124</th>\n",
       "      <th>125</th>\n",
       "      <th>126</th>\n",
       "      <th>127</th>\n",
       "      <th>128</th>\n",
       "      <th>129</th>\n",
       "      <th>130</th>\n",
       "      <th>131</th>\n",
       "      <th>132</th>\n",
       "      <th>133</th>\n",
       "      <th>134</th>\n",
       "      <th>135</th>\n",
       "      <th>136</th>\n",
       "      <th>137</th>\n",
       "      <th>138</th>\n",
       "      <th>139</th>\n",
       "      <th>140</th>\n",
       "      <th>141</th>\n",
       "      <th>142</th>\n",
       "      <th>143</th>\n",
       "      <th>144</th>\n",
       "      <th>145</th>\n",
       "      <th>146</th>\n",
       "      <th>147</th>\n",
       "      <th>148</th>\n",
       "      <th>149</th>\n",
       "      <th>150</th>\n",
       "      <th>151</th>\n",
       "      <th>152</th>\n",
       "      <th>153</th>\n",
       "      <th>154</th>\n",
       "      <th>155</th>\n",
       "      <th>156</th>\n",
       "      <th>157</th>\n",
       "      <th>158</th>\n",
       "      <th>159</th>\n",
       "      <th>160</th>\n",
       "      <th>161</th>\n",
       "      <th>162</th>\n",
       "      <th>163</th>\n",
       "      <th>164</th>\n",
       "      <th>165</th>\n",
       "      <th>166</th>\n",
       "      <th>167</th>\n",
       "      <th>168</th>\n",
       "      <th>169</th>\n",
       "      <th>170</th>\n",
       "      <th>171</th>\n",
       "      <th>172</th>\n",
       "      <th>173</th>\n",
       "      <th>174</th>\n",
       "      <th>175</th>\n",
       "      <th>176</th>\n",
       "      <th>177</th>\n",
       "      <th>178</th>\n",
       "      <th>179</th>\n",
       "      <th>180</th>\n",
       "      <th>181</th>\n",
       "      <th>182</th>\n",
       "      <th>183</th>\n",
       "      <th>184</th>\n",
       "      <th>185</th>\n",
       "      <th>186</th>\n",
       "      <th>187</th>\n",
       "      <th>188</th>\n",
       "      <th>189</th>\n",
       "      <th>190</th>\n",
       "      <th>191</th>\n",
       "      <th>192</th>\n",
       "      <th>193</th>\n",
       "      <th>194</th>\n",
       "      <th>195</th>\n",
       "      <th>196</th>\n",
       "      <th>197</th>\n",
       "      <th>198</th>\n",
       "      <th>199</th>\n",
       "      <th>200</th>\n",
       "      <th>201</th>\n",
       "      <th>202</th>\n",
       "      <th>203</th>\n",
       "      <th>204</th>\n",
       "      <th>205</th>\n",
       "      <th>206</th>\n",
       "      <th>207</th>\n",
       "      <th>208</th>\n",
       "      <th>209</th>\n",
       "      <th>210</th>\n",
       "      <th>211</th>\n",
       "      <th>212</th>\n",
       "      <th>213</th>\n",
       "      <th>214</th>\n",
       "      <th>215</th>\n",
       "      <th>216</th>\n",
       "      <th>217</th>\n",
       "      <th>218</th>\n",
       "      <th>219</th>\n",
       "      <th>220</th>\n",
       "      <th>221</th>\n",
       "      <th>222</th>\n",
       "      <th>223</th>\n",
       "      <th>224</th>\n",
       "      <th>225</th>\n",
       "      <th>226</th>\n",
       "      <th>227</th>\n",
       "      <th>228</th>\n",
       "      <th>229</th>\n",
       "      <th>230</th>\n",
       "      <th>231</th>\n",
       "      <th>232</th>\n",
       "      <th>233</th>\n",
       "      <th>234</th>\n",
       "      <th>235</th>\n",
       "      <th>236</th>\n",
       "      <th>237</th>\n",
       "      <th>238</th>\n",
       "      <th>239</th>\n",
       "      <th>240</th>\n",
       "      <th>241</th>\n",
       "      <th>242</th>\n",
       "      <th>243</th>\n",
       "      <th>244</th>\n",
       "      <th>245</th>\n",
       "      <th>246</th>\n",
       "      <th>247</th>\n",
       "      <th>248</th>\n",
       "      <th>249</th>\n",
       "      <th>250</th>\n",
       "      <th>251</th>\n",
       "      <th>252</th>\n",
       "      <th>253</th>\n",
       "      <th>254</th>\n",
       "      <th>255</th>\n",
       "      <th>256</th>\n",
       "      <th>257</th>\n",
       "      <th>258</th>\n",
       "      <th>259</th>\n",
       "      <th>260</th>\n",
       "      <th>261</th>\n",
       "      <th>262</th>\n",
       "      <th>263</th>\n",
       "      <th>264</th>\n",
       "      <th>265</th>\n",
       "      <th>266</th>\n",
       "      <th>267</th>\n",
       "      <th>268</th>\n",
       "      <th>269</th>\n",
       "      <th>270</th>\n",
       "      <th>271</th>\n",
       "      <th>272</th>\n",
       "      <th>273</th>\n",
       "      <th>274</th>\n",
       "      <th>275</th>\n",
       "      <th>276</th>\n",
       "      <th>277</th>\n",
       "      <th>278</th>\n",
       "      <th>279</th>\n",
       "      <th>280</th>\n",
       "      <th>281</th>\n",
       "      <th>282</th>\n",
       "      <th>283</th>\n",
       "      <th>284</th>\n",
       "      <th>285</th>\n",
       "      <th>286</th>\n",
       "      <th>287</th>\n",
       "      <th>288</th>\n",
       "      <th>289</th>\n",
       "      <th>290</th>\n",
       "      <th>291</th>\n",
       "      <th>292</th>\n",
       "      <th>293</th>\n",
       "      <th>294</th>\n",
       "      <th>295</th>\n",
       "      <th>296</th>\n",
       "      <th>297</th>\n",
       "      <th>298</th>\n",
       "      <th>299</th>\n",
       "      <th>300</th>\n",
       "      <th>301</th>\n",
       "      <th>302</th>\n",
       "      <th>303</th>\n",
       "      <th>304</th>\n",
       "      <th>305</th>\n",
       "      <th>306</th>\n",
       "      <th>307</th>\n",
       "      <th>308</th>\n",
       "      <th>309</th>\n",
       "      <th>310</th>\n",
       "      <th>311</th>\n",
       "      <th>312</th>\n",
       "      <th>313</th>\n",
       "      <th>314</th>\n",
       "      <th>315</th>\n",
       "      <th>316</th>\n",
       "      <th>317</th>\n",
       "      <th>318</th>\n",
       "      <th>319</th>\n",
       "      <th>320</th>\n",
       "      <th>321</th>\n",
       "      <th>322</th>\n",
       "      <th>323</th>\n",
       "      <th>324</th>\n",
       "      <th>325</th>\n",
       "      <th>326</th>\n",
       "      <th>327</th>\n",
       "      <th>328</th>\n",
       "      <th>329</th>\n",
       "      <th>330</th>\n",
       "      <th>331</th>\n",
       "      <th>332</th>\n",
       "      <th>333</th>\n",
       "      <th>334</th>\n",
       "      <th>335</th>\n",
       "      <th>336</th>\n",
       "      <th>337</th>\n",
       "      <th>338</th>\n",
       "      <th>339</th>\n",
       "      <th>340</th>\n",
       "      <th>341</th>\n",
       "      <th>342</th>\n",
       "      <th>343</th>\n",
       "      <th>344</th>\n",
       "      <th>345</th>\n",
       "      <th>346</th>\n",
       "      <th>347</th>\n",
       "      <th>348</th>\n",
       "      <th>349</th>\n",
       "      <th>350</th>\n",
       "      <th>351</th>\n",
       "      <th>352</th>\n",
       "      <th>353</th>\n",
       "      <th>354</th>\n",
       "      <th>355</th>\n",
       "      <th>356</th>\n",
       "      <th>357</th>\n",
       "      <th>358</th>\n",
       "      <th>359</th>\n",
       "      <th>360</th>\n",
       "      <th>361</th>\n",
       "      <th>362</th>\n",
       "      <th>363</th>\n",
       "      <th>364</th>\n",
       "      <th>365</th>\n",
       "      <th>366</th>\n",
       "      <th>367</th>\n",
       "      <th>368</th>\n",
       "      <th>369</th>\n",
       "      <th>370</th>\n",
       "      <th>371</th>\n",
       "      <th>372</th>\n",
       "      <th>373</th>\n",
       "      <th>374</th>\n",
       "      <th>375</th>\n",
       "      <th>376</th>\n",
       "      <th>377</th>\n",
       "      <th>378</th>\n",
       "      <th>379</th>\n",
       "      <th>380</th>\n",
       "      <th>381</th>\n",
       "      <th>382</th>\n",
       "      <th>383</th>\n",
       "      <th>384</th>\n",
       "      <th>385</th>\n",
       "      <th>386</th>\n",
       "      <th>387</th>\n",
       "      <th>388</th>\n",
       "      <th>389</th>\n",
       "      <th>390</th>\n",
       "      <th>391</th>\n",
       "      <th>392</th>\n",
       "      <th>393</th>\n",
       "      <th>394</th>\n",
       "      <th>395</th>\n",
       "      <th>396</th>\n",
       "      <th>397</th>\n",
       "      <th>398</th>\n",
       "      <th>399</th>\n",
       "      <th>400</th>\n",
       "      <th>401</th>\n",
       "      <th>402</th>\n",
       "      <th>403</th>\n",
       "      <th>404</th>\n",
       "      <th>405</th>\n",
       "      <th>406</th>\n",
       "      <th>407</th>\n",
       "      <th>408</th>\n",
       "      <th>409</th>\n",
       "      <th>410</th>\n",
       "      <th>411</th>\n",
       "      <th>412</th>\n",
       "      <th>413</th>\n",
       "      <th>414</th>\n",
       "      <th>415</th>\n",
       "      <th>416</th>\n",
       "      <th>417</th>\n",
       "      <th>418</th>\n",
       "      <th>419</th>\n",
       "      <th>420</th>\n",
       "      <th>421</th>\n",
       "      <th>422</th>\n",
       "      <th>423</th>\n",
       "      <th>424</th>\n",
       "      <th>425</th>\n",
       "      <th>426</th>\n",
       "      <th>427</th>\n",
       "      <th>428</th>\n",
       "      <th>429</th>\n",
       "      <th>430</th>\n",
       "      <th>431</th>\n",
       "      <th>432</th>\n",
       "      <th>433</th>\n",
       "      <th>434</th>\n",
       "      <th>435</th>\n",
       "      <th>436</th>\n",
       "      <th>437</th>\n",
       "      <th>438</th>\n",
       "      <th>439</th>\n",
       "      <th>440</th>\n",
       "      <th>441</th>\n",
       "      <th>442</th>\n",
       "      <th>443</th>\n",
       "      <th>444</th>\n",
       "      <th>445</th>\n",
       "      <th>446</th>\n",
       "      <th>447</th>\n",
       "      <th>448</th>\n",
       "      <th>449</th>\n",
       "      <th>450</th>\n",
       "      <th>451</th>\n",
       "      <th>452</th>\n",
       "      <th>453</th>\n",
       "      <th>454</th>\n",
       "      <th>455</th>\n",
       "      <th>456</th>\n",
       "      <th>457</th>\n",
       "      <th>458</th>\n",
       "      <th>459</th>\n",
       "      <th>460</th>\n",
       "      <th>461</th>\n",
       "      <th>462</th>\n",
       "      <th>463</th>\n",
       "      <th>464</th>\n",
       "      <th>465</th>\n",
       "      <th>466</th>\n",
       "      <th>467</th>\n",
       "      <th>468</th>\n",
       "      <th>469</th>\n",
       "      <th>470</th>\n",
       "      <th>471</th>\n",
       "      <th>472</th>\n",
       "      <th>473</th>\n",
       "      <th>474</th>\n",
       "      <th>475</th>\n",
       "      <th>476</th>\n",
       "      <th>477</th>\n",
       "      <th>478</th>\n",
       "      <th>479</th>\n",
       "      <th>480</th>\n",
       "      <th>481</th>\n",
       "      <th>482</th>\n",
       "      <th>483</th>\n",
       "      <th>484</th>\n",
       "      <th>485</th>\n",
       "      <th>486</th>\n",
       "      <th>487</th>\n",
       "      <th>488</th>\n",
       "      <th>489</th>\n",
       "      <th>490</th>\n",
       "      <th>491</th>\n",
       "      <th>492</th>\n",
       "      <th>493</th>\n",
       "      <th>494</th>\n",
       "      <th>495</th>\n",
       "      <th>496</th>\n",
       "      <th>497</th>\n",
       "      <th>498</th>\n",
       "      <th>499</th>\n",
       "      <th>500</th>\n",
       "      <th>501</th>\n",
       "      <th>502</th>\n",
       "      <th>503</th>\n",
       "      <th>504</th>\n",
       "      <th>505</th>\n",
       "      <th>506</th>\n",
       "      <th>507</th>\n",
       "      <th>508</th>\n",
       "      <th>509</th>\n",
       "      <th>510</th>\n",
       "      <th>511</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>argument</th>\n",
       "      <td>-0.006003</td>\n",
       "      <td>-0.007702</td>\n",
       "      <td>0.049104</td>\n",
       "      <td>-0.064622</td>\n",
       "      <td>-0.025032</td>\n",
       "      <td>-0.019061</td>\n",
       "      <td>-0.020415</td>\n",
       "      <td>0.091926</td>\n",
       "      <td>-0.033597</td>\n",
       "      <td>-0.028624</td>\n",
       "      <td>0.027280</td>\n",
       "      <td>0.011417</td>\n",
       "      <td>-0.053613</td>\n",
       "      <td>0.018259</td>\n",
       "      <td>0.071262</td>\n",
       "      <td>0.063713</td>\n",
       "      <td>-0.013471</td>\n",
       "      <td>-0.011661</td>\n",
       "      <td>0.035724</td>\n",
       "      <td>-0.023335</td>\n",
       "      <td>0.032925</td>\n",
       "      <td>-0.025968</td>\n",
       "      <td>0.023209</td>\n",
       "      <td>-0.007237</td>\n",
       "      <td>-0.052589</td>\n",
       "      <td>-0.054912</td>\n",
       "      <td>-0.006545</td>\n",
       "      <td>-0.034289</td>\n",
       "      <td>-0.033662</td>\n",
       "      <td>0.066789</td>\n",
       "      <td>0.037343</td>\n",
       "      <td>0.011251</td>\n",
       "      <td>0.006737</td>\n",
       "      <td>0.023766</td>\n",
       "      <td>0.017598</td>\n",
       "      <td>0.015699</td>\n",
       "      <td>-0.052477</td>\n",
       "      <td>0.024599</td>\n",
       "      <td>0.047769</td>\n",
       "      <td>-0.046031</td>\n",
       "      <td>-0.029823</td>\n",
       "      <td>0.044138</td>\n",
       "      <td>0.034726</td>\n",
       "      <td>-0.037601</td>\n",
       "      <td>0.058254</td>\n",
       "      <td>0.044123</td>\n",
       "      <td>0.063278</td>\n",
       "      <td>0.088770</td>\n",
       "      <td>0.040936</td>\n",
       "      <td>0.012084</td>\n",
       "      <td>0.021672</td>\n",
       "      <td>0.020336</td>\n",
       "      <td>0.000022</td>\n",
       "      <td>-0.000254</td>\n",
       "      <td>-0.011542</td>\n",
       "      <td>-0.058915</td>\n",
       "      <td>0.043487</td>\n",
       "      <td>0.003159</td>\n",
       "      <td>-0.004820</td>\n",
       "      <td>0.062071</td>\n",
       "      <td>-0.031914</td>\n",
       "      <td>0.032600</td>\n",
       "      <td>-0.083566</td>\n",
       "      <td>-0.030406</td>\n",
       "      <td>-0.036129</td>\n",
       "      <td>-0.050030</td>\n",
       "      <td>-0.066287</td>\n",
       "      <td>0.006014</td>\n",
       "      <td>0.014364</td>\n",
       "      <td>0.013641</td>\n",
       "      <td>-0.006052</td>\n",
       "      <td>0.000432</td>\n",
       "      <td>-0.031755</td>\n",
       "      <td>-0.062195</td>\n",
       "      <td>0.019999</td>\n",
       "      <td>0.018836</td>\n",
       "      <td>0.024018</td>\n",
       "      <td>0.012946</td>\n",
       "      <td>-0.071339</td>\n",
       "      <td>-0.012209</td>\n",
       "      <td>-0.012602</td>\n",
       "      <td>0.073238</td>\n",
       "      <td>0.092335</td>\n",
       "      <td>0.041420</td>\n",
       "      <td>-0.036912</td>\n",
       "      <td>-0.016367</td>\n",
       "      <td>-0.059622</td>\n",
       "      <td>-0.034675</td>\n",
       "      <td>-0.011695</td>\n",
       "      <td>0.008536</td>\n",
       "      <td>0.031235</td>\n",
       "      <td>-0.036399</td>\n",
       "      <td>-0.022793</td>\n",
       "      <td>-0.003573</td>\n",
       "      <td>-0.085040</td>\n",
       "      <td>0.015403</td>\n",
       "      <td>0.049900</td>\n",
       "      <td>-0.005527</td>\n",
       "      <td>-0.005082</td>\n",
       "      <td>-0.032682</td>\n",
       "      <td>0.005251</td>\n",
       "      <td>0.021066</td>\n",
       "      <td>0.010309</td>\n",
       "      <td>0.002287</td>\n",
       "      <td>-0.041193</td>\n",
       "      <td>-0.043692</td>\n",
       "      <td>-0.018276</td>\n",
       "      <td>-0.046748</td>\n",
       "      <td>-0.001182</td>\n",
       "      <td>-0.064127</td>\n",
       "      <td>-0.044554</td>\n",
       "      <td>0.018138</td>\n",
       "      <td>-0.044678</td>\n",
       "      <td>0.038087</td>\n",
       "      <td>0.021035</td>\n",
       "      <td>0.038185</td>\n",
       "      <td>-0.025295</td>\n",
       "      <td>0.016275</td>\n",
       "      <td>0.007258</td>\n",
       "      <td>0.012708</td>\n",
       "      <td>-0.023058</td>\n",
       "      <td>-0.002954</td>\n",
       "      <td>0.012793</td>\n",
       "      <td>0.024006</td>\n",
       "      <td>-0.012133</td>\n",
       "      <td>-0.075228</td>\n",
       "      <td>-0.032491</td>\n",
       "      <td>-0.037764</td>\n",
       "      <td>0.010691</td>\n",
       "      <td>0.036100</td>\n",
       "      <td>-0.003719</td>\n",
       "      <td>-0.006661</td>\n",
       "      <td>-0.028511</td>\n",
       "      <td>0.045565</td>\n",
       "      <td>-0.069914</td>\n",
       "      <td>-0.002546</td>\n",
       "      <td>-0.021120</td>\n",
       "      <td>0.020760</td>\n",
       "      <td>-0.012043</td>\n",
       "      <td>-0.025806</td>\n",
       "      <td>0.003101</td>\n",
       "      <td>-0.043906</td>\n",
       "      <td>-0.021463</td>\n",
       "      <td>-0.045797</td>\n",
       "      <td>-0.009575</td>\n",
       "      <td>0.029342</td>\n",
       "      <td>-0.026703</td>\n",
       "      <td>-0.018808</td>\n",
       "      <td>-0.005254</td>\n",
       "      <td>0.004310</td>\n",
       "      <td>-0.024730</td>\n",
       "      <td>-0.010713</td>\n",
       "      <td>0.000740</td>\n",
       "      <td>0.039211</td>\n",
       "      <td>-0.056413</td>\n",
       "      <td>0.014849</td>\n",
       "      <td>-0.023637</td>\n",
       "      <td>0.028707</td>\n",
       "      <td>-0.046382</td>\n",
       "      <td>0.039182</td>\n",
       "      <td>-0.035561</td>\n",
       "      <td>-0.000134</td>\n",
       "      <td>0.005465</td>\n",
       "      <td>-0.030363</td>\n",
       "      <td>0.003977</td>\n",
       "      <td>-0.022546</td>\n",
       "      <td>-0.002373</td>\n",
       "      <td>0.045421</td>\n",
       "      <td>-0.043740</td>\n",
       "      <td>0.032824</td>\n",
       "      <td>-0.038087</td>\n",
       "      <td>0.003259</td>\n",
       "      <td>0.013397</td>\n",
       "      <td>0.006483</td>\n",
       "      <td>0.045332</td>\n",
       "      <td>-0.017813</td>\n",
       "      <td>0.047650</td>\n",
       "      <td>-0.019894</td>\n",
       "      <td>0.001230</td>\n",
       "      <td>-0.019093</td>\n",
       "      <td>0.037615</td>\n",
       "      <td>-0.018049</td>\n",
       "      <td>0.021043</td>\n",
       "      <td>0.007654</td>\n",
       "      <td>-0.030093</td>\n",
       "      <td>-0.039901</td>\n",
       "      <td>-0.049265</td>\n",
       "      <td>-0.060078</td>\n",
       "      <td>0.017513</td>\n",
       "      <td>-0.004326</td>\n",
       "      <td>-0.017963</td>\n",
       "      <td>-0.064216</td>\n",
       "      <td>-0.015015</td>\n",
       "      <td>0.019954</td>\n",
       "      <td>0.042063</td>\n",
       "      <td>0.073215</td>\n",
       "      <td>-0.010953</td>\n",
       "      <td>0.013774</td>\n",
       "      <td>-0.027665</td>\n",
       "      <td>-0.060651</td>\n",
       "      <td>0.015127</td>\n",
       "      <td>-0.048536</td>\n",
       "      <td>-0.059498</td>\n",
       "      <td>-0.019579</td>\n",
       "      <td>-0.054471</td>\n",
       "      <td>0.070263</td>\n",
       "      <td>-0.018107</td>\n",
       "      <td>0.008655</td>\n",
       "      <td>0.071418</td>\n",
       "      <td>-0.010173</td>\n",
       "      <td>-0.090049</td>\n",
       "      <td>-0.071707</td>\n",
       "      <td>0.015849</td>\n",
       "      <td>0.041161</td>\n",
       "      <td>0.040833</td>\n",
       "      <td>0.061819</td>\n",
       "      <td>-0.001309</td>\n",
       "      <td>-0.093380</td>\n",
       "      <td>-0.101444</td>\n",
       "      <td>0.005466</td>\n",
       "      <td>0.002472</td>\n",
       "      <td>-0.004649</td>\n",
       "      <td>0.001886</td>\n",
       "      <td>-0.050198</td>\n",
       "      <td>-0.003143</td>\n",
       "      <td>-0.050877</td>\n",
       "      <td>0.007076</td>\n",
       "      <td>0.222227</td>\n",
       "      <td>0.010635</td>\n",
       "      <td>-0.028757</td>\n",
       "      <td>-0.069166</td>\n",
       "      <td>0.061245</td>\n",
       "      <td>0.000606</td>\n",
       "      <td>0.037623</td>\n",
       "      <td>-0.045572</td>\n",
       "      <td>0.104749</td>\n",
       "      <td>-0.007717</td>\n",
       "      <td>-0.009740</td>\n",
       "      <td>-0.021305</td>\n",
       "      <td>0.036857</td>\n",
       "      <td>0.001859</td>\n",
       "      <td>0.085540</td>\n",
       "      <td>0.029976</td>\n",
       "      <td>0.050665</td>\n",
       "      <td>-0.041031</td>\n",
       "      <td>-0.002015</td>\n",
       "      <td>0.062726</td>\n",
       "      <td>0.030470</td>\n",
       "      <td>-0.013613</td>\n",
       "      <td>0.014459</td>\n",
       "      <td>-0.005388</td>\n",
       "      <td>-0.055521</td>\n",
       "      <td>0.036557</td>\n",
       "      <td>-0.046752</td>\n",
       "      <td>-0.068244</td>\n",
       "      <td>-0.043407</td>\n",
       "      <td>-0.015085</td>\n",
       "      <td>-0.045892</td>\n",
       "      <td>0.046858</td>\n",
       "      <td>-0.024430</td>\n",
       "      <td>0.001555</td>\n",
       "      <td>-0.039038</td>\n",
       "      <td>0.018319</td>\n",
       "      <td>0.008762</td>\n",
       "      <td>0.022932</td>\n",
       "      <td>-0.041819</td>\n",
       "      <td>-0.087458</td>\n",
       "      <td>-0.027710</td>\n",
       "      <td>0.143462</td>\n",
       "      <td>0.068875</td>\n",
       "      <td>-0.081839</td>\n",
       "      <td>-0.029243</td>\n",
       "      <td>0.050543</td>\n",
       "      <td>-0.012008</td>\n",
       "      <td>-0.032991</td>\n",
       "      <td>0.057572</td>\n",
       "      <td>0.061028</td>\n",
       "      <td>0.009955</td>\n",
       "      <td>-0.101006</td>\n",
       "      <td>0.012150</td>\n",
       "      <td>-0.040801</td>\n",
       "      <td>0.005670</td>\n",
       "      <td>-0.033847</td>\n",
       "      <td>0.063222</td>\n",
       "      <td>0.012602</td>\n",
       "      <td>0.031961</td>\n",
       "      <td>-0.049574</td>\n",
       "      <td>0.024087</td>\n",
       "      <td>0.007286</td>\n",
       "      <td>0.021664</td>\n",
       "      <td>-0.055415</td>\n",
       "      <td>0.034154</td>\n",
       "      <td>0.037429</td>\n",
       "      <td>-0.006535</td>\n",
       "      <td>-0.043686</td>\n",
       "      <td>0.041234</td>\n",
       "      <td>-0.045761</td>\n",
       "      <td>-0.047539</td>\n",
       "      <td>-0.047382</td>\n",
       "      <td>-0.024680</td>\n",
       "      <td>-0.081747</td>\n",
       "      <td>-0.023657</td>\n",
       "      <td>0.033543</td>\n",
       "      <td>-0.020524</td>\n",
       "      <td>-0.037243</td>\n",
       "      <td>0.029723</td>\n",
       "      <td>-0.047581</td>\n",
       "      <td>0.087062</td>\n",
       "      <td>-0.032729</td>\n",
       "      <td>0.046566</td>\n",
       "      <td>0.059990</td>\n",
       "      <td>0.032156</td>\n",
       "      <td>0.053130</td>\n",
       "      <td>-0.016996</td>\n",
       "      <td>0.085229</td>\n",
       "      <td>0.033314</td>\n",
       "      <td>-0.030072</td>\n",
       "      <td>0.009963</td>\n",
       "      <td>-0.008923</td>\n",
       "      <td>0.009869</td>\n",
       "      <td>0.005263</td>\n",
       "      <td>-0.021262</td>\n",
       "      <td>-0.004458</td>\n",
       "      <td>-0.021906</td>\n",
       "      <td>0.108421</td>\n",
       "      <td>-0.009200</td>\n",
       "      <td>0.004428</td>\n",
       "      <td>0.050933</td>\n",
       "      <td>0.090360</td>\n",
       "      <td>-0.006465</td>\n",
       "      <td>-0.002345</td>\n",
       "      <td>-0.008998</td>\n",
       "      <td>0.051154</td>\n",
       "      <td>0.015261</td>\n",
       "      <td>-0.053207</td>\n",
       "      <td>0.030310</td>\n",
       "      <td>0.062576</td>\n",
       "      <td>-0.047676</td>\n",
       "      <td>0.046907</td>\n",
       "      <td>0.051223</td>\n",
       "      <td>0.021178</td>\n",
       "      <td>-0.026583</td>\n",
       "      <td>-0.092427</td>\n",
       "      <td>-0.058884</td>\n",
       "      <td>0.017933</td>\n",
       "      <td>-0.012116</td>\n",
       "      <td>-0.016891</td>\n",
       "      <td>0.034947</td>\n",
       "      <td>-0.045107</td>\n",
       "      <td>0.056104</td>\n",
       "      <td>0.046036</td>\n",
       "      <td>0.049723</td>\n",
       "      <td>0.051819</td>\n",
       "      <td>-0.005840</td>\n",
       "      <td>-0.068100</td>\n",
       "      <td>-0.020950</td>\n",
       "      <td>-0.034312</td>\n",
       "      <td>-0.054905</td>\n",
       "      <td>0.060992</td>\n",
       "      <td>0.005778</td>\n",
       "      <td>-0.015211</td>\n",
       "      <td>-0.054036</td>\n",
       "      <td>-0.036320</td>\n",
       "      <td>-0.090872</td>\n",
       "      <td>0.066721</td>\n",
       "      <td>0.034548</td>\n",
       "      <td>-0.010227</td>\n",
       "      <td>-0.019921</td>\n",
       "      <td>-0.013126</td>\n",
       "      <td>-0.000326</td>\n",
       "      <td>-0.056792</td>\n",
       "      <td>-0.027753</td>\n",
       "      <td>0.018267</td>\n",
       "      <td>-0.017541</td>\n",
       "      <td>0.040908</td>\n",
       "      <td>0.024720</td>\n",
       "      <td>-0.065316</td>\n",
       "      <td>-0.036543</td>\n",
       "      <td>0.007539</td>\n",
       "      <td>-0.045257</td>\n",
       "      <td>-0.044589</td>\n",
       "      <td>-0.049280</td>\n",
       "      <td>0.006636</td>\n",
       "      <td>-0.019829</td>\n",
       "      <td>0.024869</td>\n",
       "      <td>-0.045506</td>\n",
       "      <td>0.030502</td>\n",
       "      <td>0.034103</td>\n",
       "      <td>-0.104293</td>\n",
       "      <td>-0.002812</td>\n",
       "      <td>0.000271</td>\n",
       "      <td>-0.022931</td>\n",
       "      <td>0.031732</td>\n",
       "      <td>-0.038318</td>\n",
       "      <td>0.008057</td>\n",
       "      <td>-0.015133</td>\n",
       "      <td>-0.016214</td>\n",
       "      <td>0.029840</td>\n",
       "      <td>-0.012779</td>\n",
       "      <td>-0.128566</td>\n",
       "      <td>-0.040349</td>\n",
       "      <td>-0.013225</td>\n",
       "      <td>0.013918</td>\n",
       "      <td>0.006946</td>\n",
       "      <td>0.012714</td>\n",
       "      <td>0.054271</td>\n",
       "      <td>-0.031807</td>\n",
       "      <td>-0.041463</td>\n",
       "      <td>0.011234</td>\n",
       "      <td>-0.039616</td>\n",
       "      <td>0.044495</td>\n",
       "      <td>0.003672</td>\n",
       "      <td>0.072688</td>\n",
       "      <td>-0.027824</td>\n",
       "      <td>0.018789</td>\n",
       "      <td>-0.132182</td>\n",
       "      <td>0.020682</td>\n",
       "      <td>0.007224</td>\n",
       "      <td>-0.077860</td>\n",
       "      <td>-0.036975</td>\n",
       "      <td>0.031022</td>\n",
       "      <td>-0.069942</td>\n",
       "      <td>0.075617</td>\n",
       "      <td>0.043619</td>\n",
       "      <td>0.026726</td>\n",
       "      <td>-0.018604</td>\n",
       "      <td>-0.062158</td>\n",
       "      <td>0.007207</td>\n",
       "      <td>0.003385</td>\n",
       "      <td>0.018415</td>\n",
       "      <td>-0.071181</td>\n",
       "      <td>0.042834</td>\n",
       "      <td>-0.076972</td>\n",
       "      <td>0.004844</td>\n",
       "      <td>0.003338</td>\n",
       "      <td>-0.070391</td>\n",
       "      <td>-0.008336</td>\n",
       "      <td>-0.058938</td>\n",
       "      <td>0.017085</td>\n",
       "      <td>-0.020420</td>\n",
       "      <td>-0.004337</td>\n",
       "      <td>-0.008181</td>\n",
       "      <td>0.049004</td>\n",
       "      <td>0.026595</td>\n",
       "      <td>-0.044924</td>\n",
       "      <td>0.074895</td>\n",
       "      <td>0.009845</td>\n",
       "      <td>0.060858</td>\n",
       "      <td>-0.019436</td>\n",
       "      <td>0.068924</td>\n",
       "      <td>0.010928</td>\n",
       "      <td>0.002579</td>\n",
       "      <td>0.010506</td>\n",
       "      <td>0.047682</td>\n",
       "      <td>-0.020770</td>\n",
       "      <td>0.044593</td>\n",
       "      <td>0.017637</td>\n",
       "      <td>-0.039327</td>\n",
       "      <td>-0.028829</td>\n",
       "      <td>-0.015196</td>\n",
       "      <td>0.025572</td>\n",
       "      <td>0.017846</td>\n",
       "      <td>-0.021445</td>\n",
       "      <td>0.036358</td>\n",
       "      <td>0.050771</td>\n",
       "      <td>-0.020593</td>\n",
       "      <td>-0.035496</td>\n",
       "      <td>0.011470</td>\n",
       "      <td>-0.065424</td>\n",
       "      <td>0.063719</td>\n",
       "      <td>-0.054271</td>\n",
       "      <td>0.032851</td>\n",
       "      <td>0.028792</td>\n",
       "      <td>-0.000063</td>\n",
       "      <td>0.013526</td>\n",
       "      <td>0.029719</td>\n",
       "      <td>-0.044451</td>\n",
       "      <td>0.014399</td>\n",
       "      <td>0.245867</td>\n",
       "      <td>0.073511</td>\n",
       "      <td>0.009029</td>\n",
       "      <td>0.003520</td>\n",
       "      <td>0.038824</td>\n",
       "      <td>-0.020015</td>\n",
       "      <td>-0.036546</td>\n",
       "      <td>0.011296</td>\n",
       "      <td>-0.018885</td>\n",
       "      <td>-0.030987</td>\n",
       "      <td>-0.017351</td>\n",
       "      <td>-0.037827</td>\n",
       "      <td>-0.059293</td>\n",
       "      <td>-0.007414</td>\n",
       "      <td>0.047752</td>\n",
       "      <td>-0.038985</td>\n",
       "      <td>-0.011051</td>\n",
       "      <td>0.171788</td>\n",
       "      <td>-0.002318</td>\n",
       "      <td>-0.010284</td>\n",
       "      <td>0.043155</td>\n",
       "      <td>0.037117</td>\n",
       "      <td>0.030925</td>\n",
       "      <td>-0.038471</td>\n",
       "      <td>-0.013875</td>\n",
       "      <td>-0.070968</td>\n",
       "      <td>-0.003252</td>\n",
       "      <td>0.005956</td>\n",
       "      <td>0.021636</td>\n",
       "      <td>0.015831</td>\n",
       "      <td>0.023877</td>\n",
       "      <td>0.047711</td>\n",
       "      <td>-0.063557</td>\n",
       "      <td>0.075657</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>awful</th>\n",
       "      <td>-0.025814</td>\n",
       "      <td>-0.001224</td>\n",
       "      <td>0.035236</td>\n",
       "      <td>0.001643</td>\n",
       "      <td>-0.066600</td>\n",
       "      <td>0.017712</td>\n",
       "      <td>0.103399</td>\n",
       "      <td>0.031633</td>\n",
       "      <td>-0.017865</td>\n",
       "      <td>0.001599</td>\n",
       "      <td>0.019264</td>\n",
       "      <td>0.039432</td>\n",
       "      <td>0.017838</td>\n",
       "      <td>-0.012615</td>\n",
       "      <td>-0.122431</td>\n",
       "      <td>0.011062</td>\n",
       "      <td>-0.031063</td>\n",
       "      <td>-0.013706</td>\n",
       "      <td>0.033635</td>\n",
       "      <td>0.007671</td>\n",
       "      <td>0.015961</td>\n",
       "      <td>0.019459</td>\n",
       "      <td>0.003425</td>\n",
       "      <td>0.006021</td>\n",
       "      <td>0.057787</td>\n",
       "      <td>-0.014008</td>\n",
       "      <td>0.005105</td>\n",
       "      <td>-0.010140</td>\n",
       "      <td>0.016574</td>\n",
       "      <td>0.019606</td>\n",
       "      <td>0.015120</td>\n",
       "      <td>0.022731</td>\n",
       "      <td>-0.044560</td>\n",
       "      <td>0.029897</td>\n",
       "      <td>0.023853</td>\n",
       "      <td>0.036493</td>\n",
       "      <td>0.028849</td>\n",
       "      <td>0.043707</td>\n",
       "      <td>-0.003260</td>\n",
       "      <td>-0.030843</td>\n",
       "      <td>-0.016285</td>\n",
       "      <td>-0.009341</td>\n",
       "      <td>-0.001705</td>\n",
       "      <td>0.009599</td>\n",
       "      <td>0.019213</td>\n",
       "      <td>-0.030681</td>\n",
       "      <td>0.042644</td>\n",
       "      <td>0.030699</td>\n",
       "      <td>0.009117</td>\n",
       "      <td>-0.016227</td>\n",
       "      <td>-0.055356</td>\n",
       "      <td>0.004963</td>\n",
       "      <td>-0.032168</td>\n",
       "      <td>0.033859</td>\n",
       "      <td>0.024406</td>\n",
       "      <td>-0.057546</td>\n",
       "      <td>-0.007470</td>\n",
       "      <td>0.014877</td>\n",
       "      <td>-0.024983</td>\n",
       "      <td>-0.059320</td>\n",
       "      <td>-0.022733</td>\n",
       "      <td>0.030149</td>\n",
       "      <td>-0.048973</td>\n",
       "      <td>-0.031368</td>\n",
       "      <td>-0.052566</td>\n",
       "      <td>-0.016700</td>\n",
       "      <td>-0.042668</td>\n",
       "      <td>0.049775</td>\n",
       "      <td>0.008358</td>\n",
       "      <td>-0.021421</td>\n",
       "      <td>-0.084370</td>\n",
       "      <td>-0.004991</td>\n",
       "      <td>0.026933</td>\n",
       "      <td>0.063891</td>\n",
       "      <td>0.031296</td>\n",
       "      <td>-0.009675</td>\n",
       "      <td>0.016345</td>\n",
       "      <td>0.023671</td>\n",
       "      <td>0.050449</td>\n",
       "      <td>-0.009255</td>\n",
       "      <td>-0.033355</td>\n",
       "      <td>0.038838</td>\n",
       "      <td>0.069530</td>\n",
       "      <td>0.012205</td>\n",
       "      <td>-0.051473</td>\n",
       "      <td>0.036883</td>\n",
       "      <td>-0.044990</td>\n",
       "      <td>-0.017582</td>\n",
       "      <td>-0.058773</td>\n",
       "      <td>0.012967</td>\n",
       "      <td>-0.062246</td>\n",
       "      <td>0.001836</td>\n",
       "      <td>-0.052009</td>\n",
       "      <td>0.057569</td>\n",
       "      <td>-0.084333</td>\n",
       "      <td>-0.028085</td>\n",
       "      <td>0.025998</td>\n",
       "      <td>0.068974</td>\n",
       "      <td>-0.027156</td>\n",
       "      <td>0.011427</td>\n",
       "      <td>0.032044</td>\n",
       "      <td>0.021471</td>\n",
       "      <td>-0.053132</td>\n",
       "      <td>-0.046528</td>\n",
       "      <td>-0.046233</td>\n",
       "      <td>-0.016096</td>\n",
       "      <td>0.078329</td>\n",
       "      <td>0.004418</td>\n",
       "      <td>0.029524</td>\n",
       "      <td>0.001182</td>\n",
       "      <td>0.013785</td>\n",
       "      <td>-0.006484</td>\n",
       "      <td>-0.041348</td>\n",
       "      <td>-0.015449</td>\n",
       "      <td>0.043145</td>\n",
       "      <td>0.036085</td>\n",
       "      <td>-0.022183</td>\n",
       "      <td>-0.061391</td>\n",
       "      <td>-0.035292</td>\n",
       "      <td>0.019617</td>\n",
       "      <td>-0.048725</td>\n",
       "      <td>-0.054573</td>\n",
       "      <td>0.046135</td>\n",
       "      <td>-0.024376</td>\n",
       "      <td>-0.011189</td>\n",
       "      <td>-0.032556</td>\n",
       "      <td>-0.009461</td>\n",
       "      <td>-0.011675</td>\n",
       "      <td>0.069323</td>\n",
       "      <td>-0.042037</td>\n",
       "      <td>0.032265</td>\n",
       "      <td>-0.096520</td>\n",
       "      <td>0.022075</td>\n",
       "      <td>0.044785</td>\n",
       "      <td>0.055126</td>\n",
       "      <td>-0.055678</td>\n",
       "      <td>-0.026223</td>\n",
       "      <td>0.062629</td>\n",
       "      <td>-0.025348</td>\n",
       "      <td>-0.003328</td>\n",
       "      <td>-0.028567</td>\n",
       "      <td>-0.020788</td>\n",
       "      <td>-0.038727</td>\n",
       "      <td>-0.104016</td>\n",
       "      <td>-0.046917</td>\n",
       "      <td>0.002023</td>\n",
       "      <td>0.019458</td>\n",
       "      <td>-0.061616</td>\n",
       "      <td>0.103710</td>\n",
       "      <td>0.015812</td>\n",
       "      <td>0.037145</td>\n",
       "      <td>0.063162</td>\n",
       "      <td>-0.004835</td>\n",
       "      <td>0.021962</td>\n",
       "      <td>-0.015525</td>\n",
       "      <td>0.051988</td>\n",
       "      <td>-0.025518</td>\n",
       "      <td>0.066387</td>\n",
       "      <td>-0.016573</td>\n",
       "      <td>-0.007669</td>\n",
       "      <td>0.027609</td>\n",
       "      <td>0.032354</td>\n",
       "      <td>0.028316</td>\n",
       "      <td>0.012097</td>\n",
       "      <td>-0.038119</td>\n",
       "      <td>-0.034135</td>\n",
       "      <td>0.044899</td>\n",
       "      <td>0.024231</td>\n",
       "      <td>0.023165</td>\n",
       "      <td>-0.004083</td>\n",
       "      <td>-0.017246</td>\n",
       "      <td>0.019143</td>\n",
       "      <td>-0.008551</td>\n",
       "      <td>-0.009865</td>\n",
       "      <td>-0.033172</td>\n",
       "      <td>-0.009114</td>\n",
       "      <td>0.041588</td>\n",
       "      <td>-0.027518</td>\n",
       "      <td>0.002568</td>\n",
       "      <td>0.041306</td>\n",
       "      <td>-0.004634</td>\n",
       "      <td>0.045953</td>\n",
       "      <td>0.035884</td>\n",
       "      <td>0.004849</td>\n",
       "      <td>-0.043088</td>\n",
       "      <td>-0.075771</td>\n",
       "      <td>-0.093825</td>\n",
       "      <td>-0.038505</td>\n",
       "      <td>-0.025050</td>\n",
       "      <td>-0.012569</td>\n",
       "      <td>-0.005734</td>\n",
       "      <td>0.029197</td>\n",
       "      <td>-0.006983</td>\n",
       "      <td>-0.001606</td>\n",
       "      <td>-0.044704</td>\n",
       "      <td>0.076233</td>\n",
       "      <td>0.021628</td>\n",
       "      <td>-0.037928</td>\n",
       "      <td>-0.009255</td>\n",
       "      <td>0.029191</td>\n",
       "      <td>0.034608</td>\n",
       "      <td>-0.019100</td>\n",
       "      <td>-0.007132</td>\n",
       "      <td>-0.040019</td>\n",
       "      <td>-0.024174</td>\n",
       "      <td>0.077984</td>\n",
       "      <td>0.089024</td>\n",
       "      <td>0.007429</td>\n",
       "      <td>0.080466</td>\n",
       "      <td>-0.018046</td>\n",
       "      <td>-0.105663</td>\n",
       "      <td>-0.044170</td>\n",
       "      <td>-0.012153</td>\n",
       "      <td>0.006893</td>\n",
       "      <td>-0.116216</td>\n",
       "      <td>-0.004283</td>\n",
       "      <td>-0.008455</td>\n",
       "      <td>-0.055686</td>\n",
       "      <td>-0.012810</td>\n",
       "      <td>0.028847</td>\n",
       "      <td>-0.038166</td>\n",
       "      <td>0.061139</td>\n",
       "      <td>0.020554</td>\n",
       "      <td>-0.083702</td>\n",
       "      <td>-0.046797</td>\n",
       "      <td>-0.075130</td>\n",
       "      <td>0.048761</td>\n",
       "      <td>0.168594</td>\n",
       "      <td>0.060373</td>\n",
       "      <td>-0.006223</td>\n",
       "      <td>0.012170</td>\n",
       "      <td>0.046612</td>\n",
       "      <td>0.044808</td>\n",
       "      <td>0.022969</td>\n",
       "      <td>-0.001923</td>\n",
       "      <td>0.033051</td>\n",
       "      <td>0.016386</td>\n",
       "      <td>-0.014145</td>\n",
       "      <td>0.010384</td>\n",
       "      <td>0.008070</td>\n",
       "      <td>-0.042769</td>\n",
       "      <td>0.125319</td>\n",
       "      <td>-0.056820</td>\n",
       "      <td>-0.035894</td>\n",
       "      <td>-0.006922</td>\n",
       "      <td>-0.012677</td>\n",
       "      <td>0.007999</td>\n",
       "      <td>0.024649</td>\n",
       "      <td>0.021535</td>\n",
       "      <td>-0.033725</td>\n",
       "      <td>-0.039064</td>\n",
       "      <td>-0.021279</td>\n",
       "      <td>-0.002241</td>\n",
       "      <td>0.011400</td>\n",
       "      <td>0.006222</td>\n",
       "      <td>0.022359</td>\n",
       "      <td>-0.030097</td>\n",
       "      <td>-0.003247</td>\n",
       "      <td>0.016503</td>\n",
       "      <td>-0.030274</td>\n",
       "      <td>-0.075307</td>\n",
       "      <td>-0.016346</td>\n",
       "      <td>0.033731</td>\n",
       "      <td>-0.024291</td>\n",
       "      <td>-0.006081</td>\n",
       "      <td>0.042765</td>\n",
       "      <td>0.006900</td>\n",
       "      <td>0.000358</td>\n",
       "      <td>-0.056346</td>\n",
       "      <td>-0.006351</td>\n",
       "      <td>0.021225</td>\n",
       "      <td>0.034121</td>\n",
       "      <td>0.032833</td>\n",
       "      <td>-0.001849</td>\n",
       "      <td>-0.021615</td>\n",
       "      <td>0.047153</td>\n",
       "      <td>0.023787</td>\n",
       "      <td>0.041327</td>\n",
       "      <td>0.023487</td>\n",
       "      <td>0.005254</td>\n",
       "      <td>-0.084705</td>\n",
       "      <td>0.011369</td>\n",
       "      <td>-0.000985</td>\n",
       "      <td>0.014425</td>\n",
       "      <td>0.028045</td>\n",
       "      <td>-0.118459</td>\n",
       "      <td>-0.099329</td>\n",
       "      <td>-0.020474</td>\n",
       "      <td>0.035424</td>\n",
       "      <td>0.016551</td>\n",
       "      <td>-0.097981</td>\n",
       "      <td>-0.022505</td>\n",
       "      <td>-0.026181</td>\n",
       "      <td>-0.082360</td>\n",
       "      <td>-0.039814</td>\n",
       "      <td>0.050217</td>\n",
       "      <td>-0.073369</td>\n",
       "      <td>-0.022035</td>\n",
       "      <td>0.001041</td>\n",
       "      <td>-0.074080</td>\n",
       "      <td>0.042409</td>\n",
       "      <td>0.021382</td>\n",
       "      <td>0.059136</td>\n",
       "      <td>-0.032165</td>\n",
       "      <td>0.010374</td>\n",
       "      <td>-0.003030</td>\n",
       "      <td>0.021610</td>\n",
       "      <td>0.030048</td>\n",
       "      <td>0.048521</td>\n",
       "      <td>0.001262</td>\n",
       "      <td>-0.030480</td>\n",
       "      <td>0.043803</td>\n",
       "      <td>0.094384</td>\n",
       "      <td>0.030824</td>\n",
       "      <td>0.011029</td>\n",
       "      <td>-0.030530</td>\n",
       "      <td>0.030132</td>\n",
       "      <td>-0.008222</td>\n",
       "      <td>-0.065402</td>\n",
       "      <td>0.083882</td>\n",
       "      <td>-0.010545</td>\n",
       "      <td>0.008701</td>\n",
       "      <td>0.024967</td>\n",
       "      <td>-0.012802</td>\n",
       "      <td>0.006015</td>\n",
       "      <td>0.000518</td>\n",
       "      <td>0.044395</td>\n",
       "      <td>0.013727</td>\n",
       "      <td>0.041821</td>\n",
       "      <td>-0.046804</td>\n",
       "      <td>0.015796</td>\n",
       "      <td>0.022438</td>\n",
       "      <td>0.056414</td>\n",
       "      <td>-0.100255</td>\n",
       "      <td>-0.052801</td>\n",
       "      <td>0.072645</td>\n",
       "      <td>-0.000807</td>\n",
       "      <td>-0.015112</td>\n",
       "      <td>0.048051</td>\n",
       "      <td>0.045322</td>\n",
       "      <td>-0.000488</td>\n",
       "      <td>0.003804</td>\n",
       "      <td>-0.058851</td>\n",
       "      <td>-0.002625</td>\n",
       "      <td>-0.009554</td>\n",
       "      <td>-0.105220</td>\n",
       "      <td>0.005676</td>\n",
       "      <td>-0.033893</td>\n",
       "      <td>-0.022610</td>\n",
       "      <td>-0.027222</td>\n",
       "      <td>0.007830</td>\n",
       "      <td>0.004210</td>\n",
       "      <td>0.009940</td>\n",
       "      <td>0.044020</td>\n",
       "      <td>0.018688</td>\n",
       "      <td>0.054338</td>\n",
       "      <td>-0.068623</td>\n",
       "      <td>0.004287</td>\n",
       "      <td>0.036706</td>\n",
       "      <td>-0.036743</td>\n",
       "      <td>0.018806</td>\n",
       "      <td>-0.058223</td>\n",
       "      <td>-0.007525</td>\n",
       "      <td>0.031833</td>\n",
       "      <td>0.074131</td>\n",
       "      <td>-0.026629</td>\n",
       "      <td>-0.066046</td>\n",
       "      <td>0.022882</td>\n",
       "      <td>0.018383</td>\n",
       "      <td>0.039229</td>\n",
       "      <td>0.023241</td>\n",
       "      <td>0.041160</td>\n",
       "      <td>0.050562</td>\n",
       "      <td>0.051417</td>\n",
       "      <td>0.041208</td>\n",
       "      <td>-0.029097</td>\n",
       "      <td>-0.025675</td>\n",
       "      <td>0.064232</td>\n",
       "      <td>0.058562</td>\n",
       "      <td>0.058524</td>\n",
       "      <td>-0.060189</td>\n",
       "      <td>-0.008871</td>\n",
       "      <td>0.020446</td>\n",
       "      <td>0.011055</td>\n",
       "      <td>-0.031117</td>\n",
       "      <td>-0.062404</td>\n",
       "      <td>0.033092</td>\n",
       "      <td>-0.033073</td>\n",
       "      <td>-0.012284</td>\n",
       "      <td>0.012192</td>\n",
       "      <td>0.059441</td>\n",
       "      <td>-0.054400</td>\n",
       "      <td>0.055429</td>\n",
       "      <td>-0.075863</td>\n",
       "      <td>0.025259</td>\n",
       "      <td>0.023958</td>\n",
       "      <td>0.013989</td>\n",
       "      <td>-0.035505</td>\n",
       "      <td>0.009986</td>\n",
       "      <td>-0.009116</td>\n",
       "      <td>0.027473</td>\n",
       "      <td>-0.037504</td>\n",
       "      <td>-0.014115</td>\n",
       "      <td>0.043986</td>\n",
       "      <td>0.024755</td>\n",
       "      <td>0.047453</td>\n",
       "      <td>-0.011431</td>\n",
       "      <td>0.020090</td>\n",
       "      <td>0.024479</td>\n",
       "      <td>-0.018380</td>\n",
       "      <td>-0.026233</td>\n",
       "      <td>0.050278</td>\n",
       "      <td>-0.001687</td>\n",
       "      <td>-0.022463</td>\n",
       "      <td>-0.007484</td>\n",
       "      <td>0.020594</td>\n",
       "      <td>0.044974</td>\n",
       "      <td>-0.029187</td>\n",
       "      <td>0.001596</td>\n",
       "      <td>0.031080</td>\n",
       "      <td>0.048563</td>\n",
       "      <td>0.007506</td>\n",
       "      <td>-0.025279</td>\n",
       "      <td>0.025394</td>\n",
       "      <td>0.014345</td>\n",
       "      <td>-0.006770</td>\n",
       "      <td>0.016871</td>\n",
       "      <td>0.020040</td>\n",
       "      <td>0.088223</td>\n",
       "      <td>0.004570</td>\n",
       "      <td>0.005807</td>\n",
       "      <td>-0.028890</td>\n",
       "      <td>-0.015818</td>\n",
       "      <td>0.011849</td>\n",
       "      <td>0.054793</td>\n",
       "      <td>0.025766</td>\n",
       "      <td>-0.038911</td>\n",
       "      <td>-0.064071</td>\n",
       "      <td>-0.013066</td>\n",
       "      <td>-0.001554</td>\n",
       "      <td>0.019293</td>\n",
       "      <td>0.029381</td>\n",
       "      <td>0.102294</td>\n",
       "      <td>0.030933</td>\n",
       "      <td>0.068220</td>\n",
       "      <td>0.204491</td>\n",
       "      <td>-0.041758</td>\n",
       "      <td>-0.008564</td>\n",
       "      <td>0.061152</td>\n",
       "      <td>0.039016</td>\n",
       "      <td>0.069776</td>\n",
       "      <td>-0.041614</td>\n",
       "      <td>0.050936</td>\n",
       "      <td>-0.042269</td>\n",
       "      <td>-0.000002</td>\n",
       "      <td>0.043146</td>\n",
       "      <td>-0.048839</td>\n",
       "      <td>-0.050978</td>\n",
       "      <td>0.022706</td>\n",
       "      <td>-0.015931</td>\n",
       "      <td>-0.023049</td>\n",
       "      <td>-0.003325</td>\n",
       "      <td>0.017269</td>\n",
       "      <td>0.018861</td>\n",
       "      <td>-0.007192</td>\n",
       "      <td>0.065646</td>\n",
       "      <td>0.003827</td>\n",
       "      <td>0.026161</td>\n",
       "      <td>0.010411</td>\n",
       "      <td>-0.001105</td>\n",
       "      <td>-0.028084</td>\n",
       "      <td>-0.043691</td>\n",
       "      <td>-0.019756</td>\n",
       "      <td>-0.031428</td>\n",
       "      <td>0.018163</td>\n",
       "      <td>0.086441</td>\n",
       "      <td>-0.012338</td>\n",
       "      <td>0.027901</td>\n",
       "      <td>0.242784</td>\n",
       "      <td>0.011662</td>\n",
       "      <td>-0.045923</td>\n",
       "      <td>0.036216</td>\n",
       "      <td>-0.040422</td>\n",
       "      <td>0.006995</td>\n",
       "      <td>0.060533</td>\n",
       "      <td>-0.117955</td>\n",
       "      <td>-0.068407</td>\n",
       "      <td>-0.055641</td>\n",
       "      <td>0.000393</td>\n",
       "      <td>-0.039048</td>\n",
       "      <td>0.000108</td>\n",
       "      <td>-0.018131</td>\n",
       "      <td>-0.021651</td>\n",
       "      <td>0.064353</td>\n",
       "      <td>-0.116046</td>\n",
       "      <td>0.137493</td>\n",
       "      <td>0.017328</td>\n",
       "      <td>-0.028578</td>\n",
       "      <td>0.066309</td>\n",
       "      <td>-0.010129</td>\n",
       "      <td>-0.013693</td>\n",
       "      <td>0.036508</td>\n",
       "      <td>-0.052221</td>\n",
       "      <td>-0.052194</td>\n",
       "      <td>-0.018417</td>\n",
       "      <td>-0.035257</td>\n",
       "      <td>0.007977</td>\n",
       "      <td>0.013068</td>\n",
       "      <td>-0.020170</td>\n",
       "      <td>0.054807</td>\n",
       "      <td>-0.033252</td>\n",
       "      <td>-0.039256</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>decorator</th>\n",
       "      <td>0.028367</td>\n",
       "      <td>-0.011092</td>\n",
       "      <td>0.065735</td>\n",
       "      <td>-0.023946</td>\n",
       "      <td>0.007433</td>\n",
       "      <td>-0.042059</td>\n",
       "      <td>0.037751</td>\n",
       "      <td>-0.063795</td>\n",
       "      <td>0.006671</td>\n",
       "      <td>-0.045171</td>\n",
       "      <td>0.001256</td>\n",
       "      <td>0.005200</td>\n",
       "      <td>-0.054715</td>\n",
       "      <td>-0.010511</td>\n",
       "      <td>-0.019794</td>\n",
       "      <td>0.047819</td>\n",
       "      <td>-0.011350</td>\n",
       "      <td>-0.018326</td>\n",
       "      <td>0.035546</td>\n",
       "      <td>0.017572</td>\n",
       "      <td>0.031661</td>\n",
       "      <td>0.035187</td>\n",
       "      <td>0.109695</td>\n",
       "      <td>0.004948</td>\n",
       "      <td>0.006225</td>\n",
       "      <td>-0.083984</td>\n",
       "      <td>0.009715</td>\n",
       "      <td>0.030252</td>\n",
       "      <td>-0.002363</td>\n",
       "      <td>0.054821</td>\n",
       "      <td>0.039203</td>\n",
       "      <td>0.052721</td>\n",
       "      <td>0.082165</td>\n",
       "      <td>0.110000</td>\n",
       "      <td>-0.052309</td>\n",
       "      <td>-0.019377</td>\n",
       "      <td>0.097118</td>\n",
       "      <td>0.011658</td>\n",
       "      <td>-0.016264</td>\n",
       "      <td>0.041651</td>\n",
       "      <td>-0.066683</td>\n",
       "      <td>-0.070656</td>\n",
       "      <td>0.009422</td>\n",
       "      <td>0.061907</td>\n",
       "      <td>0.005387</td>\n",
       "      <td>-0.041726</td>\n",
       "      <td>-0.017049</td>\n",
       "      <td>0.037700</td>\n",
       "      <td>0.079488</td>\n",
       "      <td>0.000537</td>\n",
       "      <td>0.034387</td>\n",
       "      <td>0.052700</td>\n",
       "      <td>0.010411</td>\n",
       "      <td>-0.040935</td>\n",
       "      <td>-0.054288</td>\n",
       "      <td>-0.035206</td>\n",
       "      <td>0.081980</td>\n",
       "      <td>0.044010</td>\n",
       "      <td>-0.030655</td>\n",
       "      <td>-0.009150</td>\n",
       "      <td>0.017010</td>\n",
       "      <td>-0.090732</td>\n",
       "      <td>0.009580</td>\n",
       "      <td>-0.069278</td>\n",
       "      <td>-0.041877</td>\n",
       "      <td>-0.029918</td>\n",
       "      <td>0.038424</td>\n",
       "      <td>0.001841</td>\n",
       "      <td>0.007457</td>\n",
       "      <td>-0.019183</td>\n",
       "      <td>0.052403</td>\n",
       "      <td>-0.018009</td>\n",
       "      <td>-0.056961</td>\n",
       "      <td>-0.045440</td>\n",
       "      <td>0.009813</td>\n",
       "      <td>-0.048325</td>\n",
       "      <td>-0.065862</td>\n",
       "      <td>-0.119715</td>\n",
       "      <td>0.002029</td>\n",
       "      <td>-0.073085</td>\n",
       "      <td>-0.008515</td>\n",
       "      <td>0.016100</td>\n",
       "      <td>-0.006301</td>\n",
       "      <td>0.015268</td>\n",
       "      <td>-0.001366</td>\n",
       "      <td>0.026453</td>\n",
       "      <td>-0.017920</td>\n",
       "      <td>-0.023437</td>\n",
       "      <td>-0.009428</td>\n",
       "      <td>0.004971</td>\n",
       "      <td>0.038333</td>\n",
       "      <td>-0.039551</td>\n",
       "      <td>0.043570</td>\n",
       "      <td>0.050496</td>\n",
       "      <td>-0.073858</td>\n",
       "      <td>-0.070829</td>\n",
       "      <td>-0.011753</td>\n",
       "      <td>0.036846</td>\n",
       "      <td>-0.025951</td>\n",
       "      <td>-0.004652</td>\n",
       "      <td>0.007844</td>\n",
       "      <td>-0.081316</td>\n",
       "      <td>-0.030543</td>\n",
       "      <td>0.002785</td>\n",
       "      <td>-0.102660</td>\n",
       "      <td>0.018139</td>\n",
       "      <td>-0.024110</td>\n",
       "      <td>-0.009186</td>\n",
       "      <td>0.018067</td>\n",
       "      <td>-0.001253</td>\n",
       "      <td>-0.011537</td>\n",
       "      <td>0.020364</td>\n",
       "      <td>-0.065169</td>\n",
       "      <td>0.083269</td>\n",
       "      <td>0.086014</td>\n",
       "      <td>0.049683</td>\n",
       "      <td>0.022026</td>\n",
       "      <td>-0.005992</td>\n",
       "      <td>-0.021545</td>\n",
       "      <td>0.046494</td>\n",
       "      <td>0.018731</td>\n",
       "      <td>-0.045790</td>\n",
       "      <td>-0.054476</td>\n",
       "      <td>0.061659</td>\n",
       "      <td>-0.038316</td>\n",
       "      <td>0.053238</td>\n",
       "      <td>-0.044938</td>\n",
       "      <td>-0.076508</td>\n",
       "      <td>-0.024807</td>\n",
       "      <td>0.029913</td>\n",
       "      <td>-0.029004</td>\n",
       "      <td>-0.087512</td>\n",
       "      <td>0.030835</td>\n",
       "      <td>0.043007</td>\n",
       "      <td>-0.046014</td>\n",
       "      <td>-0.048448</td>\n",
       "      <td>-0.030697</td>\n",
       "      <td>0.017824</td>\n",
       "      <td>-0.011125</td>\n",
       "      <td>0.000264</td>\n",
       "      <td>-0.015512</td>\n",
       "      <td>-0.051955</td>\n",
       "      <td>0.025651</td>\n",
       "      <td>-0.032956</td>\n",
       "      <td>-0.023176</td>\n",
       "      <td>-0.028233</td>\n",
       "      <td>-0.016615</td>\n",
       "      <td>-0.011157</td>\n",
       "      <td>-0.028199</td>\n",
       "      <td>0.032952</td>\n",
       "      <td>-0.008998</td>\n",
       "      <td>-0.006820</td>\n",
       "      <td>-0.006296</td>\n",
       "      <td>0.057132</td>\n",
       "      <td>-0.018172</td>\n",
       "      <td>0.007020</td>\n",
       "      <td>-0.034926</td>\n",
       "      <td>-0.005988</td>\n",
       "      <td>-0.051606</td>\n",
       "      <td>-0.002869</td>\n",
       "      <td>-0.000451</td>\n",
       "      <td>-0.034114</td>\n",
       "      <td>0.017354</td>\n",
       "      <td>-0.056740</td>\n",
       "      <td>0.071157</td>\n",
       "      <td>-0.042537</td>\n",
       "      <td>0.072686</td>\n",
       "      <td>-0.036848</td>\n",
       "      <td>-0.073069</td>\n",
       "      <td>-0.012943</td>\n",
       "      <td>-0.008572</td>\n",
       "      <td>0.030938</td>\n",
       "      <td>-0.070706</td>\n",
       "      <td>-0.077870</td>\n",
       "      <td>0.019930</td>\n",
       "      <td>-0.037231</td>\n",
       "      <td>0.083560</td>\n",
       "      <td>0.032593</td>\n",
       "      <td>-0.075581</td>\n",
       "      <td>0.009650</td>\n",
       "      <td>-0.092142</td>\n",
       "      <td>0.001374</td>\n",
       "      <td>0.032725</td>\n",
       "      <td>-0.082506</td>\n",
       "      <td>0.000686</td>\n",
       "      <td>-0.005012</td>\n",
       "      <td>-0.084039</td>\n",
       "      <td>0.006215</td>\n",
       "      <td>-0.036115</td>\n",
       "      <td>-0.020796</td>\n",
       "      <td>-0.024117</td>\n",
       "      <td>-0.025680</td>\n",
       "      <td>0.003700</td>\n",
       "      <td>-0.014436</td>\n",
       "      <td>0.019060</td>\n",
       "      <td>0.079132</td>\n",
       "      <td>-0.030440</td>\n",
       "      <td>-0.085844</td>\n",
       "      <td>0.018209</td>\n",
       "      <td>-0.001825</td>\n",
       "      <td>0.044982</td>\n",
       "      <td>0.017309</td>\n",
       "      <td>-0.013224</td>\n",
       "      <td>-0.014878</td>\n",
       "      <td>-0.013913</td>\n",
       "      <td>0.043777</td>\n",
       "      <td>0.004967</td>\n",
       "      <td>0.023689</td>\n",
       "      <td>0.072543</td>\n",
       "      <td>-0.065101</td>\n",
       "      <td>-0.030108</td>\n",
       "      <td>0.011252</td>\n",
       "      <td>-0.007214</td>\n",
       "      <td>0.028208</td>\n",
       "      <td>0.030593</td>\n",
       "      <td>-0.079983</td>\n",
       "      <td>-0.025561</td>\n",
       "      <td>-0.103985</td>\n",
       "      <td>-0.040325</td>\n",
       "      <td>-0.055648</td>\n",
       "      <td>0.050174</td>\n",
       "      <td>-0.072414</td>\n",
       "      <td>0.035487</td>\n",
       "      <td>-0.063486</td>\n",
       "      <td>0.010625</td>\n",
       "      <td>-0.060106</td>\n",
       "      <td>0.014036</td>\n",
       "      <td>0.201772</td>\n",
       "      <td>-0.016781</td>\n",
       "      <td>-0.024157</td>\n",
       "      <td>-0.000831</td>\n",
       "      <td>0.004692</td>\n",
       "      <td>0.005389</td>\n",
       "      <td>0.011147</td>\n",
       "      <td>0.004645</td>\n",
       "      <td>0.022285</td>\n",
       "      <td>-0.004752</td>\n",
       "      <td>-0.063929</td>\n",
       "      <td>-0.014222</td>\n",
       "      <td>-0.019905</td>\n",
       "      <td>0.002861</td>\n",
       "      <td>0.108553</td>\n",
       "      <td>-0.039188</td>\n",
       "      <td>-0.012094</td>\n",
       "      <td>-0.017449</td>\n",
       "      <td>0.038870</td>\n",
       "      <td>-0.013583</td>\n",
       "      <td>0.074206</td>\n",
       "      <td>-0.014928</td>\n",
       "      <td>-0.053516</td>\n",
       "      <td>-0.060854</td>\n",
       "      <td>0.011825</td>\n",
       "      <td>0.055224</td>\n",
       "      <td>-0.067650</td>\n",
       "      <td>-0.026673</td>\n",
       "      <td>0.043983</td>\n",
       "      <td>-0.006435</td>\n",
       "      <td>-0.017319</td>\n",
       "      <td>0.053965</td>\n",
       "      <td>0.037241</td>\n",
       "      <td>0.011772</td>\n",
       "      <td>0.039992</td>\n",
       "      <td>-0.006880</td>\n",
       "      <td>0.044590</td>\n",
       "      <td>-0.003781</td>\n",
       "      <td>0.008772</td>\n",
       "      <td>-0.001212</td>\n",
       "      <td>-0.029882</td>\n",
       "      <td>0.099964</td>\n",
       "      <td>-0.003832</td>\n",
       "      <td>-0.015492</td>\n",
       "      <td>0.000826</td>\n",
       "      <td>-0.086720</td>\n",
       "      <td>0.001852</td>\n",
       "      <td>0.061735</td>\n",
       "      <td>0.098334</td>\n",
       "      <td>-0.027018</td>\n",
       "      <td>-0.002412</td>\n",
       "      <td>-0.033387</td>\n",
       "      <td>-0.041532</td>\n",
       "      <td>-0.035421</td>\n",
       "      <td>0.014119</td>\n",
       "      <td>-0.024187</td>\n",
       "      <td>0.021152</td>\n",
       "      <td>0.029263</td>\n",
       "      <td>0.021902</td>\n",
       "      <td>-0.066574</td>\n",
       "      <td>0.014818</td>\n",
       "      <td>0.023591</td>\n",
       "      <td>0.009331</td>\n",
       "      <td>-0.023462</td>\n",
       "      <td>0.070539</td>\n",
       "      <td>-0.006278</td>\n",
       "      <td>-0.031506</td>\n",
       "      <td>-0.073327</td>\n",
       "      <td>0.036041</td>\n",
       "      <td>-0.034746</td>\n",
       "      <td>0.023439</td>\n",
       "      <td>0.038204</td>\n",
       "      <td>0.044410</td>\n",
       "      <td>-0.032638</td>\n",
       "      <td>0.005619</td>\n",
       "      <td>-0.053308</td>\n",
       "      <td>0.015579</td>\n",
       "      <td>-0.002905</td>\n",
       "      <td>-0.002087</td>\n",
       "      <td>-0.020283</td>\n",
       "      <td>0.040506</td>\n",
       "      <td>-0.066762</td>\n",
       "      <td>-0.023335</td>\n",
       "      <td>-0.008828</td>\n",
       "      <td>-0.028174</td>\n",
       "      <td>0.057874</td>\n",
       "      <td>-0.007878</td>\n",
       "      <td>0.063340</td>\n",
       "      <td>-0.033202</td>\n",
       "      <td>-0.019415</td>\n",
       "      <td>0.033260</td>\n",
       "      <td>0.013230</td>\n",
       "      <td>-0.022448</td>\n",
       "      <td>-0.019014</td>\n",
       "      <td>-0.027914</td>\n",
       "      <td>0.074163</td>\n",
       "      <td>0.019025</td>\n",
       "      <td>0.063626</td>\n",
       "      <td>0.062294</td>\n",
       "      <td>0.003060</td>\n",
       "      <td>0.009223</td>\n",
       "      <td>-0.021689</td>\n",
       "      <td>-0.060590</td>\n",
       "      <td>-0.052508</td>\n",
       "      <td>-0.038133</td>\n",
       "      <td>0.055600</td>\n",
       "      <td>0.046227</td>\n",
       "      <td>-0.001290</td>\n",
       "      <td>0.007336</td>\n",
       "      <td>0.016289</td>\n",
       "      <td>-0.075799</td>\n",
       "      <td>0.039830</td>\n",
       "      <td>0.023385</td>\n",
       "      <td>0.017995</td>\n",
       "      <td>-0.046318</td>\n",
       "      <td>0.030597</td>\n",
       "      <td>0.025602</td>\n",
       "      <td>0.001760</td>\n",
       "      <td>0.079311</td>\n",
       "      <td>-0.081243</td>\n",
       "      <td>-0.036224</td>\n",
       "      <td>0.025410</td>\n",
       "      <td>0.002009</td>\n",
       "      <td>0.038523</td>\n",
       "      <td>0.055674</td>\n",
       "      <td>-0.079037</td>\n",
       "      <td>-0.022275</td>\n",
       "      <td>-0.015298</td>\n",
       "      <td>0.020786</td>\n",
       "      <td>-0.005905</td>\n",
       "      <td>0.099990</td>\n",
       "      <td>0.034154</td>\n",
       "      <td>0.001021</td>\n",
       "      <td>0.028852</td>\n",
       "      <td>-0.010889</td>\n",
       "      <td>-0.049443</td>\n",
       "      <td>-0.020298</td>\n",
       "      <td>-0.005632</td>\n",
       "      <td>0.008299</td>\n",
       "      <td>-0.026508</td>\n",
       "      <td>-0.002233</td>\n",
       "      <td>0.010184</td>\n",
       "      <td>0.015031</td>\n",
       "      <td>0.049737</td>\n",
       "      <td>-0.007136</td>\n",
       "      <td>-0.056401</td>\n",
       "      <td>0.009055</td>\n",
       "      <td>0.024226</td>\n",
       "      <td>-0.013518</td>\n",
       "      <td>-0.110892</td>\n",
       "      <td>0.076524</td>\n",
       "      <td>0.006127</td>\n",
       "      <td>-0.049789</td>\n",
       "      <td>-0.077080</td>\n",
       "      <td>0.008784</td>\n",
       "      <td>0.068113</td>\n",
       "      <td>0.003111</td>\n",
       "      <td>0.032945</td>\n",
       "      <td>-0.038457</td>\n",
       "      <td>0.053658</td>\n",
       "      <td>0.018622</td>\n",
       "      <td>0.021379</td>\n",
       "      <td>-0.066593</td>\n",
       "      <td>0.055001</td>\n",
       "      <td>0.026130</td>\n",
       "      <td>-0.005465</td>\n",
       "      <td>-0.007532</td>\n",
       "      <td>0.075804</td>\n",
       "      <td>-0.018204</td>\n",
       "      <td>0.104602</td>\n",
       "      <td>-0.023656</td>\n",
       "      <td>0.043339</td>\n",
       "      <td>-0.059699</td>\n",
       "      <td>-0.070714</td>\n",
       "      <td>-0.014626</td>\n",
       "      <td>-0.020609</td>\n",
       "      <td>-0.017734</td>\n",
       "      <td>-0.031290</td>\n",
       "      <td>-0.019365</td>\n",
       "      <td>-0.025496</td>\n",
       "      <td>0.021478</td>\n",
       "      <td>-0.020899</td>\n",
       "      <td>-0.089511</td>\n",
       "      <td>0.085749</td>\n",
       "      <td>0.031181</td>\n",
       "      <td>0.005333</td>\n",
       "      <td>-0.020131</td>\n",
       "      <td>-0.010428</td>\n",
       "      <td>0.005679</td>\n",
       "      <td>-0.036753</td>\n",
       "      <td>0.023591</td>\n",
       "      <td>-0.012101</td>\n",
       "      <td>0.018789</td>\n",
       "      <td>-0.083450</td>\n",
       "      <td>-0.055797</td>\n",
       "      <td>0.012806</td>\n",
       "      <td>-0.007653</td>\n",
       "      <td>-0.002296</td>\n",
       "      <td>-0.022206</td>\n",
       "      <td>-0.041705</td>\n",
       "      <td>0.002967</td>\n",
       "      <td>0.016144</td>\n",
       "      <td>0.019465</td>\n",
       "      <td>-0.004817</td>\n",
       "      <td>0.017198</td>\n",
       "      <td>-0.042148</td>\n",
       "      <td>0.067943</td>\n",
       "      <td>0.001053</td>\n",
       "      <td>-0.030570</td>\n",
       "      <td>-0.008952</td>\n",
       "      <td>0.046371</td>\n",
       "      <td>-0.038564</td>\n",
       "      <td>-0.025021</td>\n",
       "      <td>-0.098026</td>\n",
       "      <td>0.016978</td>\n",
       "      <td>0.022750</td>\n",
       "      <td>0.055851</td>\n",
       "      <td>0.005480</td>\n",
       "      <td>0.006505</td>\n",
       "      <td>-0.032474</td>\n",
       "      <td>0.024204</td>\n",
       "      <td>0.051205</td>\n",
       "      <td>0.019378</td>\n",
       "      <td>-0.030461</td>\n",
       "      <td>-0.021943</td>\n",
       "      <td>0.059405</td>\n",
       "      <td>0.050042</td>\n",
       "      <td>0.035683</td>\n",
       "      <td>-0.026165</td>\n",
       "      <td>-0.012636</td>\n",
       "      <td>0.016736</td>\n",
       "      <td>0.058192</td>\n",
       "      <td>-0.027293</td>\n",
       "      <td>0.009611</td>\n",
       "      <td>0.045623</td>\n",
       "      <td>-0.014549</td>\n",
       "      <td>0.026107</td>\n",
       "      <td>0.015931</td>\n",
       "      <td>0.016268</td>\n",
       "      <td>-0.047099</td>\n",
       "      <td>0.079324</td>\n",
       "      <td>-0.038588</td>\n",
       "      <td>-0.017416</td>\n",
       "      <td>-0.022368</td>\n",
       "      <td>0.003860</td>\n",
       "      <td>0.012133</td>\n",
       "      <td>0.079430</td>\n",
       "      <td>0.063534</td>\n",
       "      <td>0.045920</td>\n",
       "      <td>0.060749</td>\n",
       "      <td>0.077861</td>\n",
       "      <td>0.143558</td>\n",
       "      <td>-0.036330</td>\n",
       "      <td>-0.036515</td>\n",
       "      <td>-0.015118</td>\n",
       "      <td>-0.017046</td>\n",
       "      <td>0.021518</td>\n",
       "      <td>-0.034655</td>\n",
       "      <td>0.015253</td>\n",
       "      <td>0.040507</td>\n",
       "      <td>0.020399</td>\n",
       "      <td>0.011264</td>\n",
       "      <td>-0.025316</td>\n",
       "      <td>-0.010028</td>\n",
       "      <td>-0.013929</td>\n",
       "      <td>0.047816</td>\n",
       "      <td>0.020784</td>\n",
       "      <td>-0.000164</td>\n",
       "      <td>0.178284</td>\n",
       "      <td>-0.020195</td>\n",
       "      <td>0.000324</td>\n",
       "      <td>-0.020133</td>\n",
       "      <td>0.041894</td>\n",
       "      <td>0.024398</td>\n",
       "      <td>-0.003661</td>\n",
       "      <td>0.022667</td>\n",
       "      <td>-0.032577</td>\n",
       "      <td>-0.036059</td>\n",
       "      <td>-0.024736</td>\n",
       "      <td>-0.034689</td>\n",
       "      <td>0.064175</td>\n",
       "      <td>-0.005290</td>\n",
       "      <td>0.068678</td>\n",
       "      <td>-0.057283</td>\n",
       "      <td>0.007582</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>misconception</th>\n",
       "      <td>-0.008740</td>\n",
       "      <td>-0.051329</td>\n",
       "      <td>-0.006416</td>\n",
       "      <td>-0.016790</td>\n",
       "      <td>-0.031819</td>\n",
       "      <td>-0.076222</td>\n",
       "      <td>0.003560</td>\n",
       "      <td>0.052693</td>\n",
       "      <td>-0.010833</td>\n",
       "      <td>-0.035523</td>\n",
       "      <td>-0.018290</td>\n",
       "      <td>0.001195</td>\n",
       "      <td>0.006652</td>\n",
       "      <td>-0.021313</td>\n",
       "      <td>-0.033020</td>\n",
       "      <td>-0.007376</td>\n",
       "      <td>0.000875</td>\n",
       "      <td>0.049326</td>\n",
       "      <td>0.046574</td>\n",
       "      <td>0.003671</td>\n",
       "      <td>0.001937</td>\n",
       "      <td>0.044386</td>\n",
       "      <td>0.034593</td>\n",
       "      <td>-0.018633</td>\n",
       "      <td>-0.075689</td>\n",
       "      <td>0.000933</td>\n",
       "      <td>0.003517</td>\n",
       "      <td>-0.019119</td>\n",
       "      <td>-0.051836</td>\n",
       "      <td>-0.005512</td>\n",
       "      <td>0.005596</td>\n",
       "      <td>-0.046395</td>\n",
       "      <td>0.030977</td>\n",
       "      <td>0.023485</td>\n",
       "      <td>0.000793</td>\n",
       "      <td>-0.011765</td>\n",
       "      <td>-0.036940</td>\n",
       "      <td>-0.051507</td>\n",
       "      <td>0.019107</td>\n",
       "      <td>-0.063943</td>\n",
       "      <td>-0.034773</td>\n",
       "      <td>-0.016259</td>\n",
       "      <td>0.009174</td>\n",
       "      <td>0.027113</td>\n",
       "      <td>-0.006947</td>\n",
       "      <td>0.011418</td>\n",
       "      <td>-0.020839</td>\n",
       "      <td>0.065896</td>\n",
       "      <td>0.068960</td>\n",
       "      <td>-0.029245</td>\n",
       "      <td>0.026871</td>\n",
       "      <td>0.034483</td>\n",
       "      <td>-0.006166</td>\n",
       "      <td>-0.028449</td>\n",
       "      <td>0.072257</td>\n",
       "      <td>0.020487</td>\n",
       "      <td>0.026740</td>\n",
       "      <td>-0.017983</td>\n",
       "      <td>-0.000776</td>\n",
       "      <td>-0.038662</td>\n",
       "      <td>-0.007015</td>\n",
       "      <td>-0.042567</td>\n",
       "      <td>-0.032039</td>\n",
       "      <td>-0.063070</td>\n",
       "      <td>-0.030162</td>\n",
       "      <td>-0.062195</td>\n",
       "      <td>-0.131279</td>\n",
       "      <td>-0.033668</td>\n",
       "      <td>-0.024827</td>\n",
       "      <td>-0.037875</td>\n",
       "      <td>0.058952</td>\n",
       "      <td>0.022380</td>\n",
       "      <td>0.035568</td>\n",
       "      <td>-0.092405</td>\n",
       "      <td>0.001606</td>\n",
       "      <td>0.063155</td>\n",
       "      <td>0.012756</td>\n",
       "      <td>-0.003811</td>\n",
       "      <td>0.001569</td>\n",
       "      <td>-0.033032</td>\n",
       "      <td>-0.041889</td>\n",
       "      <td>0.039668</td>\n",
       "      <td>0.045286</td>\n",
       "      <td>0.106081</td>\n",
       "      <td>-0.015751</td>\n",
       "      <td>-0.013508</td>\n",
       "      <td>-0.044805</td>\n",
       "      <td>-0.021429</td>\n",
       "      <td>-0.002300</td>\n",
       "      <td>-0.031949</td>\n",
       "      <td>-0.013126</td>\n",
       "      <td>-0.017996</td>\n",
       "      <td>0.041479</td>\n",
       "      <td>0.077955</td>\n",
       "      <td>-0.023805</td>\n",
       "      <td>-0.083191</td>\n",
       "      <td>0.030816</td>\n",
       "      <td>-0.032733</td>\n",
       "      <td>0.004808</td>\n",
       "      <td>0.055090</td>\n",
       "      <td>0.011124</td>\n",
       "      <td>0.014574</td>\n",
       "      <td>0.006653</td>\n",
       "      <td>-0.006592</td>\n",
       "      <td>0.002484</td>\n",
       "      <td>0.069127</td>\n",
       "      <td>-0.033184</td>\n",
       "      <td>-0.008392</td>\n",
       "      <td>-0.015157</td>\n",
       "      <td>0.015916</td>\n",
       "      <td>0.028394</td>\n",
       "      <td>0.006368</td>\n",
       "      <td>0.056193</td>\n",
       "      <td>-0.097046</td>\n",
       "      <td>-0.022908</td>\n",
       "      <td>0.011579</td>\n",
       "      <td>-0.020131</td>\n",
       "      <td>-0.040010</td>\n",
       "      <td>-0.025259</td>\n",
       "      <td>0.035973</td>\n",
       "      <td>-0.006782</td>\n",
       "      <td>-0.015579</td>\n",
       "      <td>0.091322</td>\n",
       "      <td>0.015210</td>\n",
       "      <td>-0.045894</td>\n",
       "      <td>-0.065587</td>\n",
       "      <td>-0.087085</td>\n",
       "      <td>-0.010735</td>\n",
       "      <td>-0.057546</td>\n",
       "      <td>-0.017063</td>\n",
       "      <td>0.015025</td>\n",
       "      <td>-0.014781</td>\n",
       "      <td>0.008270</td>\n",
       "      <td>0.009398</td>\n",
       "      <td>-0.018601</td>\n",
       "      <td>-0.061463</td>\n",
       "      <td>0.009181</td>\n",
       "      <td>-0.007283</td>\n",
       "      <td>0.004293</td>\n",
       "      <td>0.027039</td>\n",
       "      <td>-0.017207</td>\n",
       "      <td>-0.086353</td>\n",
       "      <td>-0.020036</td>\n",
       "      <td>0.006695</td>\n",
       "      <td>-0.046034</td>\n",
       "      <td>0.045504</td>\n",
       "      <td>0.068095</td>\n",
       "      <td>0.024106</td>\n",
       "      <td>0.051508</td>\n",
       "      <td>0.039933</td>\n",
       "      <td>0.046587</td>\n",
       "      <td>0.004969</td>\n",
       "      <td>-0.039879</td>\n",
       "      <td>0.067279</td>\n",
       "      <td>0.007180</td>\n",
       "      <td>-0.029120</td>\n",
       "      <td>0.019269</td>\n",
       "      <td>0.034164</td>\n",
       "      <td>-0.070500</td>\n",
       "      <td>0.027361</td>\n",
       "      <td>0.028586</td>\n",
       "      <td>0.011200</td>\n",
       "      <td>0.023337</td>\n",
       "      <td>-0.007822</td>\n",
       "      <td>-0.014142</td>\n",
       "      <td>-0.049168</td>\n",
       "      <td>0.046043</td>\n",
       "      <td>0.036182</td>\n",
       "      <td>0.022774</td>\n",
       "      <td>0.010751</td>\n",
       "      <td>-0.078432</td>\n",
       "      <td>-0.011012</td>\n",
       "      <td>-0.052455</td>\n",
       "      <td>-0.004160</td>\n",
       "      <td>0.029537</td>\n",
       "      <td>-0.014945</td>\n",
       "      <td>0.049508</td>\n",
       "      <td>0.004427</td>\n",
       "      <td>-0.060298</td>\n",
       "      <td>0.002850</td>\n",
       "      <td>0.000894</td>\n",
       "      <td>-0.023529</td>\n",
       "      <td>0.036729</td>\n",
       "      <td>-0.002574</td>\n",
       "      <td>-0.055348</td>\n",
       "      <td>-0.035069</td>\n",
       "      <td>-0.043204</td>\n",
       "      <td>-0.018256</td>\n",
       "      <td>-0.040281</td>\n",
       "      <td>-0.033552</td>\n",
       "      <td>-0.038119</td>\n",
       "      <td>0.026945</td>\n",
       "      <td>-0.020373</td>\n",
       "      <td>-0.005260</td>\n",
       "      <td>0.060107</td>\n",
       "      <td>0.089125</td>\n",
       "      <td>-0.017129</td>\n",
       "      <td>-0.038864</td>\n",
       "      <td>-0.052481</td>\n",
       "      <td>-0.028283</td>\n",
       "      <td>0.017528</td>\n",
       "      <td>0.011551</td>\n",
       "      <td>-0.041482</td>\n",
       "      <td>-0.019878</td>\n",
       "      <td>0.006280</td>\n",
       "      <td>0.089455</td>\n",
       "      <td>-0.027106</td>\n",
       "      <td>0.115089</td>\n",
       "      <td>0.062107</td>\n",
       "      <td>0.036604</td>\n",
       "      <td>-0.031517</td>\n",
       "      <td>-0.054370</td>\n",
       "      <td>-0.017266</td>\n",
       "      <td>-0.024358</td>\n",
       "      <td>-0.013785</td>\n",
       "      <td>0.030015</td>\n",
       "      <td>-0.059942</td>\n",
       "      <td>-0.136513</td>\n",
       "      <td>-0.056101</td>\n",
       "      <td>0.063750</td>\n",
       "      <td>0.020367</td>\n",
       "      <td>-0.039145</td>\n",
       "      <td>0.007323</td>\n",
       "      <td>-0.086272</td>\n",
       "      <td>0.026548</td>\n",
       "      <td>-0.066745</td>\n",
       "      <td>-0.043001</td>\n",
       "      <td>0.213594</td>\n",
       "      <td>0.029921</td>\n",
       "      <td>-0.060267</td>\n",
       "      <td>0.029541</td>\n",
       "      <td>-0.069436</td>\n",
       "      <td>-0.004154</td>\n",
       "      <td>0.046355</td>\n",
       "      <td>0.048592</td>\n",
       "      <td>0.076948</td>\n",
       "      <td>0.029224</td>\n",
       "      <td>-0.025937</td>\n",
       "      <td>-0.009628</td>\n",
       "      <td>0.004597</td>\n",
       "      <td>-0.095912</td>\n",
       "      <td>0.116445</td>\n",
       "      <td>0.048329</td>\n",
       "      <td>-0.010976</td>\n",
       "      <td>-0.047604</td>\n",
       "      <td>0.020225</td>\n",
       "      <td>-0.015059</td>\n",
       "      <td>-0.009680</td>\n",
       "      <td>-0.083287</td>\n",
       "      <td>-0.015850</td>\n",
       "      <td>-0.015107</td>\n",
       "      <td>-0.045314</td>\n",
       "      <td>0.093198</td>\n",
       "      <td>0.031335</td>\n",
       "      <td>-0.042622</td>\n",
       "      <td>0.012149</td>\n",
       "      <td>-0.003424</td>\n",
       "      <td>0.004163</td>\n",
       "      <td>0.032577</td>\n",
       "      <td>0.034990</td>\n",
       "      <td>0.005032</td>\n",
       "      <td>0.003149</td>\n",
       "      <td>0.069475</td>\n",
       "      <td>0.026153</td>\n",
       "      <td>-0.043166</td>\n",
       "      <td>-0.015659</td>\n",
       "      <td>0.001541</td>\n",
       "      <td>-0.016054</td>\n",
       "      <td>0.083873</td>\n",
       "      <td>-0.001978</td>\n",
       "      <td>-0.007360</td>\n",
       "      <td>-0.019871</td>\n",
       "      <td>-0.009227</td>\n",
       "      <td>0.018492</td>\n",
       "      <td>0.041035</td>\n",
       "      <td>0.056705</td>\n",
       "      <td>0.051671</td>\n",
       "      <td>-0.027853</td>\n",
       "      <td>-0.041526</td>\n",
       "      <td>-0.018484</td>\n",
       "      <td>0.007721</td>\n",
       "      <td>0.041584</td>\n",
       "      <td>-0.015032</td>\n",
       "      <td>0.049796</td>\n",
       "      <td>0.018510</td>\n",
       "      <td>0.055636</td>\n",
       "      <td>-0.043135</td>\n",
       "      <td>0.041336</td>\n",
       "      <td>0.065599</td>\n",
       "      <td>-0.016202</td>\n",
       "      <td>-0.077946</td>\n",
       "      <td>-0.000349</td>\n",
       "      <td>-0.014886</td>\n",
       "      <td>0.013915</td>\n",
       "      <td>-0.052384</td>\n",
       "      <td>0.002494</td>\n",
       "      <td>0.000352</td>\n",
       "      <td>0.099715</td>\n",
       "      <td>0.024367</td>\n",
       "      <td>0.059694</td>\n",
       "      <td>-0.005762</td>\n",
       "      <td>0.029345</td>\n",
       "      <td>-0.022825</td>\n",
       "      <td>0.016579</td>\n",
       "      <td>-0.017972</td>\n",
       "      <td>-0.015370</td>\n",
       "      <td>0.011466</td>\n",
       "      <td>0.034426</td>\n",
       "      <td>0.009995</td>\n",
       "      <td>0.026683</td>\n",
       "      <td>-0.009343</td>\n",
       "      <td>-0.014509</td>\n",
       "      <td>0.048590</td>\n",
       "      <td>-0.028369</td>\n",
       "      <td>0.093055</td>\n",
       "      <td>-0.015003</td>\n",
       "      <td>-0.054098</td>\n",
       "      <td>-0.008378</td>\n",
       "      <td>-0.029254</td>\n",
       "      <td>0.037736</td>\n",
       "      <td>0.012997</td>\n",
       "      <td>-0.006899</td>\n",
       "      <td>0.063468</td>\n",
       "      <td>0.024596</td>\n",
       "      <td>-0.000979</td>\n",
       "      <td>0.047114</td>\n",
       "      <td>-0.051188</td>\n",
       "      <td>0.003591</td>\n",
       "      <td>0.079313</td>\n",
       "      <td>-0.057840</td>\n",
       "      <td>0.030539</td>\n",
       "      <td>0.026524</td>\n",
       "      <td>0.055417</td>\n",
       "      <td>0.003256</td>\n",
       "      <td>-0.031854</td>\n",
       "      <td>0.081011</td>\n",
       "      <td>-0.060259</td>\n",
       "      <td>-0.084436</td>\n",
       "      <td>0.046893</td>\n",
       "      <td>0.027551</td>\n",
       "      <td>0.013715</td>\n",
       "      <td>0.002328</td>\n",
       "      <td>-0.054635</td>\n",
       "      <td>-0.041842</td>\n",
       "      <td>-0.014738</td>\n",
       "      <td>-0.089174</td>\n",
       "      <td>0.013648</td>\n",
       "      <td>-0.086046</td>\n",
       "      <td>-0.036189</td>\n",
       "      <td>-0.011651</td>\n",
       "      <td>0.032080</td>\n",
       "      <td>-0.019531</td>\n",
       "      <td>0.032920</td>\n",
       "      <td>0.014804</td>\n",
       "      <td>0.004439</td>\n",
       "      <td>-0.001402</td>\n",
       "      <td>-0.039478</td>\n",
       "      <td>-0.003011</td>\n",
       "      <td>0.017562</td>\n",
       "      <td>-0.005941</td>\n",
       "      <td>0.061437</td>\n",
       "      <td>0.006496</td>\n",
       "      <td>-0.004199</td>\n",
       "      <td>0.049659</td>\n",
       "      <td>0.072598</td>\n",
       "      <td>0.009864</td>\n",
       "      <td>0.050740</td>\n",
       "      <td>0.019394</td>\n",
       "      <td>-0.053215</td>\n",
       "      <td>0.030262</td>\n",
       "      <td>0.107164</td>\n",
       "      <td>0.054089</td>\n",
       "      <td>-0.020286</td>\n",
       "      <td>-0.092332</td>\n",
       "      <td>0.037441</td>\n",
       "      <td>0.034548</td>\n",
       "      <td>-0.067169</td>\n",
       "      <td>0.030031</td>\n",
       "      <td>0.030816</td>\n",
       "      <td>0.073844</td>\n",
       "      <td>-0.032089</td>\n",
       "      <td>-0.010569</td>\n",
       "      <td>-0.024887</td>\n",
       "      <td>-0.011506</td>\n",
       "      <td>-0.011523</td>\n",
       "      <td>0.025564</td>\n",
       "      <td>0.040064</td>\n",
       "      <td>0.046811</td>\n",
       "      <td>-0.012338</td>\n",
       "      <td>0.041546</td>\n",
       "      <td>-0.032990</td>\n",
       "      <td>0.029368</td>\n",
       "      <td>0.061195</td>\n",
       "      <td>0.027804</td>\n",
       "      <td>0.024254</td>\n",
       "      <td>-0.043534</td>\n",
       "      <td>-0.018658</td>\n",
       "      <td>-0.043591</td>\n",
       "      <td>0.014441</td>\n",
       "      <td>-0.102390</td>\n",
       "      <td>-0.058495</td>\n",
       "      <td>-0.042832</td>\n",
       "      <td>0.019683</td>\n",
       "      <td>0.049219</td>\n",
       "      <td>-0.039676</td>\n",
       "      <td>0.013467</td>\n",
       "      <td>-0.021884</td>\n",
       "      <td>-0.026002</td>\n",
       "      <td>-0.013556</td>\n",
       "      <td>-0.004607</td>\n",
       "      <td>-0.026727</td>\n",
       "      <td>0.042340</td>\n",
       "      <td>-0.037656</td>\n",
       "      <td>0.020119</td>\n",
       "      <td>-0.002884</td>\n",
       "      <td>-0.038545</td>\n",
       "      <td>-0.006801</td>\n",
       "      <td>0.015269</td>\n",
       "      <td>0.035728</td>\n",
       "      <td>0.060773</td>\n",
       "      <td>-0.001170</td>\n",
       "      <td>-0.058063</td>\n",
       "      <td>-0.005184</td>\n",
       "      <td>0.021659</td>\n",
       "      <td>0.073308</td>\n",
       "      <td>0.050862</td>\n",
       "      <td>-0.003366</td>\n",
       "      <td>0.001381</td>\n",
       "      <td>0.026352</td>\n",
       "      <td>0.021566</td>\n",
       "      <td>-0.048172</td>\n",
       "      <td>-0.006590</td>\n",
       "      <td>0.015406</td>\n",
       "      <td>0.028573</td>\n",
       "      <td>-0.011892</td>\n",
       "      <td>-0.065901</td>\n",
       "      <td>0.000675</td>\n",
       "      <td>-0.001671</td>\n",
       "      <td>-0.027445</td>\n",
       "      <td>0.025454</td>\n",
       "      <td>-0.001399</td>\n",
       "      <td>0.002638</td>\n",
       "      <td>0.004629</td>\n",
       "      <td>-0.005780</td>\n",
       "      <td>-0.028694</td>\n",
       "      <td>0.167404</td>\n",
       "      <td>0.030653</td>\n",
       "      <td>0.017681</td>\n",
       "      <td>0.040271</td>\n",
       "      <td>0.034876</td>\n",
       "      <td>-0.056195</td>\n",
       "      <td>-0.011404</td>\n",
       "      <td>0.002961</td>\n",
       "      <td>-0.027944</td>\n",
       "      <td>0.035596</td>\n",
       "      <td>-0.039411</td>\n",
       "      <td>-0.062866</td>\n",
       "      <td>0.008502</td>\n",
       "      <td>0.017942</td>\n",
       "      <td>0.035756</td>\n",
       "      <td>0.029723</td>\n",
       "      <td>-0.020595</td>\n",
       "      <td>-0.017910</td>\n",
       "      <td>0.034796</td>\n",
       "      <td>-0.044080</td>\n",
       "      <td>0.065484</td>\n",
       "      <td>-0.014943</td>\n",
       "      <td>0.034587</td>\n",
       "      <td>-0.090077</td>\n",
       "      <td>0.057515</td>\n",
       "      <td>-0.053632</td>\n",
       "      <td>0.005629</td>\n",
       "      <td>-0.047457</td>\n",
       "      <td>-0.017616</td>\n",
       "      <td>0.035693</td>\n",
       "      <td>0.052416</td>\n",
       "      <td>0.022840</td>\n",
       "      <td>0.012177</td>\n",
       "      <td>0.204242</td>\n",
       "      <td>-0.030940</td>\n",
       "      <td>0.046657</td>\n",
       "      <td>0.045007</td>\n",
       "      <td>-0.008737</td>\n",
       "      <td>-0.019579</td>\n",
       "      <td>0.050593</td>\n",
       "      <td>-0.076919</td>\n",
       "      <td>0.013397</td>\n",
       "      <td>-0.032011</td>\n",
       "      <td>-0.004139</td>\n",
       "      <td>-0.055836</td>\n",
       "      <td>0.018894</td>\n",
       "      <td>-0.016503</td>\n",
       "      <td>0.029644</td>\n",
       "      <td>0.028216</td>\n",
       "      <td>-0.058165</td>\n",
       "      <td>0.197101</td>\n",
       "      <td>0.029622</td>\n",
       "      <td>0.061143</td>\n",
       "      <td>0.051334</td>\n",
       "      <td>0.039764</td>\n",
       "      <td>-0.007018</td>\n",
       "      <td>0.005230</td>\n",
       "      <td>-0.029945</td>\n",
       "      <td>-0.015367</td>\n",
       "      <td>0.005080</td>\n",
       "      <td>-0.029541</td>\n",
       "      <td>-0.002237</td>\n",
       "      <td>0.011822</td>\n",
       "      <td>-0.040193</td>\n",
       "      <td>-0.022842</td>\n",
       "      <td>-0.016331</td>\n",
       "      <td>-0.014565</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>crab</th>\n",
       "      <td>0.013815</td>\n",
       "      <td>0.072042</td>\n",
       "      <td>0.058833</td>\n",
       "      <td>0.021875</td>\n",
       "      <td>-0.024757</td>\n",
       "      <td>-0.017563</td>\n",
       "      <td>0.041158</td>\n",
       "      <td>-0.025145</td>\n",
       "      <td>-0.056022</td>\n",
       "      <td>0.014847</td>\n",
       "      <td>-0.030094</td>\n",
       "      <td>0.046056</td>\n",
       "      <td>0.035412</td>\n",
       "      <td>-0.029308</td>\n",
       "      <td>-0.017221</td>\n",
       "      <td>0.064846</td>\n",
       "      <td>-0.011287</td>\n",
       "      <td>-0.026741</td>\n",
       "      <td>-0.012687</td>\n",
       "      <td>-0.006003</td>\n",
       "      <td>0.009952</td>\n",
       "      <td>0.074889</td>\n",
       "      <td>0.055424</td>\n",
       "      <td>-0.060245</td>\n",
       "      <td>-0.033850</td>\n",
       "      <td>-0.033693</td>\n",
       "      <td>-0.007973</td>\n",
       "      <td>-0.058778</td>\n",
       "      <td>-0.001068</td>\n",
       "      <td>0.023246</td>\n",
       "      <td>-0.015459</td>\n",
       "      <td>0.016578</td>\n",
       "      <td>0.015308</td>\n",
       "      <td>0.032295</td>\n",
       "      <td>-0.045283</td>\n",
       "      <td>-0.001160</td>\n",
       "      <td>-0.003518</td>\n",
       "      <td>0.043212</td>\n",
       "      <td>-0.024304</td>\n",
       "      <td>0.029964</td>\n",
       "      <td>-0.048798</td>\n",
       "      <td>-0.086437</td>\n",
       "      <td>0.012914</td>\n",
       "      <td>0.034543</td>\n",
       "      <td>0.096366</td>\n",
       "      <td>0.015545</td>\n",
       "      <td>-0.126718</td>\n",
       "      <td>0.012450</td>\n",
       "      <td>0.037475</td>\n",
       "      <td>-0.029972</td>\n",
       "      <td>0.000681</td>\n",
       "      <td>-0.077662</td>\n",
       "      <td>-0.017088</td>\n",
       "      <td>0.044476</td>\n",
       "      <td>0.041312</td>\n",
       "      <td>0.049345</td>\n",
       "      <td>0.023336</td>\n",
       "      <td>-0.021704</td>\n",
       "      <td>-0.030215</td>\n",
       "      <td>-0.062838</td>\n",
       "      <td>-0.000023</td>\n",
       "      <td>-0.018700</td>\n",
       "      <td>-0.002224</td>\n",
       "      <td>0.018107</td>\n",
       "      <td>-0.004216</td>\n",
       "      <td>0.005748</td>\n",
       "      <td>0.009809</td>\n",
       "      <td>-0.032132</td>\n",
       "      <td>0.025655</td>\n",
       "      <td>-0.075112</td>\n",
       "      <td>0.002630</td>\n",
       "      <td>0.015726</td>\n",
       "      <td>0.106380</td>\n",
       "      <td>-0.016162</td>\n",
       "      <td>0.055200</td>\n",
       "      <td>-0.086515</td>\n",
       "      <td>0.005186</td>\n",
       "      <td>-0.037280</td>\n",
       "      <td>0.033714</td>\n",
       "      <td>-0.125661</td>\n",
       "      <td>0.021992</td>\n",
       "      <td>0.016013</td>\n",
       "      <td>0.054730</td>\n",
       "      <td>-0.010177</td>\n",
       "      <td>-0.017586</td>\n",
       "      <td>0.041512</td>\n",
       "      <td>0.030688</td>\n",
       "      <td>-0.077013</td>\n",
       "      <td>0.019401</td>\n",
       "      <td>0.085270</td>\n",
       "      <td>-0.010275</td>\n",
       "      <td>-0.025742</td>\n",
       "      <td>0.044508</td>\n",
       "      <td>0.001892</td>\n",
       "      <td>-0.021965</td>\n",
       "      <td>0.016240</td>\n",
       "      <td>-0.013455</td>\n",
       "      <td>0.071074</td>\n",
       "      <td>-0.098254</td>\n",
       "      <td>-0.043752</td>\n",
       "      <td>0.044720</td>\n",
       "      <td>-0.007102</td>\n",
       "      <td>-0.010289</td>\n",
       "      <td>-0.020186</td>\n",
       "      <td>-0.103446</td>\n",
       "      <td>-0.020217</td>\n",
       "      <td>-0.042713</td>\n",
       "      <td>0.004344</td>\n",
       "      <td>0.035194</td>\n",
       "      <td>-0.001463</td>\n",
       "      <td>-0.053345</td>\n",
       "      <td>0.005046</td>\n",
       "      <td>0.040522</td>\n",
       "      <td>0.015695</td>\n",
       "      <td>0.013973</td>\n",
       "      <td>0.032514</td>\n",
       "      <td>-0.071757</td>\n",
       "      <td>0.045940</td>\n",
       "      <td>0.006422</td>\n",
       "      <td>-0.012188</td>\n",
       "      <td>-0.040765</td>\n",
       "      <td>0.011898</td>\n",
       "      <td>-0.033316</td>\n",
       "      <td>-0.026340</td>\n",
       "      <td>0.048215</td>\n",
       "      <td>-0.002855</td>\n",
       "      <td>-0.008148</td>\n",
       "      <td>0.048121</td>\n",
       "      <td>0.031651</td>\n",
       "      <td>0.003180</td>\n",
       "      <td>0.008454</td>\n",
       "      <td>0.006841</td>\n",
       "      <td>0.078824</td>\n",
       "      <td>-0.036678</td>\n",
       "      <td>-0.031374</td>\n",
       "      <td>-0.036645</td>\n",
       "      <td>-0.055471</td>\n",
       "      <td>0.028440</td>\n",
       "      <td>0.080996</td>\n",
       "      <td>0.027412</td>\n",
       "      <td>0.003869</td>\n",
       "      <td>-0.065069</td>\n",
       "      <td>0.022016</td>\n",
       "      <td>-0.071869</td>\n",
       "      <td>-0.067763</td>\n",
       "      <td>0.060468</td>\n",
       "      <td>0.073570</td>\n",
       "      <td>-0.037831</td>\n",
       "      <td>0.012421</td>\n",
       "      <td>0.056417</td>\n",
       "      <td>-0.022359</td>\n",
       "      <td>0.069121</td>\n",
       "      <td>0.025398</td>\n",
       "      <td>0.042478</td>\n",
       "      <td>0.075842</td>\n",
       "      <td>0.004501</td>\n",
       "      <td>-0.032782</td>\n",
       "      <td>0.009961</td>\n",
       "      <td>-0.063708</td>\n",
       "      <td>-0.024685</td>\n",
       "      <td>0.068454</td>\n",
       "      <td>-0.049711</td>\n",
       "      <td>-0.033469</td>\n",
       "      <td>-0.009454</td>\n",
       "      <td>-0.054613</td>\n",
       "      <td>-0.033370</td>\n",
       "      <td>0.038838</td>\n",
       "      <td>0.028941</td>\n",
       "      <td>-0.006498</td>\n",
       "      <td>0.024553</td>\n",
       "      <td>-0.046694</td>\n",
       "      <td>0.029212</td>\n",
       "      <td>0.044602</td>\n",
       "      <td>0.013487</td>\n",
       "      <td>0.038077</td>\n",
       "      <td>-0.054533</td>\n",
       "      <td>-0.038407</td>\n",
       "      <td>-0.005741</td>\n",
       "      <td>-0.008698</td>\n",
       "      <td>-0.041561</td>\n",
       "      <td>0.088558</td>\n",
       "      <td>-0.016361</td>\n",
       "      <td>0.046504</td>\n",
       "      <td>-0.003924</td>\n",
       "      <td>-0.068363</td>\n",
       "      <td>0.071991</td>\n",
       "      <td>-0.040021</td>\n",
       "      <td>-0.022836</td>\n",
       "      <td>-0.007439</td>\n",
       "      <td>-0.017961</td>\n",
       "      <td>-0.041254</td>\n",
       "      <td>-0.020316</td>\n",
       "      <td>0.049874</td>\n",
       "      <td>-0.021310</td>\n",
       "      <td>-0.000732</td>\n",
       "      <td>-0.007608</td>\n",
       "      <td>-0.047637</td>\n",
       "      <td>-0.052515</td>\n",
       "      <td>0.011943</td>\n",
       "      <td>0.005381</td>\n",
       "      <td>0.104699</td>\n",
       "      <td>-0.057556</td>\n",
       "      <td>0.022107</td>\n",
       "      <td>0.043328</td>\n",
       "      <td>-0.041582</td>\n",
       "      <td>0.043138</td>\n",
       "      <td>0.016731</td>\n",
       "      <td>-0.030882</td>\n",
       "      <td>-0.054431</td>\n",
       "      <td>-0.005553</td>\n",
       "      <td>0.015920</td>\n",
       "      <td>-0.007345</td>\n",
       "      <td>-0.033191</td>\n",
       "      <td>0.025618</td>\n",
       "      <td>-0.067753</td>\n",
       "      <td>-0.030254</td>\n",
       "      <td>0.025658</td>\n",
       "      <td>-0.061674</td>\n",
       "      <td>0.059944</td>\n",
       "      <td>-0.029348</td>\n",
       "      <td>0.044588</td>\n",
       "      <td>-0.000260</td>\n",
       "      <td>0.051055</td>\n",
       "      <td>-0.038332</td>\n",
       "      <td>-0.052659</td>\n",
       "      <td>-0.014141</td>\n",
       "      <td>-0.070399</td>\n",
       "      <td>0.183042</td>\n",
       "      <td>0.007304</td>\n",
       "      <td>-0.030399</td>\n",
       "      <td>0.049387</td>\n",
       "      <td>0.051071</td>\n",
       "      <td>-0.030658</td>\n",
       "      <td>0.028370</td>\n",
       "      <td>0.020894</td>\n",
       "      <td>0.031813</td>\n",
       "      <td>0.020841</td>\n",
       "      <td>-0.010759</td>\n",
       "      <td>-0.041482</td>\n",
       "      <td>0.117905</td>\n",
       "      <td>0.030790</td>\n",
       "      <td>0.063581</td>\n",
       "      <td>-0.053461</td>\n",
       "      <td>-0.059498</td>\n",
       "      <td>-0.018792</td>\n",
       "      <td>0.023908</td>\n",
       "      <td>0.024587</td>\n",
       "      <td>0.075913</td>\n",
       "      <td>-0.003771</td>\n",
       "      <td>-0.028980</td>\n",
       "      <td>-0.042295</td>\n",
       "      <td>0.031890</td>\n",
       "      <td>-0.000690</td>\n",
       "      <td>-0.043723</td>\n",
       "      <td>-0.029190</td>\n",
       "      <td>-0.052554</td>\n",
       "      <td>-0.063053</td>\n",
       "      <td>0.054611</td>\n",
       "      <td>0.010467</td>\n",
       "      <td>0.015304</td>\n",
       "      <td>0.057608</td>\n",
       "      <td>-0.027970</td>\n",
       "      <td>-0.017528</td>\n",
       "      <td>0.045178</td>\n",
       "      <td>0.014244</td>\n",
       "      <td>0.012002</td>\n",
       "      <td>-0.040860</td>\n",
       "      <td>-0.049528</td>\n",
       "      <td>0.143483</td>\n",
       "      <td>-0.035296</td>\n",
       "      <td>-0.012473</td>\n",
       "      <td>0.050472</td>\n",
       "      <td>-0.034363</td>\n",
       "      <td>-0.029835</td>\n",
       "      <td>-0.054057</td>\n",
       "      <td>0.033151</td>\n",
       "      <td>-0.022721</td>\n",
       "      <td>-0.013383</td>\n",
       "      <td>-0.009320</td>\n",
       "      <td>-0.015628</td>\n",
       "      <td>0.057938</td>\n",
       "      <td>-0.055553</td>\n",
       "      <td>-0.057429</td>\n",
       "      <td>0.072293</td>\n",
       "      <td>0.009470</td>\n",
       "      <td>-0.020981</td>\n",
       "      <td>-0.034964</td>\n",
       "      <td>0.014203</td>\n",
       "      <td>-0.025801</td>\n",
       "      <td>-0.058271</td>\n",
       "      <td>-0.014339</td>\n",
       "      <td>0.009323</td>\n",
       "      <td>0.025900</td>\n",
       "      <td>0.035306</td>\n",
       "      <td>-0.077668</td>\n",
       "      <td>0.056561</td>\n",
       "      <td>-0.046046</td>\n",
       "      <td>0.000527</td>\n",
       "      <td>0.031723</td>\n",
       "      <td>-0.060629</td>\n",
       "      <td>-0.048830</td>\n",
       "      <td>-0.002087</td>\n",
       "      <td>-0.026408</td>\n",
       "      <td>0.003823</td>\n",
       "      <td>-0.010949</td>\n",
       "      <td>0.082873</td>\n",
       "      <td>0.053716</td>\n",
       "      <td>0.119950</td>\n",
       "      <td>-0.066832</td>\n",
       "      <td>0.007270</td>\n",
       "      <td>-0.017174</td>\n",
       "      <td>0.038950</td>\n",
       "      <td>-0.006868</td>\n",
       "      <td>0.037031</td>\n",
       "      <td>0.057090</td>\n",
       "      <td>0.008636</td>\n",
       "      <td>-0.062032</td>\n",
       "      <td>0.090315</td>\n",
       "      <td>0.001571</td>\n",
       "      <td>0.016528</td>\n",
       "      <td>0.030713</td>\n",
       "      <td>0.027391</td>\n",
       "      <td>-0.051355</td>\n",
       "      <td>0.011877</td>\n",
       "      <td>0.016769</td>\n",
       "      <td>0.024230</td>\n",
       "      <td>0.003182</td>\n",
       "      <td>0.029179</td>\n",
       "      <td>0.029243</td>\n",
       "      <td>-0.010171</td>\n",
       "      <td>0.003768</td>\n",
       "      <td>0.016986</td>\n",
       "      <td>0.027555</td>\n",
       "      <td>0.013682</td>\n",
       "      <td>-0.028953</td>\n",
       "      <td>-0.030056</td>\n",
       "      <td>-0.015043</td>\n",
       "      <td>0.042397</td>\n",
       "      <td>0.061106</td>\n",
       "      <td>0.002439</td>\n",
       "      <td>-0.048695</td>\n",
       "      <td>0.024377</td>\n",
       "      <td>0.013135</td>\n",
       "      <td>-0.038924</td>\n",
       "      <td>0.010336</td>\n",
       "      <td>0.084216</td>\n",
       "      <td>-0.018449</td>\n",
       "      <td>0.007573</td>\n",
       "      <td>0.024381</td>\n",
       "      <td>-0.006296</td>\n",
       "      <td>0.031226</td>\n",
       "      <td>0.070809</td>\n",
       "      <td>-0.036391</td>\n",
       "      <td>-0.048621</td>\n",
       "      <td>-0.027965</td>\n",
       "      <td>0.028488</td>\n",
       "      <td>0.016040</td>\n",
       "      <td>0.030347</td>\n",
       "      <td>-0.003450</td>\n",
       "      <td>-0.032385</td>\n",
       "      <td>0.003102</td>\n",
       "      <td>-0.069063</td>\n",
       "      <td>-0.008170</td>\n",
       "      <td>0.039151</td>\n",
       "      <td>-0.024040</td>\n",
       "      <td>0.053288</td>\n",
       "      <td>0.008722</td>\n",
       "      <td>0.039709</td>\n",
       "      <td>0.014760</td>\n",
       "      <td>-0.027447</td>\n",
       "      <td>-0.003469</td>\n",
       "      <td>0.091960</td>\n",
       "      <td>0.125851</td>\n",
       "      <td>0.025898</td>\n",
       "      <td>0.025300</td>\n",
       "      <td>0.024520</td>\n",
       "      <td>-0.020014</td>\n",
       "      <td>0.010261</td>\n",
       "      <td>0.007224</td>\n",
       "      <td>0.034960</td>\n",
       "      <td>-0.065069</td>\n",
       "      <td>-0.028385</td>\n",
       "      <td>0.070375</td>\n",
       "      <td>-0.061758</td>\n",
       "      <td>0.019844</td>\n",
       "      <td>-0.058402</td>\n",
       "      <td>-0.047092</td>\n",
       "      <td>-0.004439</td>\n",
       "      <td>-0.069310</td>\n",
       "      <td>-0.060501</td>\n",
       "      <td>0.012474</td>\n",
       "      <td>0.008721</td>\n",
       "      <td>0.053419</td>\n",
       "      <td>-0.039751</td>\n",
       "      <td>0.013486</td>\n",
       "      <td>0.004182</td>\n",
       "      <td>0.030708</td>\n",
       "      <td>0.052069</td>\n",
       "      <td>-0.010211</td>\n",
       "      <td>-0.013635</td>\n",
       "      <td>-0.046867</td>\n",
       "      <td>-0.073165</td>\n",
       "      <td>0.084948</td>\n",
       "      <td>0.032888</td>\n",
       "      <td>-0.035646</td>\n",
       "      <td>0.037773</td>\n",
       "      <td>-0.053069</td>\n",
       "      <td>-0.000339</td>\n",
       "      <td>-0.006851</td>\n",
       "      <td>0.075989</td>\n",
       "      <td>0.036542</td>\n",
       "      <td>-0.029959</td>\n",
       "      <td>0.036448</td>\n",
       "      <td>0.036332</td>\n",
       "      <td>-0.013668</td>\n",
       "      <td>0.031791</td>\n",
       "      <td>-0.007692</td>\n",
       "      <td>0.013750</td>\n",
       "      <td>0.054376</td>\n",
       "      <td>0.039661</td>\n",
       "      <td>0.000116</td>\n",
       "      <td>-0.073005</td>\n",
       "      <td>0.005157</td>\n",
       "      <td>0.083316</td>\n",
       "      <td>0.043276</td>\n",
       "      <td>0.004194</td>\n",
       "      <td>-0.010597</td>\n",
       "      <td>0.000929</td>\n",
       "      <td>0.072647</td>\n",
       "      <td>0.019844</td>\n",
       "      <td>-0.009845</td>\n",
       "      <td>-0.031211</td>\n",
       "      <td>-0.063922</td>\n",
       "      <td>-0.024647</td>\n",
       "      <td>0.094604</td>\n",
       "      <td>0.042762</td>\n",
       "      <td>-0.084613</td>\n",
       "      <td>-0.048249</td>\n",
       "      <td>-0.098375</td>\n",
       "      <td>-0.010282</td>\n",
       "      <td>-0.021175</td>\n",
       "      <td>0.015244</td>\n",
       "      <td>0.041014</td>\n",
       "      <td>0.030476</td>\n",
       "      <td>-0.020051</td>\n",
       "      <td>-0.015027</td>\n",
       "      <td>-0.070308</td>\n",
       "      <td>0.035357</td>\n",
       "      <td>0.008635</td>\n",
       "      <td>-0.021839</td>\n",
       "      <td>0.027600</td>\n",
       "      <td>0.020969</td>\n",
       "      <td>0.009418</td>\n",
       "      <td>0.062353</td>\n",
       "      <td>0.088083</td>\n",
       "      <td>0.014872</td>\n",
       "      <td>0.015538</td>\n",
       "      <td>0.028116</td>\n",
       "      <td>-0.019790</td>\n",
       "      <td>0.008888</td>\n",
       "      <td>0.025835</td>\n",
       "      <td>-0.019032</td>\n",
       "      <td>0.042676</td>\n",
       "      <td>0.020973</td>\n",
       "      <td>-0.002846</td>\n",
       "      <td>0.035398</td>\n",
       "      <td>0.051869</td>\n",
       "      <td>-0.025151</td>\n",
       "      <td>-0.014537</td>\n",
       "      <td>0.003352</td>\n",
       "      <td>-0.036576</td>\n",
       "      <td>-0.008851</td>\n",
       "      <td>-0.049537</td>\n",
       "      <td>0.023088</td>\n",
       "      <td>-0.040467</td>\n",
       "      <td>0.115723</td>\n",
       "      <td>0.004328</td>\n",
       "      <td>0.069223</td>\n",
       "      <td>0.140815</td>\n",
       "      <td>0.007436</td>\n",
       "      <td>-0.028549</td>\n",
       "      <td>-0.028862</td>\n",
       "      <td>-0.031053</td>\n",
       "      <td>-0.079870</td>\n",
       "      <td>0.005516</td>\n",
       "      <td>0.052036</td>\n",
       "      <td>-0.054518</td>\n",
       "      <td>-0.024591</td>\n",
       "      <td>0.011395</td>\n",
       "      <td>0.030934</td>\n",
       "      <td>-0.054522</td>\n",
       "      <td>0.019123</td>\n",
       "      <td>0.077553</td>\n",
       "      <td>-0.004958</td>\n",
       "      <td>-0.028895</td>\n",
       "      <td>0.115096</td>\n",
       "      <td>0.043067</td>\n",
       "      <td>0.002244</td>\n",
       "      <td>-0.040573</td>\n",
       "      <td>0.046142</td>\n",
       "      <td>0.000325</td>\n",
       "      <td>-0.016274</td>\n",
       "      <td>-0.059426</td>\n",
       "      <td>0.001933</td>\n",
       "      <td>-0.029121</td>\n",
       "      <td>-0.035760</td>\n",
       "      <td>0.030744</td>\n",
       "      <td>0.017002</td>\n",
       "      <td>-0.010853</td>\n",
       "      <td>0.061508</td>\n",
       "      <td>-0.028334</td>\n",
       "      <td>0.020263</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>vaccinate</th>\n",
       "      <td>0.028424</td>\n",
       "      <td>-0.005482</td>\n",
       "      <td>0.061910</td>\n",
       "      <td>0.091805</td>\n",
       "      <td>-0.068063</td>\n",
       "      <td>0.009134</td>\n",
       "      <td>0.035771</td>\n",
       "      <td>-0.064052</td>\n",
       "      <td>0.006561</td>\n",
       "      <td>-0.024170</td>\n",
       "      <td>-0.034530</td>\n",
       "      <td>0.059653</td>\n",
       "      <td>0.047069</td>\n",
       "      <td>0.018656</td>\n",
       "      <td>-0.036893</td>\n",
       "      <td>-0.066671</td>\n",
       "      <td>-0.039995</td>\n",
       "      <td>-0.000380</td>\n",
       "      <td>0.011772</td>\n",
       "      <td>-0.017885</td>\n",
       "      <td>0.080907</td>\n",
       "      <td>0.031029</td>\n",
       "      <td>0.003605</td>\n",
       "      <td>-0.062065</td>\n",
       "      <td>-0.046987</td>\n",
       "      <td>-0.028364</td>\n",
       "      <td>0.010300</td>\n",
       "      <td>0.000127</td>\n",
       "      <td>0.025504</td>\n",
       "      <td>0.051348</td>\n",
       "      <td>-0.047049</td>\n",
       "      <td>-0.001527</td>\n",
       "      <td>0.019959</td>\n",
       "      <td>0.013173</td>\n",
       "      <td>0.006596</td>\n",
       "      <td>-0.008316</td>\n",
       "      <td>-0.010457</td>\n",
       "      <td>-0.002639</td>\n",
       "      <td>-0.065755</td>\n",
       "      <td>-0.072048</td>\n",
       "      <td>-0.041362</td>\n",
       "      <td>0.035377</td>\n",
       "      <td>-0.075213</td>\n",
       "      <td>-0.037589</td>\n",
       "      <td>0.065471</td>\n",
       "      <td>-0.055836</td>\n",
       "      <td>-0.001267</td>\n",
       "      <td>0.015653</td>\n",
       "      <td>0.041979</td>\n",
       "      <td>-0.031527</td>\n",
       "      <td>0.053954</td>\n",
       "      <td>-0.027371</td>\n",
       "      <td>0.011619</td>\n",
       "      <td>0.078807</td>\n",
       "      <td>0.036055</td>\n",
       "      <td>-0.033281</td>\n",
       "      <td>-0.003390</td>\n",
       "      <td>-0.002787</td>\n",
       "      <td>0.005951</td>\n",
       "      <td>-0.000821</td>\n",
       "      <td>-0.000247</td>\n",
       "      <td>0.042917</td>\n",
       "      <td>-0.006873</td>\n",
       "      <td>0.049062</td>\n",
       "      <td>0.022954</td>\n",
       "      <td>-0.050548</td>\n",
       "      <td>0.043388</td>\n",
       "      <td>-0.021066</td>\n",
       "      <td>0.022776</td>\n",
       "      <td>-0.019568</td>\n",
       "      <td>0.063692</td>\n",
       "      <td>0.078239</td>\n",
       "      <td>0.035660</td>\n",
       "      <td>-0.029464</td>\n",
       "      <td>0.005778</td>\n",
       "      <td>-0.014027</td>\n",
       "      <td>-0.030484</td>\n",
       "      <td>-0.003974</td>\n",
       "      <td>-0.046846</td>\n",
       "      <td>0.041040</td>\n",
       "      <td>-0.074882</td>\n",
       "      <td>-0.004047</td>\n",
       "      <td>-0.006577</td>\n",
       "      <td>0.019425</td>\n",
       "      <td>0.031647</td>\n",
       "      <td>-0.036416</td>\n",
       "      <td>0.006817</td>\n",
       "      <td>-0.010599</td>\n",
       "      <td>0.075410</td>\n",
       "      <td>0.059814</td>\n",
       "      <td>-0.015807</td>\n",
       "      <td>-0.001352</td>\n",
       "      <td>-0.009438</td>\n",
       "      <td>0.020845</td>\n",
       "      <td>-0.039685</td>\n",
       "      <td>-0.044549</td>\n",
       "      <td>-0.009736</td>\n",
       "      <td>0.030648</td>\n",
       "      <td>-0.018279</td>\n",
       "      <td>0.050957</td>\n",
       "      <td>0.039124</td>\n",
       "      <td>-0.052485</td>\n",
       "      <td>-0.006788</td>\n",
       "      <td>0.010782</td>\n",
       "      <td>0.069373</td>\n",
       "      <td>0.037213</td>\n",
       "      <td>-0.006811</td>\n",
       "      <td>-0.128641</td>\n",
       "      <td>0.014097</td>\n",
       "      <td>0.034283</td>\n",
       "      <td>-0.056466</td>\n",
       "      <td>-0.038790</td>\n",
       "      <td>0.014369</td>\n",
       "      <td>-0.034366</td>\n",
       "      <td>-0.051173</td>\n",
       "      <td>-0.012156</td>\n",
       "      <td>-0.010852</td>\n",
       "      <td>0.010771</td>\n",
       "      <td>-0.096502</td>\n",
       "      <td>0.031297</td>\n",
       "      <td>-0.051673</td>\n",
       "      <td>-0.028396</td>\n",
       "      <td>-0.073332</td>\n",
       "      <td>-0.016398</td>\n",
       "      <td>-0.016643</td>\n",
       "      <td>0.033784</td>\n",
       "      <td>0.071417</td>\n",
       "      <td>0.030608</td>\n",
       "      <td>0.023413</td>\n",
       "      <td>0.003877</td>\n",
       "      <td>-0.067627</td>\n",
       "      <td>-0.027131</td>\n",
       "      <td>0.085231</td>\n",
       "      <td>0.051108</td>\n",
       "      <td>-0.018985</td>\n",
       "      <td>-0.018699</td>\n",
       "      <td>-0.044826</td>\n",
       "      <td>0.041140</td>\n",
       "      <td>-0.041869</td>\n",
       "      <td>0.025507</td>\n",
       "      <td>0.002822</td>\n",
       "      <td>-0.051402</td>\n",
       "      <td>-0.013762</td>\n",
       "      <td>-0.062223</td>\n",
       "      <td>-0.001854</td>\n",
       "      <td>0.064181</td>\n",
       "      <td>0.047847</td>\n",
       "      <td>-0.026338</td>\n",
       "      <td>0.019448</td>\n",
       "      <td>0.007904</td>\n",
       "      <td>-0.023592</td>\n",
       "      <td>0.035886</td>\n",
       "      <td>0.008865</td>\n",
       "      <td>0.038187</td>\n",
       "      <td>-0.084403</td>\n",
       "      <td>-0.023449</td>\n",
       "      <td>0.019100</td>\n",
       "      <td>0.032994</td>\n",
       "      <td>-0.066639</td>\n",
       "      <td>0.064726</td>\n",
       "      <td>0.007784</td>\n",
       "      <td>-0.012142</td>\n",
       "      <td>-0.011534</td>\n",
       "      <td>-0.087736</td>\n",
       "      <td>0.009502</td>\n",
       "      <td>-0.039093</td>\n",
       "      <td>0.048174</td>\n",
       "      <td>-0.039730</td>\n",
       "      <td>0.023950</td>\n",
       "      <td>-0.023852</td>\n",
       "      <td>0.010673</td>\n",
       "      <td>0.026071</td>\n",
       "      <td>-0.000165</td>\n",
       "      <td>-0.037096</td>\n",
       "      <td>-0.055465</td>\n",
       "      <td>-0.050319</td>\n",
       "      <td>0.034442</td>\n",
       "      <td>-0.045194</td>\n",
       "      <td>-0.019633</td>\n",
       "      <td>0.009071</td>\n",
       "      <td>0.013372</td>\n",
       "      <td>-0.021543</td>\n",
       "      <td>-0.014556</td>\n",
       "      <td>0.053793</td>\n",
       "      <td>-0.047690</td>\n",
       "      <td>-0.009929</td>\n",
       "      <td>-0.066967</td>\n",
       "      <td>-0.015392</td>\n",
       "      <td>-0.085186</td>\n",
       "      <td>0.002017</td>\n",
       "      <td>-0.029271</td>\n",
       "      <td>-0.063623</td>\n",
       "      <td>-0.025581</td>\n",
       "      <td>-0.012242</td>\n",
       "      <td>0.001018</td>\n",
       "      <td>-0.009533</td>\n",
       "      <td>0.040273</td>\n",
       "      <td>-0.032627</td>\n",
       "      <td>-0.049999</td>\n",
       "      <td>0.061210</td>\n",
       "      <td>0.072773</td>\n",
       "      <td>-0.017409</td>\n",
       "      <td>0.043491</td>\n",
       "      <td>-0.018484</td>\n",
       "      <td>-0.098696</td>\n",
       "      <td>0.000723</td>\n",
       "      <td>0.027636</td>\n",
       "      <td>0.062177</td>\n",
       "      <td>0.100626</td>\n",
       "      <td>-0.001230</td>\n",
       "      <td>0.031419</td>\n",
       "      <td>0.012575</td>\n",
       "      <td>-0.028638</td>\n",
       "      <td>-0.099404</td>\n",
       "      <td>0.002913</td>\n",
       "      <td>0.077781</td>\n",
       "      <td>-0.017047</td>\n",
       "      <td>-0.080107</td>\n",
       "      <td>-0.097192</td>\n",
       "      <td>-0.027747</td>\n",
       "      <td>0.029702</td>\n",
       "      <td>0.047839</td>\n",
       "      <td>0.020828</td>\n",
       "      <td>0.052794</td>\n",
       "      <td>-0.000693</td>\n",
       "      <td>-0.014396</td>\n",
       "      <td>0.007060</td>\n",
       "      <td>0.187270</td>\n",
       "      <td>0.026882</td>\n",
       "      <td>-0.029867</td>\n",
       "      <td>0.000131</td>\n",
       "      <td>0.017789</td>\n",
       "      <td>-0.056027</td>\n",
       "      <td>0.077322</td>\n",
       "      <td>-0.039773</td>\n",
       "      <td>0.064425</td>\n",
       "      <td>0.032336</td>\n",
       "      <td>-0.036017</td>\n",
       "      <td>0.076109</td>\n",
       "      <td>0.078220</td>\n",
       "      <td>-0.004644</td>\n",
       "      <td>0.071799</td>\n",
       "      <td>-0.021465</td>\n",
       "      <td>0.042604</td>\n",
       "      <td>-0.042295</td>\n",
       "      <td>0.043638</td>\n",
       "      <td>0.006724</td>\n",
       "      <td>0.040297</td>\n",
       "      <td>0.009434</td>\n",
       "      <td>-0.001870</td>\n",
       "      <td>-0.023685</td>\n",
       "      <td>-0.033805</td>\n",
       "      <td>0.032717</td>\n",
       "      <td>0.081422</td>\n",
       "      <td>0.016091</td>\n",
       "      <td>0.029146</td>\n",
       "      <td>0.058803</td>\n",
       "      <td>-0.092813</td>\n",
       "      <td>-0.003626</td>\n",
       "      <td>-0.046018</td>\n",
       "      <td>-0.109339</td>\n",
       "      <td>0.045569</td>\n",
       "      <td>0.072516</td>\n",
       "      <td>0.016498</td>\n",
       "      <td>0.005375</td>\n",
       "      <td>-0.048022</td>\n",
       "      <td>-0.066067</td>\n",
       "      <td>-0.062108</td>\n",
       "      <td>0.110235</td>\n",
       "      <td>0.042159</td>\n",
       "      <td>0.022910</td>\n",
       "      <td>-0.001587</td>\n",
       "      <td>0.005902</td>\n",
       "      <td>0.061990</td>\n",
       "      <td>0.019318</td>\n",
       "      <td>0.069579</td>\n",
       "      <td>0.049618</td>\n",
       "      <td>-0.094962</td>\n",
       "      <td>-0.033604</td>\n",
       "      <td>-0.044081</td>\n",
       "      <td>0.072608</td>\n",
       "      <td>0.011875</td>\n",
       "      <td>-0.005130</td>\n",
       "      <td>0.010775</td>\n",
       "      <td>0.086198</td>\n",
       "      <td>0.004783</td>\n",
       "      <td>-0.045734</td>\n",
       "      <td>-0.001165</td>\n",
       "      <td>-0.006903</td>\n",
       "      <td>0.039556</td>\n",
       "      <td>-0.028938</td>\n",
       "      <td>0.050570</td>\n",
       "      <td>0.026984</td>\n",
       "      <td>0.034070</td>\n",
       "      <td>-0.005554</td>\n",
       "      <td>-0.041676</td>\n",
       "      <td>0.008067</td>\n",
       "      <td>-0.043279</td>\n",
       "      <td>0.068548</td>\n",
       "      <td>-0.053353</td>\n",
       "      <td>-0.020968</td>\n",
       "      <td>0.027423</td>\n",
       "      <td>-0.061875</td>\n",
       "      <td>-0.026171</td>\n",
       "      <td>-0.048403</td>\n",
       "      <td>0.010833</td>\n",
       "      <td>0.049181</td>\n",
       "      <td>-0.046365</td>\n",
       "      <td>-0.048698</td>\n",
       "      <td>0.040659</td>\n",
       "      <td>0.029096</td>\n",
       "      <td>0.030901</td>\n",
       "      <td>0.004249</td>\n",
       "      <td>0.040661</td>\n",
       "      <td>0.098421</td>\n",
       "      <td>0.030173</td>\n",
       "      <td>-0.087229</td>\n",
       "      <td>-0.005163</td>\n",
       "      <td>-0.027073</td>\n",
       "      <td>-0.036554</td>\n",
       "      <td>-0.053937</td>\n",
       "      <td>0.021101</td>\n",
       "      <td>-0.074004</td>\n",
       "      <td>0.059203</td>\n",
       "      <td>0.064430</td>\n",
       "      <td>-0.016069</td>\n",
       "      <td>0.020813</td>\n",
       "      <td>-0.021506</td>\n",
       "      <td>0.021795</td>\n",
       "      <td>0.011346</td>\n",
       "      <td>-0.095042</td>\n",
       "      <td>-0.019825</td>\n",
       "      <td>-0.039136</td>\n",
       "      <td>0.088918</td>\n",
       "      <td>-0.098992</td>\n",
       "      <td>0.082056</td>\n",
       "      <td>-0.003956</td>\n",
       "      <td>-0.001478</td>\n",
       "      <td>-0.030471</td>\n",
       "      <td>-0.006722</td>\n",
       "      <td>-0.046307</td>\n",
       "      <td>-0.014848</td>\n",
       "      <td>0.024603</td>\n",
       "      <td>-0.011482</td>\n",
       "      <td>0.012439</td>\n",
       "      <td>-0.065050</td>\n",
       "      <td>0.033519</td>\n",
       "      <td>-0.004185</td>\n",
       "      <td>-0.082728</td>\n",
       "      <td>-0.022751</td>\n",
       "      <td>-0.037533</td>\n",
       "      <td>0.011552</td>\n",
       "      <td>0.087761</td>\n",
       "      <td>-0.015121</td>\n",
       "      <td>0.000084</td>\n",
       "      <td>0.010690</td>\n",
       "      <td>-0.058857</td>\n",
       "      <td>-0.012716</td>\n",
       "      <td>0.077341</td>\n",
       "      <td>-0.066953</td>\n",
       "      <td>-0.012478</td>\n",
       "      <td>-0.042921</td>\n",
       "      <td>-0.032605</td>\n",
       "      <td>0.008385</td>\n",
       "      <td>-0.020614</td>\n",
       "      <td>0.053422</td>\n",
       "      <td>-0.015643</td>\n",
       "      <td>0.035323</td>\n",
       "      <td>0.033270</td>\n",
       "      <td>0.064126</td>\n",
       "      <td>-0.058299</td>\n",
       "      <td>0.005445</td>\n",
       "      <td>-0.004697</td>\n",
       "      <td>-0.000094</td>\n",
       "      <td>-0.023874</td>\n",
       "      <td>0.049002</td>\n",
       "      <td>0.006646</td>\n",
       "      <td>-0.016067</td>\n",
       "      <td>0.043915</td>\n",
       "      <td>0.013055</td>\n",
       "      <td>-0.008762</td>\n",
       "      <td>-0.008686</td>\n",
       "      <td>-0.008644</td>\n",
       "      <td>-0.055201</td>\n",
       "      <td>-0.053289</td>\n",
       "      <td>0.033682</td>\n",
       "      <td>0.037819</td>\n",
       "      <td>-0.049233</td>\n",
       "      <td>0.003314</td>\n",
       "      <td>0.014520</td>\n",
       "      <td>0.062490</td>\n",
       "      <td>-0.046737</td>\n",
       "      <td>0.036311</td>\n",
       "      <td>0.045560</td>\n",
       "      <td>-0.008484</td>\n",
       "      <td>-0.000917</td>\n",
       "      <td>0.055566</td>\n",
       "      <td>-0.011300</td>\n",
       "      <td>0.057105</td>\n",
       "      <td>-0.052858</td>\n",
       "      <td>0.045962</td>\n",
       "      <td>-0.020470</td>\n",
       "      <td>-0.000189</td>\n",
       "      <td>-0.030196</td>\n",
       "      <td>-0.016457</td>\n",
       "      <td>0.018272</td>\n",
       "      <td>-0.001821</td>\n",
       "      <td>-0.005042</td>\n",
       "      <td>0.018443</td>\n",
       "      <td>0.021673</td>\n",
       "      <td>-0.001822</td>\n",
       "      <td>-0.013253</td>\n",
       "      <td>0.004525</td>\n",
       "      <td>-0.000161</td>\n",
       "      <td>0.019151</td>\n",
       "      <td>-0.039240</td>\n",
       "      <td>-0.074216</td>\n",
       "      <td>0.020827</td>\n",
       "      <td>0.028530</td>\n",
       "      <td>-0.025852</td>\n",
       "      <td>0.039976</td>\n",
       "      <td>-0.071466</td>\n",
       "      <td>0.000074</td>\n",
       "      <td>0.050450</td>\n",
       "      <td>0.078272</td>\n",
       "      <td>0.042439</td>\n",
       "      <td>-0.065034</td>\n",
       "      <td>0.063385</td>\n",
       "      <td>0.001497</td>\n",
       "      <td>-0.020000</td>\n",
       "      <td>-0.085082</td>\n",
       "      <td>-0.017045</td>\n",
       "      <td>0.036792</td>\n",
       "      <td>-0.028827</td>\n",
       "      <td>0.007959</td>\n",
       "      <td>-0.058976</td>\n",
       "      <td>0.037314</td>\n",
       "      <td>-0.063744</td>\n",
       "      <td>0.021911</td>\n",
       "      <td>0.095707</td>\n",
       "      <td>-0.070129</td>\n",
       "      <td>-0.006251</td>\n",
       "      <td>0.049219</td>\n",
       "      <td>0.019666</td>\n",
       "      <td>-0.038028</td>\n",
       "      <td>0.072105</td>\n",
       "      <td>-0.081174</td>\n",
       "      <td>0.006280</td>\n",
       "      <td>-0.043384</td>\n",
       "      <td>0.005142</td>\n",
       "      <td>-0.023449</td>\n",
       "      <td>0.028110</td>\n",
       "      <td>-0.013663</td>\n",
       "      <td>0.003619</td>\n",
       "      <td>-0.000790</td>\n",
       "      <td>-0.020785</td>\n",
       "      <td>0.023095</td>\n",
       "      <td>0.058947</td>\n",
       "      <td>-0.026988</td>\n",
       "      <td>-0.004328</td>\n",
       "      <td>0.013476</td>\n",
       "      <td>-0.032567</td>\n",
       "      <td>0.059173</td>\n",
       "      <td>0.057178</td>\n",
       "      <td>-0.037116</td>\n",
       "      <td>0.094095</td>\n",
       "      <td>0.017837</td>\n",
       "      <td>-0.032203</td>\n",
       "      <td>0.055226</td>\n",
       "      <td>0.040112</td>\n",
       "      <td>-0.027389</td>\n",
       "      <td>0.037445</td>\n",
       "      <td>0.016789</td>\n",
       "      <td>0.042886</td>\n",
       "      <td>0.014989</td>\n",
       "      <td>0.081078</td>\n",
       "      <td>-0.033704</td>\n",
       "      <td>0.038590</td>\n",
       "      <td>0.115601</td>\n",
       "      <td>-0.039135</td>\n",
       "      <td>0.043488</td>\n",
       "      <td>-0.047956</td>\n",
       "      <td>-0.031391</td>\n",
       "      <td>0.011976</td>\n",
       "      <td>-0.001479</td>\n",
       "      <td>0.025252</td>\n",
       "      <td>0.027826</td>\n",
       "      <td>0.021414</td>\n",
       "      <td>0.010496</td>\n",
       "      <td>0.042969</td>\n",
       "      <td>-0.015130</td>\n",
       "      <td>0.051444</td>\n",
       "      <td>0.060367</td>\n",
       "      <td>0.064192</td>\n",
       "      <td>-0.025623</td>\n",
       "      <td>0.119277</td>\n",
       "      <td>-0.031831</td>\n",
       "      <td>0.066155</td>\n",
       "      <td>0.033845</td>\n",
       "      <td>-0.016525</td>\n",
       "      <td>0.008972</td>\n",
       "      <td>-0.038890</td>\n",
       "      <td>-0.005454</td>\n",
       "      <td>0.013192</td>\n",
       "      <td>-0.009316</td>\n",
       "      <td>-0.000751</td>\n",
       "      <td>-0.021361</td>\n",
       "      <td>0.005412</td>\n",
       "      <td>0.019531</td>\n",
       "      <td>0.019365</td>\n",
       "      <td>-0.000408</td>\n",
       "      <td>0.022468</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>boastful</th>\n",
       "      <td>-0.051110</td>\n",
       "      <td>-0.024396</td>\n",
       "      <td>0.023795</td>\n",
       "      <td>-0.050308</td>\n",
       "      <td>-0.036013</td>\n",
       "      <td>0.042469</td>\n",
       "      <td>0.015321</td>\n",
       "      <td>0.072105</td>\n",
       "      <td>0.053339</td>\n",
       "      <td>0.080634</td>\n",
       "      <td>-0.006786</td>\n",
       "      <td>0.020489</td>\n",
       "      <td>0.009583</td>\n",
       "      <td>-0.044102</td>\n",
       "      <td>-0.072105</td>\n",
       "      <td>-0.044240</td>\n",
       "      <td>-0.001943</td>\n",
       "      <td>-0.011250</td>\n",
       "      <td>0.018678</td>\n",
       "      <td>-0.030220</td>\n",
       "      <td>-0.004406</td>\n",
       "      <td>-0.016332</td>\n",
       "      <td>-0.009263</td>\n",
       "      <td>-0.030748</td>\n",
       "      <td>0.002724</td>\n",
       "      <td>-0.040646</td>\n",
       "      <td>-0.019575</td>\n",
       "      <td>-0.016290</td>\n",
       "      <td>0.009505</td>\n",
       "      <td>0.048056</td>\n",
       "      <td>0.057039</td>\n",
       "      <td>0.041795</td>\n",
       "      <td>-0.018004</td>\n",
       "      <td>0.048362</td>\n",
       "      <td>-0.012203</td>\n",
       "      <td>0.007665</td>\n",
       "      <td>-0.027654</td>\n",
       "      <td>0.036928</td>\n",
       "      <td>0.005887</td>\n",
       "      <td>-0.011095</td>\n",
       "      <td>-0.048285</td>\n",
       "      <td>-0.007175</td>\n",
       "      <td>-0.004458</td>\n",
       "      <td>0.007380</td>\n",
       "      <td>0.013210</td>\n",
       "      <td>-0.036924</td>\n",
       "      <td>0.007019</td>\n",
       "      <td>0.009881</td>\n",
       "      <td>0.063554</td>\n",
       "      <td>-0.041818</td>\n",
       "      <td>0.054839</td>\n",
       "      <td>0.069288</td>\n",
       "      <td>-0.036413</td>\n",
       "      <td>-0.050277</td>\n",
       "      <td>0.020361</td>\n",
       "      <td>-0.029675</td>\n",
       "      <td>0.083413</td>\n",
       "      <td>-0.022659</td>\n",
       "      <td>-0.051213</td>\n",
       "      <td>0.033022</td>\n",
       "      <td>-0.030239</td>\n",
       "      <td>-0.049699</td>\n",
       "      <td>-0.004956</td>\n",
       "      <td>-0.008355</td>\n",
       "      <td>-0.069343</td>\n",
       "      <td>-0.003307</td>\n",
       "      <td>-0.044186</td>\n",
       "      <td>-0.052561</td>\n",
       "      <td>0.033546</td>\n",
       "      <td>-0.036092</td>\n",
       "      <td>-0.008710</td>\n",
       "      <td>0.037690</td>\n",
       "      <td>-0.059503</td>\n",
       "      <td>0.001261</td>\n",
       "      <td>0.048422</td>\n",
       "      <td>-0.062658</td>\n",
       "      <td>0.014647</td>\n",
       "      <td>-0.061985</td>\n",
       "      <td>-0.031157</td>\n",
       "      <td>0.004058</td>\n",
       "      <td>-0.084421</td>\n",
       "      <td>0.022734</td>\n",
       "      <td>0.027187</td>\n",
       "      <td>0.001669</td>\n",
       "      <td>-0.041571</td>\n",
       "      <td>-0.019596</td>\n",
       "      <td>-0.032491</td>\n",
       "      <td>-0.052717</td>\n",
       "      <td>0.026522</td>\n",
       "      <td>0.005641</td>\n",
       "      <td>-0.008179</td>\n",
       "      <td>-0.056359</td>\n",
       "      <td>0.038944</td>\n",
       "      <td>0.102061</td>\n",
       "      <td>-0.062669</td>\n",
       "      <td>-0.015704</td>\n",
       "      <td>-0.010495</td>\n",
       "      <td>0.008520</td>\n",
       "      <td>-0.013783</td>\n",
       "      <td>0.011921</td>\n",
       "      <td>-0.029634</td>\n",
       "      <td>-0.032664</td>\n",
       "      <td>-0.050840</td>\n",
       "      <td>0.050830</td>\n",
       "      <td>-0.069388</td>\n",
       "      <td>0.044583</td>\n",
       "      <td>-0.049299</td>\n",
       "      <td>0.042568</td>\n",
       "      <td>0.011506</td>\n",
       "      <td>-0.044644</td>\n",
       "      <td>0.092991</td>\n",
       "      <td>-0.020952</td>\n",
       "      <td>0.053148</td>\n",
       "      <td>0.013169</td>\n",
       "      <td>-0.058297</td>\n",
       "      <td>-0.015548</td>\n",
       "      <td>0.018060</td>\n",
       "      <td>-0.037144</td>\n",
       "      <td>0.056572</td>\n",
       "      <td>-0.040723</td>\n",
       "      <td>-0.015881</td>\n",
       "      <td>-0.029100</td>\n",
       "      <td>-0.009068</td>\n",
       "      <td>0.014715</td>\n",
       "      <td>-0.091143</td>\n",
       "      <td>-0.059306</td>\n",
       "      <td>-0.058142</td>\n",
       "      <td>-0.019943</td>\n",
       "      <td>0.007810</td>\n",
       "      <td>0.010309</td>\n",
       "      <td>0.007920</td>\n",
       "      <td>-0.034837</td>\n",
       "      <td>0.008769</td>\n",
       "      <td>0.047245</td>\n",
       "      <td>-0.008109</td>\n",
       "      <td>-0.057278</td>\n",
       "      <td>0.007334</td>\n",
       "      <td>0.015468</td>\n",
       "      <td>0.082429</td>\n",
       "      <td>0.038925</td>\n",
       "      <td>-0.027951</td>\n",
       "      <td>-0.002676</td>\n",
       "      <td>-0.011655</td>\n",
       "      <td>-0.029816</td>\n",
       "      <td>-0.037963</td>\n",
       "      <td>0.046957</td>\n",
       "      <td>0.084088</td>\n",
       "      <td>-0.016923</td>\n",
       "      <td>0.055365</td>\n",
       "      <td>0.011903</td>\n",
       "      <td>0.047906</td>\n",
       "      <td>0.049921</td>\n",
       "      <td>-0.022917</td>\n",
       "      <td>0.086332</td>\n",
       "      <td>-0.028154</td>\n",
       "      <td>-0.022331</td>\n",
       "      <td>-0.019451</td>\n",
       "      <td>0.081395</td>\n",
       "      <td>0.012887</td>\n",
       "      <td>0.025114</td>\n",
       "      <td>-0.048578</td>\n",
       "      <td>-0.071913</td>\n",
       "      <td>-0.001807</td>\n",
       "      <td>-0.019900</td>\n",
       "      <td>0.049445</td>\n",
       "      <td>-0.052497</td>\n",
       "      <td>0.050448</td>\n",
       "      <td>-0.000926</td>\n",
       "      <td>0.001319</td>\n",
       "      <td>-0.075955</td>\n",
       "      <td>-0.004226</td>\n",
       "      <td>0.039309</td>\n",
       "      <td>-0.025777</td>\n",
       "      <td>-0.029057</td>\n",
       "      <td>0.051345</td>\n",
       "      <td>-0.080120</td>\n",
       "      <td>0.014854</td>\n",
       "      <td>0.001715</td>\n",
       "      <td>-0.044022</td>\n",
       "      <td>0.014153</td>\n",
       "      <td>-0.016756</td>\n",
       "      <td>-0.015527</td>\n",
       "      <td>0.080063</td>\n",
       "      <td>-0.007534</td>\n",
       "      <td>0.005873</td>\n",
       "      <td>0.037034</td>\n",
       "      <td>-0.092264</td>\n",
       "      <td>-0.014186</td>\n",
       "      <td>0.012880</td>\n",
       "      <td>0.025195</td>\n",
       "      <td>-0.027933</td>\n",
       "      <td>0.019777</td>\n",
       "      <td>-0.077407</td>\n",
       "      <td>0.013706</td>\n",
       "      <td>0.000021</td>\n",
       "      <td>0.067735</td>\n",
       "      <td>0.046901</td>\n",
       "      <td>0.023685</td>\n",
       "      <td>0.011180</td>\n",
       "      <td>0.001189</td>\n",
       "      <td>0.049485</td>\n",
       "      <td>-0.024911</td>\n",
       "      <td>0.045043</td>\n",
       "      <td>-0.019323</td>\n",
       "      <td>0.008287</td>\n",
       "      <td>0.059856</td>\n",
       "      <td>0.016736</td>\n",
       "      <td>0.089371</td>\n",
       "      <td>0.070406</td>\n",
       "      <td>0.043941</td>\n",
       "      <td>-0.049324</td>\n",
       "      <td>-0.017462</td>\n",
       "      <td>-0.005426</td>\n",
       "      <td>-0.012474</td>\n",
       "      <td>-0.052949</td>\n",
       "      <td>-0.018070</td>\n",
       "      <td>-0.069957</td>\n",
       "      <td>-0.096576</td>\n",
       "      <td>-0.029859</td>\n",
       "      <td>0.029972</td>\n",
       "      <td>0.039913</td>\n",
       "      <td>-0.037258</td>\n",
       "      <td>0.023936</td>\n",
       "      <td>-0.047566</td>\n",
       "      <td>0.052914</td>\n",
       "      <td>-0.058365</td>\n",
       "      <td>0.033227</td>\n",
       "      <td>0.194536</td>\n",
       "      <td>-0.000754</td>\n",
       "      <td>-0.039614</td>\n",
       "      <td>-0.050341</td>\n",
       "      <td>-0.048408</td>\n",
       "      <td>-0.025505</td>\n",
       "      <td>0.030747</td>\n",
       "      <td>0.004464</td>\n",
       "      <td>-0.043184</td>\n",
       "      <td>-0.042398</td>\n",
       "      <td>-0.045265</td>\n",
       "      <td>-0.057277</td>\n",
       "      <td>-0.035369</td>\n",
       "      <td>-0.049776</td>\n",
       "      <td>0.118891</td>\n",
       "      <td>-0.013536</td>\n",
       "      <td>-0.033841</td>\n",
       "      <td>-0.051081</td>\n",
       "      <td>0.067823</td>\n",
       "      <td>-0.052312</td>\n",
       "      <td>-0.003320</td>\n",
       "      <td>-0.005112</td>\n",
       "      <td>-0.036295</td>\n",
       "      <td>-0.005148</td>\n",
       "      <td>-0.054336</td>\n",
       "      <td>0.057668</td>\n",
       "      <td>-0.015315</td>\n",
       "      <td>0.027973</td>\n",
       "      <td>0.005714</td>\n",
       "      <td>0.026554</td>\n",
       "      <td>-0.029522</td>\n",
       "      <td>0.024648</td>\n",
       "      <td>0.014269</td>\n",
       "      <td>0.076719</td>\n",
       "      <td>0.048317</td>\n",
       "      <td>0.069780</td>\n",
       "      <td>-0.023885</td>\n",
       "      <td>-0.033882</td>\n",
       "      <td>0.063300</td>\n",
       "      <td>0.034753</td>\n",
       "      <td>-0.028121</td>\n",
       "      <td>0.086903</td>\n",
       "      <td>-0.004895</td>\n",
       "      <td>-0.036726</td>\n",
       "      <td>-0.006226</td>\n",
       "      <td>-0.002396</td>\n",
       "      <td>0.038949</td>\n",
       "      <td>-0.037429</td>\n",
       "      <td>0.038245</td>\n",
       "      <td>-0.032793</td>\n",
       "      <td>0.010142</td>\n",
       "      <td>0.013419</td>\n",
       "      <td>-0.004295</td>\n",
       "      <td>0.020863</td>\n",
       "      <td>0.005318</td>\n",
       "      <td>0.107003</td>\n",
       "      <td>-0.080659</td>\n",
       "      <td>0.019166</td>\n",
       "      <td>-0.006344</td>\n",
       "      <td>-0.075339</td>\n",
       "      <td>-0.016006</td>\n",
       "      <td>0.109040</td>\n",
       "      <td>-0.034634</td>\n",
       "      <td>-0.060696</td>\n",
       "      <td>0.015021</td>\n",
       "      <td>0.001204</td>\n",
       "      <td>0.077196</td>\n",
       "      <td>0.017214</td>\n",
       "      <td>0.073040</td>\n",
       "      <td>0.002626</td>\n",
       "      <td>0.069214</td>\n",
       "      <td>0.084182</td>\n",
       "      <td>-0.003112</td>\n",
       "      <td>-0.035870</td>\n",
       "      <td>-0.010623</td>\n",
       "      <td>-0.034958</td>\n",
       "      <td>-0.046201</td>\n",
       "      <td>-0.042353</td>\n",
       "      <td>0.003376</td>\n",
       "      <td>0.047275</td>\n",
       "      <td>0.091598</td>\n",
       "      <td>-0.038574</td>\n",
       "      <td>0.034940</td>\n",
       "      <td>-0.020772</td>\n",
       "      <td>0.047630</td>\n",
       "      <td>-0.070728</td>\n",
       "      <td>-0.013228</td>\n",
       "      <td>0.009983</td>\n",
       "      <td>0.055462</td>\n",
       "      <td>-0.011829</td>\n",
       "      <td>0.026638</td>\n",
       "      <td>-0.018852</td>\n",
       "      <td>0.018976</td>\n",
       "      <td>-0.071261</td>\n",
       "      <td>-0.055114</td>\n",
       "      <td>-0.015510</td>\n",
       "      <td>0.014190</td>\n",
       "      <td>0.071436</td>\n",
       "      <td>0.074714</td>\n",
       "      <td>0.054551</td>\n",
       "      <td>0.051625</td>\n",
       "      <td>0.053652</td>\n",
       "      <td>-0.000755</td>\n",
       "      <td>0.048550</td>\n",
       "      <td>0.036351</td>\n",
       "      <td>0.034828</td>\n",
       "      <td>0.069793</td>\n",
       "      <td>-0.036032</td>\n",
       "      <td>0.050542</td>\n",
       "      <td>0.059103</td>\n",
       "      <td>-0.023277</td>\n",
       "      <td>0.038137</td>\n",
       "      <td>0.022458</td>\n",
       "      <td>0.020398</td>\n",
       "      <td>-0.020471</td>\n",
       "      <td>-0.048897</td>\n",
       "      <td>-0.062188</td>\n",
       "      <td>-0.006658</td>\n",
       "      <td>-0.024584</td>\n",
       "      <td>-0.042119</td>\n",
       "      <td>0.029949</td>\n",
       "      <td>-0.031556</td>\n",
       "      <td>0.013961</td>\n",
       "      <td>0.123659</td>\n",
       "      <td>0.005216</td>\n",
       "      <td>0.029570</td>\n",
       "      <td>0.010176</td>\n",
       "      <td>-0.018344</td>\n",
       "      <td>0.053437</td>\n",
       "      <td>0.041651</td>\n",
       "      <td>-0.007257</td>\n",
       "      <td>-0.003924</td>\n",
       "      <td>-0.021138</td>\n",
       "      <td>0.014091</td>\n",
       "      <td>-0.063302</td>\n",
       "      <td>0.002885</td>\n",
       "      <td>0.009757</td>\n",
       "      <td>0.052025</td>\n",
       "      <td>0.004332</td>\n",
       "      <td>0.021721</td>\n",
       "      <td>0.014936</td>\n",
       "      <td>-0.034612</td>\n",
       "      <td>0.017472</td>\n",
       "      <td>0.078167</td>\n",
       "      <td>0.029481</td>\n",
       "      <td>0.007195</td>\n",
       "      <td>-0.046038</td>\n",
       "      <td>0.029356</td>\n",
       "      <td>0.013884</td>\n",
       "      <td>-0.034432</td>\n",
       "      <td>0.049890</td>\n",
       "      <td>-0.055426</td>\n",
       "      <td>-0.005104</td>\n",
       "      <td>-0.041903</td>\n",
       "      <td>0.036163</td>\n",
       "      <td>0.007727</td>\n",
       "      <td>-0.074569</td>\n",
       "      <td>-0.032194</td>\n",
       "      <td>0.012535</td>\n",
       "      <td>0.007426</td>\n",
       "      <td>0.041600</td>\n",
       "      <td>-0.027705</td>\n",
       "      <td>-0.067641</td>\n",
       "      <td>-0.019714</td>\n",
       "      <td>-0.038719</td>\n",
       "      <td>0.035367</td>\n",
       "      <td>-0.012464</td>\n",
       "      <td>-0.004674</td>\n",
       "      <td>-0.066688</td>\n",
       "      <td>0.069571</td>\n",
       "      <td>-0.005903</td>\n",
       "      <td>0.081156</td>\n",
       "      <td>-0.021215</td>\n",
       "      <td>-0.053401</td>\n",
       "      <td>-0.014014</td>\n",
       "      <td>0.071768</td>\n",
       "      <td>0.044162</td>\n",
       "      <td>-0.001052</td>\n",
       "      <td>0.002423</td>\n",
       "      <td>-0.040683</td>\n",
       "      <td>0.023525</td>\n",
       "      <td>-0.018400</td>\n",
       "      <td>-0.043960</td>\n",
       "      <td>-0.079991</td>\n",
       "      <td>-0.023481</td>\n",
       "      <td>-0.050103</td>\n",
       "      <td>-0.037227</td>\n",
       "      <td>-0.037890</td>\n",
       "      <td>-0.069324</td>\n",
       "      <td>0.012661</td>\n",
       "      <td>-0.026899</td>\n",
       "      <td>0.035687</td>\n",
       "      <td>0.030884</td>\n",
       "      <td>-0.049208</td>\n",
       "      <td>-0.051994</td>\n",
       "      <td>0.000854</td>\n",
       "      <td>0.043943</td>\n",
       "      <td>0.093851</td>\n",
       "      <td>0.064736</td>\n",
       "      <td>-0.052860</td>\n",
       "      <td>-0.023531</td>\n",
       "      <td>0.080256</td>\n",
       "      <td>0.008760</td>\n",
       "      <td>0.005182</td>\n",
       "      <td>0.010996</td>\n",
       "      <td>-0.084760</td>\n",
       "      <td>0.010517</td>\n",
       "      <td>0.002839</td>\n",
       "      <td>0.028523</td>\n",
       "      <td>-0.008092</td>\n",
       "      <td>-0.018446</td>\n",
       "      <td>0.074432</td>\n",
       "      <td>-0.017187</td>\n",
       "      <td>0.020271</td>\n",
       "      <td>-0.034357</td>\n",
       "      <td>0.010760</td>\n",
       "      <td>0.020533</td>\n",
       "      <td>-0.028674</td>\n",
       "      <td>0.056149</td>\n",
       "      <td>-0.031791</td>\n",
       "      <td>-0.002711</td>\n",
       "      <td>0.099564</td>\n",
       "      <td>0.050883</td>\n",
       "      <td>0.003158</td>\n",
       "      <td>0.020939</td>\n",
       "      <td>0.033169</td>\n",
       "      <td>-0.028066</td>\n",
       "      <td>0.045399</td>\n",
       "      <td>0.001880</td>\n",
       "      <td>-0.035072</td>\n",
       "      <td>0.026681</td>\n",
       "      <td>0.037922</td>\n",
       "      <td>0.011587</td>\n",
       "      <td>0.006886</td>\n",
       "      <td>-0.020961</td>\n",
       "      <td>0.018029</td>\n",
       "      <td>0.078361</td>\n",
       "      <td>0.002040</td>\n",
       "      <td>0.019554</td>\n",
       "      <td>0.017028</td>\n",
       "      <td>-0.033588</td>\n",
       "      <td>-0.073720</td>\n",
       "      <td>0.046121</td>\n",
       "      <td>-0.052128</td>\n",
       "      <td>0.092784</td>\n",
       "      <td>0.048110</td>\n",
       "      <td>0.036085</td>\n",
       "      <td>0.017367</td>\n",
       "      <td>0.068189</td>\n",
       "      <td>-0.002608</td>\n",
       "      <td>0.031693</td>\n",
       "      <td>0.144429</td>\n",
       "      <td>-0.050009</td>\n",
       "      <td>-0.020675</td>\n",
       "      <td>0.028418</td>\n",
       "      <td>0.035298</td>\n",
       "      <td>-0.007077</td>\n",
       "      <td>-0.074961</td>\n",
       "      <td>-0.011768</td>\n",
       "      <td>0.002609</td>\n",
       "      <td>-0.052619</td>\n",
       "      <td>-0.003499</td>\n",
       "      <td>-0.026776</td>\n",
       "      <td>-0.004766</td>\n",
       "      <td>-0.032794</td>\n",
       "      <td>0.060407</td>\n",
       "      <td>0.039198</td>\n",
       "      <td>-0.095160</td>\n",
       "      <td>0.155760</td>\n",
       "      <td>-0.001979</td>\n",
       "      <td>-0.001395</td>\n",
       "      <td>-0.022317</td>\n",
       "      <td>0.028448</td>\n",
       "      <td>0.029504</td>\n",
       "      <td>-0.056538</td>\n",
       "      <td>0.030101</td>\n",
       "      <td>-0.014380</td>\n",
       "      <td>-0.064370</td>\n",
       "      <td>-0.017066</td>\n",
       "      <td>-0.008721</td>\n",
       "      <td>0.033386</td>\n",
       "      <td>-0.040513</td>\n",
       "      <td>0.002530</td>\n",
       "      <td>-0.039945</td>\n",
       "      <td>-0.017599</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>phrase</th>\n",
       "      <td>-0.032291</td>\n",
       "      <td>-0.037546</td>\n",
       "      <td>0.027556</td>\n",
       "      <td>-0.001748</td>\n",
       "      <td>0.030524</td>\n",
       "      <td>0.006126</td>\n",
       "      <td>-0.000658</td>\n",
       "      <td>0.024837</td>\n",
       "      <td>-0.024779</td>\n",
       "      <td>-0.021142</td>\n",
       "      <td>-0.021418</td>\n",
       "      <td>0.016284</td>\n",
       "      <td>0.075576</td>\n",
       "      <td>-0.089324</td>\n",
       "      <td>0.026687</td>\n",
       "      <td>0.044783</td>\n",
       "      <td>-0.060943</td>\n",
       "      <td>-0.035026</td>\n",
       "      <td>0.042232</td>\n",
       "      <td>-0.005213</td>\n",
       "      <td>0.019285</td>\n",
       "      <td>-0.013516</td>\n",
       "      <td>-0.018106</td>\n",
       "      <td>-0.045093</td>\n",
       "      <td>-0.000340</td>\n",
       "      <td>-0.014059</td>\n",
       "      <td>-0.022098</td>\n",
       "      <td>0.001560</td>\n",
       "      <td>-0.024134</td>\n",
       "      <td>0.040805</td>\n",
       "      <td>-0.011890</td>\n",
       "      <td>0.082071</td>\n",
       "      <td>0.079723</td>\n",
       "      <td>0.073002</td>\n",
       "      <td>-0.061001</td>\n",
       "      <td>0.043345</td>\n",
       "      <td>-0.057310</td>\n",
       "      <td>0.031074</td>\n",
       "      <td>0.021179</td>\n",
       "      <td>-0.029621</td>\n",
       "      <td>-0.013334</td>\n",
       "      <td>0.043421</td>\n",
       "      <td>0.003129</td>\n",
       "      <td>-0.001024</td>\n",
       "      <td>-0.003668</td>\n",
       "      <td>0.008665</td>\n",
       "      <td>0.009455</td>\n",
       "      <td>0.025489</td>\n",
       "      <td>0.031866</td>\n",
       "      <td>0.049585</td>\n",
       "      <td>0.008313</td>\n",
       "      <td>-0.036805</td>\n",
       "      <td>0.011974</td>\n",
       "      <td>0.002010</td>\n",
       "      <td>0.042235</td>\n",
       "      <td>-0.038181</td>\n",
       "      <td>0.038660</td>\n",
       "      <td>0.070997</td>\n",
       "      <td>0.033646</td>\n",
       "      <td>0.114800</td>\n",
       "      <td>-0.050354</td>\n",
       "      <td>0.013304</td>\n",
       "      <td>-0.067767</td>\n",
       "      <td>0.013132</td>\n",
       "      <td>-0.045834</td>\n",
       "      <td>-0.073161</td>\n",
       "      <td>-0.160033</td>\n",
       "      <td>-0.026870</td>\n",
       "      <td>-0.003374</td>\n",
       "      <td>-0.030343</td>\n",
       "      <td>0.037477</td>\n",
       "      <td>0.053054</td>\n",
       "      <td>0.058240</td>\n",
       "      <td>0.028669</td>\n",
       "      <td>-0.018565</td>\n",
       "      <td>-0.003153</td>\n",
       "      <td>0.027676</td>\n",
       "      <td>0.070613</td>\n",
       "      <td>0.002926</td>\n",
       "      <td>0.051479</td>\n",
       "      <td>-0.026535</td>\n",
       "      <td>0.055758</td>\n",
       "      <td>0.025351</td>\n",
       "      <td>0.066969</td>\n",
       "      <td>-0.029686</td>\n",
       "      <td>-0.005180</td>\n",
       "      <td>-0.016695</td>\n",
       "      <td>-0.013679</td>\n",
       "      <td>-0.021277</td>\n",
       "      <td>-0.011967</td>\n",
       "      <td>0.033314</td>\n",
       "      <td>0.005718</td>\n",
       "      <td>0.037079</td>\n",
       "      <td>0.027994</td>\n",
       "      <td>-0.046271</td>\n",
       "      <td>-0.085550</td>\n",
       "      <td>-0.011605</td>\n",
       "      <td>0.044689</td>\n",
       "      <td>0.054081</td>\n",
       "      <td>-0.016777</td>\n",
       "      <td>-0.010656</td>\n",
       "      <td>-0.049546</td>\n",
       "      <td>-0.015123</td>\n",
       "      <td>0.008269</td>\n",
       "      <td>-0.024772</td>\n",
       "      <td>0.005441</td>\n",
       "      <td>-0.030478</td>\n",
       "      <td>-0.034255</td>\n",
       "      <td>-0.050045</td>\n",
       "      <td>0.017429</td>\n",
       "      <td>-0.049187</td>\n",
       "      <td>0.053201</td>\n",
       "      <td>0.001476</td>\n",
       "      <td>0.003143</td>\n",
       "      <td>-0.046732</td>\n",
       "      <td>0.006148</td>\n",
       "      <td>-0.018519</td>\n",
       "      <td>0.040934</td>\n",
       "      <td>-0.028465</td>\n",
       "      <td>-0.059986</td>\n",
       "      <td>-0.045956</td>\n",
       "      <td>-0.019657</td>\n",
       "      <td>-0.023675</td>\n",
       "      <td>0.023598</td>\n",
       "      <td>0.002842</td>\n",
       "      <td>-0.012999</td>\n",
       "      <td>-0.002222</td>\n",
       "      <td>-0.089879</td>\n",
       "      <td>0.005517</td>\n",
       "      <td>0.016177</td>\n",
       "      <td>0.001155</td>\n",
       "      <td>-0.027170</td>\n",
       "      <td>-0.005817</td>\n",
       "      <td>-0.032664</td>\n",
       "      <td>-0.124274</td>\n",
       "      <td>0.011576</td>\n",
       "      <td>-0.025072</td>\n",
       "      <td>0.001185</td>\n",
       "      <td>-0.002416</td>\n",
       "      <td>-0.080268</td>\n",
       "      <td>-0.072213</td>\n",
       "      <td>-0.042050</td>\n",
       "      <td>-0.019787</td>\n",
       "      <td>-0.033568</td>\n",
       "      <td>-0.051008</td>\n",
       "      <td>0.056745</td>\n",
       "      <td>-0.001991</td>\n",
       "      <td>-0.069274</td>\n",
       "      <td>-0.032142</td>\n",
       "      <td>-0.012820</td>\n",
       "      <td>0.041061</td>\n",
       "      <td>0.117455</td>\n",
       "      <td>0.019461</td>\n",
       "      <td>0.175871</td>\n",
       "      <td>-0.028394</td>\n",
       "      <td>0.050680</td>\n",
       "      <td>-0.095411</td>\n",
       "      <td>0.081329</td>\n",
       "      <td>-0.048965</td>\n",
       "      <td>0.016566</td>\n",
       "      <td>-0.031290</td>\n",
       "      <td>0.008215</td>\n",
       "      <td>0.064108</td>\n",
       "      <td>-0.012911</td>\n",
       "      <td>0.028826</td>\n",
       "      <td>-0.015281</td>\n",
       "      <td>-0.077419</td>\n",
       "      <td>0.029784</td>\n",
       "      <td>-0.008172</td>\n",
       "      <td>0.077687</td>\n",
       "      <td>0.028355</td>\n",
       "      <td>0.009159</td>\n",
       "      <td>-0.018345</td>\n",
       "      <td>0.022619</td>\n",
       "      <td>-0.013331</td>\n",
       "      <td>-0.009165</td>\n",
       "      <td>0.027230</td>\n",
       "      <td>-0.015977</td>\n",
       "      <td>0.028337</td>\n",
       "      <td>0.014322</td>\n",
       "      <td>0.009745</td>\n",
       "      <td>-0.068717</td>\n",
       "      <td>-0.008450</td>\n",
       "      <td>0.043847</td>\n",
       "      <td>0.023996</td>\n",
       "      <td>-0.018301</td>\n",
       "      <td>-0.076823</td>\n",
       "      <td>0.015685</td>\n",
       "      <td>0.011264</td>\n",
       "      <td>-0.041260</td>\n",
       "      <td>0.027267</td>\n",
       "      <td>0.007250</td>\n",
       "      <td>0.017576</td>\n",
       "      <td>0.000723</td>\n",
       "      <td>0.016673</td>\n",
       "      <td>0.067034</td>\n",
       "      <td>0.020061</td>\n",
       "      <td>0.009841</td>\n",
       "      <td>0.066789</td>\n",
       "      <td>0.004518</td>\n",
       "      <td>-0.013099</td>\n",
       "      <td>0.033045</td>\n",
       "      <td>-0.018943</td>\n",
       "      <td>-0.038036</td>\n",
       "      <td>-0.025388</td>\n",
       "      <td>0.023114</td>\n",
       "      <td>0.006939</td>\n",
       "      <td>0.056635</td>\n",
       "      <td>0.048861</td>\n",
       "      <td>0.018489</td>\n",
       "      <td>-0.079867</td>\n",
       "      <td>-0.004178</td>\n",
       "      <td>0.021219</td>\n",
       "      <td>-0.027407</td>\n",
       "      <td>-0.026955</td>\n",
       "      <td>-0.046196</td>\n",
       "      <td>-0.015740</td>\n",
       "      <td>-0.102883</td>\n",
       "      <td>-0.009445</td>\n",
       "      <td>0.046316</td>\n",
       "      <td>-0.007780</td>\n",
       "      <td>-0.002280</td>\n",
       "      <td>-0.000415</td>\n",
       "      <td>-0.016831</td>\n",
       "      <td>-0.031255</td>\n",
       "      <td>-0.119046</td>\n",
       "      <td>0.036523</td>\n",
       "      <td>0.220437</td>\n",
       "      <td>0.028056</td>\n",
       "      <td>-0.052936</td>\n",
       "      <td>0.007023</td>\n",
       "      <td>-0.026899</td>\n",
       "      <td>-0.005849</td>\n",
       "      <td>0.040759</td>\n",
       "      <td>-0.017490</td>\n",
       "      <td>-0.032503</td>\n",
       "      <td>-0.052479</td>\n",
       "      <td>0.015019</td>\n",
       "      <td>-0.008651</td>\n",
       "      <td>-0.017456</td>\n",
       "      <td>-0.043198</td>\n",
       "      <td>0.124263</td>\n",
       "      <td>0.002850</td>\n",
       "      <td>-0.009832</td>\n",
       "      <td>-0.063221</td>\n",
       "      <td>-0.067660</td>\n",
       "      <td>0.026603</td>\n",
       "      <td>0.027954</td>\n",
       "      <td>-0.024656</td>\n",
       "      <td>0.030133</td>\n",
       "      <td>-0.033378</td>\n",
       "      <td>0.057579</td>\n",
       "      <td>-0.003952</td>\n",
       "      <td>-0.028255</td>\n",
       "      <td>-0.028429</td>\n",
       "      <td>-0.042290</td>\n",
       "      <td>-0.016476</td>\n",
       "      <td>-0.011721</td>\n",
       "      <td>-0.008798</td>\n",
       "      <td>0.012879</td>\n",
       "      <td>0.051688</td>\n",
       "      <td>-0.065220</td>\n",
       "      <td>0.030203</td>\n",
       "      <td>-0.057402</td>\n",
       "      <td>0.029101</td>\n",
       "      <td>0.001568</td>\n",
       "      <td>-0.047203</td>\n",
       "      <td>-0.031695</td>\n",
       "      <td>0.127186</td>\n",
       "      <td>-0.018315</td>\n",
       "      <td>-0.006690</td>\n",
       "      <td>-0.084850</td>\n",
       "      <td>-0.004090</td>\n",
       "      <td>-0.026543</td>\n",
       "      <td>-0.008540</td>\n",
       "      <td>0.111074</td>\n",
       "      <td>0.004941</td>\n",
       "      <td>0.024492</td>\n",
       "      <td>-0.038087</td>\n",
       "      <td>0.004840</td>\n",
       "      <td>0.064414</td>\n",
       "      <td>-0.037048</td>\n",
       "      <td>-0.002373</td>\n",
       "      <td>-0.016628</td>\n",
       "      <td>0.020384</td>\n",
       "      <td>-0.016905</td>\n",
       "      <td>0.020041</td>\n",
       "      <td>0.046781</td>\n",
       "      <td>0.036568</td>\n",
       "      <td>0.017206</td>\n",
       "      <td>0.043423</td>\n",
       "      <td>0.006018</td>\n",
       "      <td>-0.069732</td>\n",
       "      <td>0.015206</td>\n",
       "      <td>-0.089532</td>\n",
       "      <td>0.019574</td>\n",
       "      <td>0.018102</td>\n",
       "      <td>-0.014253</td>\n",
       "      <td>0.039520</td>\n",
       "      <td>-0.011853</td>\n",
       "      <td>0.041919</td>\n",
       "      <td>0.013867</td>\n",
       "      <td>-0.025970</td>\n",
       "      <td>-0.009378</td>\n",
       "      <td>-0.033706</td>\n",
       "      <td>-0.007861</td>\n",
       "      <td>-0.094755</td>\n",
       "      <td>-0.015332</td>\n",
       "      <td>-0.028955</td>\n",
       "      <td>-0.021220</td>\n",
       "      <td>-0.025419</td>\n",
       "      <td>-0.000249</td>\n",
       "      <td>0.033599</td>\n",
       "      <td>0.005672</td>\n",
       "      <td>0.047307</td>\n",
       "      <td>0.042933</td>\n",
       "      <td>-0.063510</td>\n",
       "      <td>0.002963</td>\n",
       "      <td>0.009207</td>\n",
       "      <td>0.022154</td>\n",
       "      <td>0.044349</td>\n",
       "      <td>0.021297</td>\n",
       "      <td>0.003317</td>\n",
       "      <td>-0.040126</td>\n",
       "      <td>0.116776</td>\n",
       "      <td>-0.022152</td>\n",
       "      <td>-0.005408</td>\n",
       "      <td>0.107770</td>\n",
       "      <td>0.055286</td>\n",
       "      <td>-0.022186</td>\n",
       "      <td>-0.065453</td>\n",
       "      <td>0.040531</td>\n",
       "      <td>-0.036721</td>\n",
       "      <td>0.028349</td>\n",
       "      <td>-0.058334</td>\n",
       "      <td>0.079651</td>\n",
       "      <td>0.041563</td>\n",
       "      <td>-0.047876</td>\n",
       "      <td>0.023672</td>\n",
       "      <td>0.062529</td>\n",
       "      <td>-0.032817</td>\n",
       "      <td>-0.012952</td>\n",
       "      <td>-0.002632</td>\n",
       "      <td>0.008558</td>\n",
       "      <td>-0.003629</td>\n",
       "      <td>0.033281</td>\n",
       "      <td>0.015877</td>\n",
       "      <td>-0.017730</td>\n",
       "      <td>0.018401</td>\n",
       "      <td>-0.013835</td>\n",
       "      <td>-0.010461</td>\n",
       "      <td>0.072611</td>\n",
       "      <td>0.102597</td>\n",
       "      <td>0.012783</td>\n",
       "      <td>-0.030518</td>\n",
       "      <td>-0.001271</td>\n",
       "      <td>-0.014921</td>\n",
       "      <td>-0.029244</td>\n",
       "      <td>-0.012678</td>\n",
       "      <td>-0.000284</td>\n",
       "      <td>0.021043</td>\n",
       "      <td>0.005935</td>\n",
       "      <td>-0.034138</td>\n",
       "      <td>0.019013</td>\n",
       "      <td>0.026799</td>\n",
       "      <td>-0.030454</td>\n",
       "      <td>-0.014222</td>\n",
       "      <td>0.009316</td>\n",
       "      <td>0.002893</td>\n",
       "      <td>0.035156</td>\n",
       "      <td>-0.021969</td>\n",
       "      <td>-0.017892</td>\n",
       "      <td>-0.045033</td>\n",
       "      <td>0.034003</td>\n",
       "      <td>0.041200</td>\n",
       "      <td>-0.011200</td>\n",
       "      <td>-0.047523</td>\n",
       "      <td>0.090632</td>\n",
       "      <td>0.008182</td>\n",
       "      <td>0.074359</td>\n",
       "      <td>-0.030094</td>\n",
       "      <td>-0.002536</td>\n",
       "      <td>0.054035</td>\n",
       "      <td>-0.070746</td>\n",
       "      <td>-0.033782</td>\n",
       "      <td>-0.051755</td>\n",
       "      <td>0.020351</td>\n",
       "      <td>0.044263</td>\n",
       "      <td>0.007501</td>\n",
       "      <td>0.002558</td>\n",
       "      <td>-0.053835</td>\n",
       "      <td>-0.014903</td>\n",
       "      <td>0.001218</td>\n",
       "      <td>-0.012547</td>\n",
       "      <td>-0.006464</td>\n",
       "      <td>-0.024044</td>\n",
       "      <td>-0.058528</td>\n",
       "      <td>0.065285</td>\n",
       "      <td>-0.087513</td>\n",
       "      <td>-0.115825</td>\n",
       "      <td>0.022499</td>\n",
       "      <td>-0.024774</td>\n",
       "      <td>0.036103</td>\n",
       "      <td>-0.004231</td>\n",
       "      <td>0.024775</td>\n",
       "      <td>0.004198</td>\n",
       "      <td>-0.019795</td>\n",
       "      <td>-0.064633</td>\n",
       "      <td>-0.001469</td>\n",
       "      <td>-0.016869</td>\n",
       "      <td>0.032951</td>\n",
       "      <td>-0.003615</td>\n",
       "      <td>0.019269</td>\n",
       "      <td>-0.050498</td>\n",
       "      <td>0.069507</td>\n",
       "      <td>-0.036685</td>\n",
       "      <td>0.082373</td>\n",
       "      <td>-0.008827</td>\n",
       "      <td>-0.008564</td>\n",
       "      <td>0.063305</td>\n",
       "      <td>0.010689</td>\n",
       "      <td>-0.038379</td>\n",
       "      <td>-0.025952</td>\n",
       "      <td>-0.003131</td>\n",
       "      <td>-0.014177</td>\n",
       "      <td>0.033302</td>\n",
       "      <td>0.019657</td>\n",
       "      <td>0.011582</td>\n",
       "      <td>0.018964</td>\n",
       "      <td>-0.015687</td>\n",
       "      <td>-0.058643</td>\n",
       "      <td>0.007014</td>\n",
       "      <td>-0.011122</td>\n",
       "      <td>-0.021585</td>\n",
       "      <td>0.020099</td>\n",
       "      <td>-0.009868</td>\n",
       "      <td>-0.019699</td>\n",
       "      <td>0.029781</td>\n",
       "      <td>-0.020384</td>\n",
       "      <td>-0.020782</td>\n",
       "      <td>0.045475</td>\n",
       "      <td>-0.009865</td>\n",
       "      <td>0.002705</td>\n",
       "      <td>0.110405</td>\n",
       "      <td>-0.055338</td>\n",
       "      <td>0.021207</td>\n",
       "      <td>-0.030944</td>\n",
       "      <td>0.030240</td>\n",
       "      <td>0.012771</td>\n",
       "      <td>0.012619</td>\n",
       "      <td>-0.022920</td>\n",
       "      <td>0.070540</td>\n",
       "      <td>-0.007604</td>\n",
       "      <td>0.024985</td>\n",
       "      <td>0.039168</td>\n",
       "      <td>-0.003487</td>\n",
       "      <td>-0.036396</td>\n",
       "      <td>0.015911</td>\n",
       "      <td>-0.036793</td>\n",
       "      <td>0.023608</td>\n",
       "      <td>0.041129</td>\n",
       "      <td>0.008881</td>\n",
       "      <td>-0.003117</td>\n",
       "      <td>0.045816</td>\n",
       "      <td>0.020293</td>\n",
       "      <td>-0.037319</td>\n",
       "      <td>0.020574</td>\n",
       "      <td>-0.001703</td>\n",
       "      <td>-0.003296</td>\n",
       "      <td>0.004238</td>\n",
       "      <td>-0.031665</td>\n",
       "      <td>0.022885</td>\n",
       "      <td>0.006250</td>\n",
       "      <td>0.055551</td>\n",
       "      <td>-0.007252</td>\n",
       "      <td>-0.003325</td>\n",
       "      <td>0.050446</td>\n",
       "      <td>-0.000391</td>\n",
       "      <td>0.191838</td>\n",
       "      <td>0.008870</td>\n",
       "      <td>0.000128</td>\n",
       "      <td>0.089973</td>\n",
       "      <td>0.063521</td>\n",
       "      <td>0.013788</td>\n",
       "      <td>0.016724</td>\n",
       "      <td>0.050653</td>\n",
       "      <td>-0.086953</td>\n",
       "      <td>0.000160</td>\n",
       "      <td>0.043096</td>\n",
       "      <td>0.002811</td>\n",
       "      <td>-0.021237</td>\n",
       "      <td>-0.043554</td>\n",
       "      <td>-0.012026</td>\n",
       "      <td>0.027210</td>\n",
       "      <td>-0.022110</td>\n",
       "      <td>0.194198</td>\n",
       "      <td>0.061368</td>\n",
       "      <td>0.005018</td>\n",
       "      <td>0.004373</td>\n",
       "      <td>0.032080</td>\n",
       "      <td>-0.011544</td>\n",
       "      <td>0.013090</td>\n",
       "      <td>-0.017780</td>\n",
       "      <td>0.015126</td>\n",
       "      <td>0.031574</td>\n",
       "      <td>0.023818</td>\n",
       "      <td>-0.013850</td>\n",
       "      <td>-0.041078</td>\n",
       "      <td>0.043683</td>\n",
       "      <td>0.046820</td>\n",
       "      <td>-0.055000</td>\n",
       "      <td>-0.029599</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>ski</th>\n",
       "      <td>-0.013872</td>\n",
       "      <td>0.016587</td>\n",
       "      <td>0.053483</td>\n",
       "      <td>-0.001593</td>\n",
       "      <td>-0.056213</td>\n",
       "      <td>0.053225</td>\n",
       "      <td>0.024746</td>\n",
       "      <td>-0.038070</td>\n",
       "      <td>-0.001304</td>\n",
       "      <td>-0.064059</td>\n",
       "      <td>-0.004609</td>\n",
       "      <td>-0.019647</td>\n",
       "      <td>0.007063</td>\n",
       "      <td>-0.066592</td>\n",
       "      <td>-0.017022</td>\n",
       "      <td>-0.028749</td>\n",
       "      <td>-0.028093</td>\n",
       "      <td>-0.050822</td>\n",
       "      <td>0.023928</td>\n",
       "      <td>-0.057207</td>\n",
       "      <td>0.047605</td>\n",
       "      <td>0.038898</td>\n",
       "      <td>-0.001860</td>\n",
       "      <td>-0.034913</td>\n",
       "      <td>0.025796</td>\n",
       "      <td>-0.021582</td>\n",
       "      <td>0.023413</td>\n",
       "      <td>-0.028462</td>\n",
       "      <td>0.043870</td>\n",
       "      <td>-0.015521</td>\n",
       "      <td>0.028640</td>\n",
       "      <td>0.007721</td>\n",
       "      <td>0.022624</td>\n",
       "      <td>0.029843</td>\n",
       "      <td>0.054276</td>\n",
       "      <td>-0.042778</td>\n",
       "      <td>-0.020132</td>\n",
       "      <td>0.087550</td>\n",
       "      <td>-0.025783</td>\n",
       "      <td>-0.058086</td>\n",
       "      <td>0.008229</td>\n",
       "      <td>-0.002663</td>\n",
       "      <td>0.116066</td>\n",
       "      <td>0.005409</td>\n",
       "      <td>-0.019686</td>\n",
       "      <td>0.042904</td>\n",
       "      <td>0.001521</td>\n",
       "      <td>0.038546</td>\n",
       "      <td>0.032056</td>\n",
       "      <td>-0.016908</td>\n",
       "      <td>0.003146</td>\n",
       "      <td>0.041120</td>\n",
       "      <td>-0.007723</td>\n",
       "      <td>-0.065293</td>\n",
       "      <td>0.099305</td>\n",
       "      <td>-0.015566</td>\n",
       "      <td>0.055894</td>\n",
       "      <td>0.042118</td>\n",
       "      <td>0.000059</td>\n",
       "      <td>-0.069229</td>\n",
       "      <td>-0.055996</td>\n",
       "      <td>0.067214</td>\n",
       "      <td>-0.049982</td>\n",
       "      <td>-0.049761</td>\n",
       "      <td>-0.052199</td>\n",
       "      <td>0.019055</td>\n",
       "      <td>0.030435</td>\n",
       "      <td>-0.076975</td>\n",
       "      <td>0.018795</td>\n",
       "      <td>-0.031994</td>\n",
       "      <td>0.046610</td>\n",
       "      <td>0.061748</td>\n",
       "      <td>0.047206</td>\n",
       "      <td>-0.040223</td>\n",
       "      <td>-0.009654</td>\n",
       "      <td>-0.035785</td>\n",
       "      <td>0.011235</td>\n",
       "      <td>0.039858</td>\n",
       "      <td>-0.045656</td>\n",
       "      <td>0.015985</td>\n",
       "      <td>0.036816</td>\n",
       "      <td>-0.049957</td>\n",
       "      <td>-0.010134</td>\n",
       "      <td>0.047293</td>\n",
       "      <td>-0.006938</td>\n",
       "      <td>-0.110578</td>\n",
       "      <td>-0.004824</td>\n",
       "      <td>0.032882</td>\n",
       "      <td>0.055943</td>\n",
       "      <td>0.039649</td>\n",
       "      <td>0.003459</td>\n",
       "      <td>-0.029714</td>\n",
       "      <td>-0.029203</td>\n",
       "      <td>0.062296</td>\n",
       "      <td>-0.037753</td>\n",
       "      <td>-0.019547</td>\n",
       "      <td>0.046419</td>\n",
       "      <td>0.008823</td>\n",
       "      <td>-0.038662</td>\n",
       "      <td>-0.012225</td>\n",
       "      <td>0.045851</td>\n",
       "      <td>-0.035496</td>\n",
       "      <td>-0.036195</td>\n",
       "      <td>0.034603</td>\n",
       "      <td>-0.053700</td>\n",
       "      <td>-0.026778</td>\n",
       "      <td>-0.032472</td>\n",
       "      <td>0.011841</td>\n",
       "      <td>0.046940</td>\n",
       "      <td>-0.043306</td>\n",
       "      <td>-0.005260</td>\n",
       "      <td>-0.001871</td>\n",
       "      <td>-0.014692</td>\n",
       "      <td>0.016023</td>\n",
       "      <td>-0.056307</td>\n",
       "      <td>0.067241</td>\n",
       "      <td>-0.089683</td>\n",
       "      <td>-0.066270</td>\n",
       "      <td>-0.077262</td>\n",
       "      <td>-0.013772</td>\n",
       "      <td>-0.068216</td>\n",
       "      <td>-0.051956</td>\n",
       "      <td>-0.040365</td>\n",
       "      <td>-0.069705</td>\n",
       "      <td>0.032662</td>\n",
       "      <td>0.032440</td>\n",
       "      <td>0.021780</td>\n",
       "      <td>-0.019936</td>\n",
       "      <td>-0.059833</td>\n",
       "      <td>0.082245</td>\n",
       "      <td>-0.056411</td>\n",
       "      <td>-0.034405</td>\n",
       "      <td>0.003386</td>\n",
       "      <td>0.014954</td>\n",
       "      <td>-0.055937</td>\n",
       "      <td>0.013666</td>\n",
       "      <td>-0.023110</td>\n",
       "      <td>0.018982</td>\n",
       "      <td>-0.014375</td>\n",
       "      <td>-0.017691</td>\n",
       "      <td>-0.004931</td>\n",
       "      <td>0.038707</td>\n",
       "      <td>-0.081328</td>\n",
       "      <td>-0.091864</td>\n",
       "      <td>-0.015918</td>\n",
       "      <td>0.023663</td>\n",
       "      <td>-0.003527</td>\n",
       "      <td>-0.028469</td>\n",
       "      <td>0.006176</td>\n",
       "      <td>-0.008459</td>\n",
       "      <td>-0.041107</td>\n",
       "      <td>0.024031</td>\n",
       "      <td>0.039680</td>\n",
       "      <td>0.020315</td>\n",
       "      <td>-0.041719</td>\n",
       "      <td>0.002327</td>\n",
       "      <td>-0.038460</td>\n",
       "      <td>-0.021237</td>\n",
       "      <td>-0.027125</td>\n",
       "      <td>0.067381</td>\n",
       "      <td>0.028438</td>\n",
       "      <td>-0.075094</td>\n",
       "      <td>0.039279</td>\n",
       "      <td>-0.067943</td>\n",
       "      <td>0.016289</td>\n",
       "      <td>-0.053202</td>\n",
       "      <td>-0.019049</td>\n",
       "      <td>0.000292</td>\n",
       "      <td>-0.043234</td>\n",
       "      <td>0.020842</td>\n",
       "      <td>-0.011141</td>\n",
       "      <td>0.015587</td>\n",
       "      <td>0.054630</td>\n",
       "      <td>0.003525</td>\n",
       "      <td>-0.013895</td>\n",
       "      <td>-0.086119</td>\n",
       "      <td>0.037069</td>\n",
       "      <td>0.029416</td>\n",
       "      <td>0.015883</td>\n",
       "      <td>-0.016231</td>\n",
       "      <td>0.076575</td>\n",
       "      <td>-0.003593</td>\n",
       "      <td>0.039219</td>\n",
       "      <td>-0.048926</td>\n",
       "      <td>0.008964</td>\n",
       "      <td>-0.066959</td>\n",
       "      <td>-0.060414</td>\n",
       "      <td>-0.052575</td>\n",
       "      <td>0.064279</td>\n",
       "      <td>-0.030092</td>\n",
       "      <td>-0.024110</td>\n",
       "      <td>0.000552</td>\n",
       "      <td>-0.006076</td>\n",
       "      <td>0.033648</td>\n",
       "      <td>0.028541</td>\n",
       "      <td>0.013798</td>\n",
       "      <td>-0.055113</td>\n",
       "      <td>-0.035938</td>\n",
       "      <td>-0.058438</td>\n",
       "      <td>0.038530</td>\n",
       "      <td>0.085925</td>\n",
       "      <td>0.020296</td>\n",
       "      <td>0.006213</td>\n",
       "      <td>-0.005141</td>\n",
       "      <td>-0.062642</td>\n",
       "      <td>-0.011234</td>\n",
       "      <td>0.007052</td>\n",
       "      <td>0.053659</td>\n",
       "      <td>0.057046</td>\n",
       "      <td>-0.030005</td>\n",
       "      <td>0.020388</td>\n",
       "      <td>0.022911</td>\n",
       "      <td>-0.037419</td>\n",
       "      <td>-0.106842</td>\n",
       "      <td>-0.047479</td>\n",
       "      <td>0.050884</td>\n",
       "      <td>-0.028620</td>\n",
       "      <td>-0.061798</td>\n",
       "      <td>-0.007032</td>\n",
       "      <td>0.024784</td>\n",
       "      <td>0.037960</td>\n",
       "      <td>0.025840</td>\n",
       "      <td>0.028836</td>\n",
       "      <td>-0.011317</td>\n",
       "      <td>-0.086174</td>\n",
       "      <td>-0.020134</td>\n",
       "      <td>-0.032978</td>\n",
       "      <td>0.202393</td>\n",
       "      <td>0.003158</td>\n",
       "      <td>-0.025799</td>\n",
       "      <td>0.055955</td>\n",
       "      <td>0.032788</td>\n",
       "      <td>0.004367</td>\n",
       "      <td>-0.011174</td>\n",
       "      <td>-0.059602</td>\n",
       "      <td>0.048104</td>\n",
       "      <td>-0.050321</td>\n",
       "      <td>0.000620</td>\n",
       "      <td>-0.018958</td>\n",
       "      <td>-0.005440</td>\n",
       "      <td>-0.004009</td>\n",
       "      <td>0.089833</td>\n",
       "      <td>-0.031880</td>\n",
       "      <td>-0.079372</td>\n",
       "      <td>-0.020683</td>\n",
       "      <td>0.022744</td>\n",
       "      <td>0.040849</td>\n",
       "      <td>-0.033132</td>\n",
       "      <td>-0.065669</td>\n",
       "      <td>-0.045235</td>\n",
       "      <td>0.018817</td>\n",
       "      <td>-0.031434</td>\n",
       "      <td>-0.030909</td>\n",
       "      <td>-0.031969</td>\n",
       "      <td>0.012048</td>\n",
       "      <td>0.052537</td>\n",
       "      <td>0.032010</td>\n",
       "      <td>-0.038205</td>\n",
       "      <td>0.005552</td>\n",
       "      <td>-0.019587</td>\n",
       "      <td>-0.041832</td>\n",
       "      <td>-0.025522</td>\n",
       "      <td>0.018393</td>\n",
       "      <td>-0.013667</td>\n",
       "      <td>0.011569</td>\n",
       "      <td>-0.003874</td>\n",
       "      <td>-0.021186</td>\n",
       "      <td>-0.041324</td>\n",
       "      <td>0.143349</td>\n",
       "      <td>-0.028088</td>\n",
       "      <td>-0.021641</td>\n",
       "      <td>0.013739</td>\n",
       "      <td>-0.121819</td>\n",
       "      <td>0.034780</td>\n",
       "      <td>-0.071066</td>\n",
       "      <td>0.050635</td>\n",
       "      <td>0.009606</td>\n",
       "      <td>0.002485</td>\n",
       "      <td>-0.059402</td>\n",
       "      <td>-0.043550</td>\n",
       "      <td>0.024906</td>\n",
       "      <td>-0.061689</td>\n",
       "      <td>0.008869</td>\n",
       "      <td>-0.058233</td>\n",
       "      <td>0.017211</td>\n",
       "      <td>-0.004470</td>\n",
       "      <td>-0.055263</td>\n",
       "      <td>-0.021396</td>\n",
       "      <td>-0.051851</td>\n",
       "      <td>-0.053350</td>\n",
       "      <td>0.019689</td>\n",
       "      <td>-0.036087</td>\n",
       "      <td>0.002624</td>\n",
       "      <td>0.036642</td>\n",
       "      <td>-0.036842</td>\n",
       "      <td>0.077346</td>\n",
       "      <td>0.010380</td>\n",
       "      <td>-0.022904</td>\n",
       "      <td>0.069120</td>\n",
       "      <td>-0.059792</td>\n",
       "      <td>0.034238</td>\n",
       "      <td>-0.024774</td>\n",
       "      <td>-0.053017</td>\n",
       "      <td>-0.097406</td>\n",
       "      <td>0.023633</td>\n",
       "      <td>-0.024530</td>\n",
       "      <td>0.000952</td>\n",
       "      <td>-0.042852</td>\n",
       "      <td>-0.011266</td>\n",
       "      <td>0.018251</td>\n",
       "      <td>-0.006567</td>\n",
       "      <td>0.052853</td>\n",
       "      <td>0.099428</td>\n",
       "      <td>0.051638</td>\n",
       "      <td>0.087469</td>\n",
       "      <td>0.024311</td>\n",
       "      <td>0.016170</td>\n",
       "      <td>-0.020217</td>\n",
       "      <td>-0.005375</td>\n",
       "      <td>-0.044012</td>\n",
       "      <td>-0.007893</td>\n",
       "      <td>0.026649</td>\n",
       "      <td>-0.014159</td>\n",
       "      <td>-0.005886</td>\n",
       "      <td>0.034076</td>\n",
       "      <td>0.050464</td>\n",
       "      <td>0.015244</td>\n",
       "      <td>0.011641</td>\n",
       "      <td>0.003350</td>\n",
       "      <td>0.021419</td>\n",
       "      <td>0.023190</td>\n",
       "      <td>0.039329</td>\n",
       "      <td>0.043227</td>\n",
       "      <td>-0.043292</td>\n",
       "      <td>0.010944</td>\n",
       "      <td>0.085633</td>\n",
       "      <td>0.057458</td>\n",
       "      <td>0.070978</td>\n",
       "      <td>0.026883</td>\n",
       "      <td>0.020939</td>\n",
       "      <td>-0.000073</td>\n",
       "      <td>0.040886</td>\n",
       "      <td>0.051017</td>\n",
       "      <td>-0.004203</td>\n",
       "      <td>0.001547</td>\n",
       "      <td>-0.015940</td>\n",
       "      <td>0.000884</td>\n",
       "      <td>0.017177</td>\n",
       "      <td>-0.015062</td>\n",
       "      <td>0.000966</td>\n",
       "      <td>-0.010336</td>\n",
       "      <td>0.043247</td>\n",
       "      <td>0.043357</td>\n",
       "      <td>-0.067278</td>\n",
       "      <td>-0.011968</td>\n",
       "      <td>-0.026605</td>\n",
       "      <td>0.068790</td>\n",
       "      <td>0.023970</td>\n",
       "      <td>0.065471</td>\n",
       "      <td>0.029704</td>\n",
       "      <td>0.005881</td>\n",
       "      <td>-0.025308</td>\n",
       "      <td>0.001190</td>\n",
       "      <td>0.023001</td>\n",
       "      <td>0.008168</td>\n",
       "      <td>0.032901</td>\n",
       "      <td>-0.033395</td>\n",
       "      <td>0.052650</td>\n",
       "      <td>0.055671</td>\n",
       "      <td>-0.110313</td>\n",
       "      <td>-0.001026</td>\n",
       "      <td>-0.082437</td>\n",
       "      <td>-0.009072</td>\n",
       "      <td>-0.000208</td>\n",
       "      <td>0.036476</td>\n",
       "      <td>0.015602</td>\n",
       "      <td>-0.011681</td>\n",
       "      <td>-0.072246</td>\n",
       "      <td>-0.029016</td>\n",
       "      <td>0.027394</td>\n",
       "      <td>-0.027655</td>\n",
       "      <td>0.030420</td>\n",
       "      <td>0.053756</td>\n",
       "      <td>-0.034217</td>\n",
       "      <td>-0.029213</td>\n",
       "      <td>-0.045368</td>\n",
       "      <td>0.001857</td>\n",
       "      <td>-0.033028</td>\n",
       "      <td>0.050661</td>\n",
       "      <td>0.060942</td>\n",
       "      <td>0.055785</td>\n",
       "      <td>-0.046167</td>\n",
       "      <td>-0.016992</td>\n",
       "      <td>0.009614</td>\n",
       "      <td>0.030656</td>\n",
       "      <td>-0.017616</td>\n",
       "      <td>0.008382</td>\n",
       "      <td>0.021332</td>\n",
       "      <td>-0.007910</td>\n",
       "      <td>-0.039261</td>\n",
       "      <td>-0.004204</td>\n",
       "      <td>-0.034923</td>\n",
       "      <td>0.066608</td>\n",
       "      <td>0.001291</td>\n",
       "      <td>0.086285</td>\n",
       "      <td>0.034908</td>\n",
       "      <td>-0.015330</td>\n",
       "      <td>0.012493</td>\n",
       "      <td>-0.051689</td>\n",
       "      <td>-0.003796</td>\n",
       "      <td>-0.012618</td>\n",
       "      <td>0.021196</td>\n",
       "      <td>-0.013685</td>\n",
       "      <td>-0.061384</td>\n",
       "      <td>-0.013458</td>\n",
       "      <td>0.028805</td>\n",
       "      <td>0.007108</td>\n",
       "      <td>0.051880</td>\n",
       "      <td>0.088713</td>\n",
       "      <td>-0.001977</td>\n",
       "      <td>-0.000163</td>\n",
       "      <td>-0.029829</td>\n",
       "      <td>0.018122</td>\n",
       "      <td>0.005632</td>\n",
       "      <td>0.029128</td>\n",
       "      <td>0.041451</td>\n",
       "      <td>0.010574</td>\n",
       "      <td>-0.046687</td>\n",
       "      <td>0.053688</td>\n",
       "      <td>-0.039152</td>\n",
       "      <td>-0.066719</td>\n",
       "      <td>-0.007831</td>\n",
       "      <td>0.020553</td>\n",
       "      <td>0.027382</td>\n",
       "      <td>-0.001871</td>\n",
       "      <td>-0.087949</td>\n",
       "      <td>-0.014861</td>\n",
       "      <td>-0.013398</td>\n",
       "      <td>0.074896</td>\n",
       "      <td>-0.009438</td>\n",
       "      <td>-0.018144</td>\n",
       "      <td>0.018258</td>\n",
       "      <td>0.045537</td>\n",
       "      <td>0.063243</td>\n",
       "      <td>0.007885</td>\n",
       "      <td>-0.006380</td>\n",
       "      <td>-0.017508</td>\n",
       "      <td>0.033997</td>\n",
       "      <td>-0.003291</td>\n",
       "      <td>-0.007959</td>\n",
       "      <td>0.042868</td>\n",
       "      <td>0.026891</td>\n",
       "      <td>-0.049395</td>\n",
       "      <td>0.076976</td>\n",
       "      <td>0.051432</td>\n",
       "      <td>-0.007085</td>\n",
       "      <td>-0.049923</td>\n",
       "      <td>0.058704</td>\n",
       "      <td>-0.037711</td>\n",
       "      <td>0.047211</td>\n",
       "      <td>0.057437</td>\n",
       "      <td>-0.110134</td>\n",
       "      <td>-0.013813</td>\n",
       "      <td>0.033519</td>\n",
       "      <td>-0.058675</td>\n",
       "      <td>0.060776</td>\n",
       "      <td>-0.020869</td>\n",
       "      <td>0.049130</td>\n",
       "      <td>-0.012893</td>\n",
       "      <td>0.104267</td>\n",
       "      <td>-0.016780</td>\n",
       "      <td>0.033721</td>\n",
       "      <td>-0.040861</td>\n",
       "      <td>0.018616</td>\n",
       "      <td>-0.002271</td>\n",
       "      <td>0.082381</td>\n",
       "      <td>-0.041468</td>\n",
       "      <td>0.070440</td>\n",
       "      <td>0.158369</td>\n",
       "      <td>0.008520</td>\n",
       "      <td>-0.031631</td>\n",
       "      <td>-0.011209</td>\n",
       "      <td>-0.038880</td>\n",
       "      <td>-0.016970</td>\n",
       "      <td>0.009048</td>\n",
       "      <td>-0.050233</td>\n",
       "      <td>-0.002430</td>\n",
       "      <td>0.074730</td>\n",
       "      <td>-0.040414</td>\n",
       "      <td>-0.076996</td>\n",
       "      <td>-0.029274</td>\n",
       "      <td>0.024709</td>\n",
       "      <td>0.062133</td>\n",
       "      <td>0.046645</td>\n",
       "      <td>-0.007068</td>\n",
       "      <td>0.159120</td>\n",
       "      <td>0.006680</td>\n",
       "      <td>0.036902</td>\n",
       "      <td>0.049515</td>\n",
       "      <td>0.050404</td>\n",
       "      <td>0.039928</td>\n",
       "      <td>-0.002150</td>\n",
       "      <td>-0.034333</td>\n",
       "      <td>-0.093344</td>\n",
       "      <td>-0.002908</td>\n",
       "      <td>-0.009015</td>\n",
       "      <td>-0.003638</td>\n",
       "      <td>-0.008709</td>\n",
       "      <td>0.021233</td>\n",
       "      <td>0.045306</td>\n",
       "      <td>0.001246</td>\n",
       "      <td>-0.020212</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>commanding</th>\n",
       "      <td>-0.021643</td>\n",
       "      <td>-0.001753</td>\n",
       "      <td>0.059346</td>\n",
       "      <td>0.009933</td>\n",
       "      <td>0.009539</td>\n",
       "      <td>-0.128928</td>\n",
       "      <td>0.032531</td>\n",
       "      <td>0.017477</td>\n",
       "      <td>0.013784</td>\n",
       "      <td>0.025225</td>\n",
       "      <td>0.020248</td>\n",
       "      <td>0.081404</td>\n",
       "      <td>-0.026698</td>\n",
       "      <td>-0.047578</td>\n",
       "      <td>-0.023229</td>\n",
       "      <td>0.027368</td>\n",
       "      <td>0.003698</td>\n",
       "      <td>-0.001569</td>\n",
       "      <td>-0.001780</td>\n",
       "      <td>0.018005</td>\n",
       "      <td>0.057034</td>\n",
       "      <td>0.032284</td>\n",
       "      <td>0.042459</td>\n",
       "      <td>-0.039614</td>\n",
       "      <td>-0.006877</td>\n",
       "      <td>-0.080075</td>\n",
       "      <td>0.009791</td>\n",
       "      <td>0.086474</td>\n",
       "      <td>-0.011625</td>\n",
       "      <td>0.048725</td>\n",
       "      <td>0.033478</td>\n",
       "      <td>0.043670</td>\n",
       "      <td>-0.027102</td>\n",
       "      <td>0.124842</td>\n",
       "      <td>-0.000782</td>\n",
       "      <td>-0.089207</td>\n",
       "      <td>0.079469</td>\n",
       "      <td>0.029341</td>\n",
       "      <td>-0.055349</td>\n",
       "      <td>0.019166</td>\n",
       "      <td>0.003231</td>\n",
       "      <td>0.044943</td>\n",
       "      <td>0.038837</td>\n",
       "      <td>-0.033651</td>\n",
       "      <td>0.025904</td>\n",
       "      <td>0.037340</td>\n",
       "      <td>0.015126</td>\n",
       "      <td>0.018474</td>\n",
       "      <td>0.072318</td>\n",
       "      <td>-0.004525</td>\n",
       "      <td>0.034089</td>\n",
       "      <td>0.070720</td>\n",
       "      <td>-0.005002</td>\n",
       "      <td>-0.018145</td>\n",
       "      <td>0.018852</td>\n",
       "      <td>-0.056592</td>\n",
       "      <td>0.078702</td>\n",
       "      <td>-0.040465</td>\n",
       "      <td>0.031717</td>\n",
       "      <td>-0.021126</td>\n",
       "      <td>-0.019583</td>\n",
       "      <td>-0.009596</td>\n",
       "      <td>-0.042663</td>\n",
       "      <td>-0.052976</td>\n",
       "      <td>-0.089164</td>\n",
       "      <td>-0.018692</td>\n",
       "      <td>-0.007255</td>\n",
       "      <td>0.011200</td>\n",
       "      <td>-0.021510</td>\n",
       "      <td>-0.027698</td>\n",
       "      <td>0.037555</td>\n",
       "      <td>0.044546</td>\n",
       "      <td>-0.052333</td>\n",
       "      <td>-0.013833</td>\n",
       "      <td>-0.015568</td>\n",
       "      <td>-0.019458</td>\n",
       "      <td>0.048057</td>\n",
       "      <td>-0.056151</td>\n",
       "      <td>-0.008476</td>\n",
       "      <td>0.003763</td>\n",
       "      <td>0.014685</td>\n",
       "      <td>0.021772</td>\n",
       "      <td>0.026899</td>\n",
       "      <td>-0.019591</td>\n",
       "      <td>-0.024272</td>\n",
       "      <td>0.051466</td>\n",
       "      <td>-0.018622</td>\n",
       "      <td>0.065751</td>\n",
       "      <td>0.003105</td>\n",
       "      <td>0.002518</td>\n",
       "      <td>-0.076166</td>\n",
       "      <td>-0.026585</td>\n",
       "      <td>-0.029944</td>\n",
       "      <td>-0.015349</td>\n",
       "      <td>-0.047645</td>\n",
       "      <td>-0.007413</td>\n",
       "      <td>0.087120</td>\n",
       "      <td>-0.035157</td>\n",
       "      <td>-0.071460</td>\n",
       "      <td>0.007088</td>\n",
       "      <td>-0.073490</td>\n",
       "      <td>-0.055170</td>\n",
       "      <td>-0.081992</td>\n",
       "      <td>0.001517</td>\n",
       "      <td>-0.016964</td>\n",
       "      <td>0.030018</td>\n",
       "      <td>0.004160</td>\n",
       "      <td>-0.022467</td>\n",
       "      <td>-0.006657</td>\n",
       "      <td>-0.035748</td>\n",
       "      <td>0.040980</td>\n",
       "      <td>-0.053014</td>\n",
       "      <td>0.044824</td>\n",
       "      <td>-0.008718</td>\n",
       "      <td>0.003849</td>\n",
       "      <td>-0.025019</td>\n",
       "      <td>0.011426</td>\n",
       "      <td>-0.066660</td>\n",
       "      <td>-0.030656</td>\n",
       "      <td>-0.052554</td>\n",
       "      <td>-0.057266</td>\n",
       "      <td>0.024076</td>\n",
       "      <td>-0.052575</td>\n",
       "      <td>0.026837</td>\n",
       "      <td>-0.020632</td>\n",
       "      <td>-0.023704</td>\n",
       "      <td>0.010053</td>\n",
       "      <td>-0.010024</td>\n",
       "      <td>0.051221</td>\n",
       "      <td>-0.026290</td>\n",
       "      <td>-0.033514</td>\n",
       "      <td>-0.086520</td>\n",
       "      <td>-0.065273</td>\n",
       "      <td>0.034437</td>\n",
       "      <td>-0.032389</td>\n",
       "      <td>-0.058795</td>\n",
       "      <td>-0.006822</td>\n",
       "      <td>0.023129</td>\n",
       "      <td>-0.013649</td>\n",
       "      <td>-0.019729</td>\n",
       "      <td>-0.059690</td>\n",
       "      <td>-0.010782</td>\n",
       "      <td>-0.017630</td>\n",
       "      <td>-0.035718</td>\n",
       "      <td>0.034197</td>\n",
       "      <td>0.003572</td>\n",
       "      <td>0.014439</td>\n",
       "      <td>-0.027348</td>\n",
       "      <td>0.027440</td>\n",
       "      <td>0.063121</td>\n",
       "      <td>0.030407</td>\n",
       "      <td>0.044482</td>\n",
       "      <td>0.015033</td>\n",
       "      <td>0.070332</td>\n",
       "      <td>-0.011271</td>\n",
       "      <td>0.007824</td>\n",
       "      <td>0.004222</td>\n",
       "      <td>0.007239</td>\n",
       "      <td>-0.077934</td>\n",
       "      <td>0.019962</td>\n",
       "      <td>0.035756</td>\n",
       "      <td>-0.044720</td>\n",
       "      <td>-0.004919</td>\n",
       "      <td>-0.065912</td>\n",
       "      <td>-0.000597</td>\n",
       "      <td>-0.040960</td>\n",
       "      <td>0.019029</td>\n",
       "      <td>0.003117</td>\n",
       "      <td>0.012346</td>\n",
       "      <td>0.004021</td>\n",
       "      <td>0.013941</td>\n",
       "      <td>0.025210</td>\n",
       "      <td>-0.003407</td>\n",
       "      <td>-0.070864</td>\n",
       "      <td>0.016920</td>\n",
       "      <td>0.000896</td>\n",
       "      <td>0.039064</td>\n",
       "      <td>-0.011389</td>\n",
       "      <td>-0.036745</td>\n",
       "      <td>-0.067944</td>\n",
       "      <td>0.020702</td>\n",
       "      <td>-0.025541</td>\n",
       "      <td>-0.003371</td>\n",
       "      <td>0.008871</td>\n",
       "      <td>0.048331</td>\n",
       "      <td>-0.015311</td>\n",
       "      <td>-0.113575</td>\n",
       "      <td>-0.009840</td>\n",
       "      <td>0.013920</td>\n",
       "      <td>-0.006402</td>\n",
       "      <td>-0.028837</td>\n",
       "      <td>-0.041043</td>\n",
       "      <td>-0.006534</td>\n",
       "      <td>-0.020197</td>\n",
       "      <td>-0.015948</td>\n",
       "      <td>0.081412</td>\n",
       "      <td>0.016552</td>\n",
       "      <td>-0.014702</td>\n",
       "      <td>0.052325</td>\n",
       "      <td>-0.017257</td>\n",
       "      <td>0.025041</td>\n",
       "      <td>0.032561</td>\n",
       "      <td>-0.002362</td>\n",
       "      <td>-0.044448</td>\n",
       "      <td>-0.062082</td>\n",
       "      <td>0.024154</td>\n",
       "      <td>0.060061</td>\n",
       "      <td>0.034234</td>\n",
       "      <td>0.089554</td>\n",
       "      <td>-0.006306</td>\n",
       "      <td>-0.039365</td>\n",
       "      <td>0.043834</td>\n",
       "      <td>0.072385</td>\n",
       "      <td>0.042458</td>\n",
       "      <td>0.028361</td>\n",
       "      <td>-0.000684</td>\n",
       "      <td>0.001104</td>\n",
       "      <td>-0.120564</td>\n",
       "      <td>-0.027148</td>\n",
       "      <td>0.047761</td>\n",
       "      <td>0.034415</td>\n",
       "      <td>-0.051212</td>\n",
       "      <td>0.034130</td>\n",
       "      <td>-0.037960</td>\n",
       "      <td>0.091482</td>\n",
       "      <td>-0.027058</td>\n",
       "      <td>0.056124</td>\n",
       "      <td>0.195167</td>\n",
       "      <td>-0.011711</td>\n",
       "      <td>-0.007206</td>\n",
       "      <td>0.041300</td>\n",
       "      <td>0.064358</td>\n",
       "      <td>-0.035460</td>\n",
       "      <td>0.009123</td>\n",
       "      <td>-0.007125</td>\n",
       "      <td>-0.023654</td>\n",
       "      <td>-0.009742</td>\n",
       "      <td>-0.060939</td>\n",
       "      <td>-0.057435</td>\n",
       "      <td>0.057011</td>\n",
       "      <td>-0.002648</td>\n",
       "      <td>0.123051</td>\n",
       "      <td>-0.012610</td>\n",
       "      <td>0.001368</td>\n",
       "      <td>-0.033395</td>\n",
       "      <td>0.007712</td>\n",
       "      <td>-0.022468</td>\n",
       "      <td>0.007765</td>\n",
       "      <td>0.038097</td>\n",
       "      <td>0.017668</td>\n",
       "      <td>-0.090918</td>\n",
       "      <td>-0.028575</td>\n",
       "      <td>0.040213</td>\n",
       "      <td>-0.018307</td>\n",
       "      <td>-0.049447</td>\n",
       "      <td>0.005271</td>\n",
       "      <td>0.055747</td>\n",
       "      <td>-0.005953</td>\n",
       "      <td>0.077284</td>\n",
       "      <td>0.000709</td>\n",
       "      <td>0.023473</td>\n",
       "      <td>-0.017973</td>\n",
       "      <td>-0.015217</td>\n",
       "      <td>-0.030226</td>\n",
       "      <td>-0.028576</td>\n",
       "      <td>0.013981</td>\n",
       "      <td>0.044259</td>\n",
       "      <td>0.010497</td>\n",
       "      <td>0.087216</td>\n",
       "      <td>-0.013632</td>\n",
       "      <td>-0.035985</td>\n",
       "      <td>-0.054465</td>\n",
       "      <td>-0.009651</td>\n",
       "      <td>0.005737</td>\n",
       "      <td>-0.077592</td>\n",
       "      <td>0.091128</td>\n",
       "      <td>0.071484</td>\n",
       "      <td>-0.016924</td>\n",
       "      <td>-0.006736</td>\n",
       "      <td>-0.026583</td>\n",
       "      <td>0.029587</td>\n",
       "      <td>-0.010142</td>\n",
       "      <td>-0.057059</td>\n",
       "      <td>0.007034</td>\n",
       "      <td>0.068796</td>\n",
       "      <td>0.003673</td>\n",
       "      <td>-0.116480</td>\n",
       "      <td>0.072480</td>\n",
       "      <td>0.000154</td>\n",
       "      <td>-0.019958</td>\n",
       "      <td>-0.117175</td>\n",
       "      <td>0.005522</td>\n",
       "      <td>-0.047364</td>\n",
       "      <td>0.043816</td>\n",
       "      <td>0.010554</td>\n",
       "      <td>0.076083</td>\n",
       "      <td>-0.051996</td>\n",
       "      <td>-0.025430</td>\n",
       "      <td>0.080790</td>\n",
       "      <td>-0.060167</td>\n",
       "      <td>0.042614</td>\n",
       "      <td>-0.009178</td>\n",
       "      <td>-0.034335</td>\n",
       "      <td>-0.065378</td>\n",
       "      <td>-0.016178</td>\n",
       "      <td>-0.022129</td>\n",
       "      <td>-0.015178</td>\n",
       "      <td>0.000220</td>\n",
       "      <td>0.024351</td>\n",
       "      <td>0.038405</td>\n",
       "      <td>-0.008805</td>\n",
       "      <td>0.016085</td>\n",
       "      <td>0.039612</td>\n",
       "      <td>0.044743</td>\n",
       "      <td>0.083379</td>\n",
       "      <td>0.020162</td>\n",
       "      <td>0.030495</td>\n",
       "      <td>0.043775</td>\n",
       "      <td>-0.000040</td>\n",
       "      <td>0.022129</td>\n",
       "      <td>0.005051</td>\n",
       "      <td>-0.043426</td>\n",
       "      <td>0.035729</td>\n",
       "      <td>-0.000915</td>\n",
       "      <td>0.055961</td>\n",
       "      <td>-0.035329</td>\n",
       "      <td>0.028746</td>\n",
       "      <td>-0.028187</td>\n",
       "      <td>0.036928</td>\n",
       "      <td>0.021690</td>\n",
       "      <td>-0.027648</td>\n",
       "      <td>0.002086</td>\n",
       "      <td>0.019244</td>\n",
       "      <td>0.056314</td>\n",
       "      <td>-0.047314</td>\n",
       "      <td>0.067922</td>\n",
       "      <td>-0.127662</td>\n",
       "      <td>-0.007581</td>\n",
       "      <td>0.059841</td>\n",
       "      <td>0.000360</td>\n",
       "      <td>0.003862</td>\n",
       "      <td>-0.096651</td>\n",
       "      <td>-0.046278</td>\n",
       "      <td>-0.042496</td>\n",
       "      <td>-0.005145</td>\n",
       "      <td>-0.025536</td>\n",
       "      <td>-0.012258</td>\n",
       "      <td>-0.027964</td>\n",
       "      <td>-0.007082</td>\n",
       "      <td>0.021499</td>\n",
       "      <td>0.049162</td>\n",
       "      <td>0.067834</td>\n",
       "      <td>0.034015</td>\n",
       "      <td>-0.033849</td>\n",
       "      <td>-0.017583</td>\n",
       "      <td>0.039971</td>\n",
       "      <td>0.075746</td>\n",
       "      <td>-0.031029</td>\n",
       "      <td>0.064802</td>\n",
       "      <td>-0.036586</td>\n",
       "      <td>0.006019</td>\n",
       "      <td>-0.043170</td>\n",
       "      <td>-0.053748</td>\n",
       "      <td>-0.055148</td>\n",
       "      <td>0.041165</td>\n",
       "      <td>-0.003597</td>\n",
       "      <td>-0.058903</td>\n",
       "      <td>0.002993</td>\n",
       "      <td>0.052532</td>\n",
       "      <td>-0.023457</td>\n",
       "      <td>0.031152</td>\n",
       "      <td>-0.002807</td>\n",
       "      <td>0.016733</td>\n",
       "      <td>0.062071</td>\n",
       "      <td>0.017900</td>\n",
       "      <td>0.014274</td>\n",
       "      <td>-0.049826</td>\n",
       "      <td>0.008016</td>\n",
       "      <td>0.007972</td>\n",
       "      <td>-0.002798</td>\n",
       "      <td>-0.053569</td>\n",
       "      <td>0.029696</td>\n",
       "      <td>0.049230</td>\n",
       "      <td>-0.052330</td>\n",
       "      <td>-0.070980</td>\n",
       "      <td>-0.055893</td>\n",
       "      <td>-0.000766</td>\n",
       "      <td>0.019126</td>\n",
       "      <td>-0.068870</td>\n",
       "      <td>-0.058030</td>\n",
       "      <td>0.049417</td>\n",
       "      <td>-0.013663</td>\n",
       "      <td>-0.054767</td>\n",
       "      <td>-0.066311</td>\n",
       "      <td>-0.044828</td>\n",
       "      <td>-0.049260</td>\n",
       "      <td>0.074690</td>\n",
       "      <td>-0.032016</td>\n",
       "      <td>0.092839</td>\n",
       "      <td>-0.057402</td>\n",
       "      <td>-0.025372</td>\n",
       "      <td>0.003580</td>\n",
       "      <td>0.023142</td>\n",
       "      <td>0.016646</td>\n",
       "      <td>-0.068957</td>\n",
       "      <td>-0.026307</td>\n",
       "      <td>-0.038541</td>\n",
       "      <td>0.019094</td>\n",
       "      <td>-0.042331</td>\n",
       "      <td>-0.030626</td>\n",
       "      <td>-0.002468</td>\n",
       "      <td>-0.004777</td>\n",
       "      <td>-0.029731</td>\n",
       "      <td>-0.003758</td>\n",
       "      <td>-0.038012</td>\n",
       "      <td>-0.019335</td>\n",
       "      <td>-0.002208</td>\n",
       "      <td>-0.001038</td>\n",
       "      <td>0.051712</td>\n",
       "      <td>0.098190</td>\n",
       "      <td>-0.016959</td>\n",
       "      <td>-0.008214</td>\n",
       "      <td>-0.018745</td>\n",
       "      <td>-0.002664</td>\n",
       "      <td>0.052733</td>\n",
       "      <td>0.029994</td>\n",
       "      <td>0.022315</td>\n",
       "      <td>0.000283</td>\n",
       "      <td>0.030527</td>\n",
       "      <td>0.008958</td>\n",
       "      <td>-0.049916</td>\n",
       "      <td>0.025179</td>\n",
       "      <td>-0.063281</td>\n",
       "      <td>-0.019849</td>\n",
       "      <td>-0.046519</td>\n",
       "      <td>0.022930</td>\n",
       "      <td>0.024403</td>\n",
       "      <td>0.036987</td>\n",
       "      <td>-0.013033</td>\n",
       "      <td>0.056618</td>\n",
       "      <td>-0.010980</td>\n",
       "      <td>0.030294</td>\n",
       "      <td>0.023763</td>\n",
       "      <td>0.021788</td>\n",
       "      <td>0.003277</td>\n",
       "      <td>-0.025218</td>\n",
       "      <td>0.013912</td>\n",
       "      <td>0.032784</td>\n",
       "      <td>0.007840</td>\n",
       "      <td>0.015517</td>\n",
       "      <td>0.085115</td>\n",
       "      <td>0.021464</td>\n",
       "      <td>0.061205</td>\n",
       "      <td>-0.009277</td>\n",
       "      <td>0.031175</td>\n",
       "      <td>0.026020</td>\n",
       "      <td>0.004564</td>\n",
       "      <td>0.050285</td>\n",
       "      <td>-0.025953</td>\n",
       "      <td>0.016942</td>\n",
       "      <td>-0.017027</td>\n",
       "      <td>0.037061</td>\n",
       "      <td>0.048977</td>\n",
       "      <td>0.028693</td>\n",
       "      <td>0.002752</td>\n",
       "      <td>0.068661</td>\n",
       "      <td>-0.062564</td>\n",
       "      <td>0.048838</td>\n",
       "      <td>0.011330</td>\n",
       "      <td>0.027440</td>\n",
       "      <td>-0.058865</td>\n",
       "      <td>0.012646</td>\n",
       "      <td>0.017443</td>\n",
       "      <td>0.050385</td>\n",
       "      <td>-0.003471</td>\n",
       "      <td>0.044069</td>\n",
       "      <td>-0.052522</td>\n",
       "      <td>0.083973</td>\n",
       "      <td>0.163117</td>\n",
       "      <td>-0.027056</td>\n",
       "      <td>-0.031732</td>\n",
       "      <td>0.024662</td>\n",
       "      <td>-0.002531</td>\n",
       "      <td>-0.036302</td>\n",
       "      <td>-0.077429</td>\n",
       "      <td>0.073330</td>\n",
       "      <td>0.051751</td>\n",
       "      <td>-0.077653</td>\n",
       "      <td>-0.036914</td>\n",
       "      <td>0.001558</td>\n",
       "      <td>0.018197</td>\n",
       "      <td>-0.016465</td>\n",
       "      <td>0.025043</td>\n",
       "      <td>-0.010481</td>\n",
       "      <td>-0.060741</td>\n",
       "      <td>0.184386</td>\n",
       "      <td>-0.019070</td>\n",
       "      <td>0.036520</td>\n",
       "      <td>-0.003032</td>\n",
       "      <td>0.044082</td>\n",
       "      <td>0.018076</td>\n",
       "      <td>0.028879</td>\n",
       "      <td>-0.041535</td>\n",
       "      <td>0.009056</td>\n",
       "      <td>0.019925</td>\n",
       "      <td>-0.036146</td>\n",
       "      <td>-0.019906</td>\n",
       "      <td>-0.011048</td>\n",
       "      <td>-0.014468</td>\n",
       "      <td>0.003710</td>\n",
       "      <td>-0.038677</td>\n",
       "      <td>0.002350</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>3512 rows × 512 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                    0         1         2         3         4         5         6         7         8         9         10        11        12        13        14        15        16        17        18        19        20        21        22        23        24        25        26        27        28        29        30        31        32        33        34        35        36        37        38        39        40        41        42        43        44        45        46        47        48        49        50        51        52        53        54        55        56        57        58        59        60        61        62        63        64        65        66        67        68        69        70        71        72        73        74        75        76        77        78        79        80        81        82        83        84        85        86        87        88        89        90        91        92        93        94        95        96        97        98        99        100       101       102       103       104       105       106       107       108       109       110       111       112       113       114       115       116       117       118       119       120       121       122       123       124       125       126       127       128       129       130       131       132       133       134       135       136       137       138       139       140       141       142       143       144       145       146       147       148       149       150       151       152       153       154       155       156       157       158       159       160       161       162       163       164       165       166       167       168       169       170       171       172       173       174       175       176       177       178       179       180       181       182       183       184       185       186       187       188       189       190       191       192       193       194       195       196       197  \\\n",
       "argument      -0.006003 -0.007702  0.049104 -0.064622 -0.025032 -0.019061 -0.020415  0.091926 -0.033597 -0.028624  0.027280  0.011417 -0.053613  0.018259  0.071262  0.063713 -0.013471 -0.011661  0.035724 -0.023335  0.032925 -0.025968  0.023209 -0.007237 -0.052589 -0.054912 -0.006545 -0.034289 -0.033662  0.066789  0.037343  0.011251  0.006737  0.023766  0.017598  0.015699 -0.052477  0.024599  0.047769 -0.046031 -0.029823  0.044138  0.034726 -0.037601  0.058254  0.044123  0.063278  0.088770  0.040936  0.012084  0.021672  0.020336  0.000022 -0.000254 -0.011542 -0.058915  0.043487  0.003159 -0.004820  0.062071 -0.031914  0.032600 -0.083566 -0.030406 -0.036129 -0.050030 -0.066287  0.006014  0.014364  0.013641 -0.006052  0.000432 -0.031755 -0.062195  0.019999  0.018836  0.024018  0.012946 -0.071339 -0.012209 -0.012602  0.073238  0.092335  0.041420 -0.036912 -0.016367 -0.059622 -0.034675 -0.011695  0.008536  0.031235 -0.036399 -0.022793 -0.003573 -0.085040  0.015403  0.049900 -0.005527 -0.005082 -0.032682  0.005251  0.021066  0.010309  0.002287 -0.041193 -0.043692 -0.018276 -0.046748 -0.001182 -0.064127 -0.044554  0.018138 -0.044678  0.038087  0.021035  0.038185 -0.025295  0.016275  0.007258  0.012708 -0.023058 -0.002954  0.012793  0.024006 -0.012133 -0.075228 -0.032491 -0.037764  0.010691  0.036100 -0.003719 -0.006661 -0.028511  0.045565 -0.069914 -0.002546 -0.021120  0.020760 -0.012043 -0.025806  0.003101 -0.043906 -0.021463 -0.045797 -0.009575  0.029342 -0.026703 -0.018808 -0.005254  0.004310 -0.024730 -0.010713  0.000740  0.039211 -0.056413  0.014849 -0.023637  0.028707 -0.046382  0.039182 -0.035561 -0.000134  0.005465 -0.030363  0.003977 -0.022546 -0.002373  0.045421 -0.043740  0.032824 -0.038087  0.003259  0.013397  0.006483  0.045332 -0.017813  0.047650 -0.019894  0.001230 -0.019093  0.037615 -0.018049  0.021043  0.007654 -0.030093 -0.039901 -0.049265 -0.060078  0.017513 -0.004326 -0.017963 -0.064216 -0.015015  0.019954  0.042063  0.073215 -0.010953  0.013774   \n",
       "awful         -0.025814 -0.001224  0.035236  0.001643 -0.066600  0.017712  0.103399  0.031633 -0.017865  0.001599  0.019264  0.039432  0.017838 -0.012615 -0.122431  0.011062 -0.031063 -0.013706  0.033635  0.007671  0.015961  0.019459  0.003425  0.006021  0.057787 -0.014008  0.005105 -0.010140  0.016574  0.019606  0.015120  0.022731 -0.044560  0.029897  0.023853  0.036493  0.028849  0.043707 -0.003260 -0.030843 -0.016285 -0.009341 -0.001705  0.009599  0.019213 -0.030681  0.042644  0.030699  0.009117 -0.016227 -0.055356  0.004963 -0.032168  0.033859  0.024406 -0.057546 -0.007470  0.014877 -0.024983 -0.059320 -0.022733  0.030149 -0.048973 -0.031368 -0.052566 -0.016700 -0.042668  0.049775  0.008358 -0.021421 -0.084370 -0.004991  0.026933  0.063891  0.031296 -0.009675  0.016345  0.023671  0.050449 -0.009255 -0.033355  0.038838  0.069530  0.012205 -0.051473  0.036883 -0.044990 -0.017582 -0.058773  0.012967 -0.062246  0.001836 -0.052009  0.057569 -0.084333 -0.028085  0.025998  0.068974 -0.027156  0.011427  0.032044  0.021471 -0.053132 -0.046528 -0.046233 -0.016096  0.078329  0.004418  0.029524  0.001182  0.013785 -0.006484 -0.041348 -0.015449  0.043145  0.036085 -0.022183 -0.061391 -0.035292  0.019617 -0.048725 -0.054573  0.046135 -0.024376 -0.011189 -0.032556 -0.009461 -0.011675  0.069323 -0.042037  0.032265 -0.096520  0.022075  0.044785  0.055126 -0.055678 -0.026223  0.062629 -0.025348 -0.003328 -0.028567 -0.020788 -0.038727 -0.104016 -0.046917  0.002023  0.019458 -0.061616  0.103710  0.015812  0.037145  0.063162 -0.004835  0.021962 -0.015525  0.051988 -0.025518  0.066387 -0.016573 -0.007669  0.027609  0.032354  0.028316  0.012097 -0.038119 -0.034135  0.044899  0.024231  0.023165 -0.004083 -0.017246  0.019143 -0.008551 -0.009865 -0.033172 -0.009114  0.041588 -0.027518  0.002568  0.041306 -0.004634  0.045953  0.035884  0.004849 -0.043088 -0.075771 -0.093825 -0.038505 -0.025050 -0.012569 -0.005734  0.029197 -0.006983 -0.001606 -0.044704  0.076233  0.021628 -0.037928   \n",
       "decorator      0.028367 -0.011092  0.065735 -0.023946  0.007433 -0.042059  0.037751 -0.063795  0.006671 -0.045171  0.001256  0.005200 -0.054715 -0.010511 -0.019794  0.047819 -0.011350 -0.018326  0.035546  0.017572  0.031661  0.035187  0.109695  0.004948  0.006225 -0.083984  0.009715  0.030252 -0.002363  0.054821  0.039203  0.052721  0.082165  0.110000 -0.052309 -0.019377  0.097118  0.011658 -0.016264  0.041651 -0.066683 -0.070656  0.009422  0.061907  0.005387 -0.041726 -0.017049  0.037700  0.079488  0.000537  0.034387  0.052700  0.010411 -0.040935 -0.054288 -0.035206  0.081980  0.044010 -0.030655 -0.009150  0.017010 -0.090732  0.009580 -0.069278 -0.041877 -0.029918  0.038424  0.001841  0.007457 -0.019183  0.052403 -0.018009 -0.056961 -0.045440  0.009813 -0.048325 -0.065862 -0.119715  0.002029 -0.073085 -0.008515  0.016100 -0.006301  0.015268 -0.001366  0.026453 -0.017920 -0.023437 -0.009428  0.004971  0.038333 -0.039551  0.043570  0.050496 -0.073858 -0.070829 -0.011753  0.036846 -0.025951 -0.004652  0.007844 -0.081316 -0.030543  0.002785 -0.102660  0.018139 -0.024110 -0.009186  0.018067 -0.001253 -0.011537  0.020364 -0.065169  0.083269  0.086014  0.049683  0.022026 -0.005992 -0.021545  0.046494  0.018731 -0.045790 -0.054476  0.061659 -0.038316  0.053238 -0.044938 -0.076508 -0.024807  0.029913 -0.029004 -0.087512  0.030835  0.043007 -0.046014 -0.048448 -0.030697  0.017824 -0.011125  0.000264 -0.015512 -0.051955  0.025651 -0.032956 -0.023176 -0.028233 -0.016615 -0.011157 -0.028199  0.032952 -0.008998 -0.006820 -0.006296  0.057132 -0.018172  0.007020 -0.034926 -0.005988 -0.051606 -0.002869 -0.000451 -0.034114  0.017354 -0.056740  0.071157 -0.042537  0.072686 -0.036848 -0.073069 -0.012943 -0.008572  0.030938 -0.070706 -0.077870  0.019930 -0.037231  0.083560  0.032593 -0.075581  0.009650 -0.092142  0.001374  0.032725 -0.082506  0.000686 -0.005012 -0.084039  0.006215 -0.036115 -0.020796 -0.024117 -0.025680  0.003700 -0.014436  0.019060  0.079132 -0.030440 -0.085844   \n",
       "misconception -0.008740 -0.051329 -0.006416 -0.016790 -0.031819 -0.076222  0.003560  0.052693 -0.010833 -0.035523 -0.018290  0.001195  0.006652 -0.021313 -0.033020 -0.007376  0.000875  0.049326  0.046574  0.003671  0.001937  0.044386  0.034593 -0.018633 -0.075689  0.000933  0.003517 -0.019119 -0.051836 -0.005512  0.005596 -0.046395  0.030977  0.023485  0.000793 -0.011765 -0.036940 -0.051507  0.019107 -0.063943 -0.034773 -0.016259  0.009174  0.027113 -0.006947  0.011418 -0.020839  0.065896  0.068960 -0.029245  0.026871  0.034483 -0.006166 -0.028449  0.072257  0.020487  0.026740 -0.017983 -0.000776 -0.038662 -0.007015 -0.042567 -0.032039 -0.063070 -0.030162 -0.062195 -0.131279 -0.033668 -0.024827 -0.037875  0.058952  0.022380  0.035568 -0.092405  0.001606  0.063155  0.012756 -0.003811  0.001569 -0.033032 -0.041889  0.039668  0.045286  0.106081 -0.015751 -0.013508 -0.044805 -0.021429 -0.002300 -0.031949 -0.013126 -0.017996  0.041479  0.077955 -0.023805 -0.083191  0.030816 -0.032733  0.004808  0.055090  0.011124  0.014574  0.006653 -0.006592  0.002484  0.069127 -0.033184 -0.008392 -0.015157  0.015916  0.028394  0.006368  0.056193 -0.097046 -0.022908  0.011579 -0.020131 -0.040010 -0.025259  0.035973 -0.006782 -0.015579  0.091322  0.015210 -0.045894 -0.065587 -0.087085 -0.010735 -0.057546 -0.017063  0.015025 -0.014781  0.008270  0.009398 -0.018601 -0.061463  0.009181 -0.007283  0.004293  0.027039 -0.017207 -0.086353 -0.020036  0.006695 -0.046034  0.045504  0.068095  0.024106  0.051508  0.039933  0.046587  0.004969 -0.039879  0.067279  0.007180 -0.029120  0.019269  0.034164 -0.070500  0.027361  0.028586  0.011200  0.023337 -0.007822 -0.014142 -0.049168  0.046043  0.036182  0.022774  0.010751 -0.078432 -0.011012 -0.052455 -0.004160  0.029537 -0.014945  0.049508  0.004427 -0.060298  0.002850  0.000894 -0.023529  0.036729 -0.002574 -0.055348 -0.035069 -0.043204 -0.018256 -0.040281 -0.033552 -0.038119  0.026945 -0.020373 -0.005260  0.060107  0.089125 -0.017129 -0.038864   \n",
       "crab           0.013815  0.072042  0.058833  0.021875 -0.024757 -0.017563  0.041158 -0.025145 -0.056022  0.014847 -0.030094  0.046056  0.035412 -0.029308 -0.017221  0.064846 -0.011287 -0.026741 -0.012687 -0.006003  0.009952  0.074889  0.055424 -0.060245 -0.033850 -0.033693 -0.007973 -0.058778 -0.001068  0.023246 -0.015459  0.016578  0.015308  0.032295 -0.045283 -0.001160 -0.003518  0.043212 -0.024304  0.029964 -0.048798 -0.086437  0.012914  0.034543  0.096366  0.015545 -0.126718  0.012450  0.037475 -0.029972  0.000681 -0.077662 -0.017088  0.044476  0.041312  0.049345  0.023336 -0.021704 -0.030215 -0.062838 -0.000023 -0.018700 -0.002224  0.018107 -0.004216  0.005748  0.009809 -0.032132  0.025655 -0.075112  0.002630  0.015726  0.106380 -0.016162  0.055200 -0.086515  0.005186 -0.037280  0.033714 -0.125661  0.021992  0.016013  0.054730 -0.010177 -0.017586  0.041512  0.030688 -0.077013  0.019401  0.085270 -0.010275 -0.025742  0.044508  0.001892 -0.021965  0.016240 -0.013455  0.071074 -0.098254 -0.043752  0.044720 -0.007102 -0.010289 -0.020186 -0.103446 -0.020217 -0.042713  0.004344  0.035194 -0.001463 -0.053345  0.005046  0.040522  0.015695  0.013973  0.032514 -0.071757  0.045940  0.006422 -0.012188 -0.040765  0.011898 -0.033316 -0.026340  0.048215 -0.002855 -0.008148  0.048121  0.031651  0.003180  0.008454  0.006841  0.078824 -0.036678 -0.031374 -0.036645 -0.055471  0.028440  0.080996  0.027412  0.003869 -0.065069  0.022016 -0.071869 -0.067763  0.060468  0.073570 -0.037831  0.012421  0.056417 -0.022359  0.069121  0.025398  0.042478  0.075842  0.004501 -0.032782  0.009961 -0.063708 -0.024685  0.068454 -0.049711 -0.033469 -0.009454 -0.054613 -0.033370  0.038838  0.028941 -0.006498  0.024553 -0.046694  0.029212  0.044602  0.013487  0.038077 -0.054533 -0.038407 -0.005741 -0.008698 -0.041561  0.088558 -0.016361  0.046504 -0.003924 -0.068363  0.071991 -0.040021 -0.022836 -0.007439 -0.017961 -0.041254 -0.020316  0.049874 -0.021310 -0.000732 -0.007608 -0.047637 -0.052515   \n",
       "...                 ...       ...       ...       ...       ...       ...       ...       ...       ...       ...       ...       ...       ...       ...       ...       ...       ...       ...       ...       ...       ...       ...       ...       ...       ...       ...       ...       ...       ...       ...       ...       ...       ...       ...       ...       ...       ...       ...       ...       ...       ...       ...       ...       ...       ...       ...       ...       ...       ...       ...       ...       ...       ...       ...       ...       ...       ...       ...       ...       ...       ...       ...       ...       ...       ...       ...       ...       ...       ...       ...       ...       ...       ...       ...       ...       ...       ...       ...       ...       ...       ...       ...       ...       ...       ...       ...       ...       ...       ...       ...       ...       ...       ...       ...       ...       ...       ...       ...       ...       ...       ...       ...       ...       ...       ...       ...       ...       ...       ...       ...       ...       ...       ...       ...       ...       ...       ...       ...       ...       ...       ...       ...       ...       ...       ...       ...       ...       ...       ...       ...       ...       ...       ...       ...       ...       ...       ...       ...       ...       ...       ...       ...       ...       ...       ...       ...       ...       ...       ...       ...       ...       ...       ...       ...       ...       ...       ...       ...       ...       ...       ...       ...       ...       ...       ...       ...       ...       ...       ...       ...       ...       ...       ...       ...       ...       ...       ...       ...       ...       ...       ...       ...       ...       ...       ...       ...       ...       ...       ...       ...       ...       ...       ...       ...       ...       ...       ...       ...   \n",
       "vaccinate      0.028424 -0.005482  0.061910  0.091805 -0.068063  0.009134  0.035771 -0.064052  0.006561 -0.024170 -0.034530  0.059653  0.047069  0.018656 -0.036893 -0.066671 -0.039995 -0.000380  0.011772 -0.017885  0.080907  0.031029  0.003605 -0.062065 -0.046987 -0.028364  0.010300  0.000127  0.025504  0.051348 -0.047049 -0.001527  0.019959  0.013173  0.006596 -0.008316 -0.010457 -0.002639 -0.065755 -0.072048 -0.041362  0.035377 -0.075213 -0.037589  0.065471 -0.055836 -0.001267  0.015653  0.041979 -0.031527  0.053954 -0.027371  0.011619  0.078807  0.036055 -0.033281 -0.003390 -0.002787  0.005951 -0.000821 -0.000247  0.042917 -0.006873  0.049062  0.022954 -0.050548  0.043388 -0.021066  0.022776 -0.019568  0.063692  0.078239  0.035660 -0.029464  0.005778 -0.014027 -0.030484 -0.003974 -0.046846  0.041040 -0.074882 -0.004047 -0.006577  0.019425  0.031647 -0.036416  0.006817 -0.010599  0.075410  0.059814 -0.015807 -0.001352 -0.009438  0.020845 -0.039685 -0.044549 -0.009736  0.030648 -0.018279  0.050957  0.039124 -0.052485 -0.006788  0.010782  0.069373  0.037213 -0.006811 -0.128641  0.014097  0.034283 -0.056466 -0.038790  0.014369 -0.034366 -0.051173 -0.012156 -0.010852  0.010771 -0.096502  0.031297 -0.051673 -0.028396 -0.073332 -0.016398 -0.016643  0.033784  0.071417  0.030608  0.023413  0.003877 -0.067627 -0.027131  0.085231  0.051108 -0.018985 -0.018699 -0.044826  0.041140 -0.041869  0.025507  0.002822 -0.051402 -0.013762 -0.062223 -0.001854  0.064181  0.047847 -0.026338  0.019448  0.007904 -0.023592  0.035886  0.008865  0.038187 -0.084403 -0.023449  0.019100  0.032994 -0.066639  0.064726  0.007784 -0.012142 -0.011534 -0.087736  0.009502 -0.039093  0.048174 -0.039730  0.023950 -0.023852  0.010673  0.026071 -0.000165 -0.037096 -0.055465 -0.050319  0.034442 -0.045194 -0.019633  0.009071  0.013372 -0.021543 -0.014556  0.053793 -0.047690 -0.009929 -0.066967 -0.015392 -0.085186  0.002017 -0.029271 -0.063623 -0.025581 -0.012242  0.001018 -0.009533  0.040273 -0.032627   \n",
       "boastful      -0.051110 -0.024396  0.023795 -0.050308 -0.036013  0.042469  0.015321  0.072105  0.053339  0.080634 -0.006786  0.020489  0.009583 -0.044102 -0.072105 -0.044240 -0.001943 -0.011250  0.018678 -0.030220 -0.004406 -0.016332 -0.009263 -0.030748  0.002724 -0.040646 -0.019575 -0.016290  0.009505  0.048056  0.057039  0.041795 -0.018004  0.048362 -0.012203  0.007665 -0.027654  0.036928  0.005887 -0.011095 -0.048285 -0.007175 -0.004458  0.007380  0.013210 -0.036924  0.007019  0.009881  0.063554 -0.041818  0.054839  0.069288 -0.036413 -0.050277  0.020361 -0.029675  0.083413 -0.022659 -0.051213  0.033022 -0.030239 -0.049699 -0.004956 -0.008355 -0.069343 -0.003307 -0.044186 -0.052561  0.033546 -0.036092 -0.008710  0.037690 -0.059503  0.001261  0.048422 -0.062658  0.014647 -0.061985 -0.031157  0.004058 -0.084421  0.022734  0.027187  0.001669 -0.041571 -0.019596 -0.032491 -0.052717  0.026522  0.005641 -0.008179 -0.056359  0.038944  0.102061 -0.062669 -0.015704 -0.010495  0.008520 -0.013783  0.011921 -0.029634 -0.032664 -0.050840  0.050830 -0.069388  0.044583 -0.049299  0.042568  0.011506 -0.044644  0.092991 -0.020952  0.053148  0.013169 -0.058297 -0.015548  0.018060 -0.037144  0.056572 -0.040723 -0.015881 -0.029100 -0.009068  0.014715 -0.091143 -0.059306 -0.058142 -0.019943  0.007810  0.010309  0.007920 -0.034837  0.008769  0.047245 -0.008109 -0.057278  0.007334  0.015468  0.082429  0.038925 -0.027951 -0.002676 -0.011655 -0.029816 -0.037963  0.046957  0.084088 -0.016923  0.055365  0.011903  0.047906  0.049921 -0.022917  0.086332 -0.028154 -0.022331 -0.019451  0.081395  0.012887  0.025114 -0.048578 -0.071913 -0.001807 -0.019900  0.049445 -0.052497  0.050448 -0.000926  0.001319 -0.075955 -0.004226  0.039309 -0.025777 -0.029057  0.051345 -0.080120  0.014854  0.001715 -0.044022  0.014153 -0.016756 -0.015527  0.080063 -0.007534  0.005873  0.037034 -0.092264 -0.014186  0.012880  0.025195 -0.027933  0.019777 -0.077407  0.013706  0.000021  0.067735  0.046901  0.023685   \n",
       "phrase        -0.032291 -0.037546  0.027556 -0.001748  0.030524  0.006126 -0.000658  0.024837 -0.024779 -0.021142 -0.021418  0.016284  0.075576 -0.089324  0.026687  0.044783 -0.060943 -0.035026  0.042232 -0.005213  0.019285 -0.013516 -0.018106 -0.045093 -0.000340 -0.014059 -0.022098  0.001560 -0.024134  0.040805 -0.011890  0.082071  0.079723  0.073002 -0.061001  0.043345 -0.057310  0.031074  0.021179 -0.029621 -0.013334  0.043421  0.003129 -0.001024 -0.003668  0.008665  0.009455  0.025489  0.031866  0.049585  0.008313 -0.036805  0.011974  0.002010  0.042235 -0.038181  0.038660  0.070997  0.033646  0.114800 -0.050354  0.013304 -0.067767  0.013132 -0.045834 -0.073161 -0.160033 -0.026870 -0.003374 -0.030343  0.037477  0.053054  0.058240  0.028669 -0.018565 -0.003153  0.027676  0.070613  0.002926  0.051479 -0.026535  0.055758  0.025351  0.066969 -0.029686 -0.005180 -0.016695 -0.013679 -0.021277 -0.011967  0.033314  0.005718  0.037079  0.027994 -0.046271 -0.085550 -0.011605  0.044689  0.054081 -0.016777 -0.010656 -0.049546 -0.015123  0.008269 -0.024772  0.005441 -0.030478 -0.034255 -0.050045  0.017429 -0.049187  0.053201  0.001476  0.003143 -0.046732  0.006148 -0.018519  0.040934 -0.028465 -0.059986 -0.045956 -0.019657 -0.023675  0.023598  0.002842 -0.012999 -0.002222 -0.089879  0.005517  0.016177  0.001155 -0.027170 -0.005817 -0.032664 -0.124274  0.011576 -0.025072  0.001185 -0.002416 -0.080268 -0.072213 -0.042050 -0.019787 -0.033568 -0.051008  0.056745 -0.001991 -0.069274 -0.032142 -0.012820  0.041061  0.117455  0.019461  0.175871 -0.028394  0.050680 -0.095411  0.081329 -0.048965  0.016566 -0.031290  0.008215  0.064108 -0.012911  0.028826 -0.015281 -0.077419  0.029784 -0.008172  0.077687  0.028355  0.009159 -0.018345  0.022619 -0.013331 -0.009165  0.027230 -0.015977  0.028337  0.014322  0.009745 -0.068717 -0.008450  0.043847  0.023996 -0.018301 -0.076823  0.015685  0.011264 -0.041260  0.027267  0.007250  0.017576  0.000723  0.016673  0.067034  0.020061  0.009841   \n",
       "ski           -0.013872  0.016587  0.053483 -0.001593 -0.056213  0.053225  0.024746 -0.038070 -0.001304 -0.064059 -0.004609 -0.019647  0.007063 -0.066592 -0.017022 -0.028749 -0.028093 -0.050822  0.023928 -0.057207  0.047605  0.038898 -0.001860 -0.034913  0.025796 -0.021582  0.023413 -0.028462  0.043870 -0.015521  0.028640  0.007721  0.022624  0.029843  0.054276 -0.042778 -0.020132  0.087550 -0.025783 -0.058086  0.008229 -0.002663  0.116066  0.005409 -0.019686  0.042904  0.001521  0.038546  0.032056 -0.016908  0.003146  0.041120 -0.007723 -0.065293  0.099305 -0.015566  0.055894  0.042118  0.000059 -0.069229 -0.055996  0.067214 -0.049982 -0.049761 -0.052199  0.019055  0.030435 -0.076975  0.018795 -0.031994  0.046610  0.061748  0.047206 -0.040223 -0.009654 -0.035785  0.011235  0.039858 -0.045656  0.015985  0.036816 -0.049957 -0.010134  0.047293 -0.006938 -0.110578 -0.004824  0.032882  0.055943  0.039649  0.003459 -0.029714 -0.029203  0.062296 -0.037753 -0.019547  0.046419  0.008823 -0.038662 -0.012225  0.045851 -0.035496 -0.036195  0.034603 -0.053700 -0.026778 -0.032472  0.011841  0.046940 -0.043306 -0.005260 -0.001871 -0.014692  0.016023 -0.056307  0.067241 -0.089683 -0.066270 -0.077262 -0.013772 -0.068216 -0.051956 -0.040365 -0.069705  0.032662  0.032440  0.021780 -0.019936 -0.059833  0.082245 -0.056411 -0.034405  0.003386  0.014954 -0.055937  0.013666 -0.023110  0.018982 -0.014375 -0.017691 -0.004931  0.038707 -0.081328 -0.091864 -0.015918  0.023663 -0.003527 -0.028469  0.006176 -0.008459 -0.041107  0.024031  0.039680  0.020315 -0.041719  0.002327 -0.038460 -0.021237 -0.027125  0.067381  0.028438 -0.075094  0.039279 -0.067943  0.016289 -0.053202 -0.019049  0.000292 -0.043234  0.020842 -0.011141  0.015587  0.054630  0.003525 -0.013895 -0.086119  0.037069  0.029416  0.015883 -0.016231  0.076575 -0.003593  0.039219 -0.048926  0.008964 -0.066959 -0.060414 -0.052575  0.064279 -0.030092 -0.024110  0.000552 -0.006076  0.033648  0.028541  0.013798 -0.055113 -0.035938   \n",
       "commanding    -0.021643 -0.001753  0.059346  0.009933  0.009539 -0.128928  0.032531  0.017477  0.013784  0.025225  0.020248  0.081404 -0.026698 -0.047578 -0.023229  0.027368  0.003698 -0.001569 -0.001780  0.018005  0.057034  0.032284  0.042459 -0.039614 -0.006877 -0.080075  0.009791  0.086474 -0.011625  0.048725  0.033478  0.043670 -0.027102  0.124842 -0.000782 -0.089207  0.079469  0.029341 -0.055349  0.019166  0.003231  0.044943  0.038837 -0.033651  0.025904  0.037340  0.015126  0.018474  0.072318 -0.004525  0.034089  0.070720 -0.005002 -0.018145  0.018852 -0.056592  0.078702 -0.040465  0.031717 -0.021126 -0.019583 -0.009596 -0.042663 -0.052976 -0.089164 -0.018692 -0.007255  0.011200 -0.021510 -0.027698  0.037555  0.044546 -0.052333 -0.013833 -0.015568 -0.019458  0.048057 -0.056151 -0.008476  0.003763  0.014685  0.021772  0.026899 -0.019591 -0.024272  0.051466 -0.018622  0.065751  0.003105  0.002518 -0.076166 -0.026585 -0.029944 -0.015349 -0.047645 -0.007413  0.087120 -0.035157 -0.071460  0.007088 -0.073490 -0.055170 -0.081992  0.001517 -0.016964  0.030018  0.004160 -0.022467 -0.006657 -0.035748  0.040980 -0.053014  0.044824 -0.008718  0.003849 -0.025019  0.011426 -0.066660 -0.030656 -0.052554 -0.057266  0.024076 -0.052575  0.026837 -0.020632 -0.023704  0.010053 -0.010024  0.051221 -0.026290 -0.033514 -0.086520 -0.065273  0.034437 -0.032389 -0.058795 -0.006822  0.023129 -0.013649 -0.019729 -0.059690 -0.010782 -0.017630 -0.035718  0.034197  0.003572  0.014439 -0.027348  0.027440  0.063121  0.030407  0.044482  0.015033  0.070332 -0.011271  0.007824  0.004222  0.007239 -0.077934  0.019962  0.035756 -0.044720 -0.004919 -0.065912 -0.000597 -0.040960  0.019029  0.003117  0.012346  0.004021  0.013941  0.025210 -0.003407 -0.070864  0.016920  0.000896  0.039064 -0.011389 -0.036745 -0.067944  0.020702 -0.025541 -0.003371  0.008871  0.048331 -0.015311 -0.113575 -0.009840  0.013920 -0.006402 -0.028837 -0.041043 -0.006534 -0.020197 -0.015948  0.081412  0.016552 -0.014702   \n",
       "\n",
       "                    198       199       200       201       202       203       204       205       206       207       208       209       210       211       212       213       214       215       216       217       218       219       220       221       222       223       224       225       226       227       228       229       230       231       232       233       234       235       236       237       238       239       240       241       242       243       244       245       246       247       248       249       250       251       252       253       254       255       256       257       258       259       260       261       262       263       264       265       266       267       268       269       270       271       272       273       274       275       276       277       278       279       280       281       282       283       284       285       286       287       288       289       290       291       292       293       294       295       296       297       298       299       300       301       302       303       304       305       306       307       308       309       310       311       312       313       314       315       316       317       318       319       320       321       322       323       324       325       326       327       328       329       330       331       332       333       334       335       336       337       338       339       340       341       342       343       344       345       346       347       348       349       350       351       352       353       354       355       356       357       358       359       360       361       362       363       364       365       366       367       368       369       370       371       372       373       374       375       376       377       378       379       380       381       382       383       384       385       386       387       388       389       390       391       392       393       394       395  \\\n",
       "argument      -0.027665 -0.060651  0.015127 -0.048536 -0.059498 -0.019579 -0.054471  0.070263 -0.018107  0.008655  0.071418 -0.010173 -0.090049 -0.071707  0.015849  0.041161  0.040833  0.061819 -0.001309 -0.093380 -0.101444  0.005466  0.002472 -0.004649  0.001886 -0.050198 -0.003143 -0.050877  0.007076  0.222227  0.010635 -0.028757 -0.069166  0.061245  0.000606  0.037623 -0.045572  0.104749 -0.007717 -0.009740 -0.021305  0.036857  0.001859  0.085540  0.029976  0.050665 -0.041031 -0.002015  0.062726  0.030470 -0.013613  0.014459 -0.005388 -0.055521  0.036557 -0.046752 -0.068244 -0.043407 -0.015085 -0.045892  0.046858 -0.024430  0.001555 -0.039038  0.018319  0.008762  0.022932 -0.041819 -0.087458 -0.027710  0.143462  0.068875 -0.081839 -0.029243  0.050543 -0.012008 -0.032991  0.057572  0.061028  0.009955 -0.101006  0.012150 -0.040801  0.005670 -0.033847  0.063222  0.012602  0.031961 -0.049574  0.024087  0.007286  0.021664 -0.055415  0.034154  0.037429 -0.006535 -0.043686  0.041234 -0.045761 -0.047539 -0.047382 -0.024680 -0.081747 -0.023657  0.033543 -0.020524 -0.037243  0.029723 -0.047581  0.087062 -0.032729  0.046566  0.059990  0.032156  0.053130 -0.016996  0.085229  0.033314 -0.030072  0.009963 -0.008923  0.009869  0.005263 -0.021262 -0.004458 -0.021906  0.108421 -0.009200  0.004428  0.050933  0.090360 -0.006465 -0.002345 -0.008998  0.051154  0.015261 -0.053207  0.030310  0.062576 -0.047676  0.046907  0.051223  0.021178 -0.026583 -0.092427 -0.058884  0.017933 -0.012116 -0.016891  0.034947 -0.045107  0.056104  0.046036  0.049723  0.051819 -0.005840 -0.068100 -0.020950 -0.034312 -0.054905  0.060992  0.005778 -0.015211 -0.054036 -0.036320 -0.090872  0.066721  0.034548 -0.010227 -0.019921 -0.013126 -0.000326 -0.056792 -0.027753  0.018267 -0.017541  0.040908  0.024720 -0.065316 -0.036543  0.007539 -0.045257 -0.044589 -0.049280  0.006636 -0.019829  0.024869 -0.045506  0.030502  0.034103 -0.104293 -0.002812  0.000271 -0.022931  0.031732 -0.038318  0.008057 -0.015133   \n",
       "awful         -0.009255  0.029191  0.034608 -0.019100 -0.007132 -0.040019 -0.024174  0.077984  0.089024  0.007429  0.080466 -0.018046 -0.105663 -0.044170 -0.012153  0.006893 -0.116216 -0.004283 -0.008455 -0.055686 -0.012810  0.028847 -0.038166  0.061139  0.020554 -0.083702 -0.046797 -0.075130  0.048761  0.168594  0.060373 -0.006223  0.012170  0.046612  0.044808  0.022969 -0.001923  0.033051  0.016386 -0.014145  0.010384  0.008070 -0.042769  0.125319 -0.056820 -0.035894 -0.006922 -0.012677  0.007999  0.024649  0.021535 -0.033725 -0.039064 -0.021279 -0.002241  0.011400  0.006222  0.022359 -0.030097 -0.003247  0.016503 -0.030274 -0.075307 -0.016346  0.033731 -0.024291 -0.006081  0.042765  0.006900  0.000358 -0.056346 -0.006351  0.021225  0.034121  0.032833 -0.001849 -0.021615  0.047153  0.023787  0.041327  0.023487  0.005254 -0.084705  0.011369 -0.000985  0.014425  0.028045 -0.118459 -0.099329 -0.020474  0.035424  0.016551 -0.097981 -0.022505 -0.026181 -0.082360 -0.039814  0.050217 -0.073369 -0.022035  0.001041 -0.074080  0.042409  0.021382  0.059136 -0.032165  0.010374 -0.003030  0.021610  0.030048  0.048521  0.001262 -0.030480  0.043803  0.094384  0.030824  0.011029 -0.030530  0.030132 -0.008222 -0.065402  0.083882 -0.010545  0.008701  0.024967 -0.012802  0.006015  0.000518  0.044395  0.013727  0.041821 -0.046804  0.015796  0.022438  0.056414 -0.100255 -0.052801  0.072645 -0.000807 -0.015112  0.048051  0.045322 -0.000488  0.003804 -0.058851 -0.002625 -0.009554 -0.105220  0.005676 -0.033893 -0.022610 -0.027222  0.007830  0.004210  0.009940  0.044020  0.018688  0.054338 -0.068623  0.004287  0.036706 -0.036743  0.018806 -0.058223 -0.007525  0.031833  0.074131 -0.026629 -0.066046  0.022882  0.018383  0.039229  0.023241  0.041160  0.050562  0.051417  0.041208 -0.029097 -0.025675  0.064232  0.058562  0.058524 -0.060189 -0.008871  0.020446  0.011055 -0.031117 -0.062404  0.033092 -0.033073 -0.012284  0.012192  0.059441 -0.054400  0.055429 -0.075863  0.025259  0.023958   \n",
       "decorator      0.018209 -0.001825  0.044982  0.017309 -0.013224 -0.014878 -0.013913  0.043777  0.004967  0.023689  0.072543 -0.065101 -0.030108  0.011252 -0.007214  0.028208  0.030593 -0.079983 -0.025561 -0.103985 -0.040325 -0.055648  0.050174 -0.072414  0.035487 -0.063486  0.010625 -0.060106  0.014036  0.201772 -0.016781 -0.024157 -0.000831  0.004692  0.005389  0.011147  0.004645  0.022285 -0.004752 -0.063929 -0.014222 -0.019905  0.002861  0.108553 -0.039188 -0.012094 -0.017449  0.038870 -0.013583  0.074206 -0.014928 -0.053516 -0.060854  0.011825  0.055224 -0.067650 -0.026673  0.043983 -0.006435 -0.017319  0.053965  0.037241  0.011772  0.039992 -0.006880  0.044590 -0.003781  0.008772 -0.001212 -0.029882  0.099964 -0.003832 -0.015492  0.000826 -0.086720  0.001852  0.061735  0.098334 -0.027018 -0.002412 -0.033387 -0.041532 -0.035421  0.014119 -0.024187  0.021152  0.029263  0.021902 -0.066574  0.014818  0.023591  0.009331 -0.023462  0.070539 -0.006278 -0.031506 -0.073327  0.036041 -0.034746  0.023439  0.038204  0.044410 -0.032638  0.005619 -0.053308  0.015579 -0.002905 -0.002087 -0.020283  0.040506 -0.066762 -0.023335 -0.008828 -0.028174  0.057874 -0.007878  0.063340 -0.033202 -0.019415  0.033260  0.013230 -0.022448 -0.019014 -0.027914  0.074163  0.019025  0.063626  0.062294  0.003060  0.009223 -0.021689 -0.060590 -0.052508 -0.038133  0.055600  0.046227 -0.001290  0.007336  0.016289 -0.075799  0.039830  0.023385  0.017995 -0.046318  0.030597  0.025602  0.001760  0.079311 -0.081243 -0.036224  0.025410  0.002009  0.038523  0.055674 -0.079037 -0.022275 -0.015298  0.020786 -0.005905  0.099990  0.034154  0.001021  0.028852 -0.010889 -0.049443 -0.020298 -0.005632  0.008299 -0.026508 -0.002233  0.010184  0.015031  0.049737 -0.007136 -0.056401  0.009055  0.024226 -0.013518 -0.110892  0.076524  0.006127 -0.049789 -0.077080  0.008784  0.068113  0.003111  0.032945 -0.038457  0.053658  0.018622  0.021379 -0.066593  0.055001  0.026130 -0.005465 -0.007532  0.075804 -0.018204   \n",
       "misconception -0.052481 -0.028283  0.017528  0.011551 -0.041482 -0.019878  0.006280  0.089455 -0.027106  0.115089  0.062107  0.036604 -0.031517 -0.054370 -0.017266 -0.024358 -0.013785  0.030015 -0.059942 -0.136513 -0.056101  0.063750  0.020367 -0.039145  0.007323 -0.086272  0.026548 -0.066745 -0.043001  0.213594  0.029921 -0.060267  0.029541 -0.069436 -0.004154  0.046355  0.048592  0.076948  0.029224 -0.025937 -0.009628  0.004597 -0.095912  0.116445  0.048329 -0.010976 -0.047604  0.020225 -0.015059 -0.009680 -0.083287 -0.015850 -0.015107 -0.045314  0.093198  0.031335 -0.042622  0.012149 -0.003424  0.004163  0.032577  0.034990  0.005032  0.003149  0.069475  0.026153 -0.043166 -0.015659  0.001541 -0.016054  0.083873 -0.001978 -0.007360 -0.019871 -0.009227  0.018492  0.041035  0.056705  0.051671 -0.027853 -0.041526 -0.018484  0.007721  0.041584 -0.015032  0.049796  0.018510  0.055636 -0.043135  0.041336  0.065599 -0.016202 -0.077946 -0.000349 -0.014886  0.013915 -0.052384  0.002494  0.000352  0.099715  0.024367  0.059694 -0.005762  0.029345 -0.022825  0.016579 -0.017972 -0.015370  0.011466  0.034426  0.009995  0.026683 -0.009343 -0.014509  0.048590 -0.028369  0.093055 -0.015003 -0.054098 -0.008378 -0.029254  0.037736  0.012997 -0.006899  0.063468  0.024596 -0.000979  0.047114 -0.051188  0.003591  0.079313 -0.057840  0.030539  0.026524  0.055417  0.003256 -0.031854  0.081011 -0.060259 -0.084436  0.046893  0.027551  0.013715  0.002328 -0.054635 -0.041842 -0.014738 -0.089174  0.013648 -0.086046 -0.036189 -0.011651  0.032080 -0.019531  0.032920  0.014804  0.004439 -0.001402 -0.039478 -0.003011  0.017562 -0.005941  0.061437  0.006496 -0.004199  0.049659  0.072598  0.009864  0.050740  0.019394 -0.053215  0.030262  0.107164  0.054089 -0.020286 -0.092332  0.037441  0.034548 -0.067169  0.030031  0.030816  0.073844 -0.032089 -0.010569 -0.024887 -0.011506 -0.011523  0.025564  0.040064  0.046811 -0.012338  0.041546 -0.032990  0.029368  0.061195  0.027804  0.024254 -0.043534   \n",
       "crab           0.011943  0.005381  0.104699 -0.057556  0.022107  0.043328 -0.041582  0.043138  0.016731 -0.030882 -0.054431 -0.005553  0.015920 -0.007345 -0.033191  0.025618 -0.067753 -0.030254  0.025658 -0.061674  0.059944 -0.029348  0.044588 -0.000260  0.051055 -0.038332 -0.052659 -0.014141 -0.070399  0.183042  0.007304 -0.030399  0.049387  0.051071 -0.030658  0.028370  0.020894  0.031813  0.020841 -0.010759 -0.041482  0.117905  0.030790  0.063581 -0.053461 -0.059498 -0.018792  0.023908  0.024587  0.075913 -0.003771 -0.028980 -0.042295  0.031890 -0.000690 -0.043723 -0.029190 -0.052554 -0.063053  0.054611  0.010467  0.015304  0.057608 -0.027970 -0.017528  0.045178  0.014244  0.012002 -0.040860 -0.049528  0.143483 -0.035296 -0.012473  0.050472 -0.034363 -0.029835 -0.054057  0.033151 -0.022721 -0.013383 -0.009320 -0.015628  0.057938 -0.055553 -0.057429  0.072293  0.009470 -0.020981 -0.034964  0.014203 -0.025801 -0.058271 -0.014339  0.009323  0.025900  0.035306 -0.077668  0.056561 -0.046046  0.000527  0.031723 -0.060629 -0.048830 -0.002087 -0.026408  0.003823 -0.010949  0.082873  0.053716  0.119950 -0.066832  0.007270 -0.017174  0.038950 -0.006868  0.037031  0.057090  0.008636 -0.062032  0.090315  0.001571  0.016528  0.030713  0.027391 -0.051355  0.011877  0.016769  0.024230  0.003182  0.029179  0.029243 -0.010171  0.003768  0.016986  0.027555  0.013682 -0.028953 -0.030056 -0.015043  0.042397  0.061106  0.002439 -0.048695  0.024377  0.013135 -0.038924  0.010336  0.084216 -0.018449  0.007573  0.024381 -0.006296  0.031226  0.070809 -0.036391 -0.048621 -0.027965  0.028488  0.016040  0.030347 -0.003450 -0.032385  0.003102 -0.069063 -0.008170  0.039151 -0.024040  0.053288  0.008722  0.039709  0.014760 -0.027447 -0.003469  0.091960  0.125851  0.025898  0.025300  0.024520 -0.020014  0.010261  0.007224  0.034960 -0.065069 -0.028385  0.070375 -0.061758  0.019844 -0.058402 -0.047092 -0.004439 -0.069310 -0.060501  0.012474  0.008721  0.053419 -0.039751  0.013486  0.004182   \n",
       "...                 ...       ...       ...       ...       ...       ...       ...       ...       ...       ...       ...       ...       ...       ...       ...       ...       ...       ...       ...       ...       ...       ...       ...       ...       ...       ...       ...       ...       ...       ...       ...       ...       ...       ...       ...       ...       ...       ...       ...       ...       ...       ...       ...       ...       ...       ...       ...       ...       ...       ...       ...       ...       ...       ...       ...       ...       ...       ...       ...       ...       ...       ...       ...       ...       ...       ...       ...       ...       ...       ...       ...       ...       ...       ...       ...       ...       ...       ...       ...       ...       ...       ...       ...       ...       ...       ...       ...       ...       ...       ...       ...       ...       ...       ...       ...       ...       ...       ...       ...       ...       ...       ...       ...       ...       ...       ...       ...       ...       ...       ...       ...       ...       ...       ...       ...       ...       ...       ...       ...       ...       ...       ...       ...       ...       ...       ...       ...       ...       ...       ...       ...       ...       ...       ...       ...       ...       ...       ...       ...       ...       ...       ...       ...       ...       ...       ...       ...       ...       ...       ...       ...       ...       ...       ...       ...       ...       ...       ...       ...       ...       ...       ...       ...       ...       ...       ...       ...       ...       ...       ...       ...       ...       ...       ...       ...       ...       ...       ...       ...       ...       ...       ...       ...       ...       ...       ...       ...       ...       ...       ...       ...       ...       ...       ...       ...       ...       ...       ...   \n",
       "vaccinate     -0.049999  0.061210  0.072773 -0.017409  0.043491 -0.018484 -0.098696  0.000723  0.027636  0.062177  0.100626 -0.001230  0.031419  0.012575 -0.028638 -0.099404  0.002913  0.077781 -0.017047 -0.080107 -0.097192 -0.027747  0.029702  0.047839  0.020828  0.052794 -0.000693 -0.014396  0.007060  0.187270  0.026882 -0.029867  0.000131  0.017789 -0.056027  0.077322 -0.039773  0.064425  0.032336 -0.036017  0.076109  0.078220 -0.004644  0.071799 -0.021465  0.042604 -0.042295  0.043638  0.006724  0.040297  0.009434 -0.001870 -0.023685 -0.033805  0.032717  0.081422  0.016091  0.029146  0.058803 -0.092813 -0.003626 -0.046018 -0.109339  0.045569  0.072516  0.016498  0.005375 -0.048022 -0.066067 -0.062108  0.110235  0.042159  0.022910 -0.001587  0.005902  0.061990  0.019318  0.069579  0.049618 -0.094962 -0.033604 -0.044081  0.072608  0.011875 -0.005130  0.010775  0.086198  0.004783 -0.045734 -0.001165 -0.006903  0.039556 -0.028938  0.050570  0.026984  0.034070 -0.005554 -0.041676  0.008067 -0.043279  0.068548 -0.053353 -0.020968  0.027423 -0.061875 -0.026171 -0.048403  0.010833  0.049181 -0.046365 -0.048698  0.040659  0.029096  0.030901  0.004249  0.040661  0.098421  0.030173 -0.087229 -0.005163 -0.027073 -0.036554 -0.053937  0.021101 -0.074004  0.059203  0.064430 -0.016069  0.020813 -0.021506  0.021795  0.011346 -0.095042 -0.019825 -0.039136  0.088918 -0.098992  0.082056 -0.003956 -0.001478 -0.030471 -0.006722 -0.046307 -0.014848  0.024603 -0.011482  0.012439 -0.065050  0.033519 -0.004185 -0.082728 -0.022751 -0.037533  0.011552  0.087761 -0.015121  0.000084  0.010690 -0.058857 -0.012716  0.077341 -0.066953 -0.012478 -0.042921 -0.032605  0.008385 -0.020614  0.053422 -0.015643  0.035323  0.033270  0.064126 -0.058299  0.005445 -0.004697 -0.000094 -0.023874  0.049002  0.006646 -0.016067  0.043915  0.013055 -0.008762 -0.008686 -0.008644 -0.055201 -0.053289  0.033682  0.037819 -0.049233  0.003314  0.014520  0.062490 -0.046737  0.036311  0.045560 -0.008484 -0.000917   \n",
       "boastful       0.011180  0.001189  0.049485 -0.024911  0.045043 -0.019323  0.008287  0.059856  0.016736  0.089371  0.070406  0.043941 -0.049324 -0.017462 -0.005426 -0.012474 -0.052949 -0.018070 -0.069957 -0.096576 -0.029859  0.029972  0.039913 -0.037258  0.023936 -0.047566  0.052914 -0.058365  0.033227  0.194536 -0.000754 -0.039614 -0.050341 -0.048408 -0.025505  0.030747  0.004464 -0.043184 -0.042398 -0.045265 -0.057277 -0.035369 -0.049776  0.118891 -0.013536 -0.033841 -0.051081  0.067823 -0.052312 -0.003320 -0.005112 -0.036295 -0.005148 -0.054336  0.057668 -0.015315  0.027973  0.005714  0.026554 -0.029522  0.024648  0.014269  0.076719  0.048317  0.069780 -0.023885 -0.033882  0.063300  0.034753 -0.028121  0.086903 -0.004895 -0.036726 -0.006226 -0.002396  0.038949 -0.037429  0.038245 -0.032793  0.010142  0.013419 -0.004295  0.020863  0.005318  0.107003 -0.080659  0.019166 -0.006344 -0.075339 -0.016006  0.109040 -0.034634 -0.060696  0.015021  0.001204  0.077196  0.017214  0.073040  0.002626  0.069214  0.084182 -0.003112 -0.035870 -0.010623 -0.034958 -0.046201 -0.042353  0.003376  0.047275  0.091598 -0.038574  0.034940 -0.020772  0.047630 -0.070728 -0.013228  0.009983  0.055462 -0.011829  0.026638 -0.018852  0.018976 -0.071261 -0.055114 -0.015510  0.014190  0.071436  0.074714  0.054551  0.051625  0.053652 -0.000755  0.048550  0.036351  0.034828  0.069793 -0.036032  0.050542  0.059103 -0.023277  0.038137  0.022458  0.020398 -0.020471 -0.048897 -0.062188 -0.006658 -0.024584 -0.042119  0.029949 -0.031556  0.013961  0.123659  0.005216  0.029570  0.010176 -0.018344  0.053437  0.041651 -0.007257 -0.003924 -0.021138  0.014091 -0.063302  0.002885  0.009757  0.052025  0.004332  0.021721  0.014936 -0.034612  0.017472  0.078167  0.029481  0.007195 -0.046038  0.029356  0.013884 -0.034432  0.049890 -0.055426 -0.005104 -0.041903  0.036163  0.007727 -0.074569 -0.032194  0.012535  0.007426  0.041600 -0.027705 -0.067641 -0.019714 -0.038719  0.035367 -0.012464 -0.004674 -0.066688   \n",
       "phrase         0.066789  0.004518 -0.013099  0.033045 -0.018943 -0.038036 -0.025388  0.023114  0.006939  0.056635  0.048861  0.018489 -0.079867 -0.004178  0.021219 -0.027407 -0.026955 -0.046196 -0.015740 -0.102883 -0.009445  0.046316 -0.007780 -0.002280 -0.000415 -0.016831 -0.031255 -0.119046  0.036523  0.220437  0.028056 -0.052936  0.007023 -0.026899 -0.005849  0.040759 -0.017490 -0.032503 -0.052479  0.015019 -0.008651 -0.017456 -0.043198  0.124263  0.002850 -0.009832 -0.063221 -0.067660  0.026603  0.027954 -0.024656  0.030133 -0.033378  0.057579 -0.003952 -0.028255 -0.028429 -0.042290 -0.016476 -0.011721 -0.008798  0.012879  0.051688 -0.065220  0.030203 -0.057402  0.029101  0.001568 -0.047203 -0.031695  0.127186 -0.018315 -0.006690 -0.084850 -0.004090 -0.026543 -0.008540  0.111074  0.004941  0.024492 -0.038087  0.004840  0.064414 -0.037048 -0.002373 -0.016628  0.020384 -0.016905  0.020041  0.046781  0.036568  0.017206  0.043423  0.006018 -0.069732  0.015206 -0.089532  0.019574  0.018102 -0.014253  0.039520 -0.011853  0.041919  0.013867 -0.025970 -0.009378 -0.033706 -0.007861 -0.094755 -0.015332 -0.028955 -0.021220 -0.025419 -0.000249  0.033599  0.005672  0.047307  0.042933 -0.063510  0.002963  0.009207  0.022154  0.044349  0.021297  0.003317 -0.040126  0.116776 -0.022152 -0.005408  0.107770  0.055286 -0.022186 -0.065453  0.040531 -0.036721  0.028349 -0.058334  0.079651  0.041563 -0.047876  0.023672  0.062529 -0.032817 -0.012952 -0.002632  0.008558 -0.003629  0.033281  0.015877 -0.017730  0.018401 -0.013835 -0.010461  0.072611  0.102597  0.012783 -0.030518 -0.001271 -0.014921 -0.029244 -0.012678 -0.000284  0.021043  0.005935 -0.034138  0.019013  0.026799 -0.030454 -0.014222  0.009316  0.002893  0.035156 -0.021969 -0.017892 -0.045033  0.034003  0.041200 -0.011200 -0.047523  0.090632  0.008182  0.074359 -0.030094 -0.002536  0.054035 -0.070746 -0.033782 -0.051755  0.020351  0.044263  0.007501  0.002558 -0.053835 -0.014903  0.001218 -0.012547 -0.006464 -0.024044   \n",
       "ski           -0.058438  0.038530  0.085925  0.020296  0.006213 -0.005141 -0.062642 -0.011234  0.007052  0.053659  0.057046 -0.030005  0.020388  0.022911 -0.037419 -0.106842 -0.047479  0.050884 -0.028620 -0.061798 -0.007032  0.024784  0.037960  0.025840  0.028836 -0.011317 -0.086174 -0.020134 -0.032978  0.202393  0.003158 -0.025799  0.055955  0.032788  0.004367 -0.011174 -0.059602  0.048104 -0.050321  0.000620 -0.018958 -0.005440 -0.004009  0.089833 -0.031880 -0.079372 -0.020683  0.022744  0.040849 -0.033132 -0.065669 -0.045235  0.018817 -0.031434 -0.030909 -0.031969  0.012048  0.052537  0.032010 -0.038205  0.005552 -0.019587 -0.041832 -0.025522  0.018393 -0.013667  0.011569 -0.003874 -0.021186 -0.041324  0.143349 -0.028088 -0.021641  0.013739 -0.121819  0.034780 -0.071066  0.050635  0.009606  0.002485 -0.059402 -0.043550  0.024906 -0.061689  0.008869 -0.058233  0.017211 -0.004470 -0.055263 -0.021396 -0.051851 -0.053350  0.019689 -0.036087  0.002624  0.036642 -0.036842  0.077346  0.010380 -0.022904  0.069120 -0.059792  0.034238 -0.024774 -0.053017 -0.097406  0.023633 -0.024530  0.000952 -0.042852 -0.011266  0.018251 -0.006567  0.052853  0.099428  0.051638  0.087469  0.024311  0.016170 -0.020217 -0.005375 -0.044012 -0.007893  0.026649 -0.014159 -0.005886  0.034076  0.050464  0.015244  0.011641  0.003350  0.021419  0.023190  0.039329  0.043227 -0.043292  0.010944  0.085633  0.057458  0.070978  0.026883  0.020939 -0.000073  0.040886  0.051017 -0.004203  0.001547 -0.015940  0.000884  0.017177 -0.015062  0.000966 -0.010336  0.043247  0.043357 -0.067278 -0.011968 -0.026605  0.068790  0.023970  0.065471  0.029704  0.005881 -0.025308  0.001190  0.023001  0.008168  0.032901 -0.033395  0.052650  0.055671 -0.110313 -0.001026 -0.082437 -0.009072 -0.000208  0.036476  0.015602 -0.011681 -0.072246 -0.029016  0.027394 -0.027655  0.030420  0.053756 -0.034217 -0.029213 -0.045368  0.001857 -0.033028  0.050661  0.060942  0.055785 -0.046167 -0.016992  0.009614  0.030656 -0.017616   \n",
       "commanding     0.052325 -0.017257  0.025041  0.032561 -0.002362 -0.044448 -0.062082  0.024154  0.060061  0.034234  0.089554 -0.006306 -0.039365  0.043834  0.072385  0.042458  0.028361 -0.000684  0.001104 -0.120564 -0.027148  0.047761  0.034415 -0.051212  0.034130 -0.037960  0.091482 -0.027058  0.056124  0.195167 -0.011711 -0.007206  0.041300  0.064358 -0.035460  0.009123 -0.007125 -0.023654 -0.009742 -0.060939 -0.057435  0.057011 -0.002648  0.123051 -0.012610  0.001368 -0.033395  0.007712 -0.022468  0.007765  0.038097  0.017668 -0.090918 -0.028575  0.040213 -0.018307 -0.049447  0.005271  0.055747 -0.005953  0.077284  0.000709  0.023473 -0.017973 -0.015217 -0.030226 -0.028576  0.013981  0.044259  0.010497  0.087216 -0.013632 -0.035985 -0.054465 -0.009651  0.005737 -0.077592  0.091128  0.071484 -0.016924 -0.006736 -0.026583  0.029587 -0.010142 -0.057059  0.007034  0.068796  0.003673 -0.116480  0.072480  0.000154 -0.019958 -0.117175  0.005522 -0.047364  0.043816  0.010554  0.076083 -0.051996 -0.025430  0.080790 -0.060167  0.042614 -0.009178 -0.034335 -0.065378 -0.016178 -0.022129 -0.015178  0.000220  0.024351  0.038405 -0.008805  0.016085  0.039612  0.044743  0.083379  0.020162  0.030495  0.043775 -0.000040  0.022129  0.005051 -0.043426  0.035729 -0.000915  0.055961 -0.035329  0.028746 -0.028187  0.036928  0.021690 -0.027648  0.002086  0.019244  0.056314 -0.047314  0.067922 -0.127662 -0.007581  0.059841  0.000360  0.003862 -0.096651 -0.046278 -0.042496 -0.005145 -0.025536 -0.012258 -0.027964 -0.007082  0.021499  0.049162  0.067834  0.034015 -0.033849 -0.017583  0.039971  0.075746 -0.031029  0.064802 -0.036586  0.006019 -0.043170 -0.053748 -0.055148  0.041165 -0.003597 -0.058903  0.002993  0.052532 -0.023457  0.031152 -0.002807  0.016733  0.062071  0.017900  0.014274 -0.049826  0.008016  0.007972 -0.002798 -0.053569  0.029696  0.049230 -0.052330 -0.070980 -0.055893 -0.000766  0.019126 -0.068870 -0.058030  0.049417 -0.013663 -0.054767 -0.066311 -0.044828 -0.049260   \n",
       "\n",
       "                    396       397       398       399       400       401       402       403       404       405       406       407       408       409       410       411       412       413       414       415       416       417       418       419       420       421       422       423       424       425       426       427       428       429       430       431       432       433       434       435       436       437       438       439       440       441       442       443       444       445       446       447       448       449       450       451       452       453       454       455       456       457       458       459       460       461       462       463       464       465       466       467       468       469       470       471       472       473       474       475       476       477       478       479       480       481       482       483       484       485       486       487       488       489       490       491       492       493       494       495       496       497       498       499       500       501       502       503       504       505       506       507       508       509       510       511  \n",
       "argument      -0.016214  0.029840 -0.012779 -0.128566 -0.040349 -0.013225  0.013918  0.006946  0.012714  0.054271 -0.031807 -0.041463  0.011234 -0.039616  0.044495  0.003672  0.072688 -0.027824  0.018789 -0.132182  0.020682  0.007224 -0.077860 -0.036975  0.031022 -0.069942  0.075617  0.043619  0.026726 -0.018604 -0.062158  0.007207  0.003385  0.018415 -0.071181  0.042834 -0.076972  0.004844  0.003338 -0.070391 -0.008336 -0.058938  0.017085 -0.020420 -0.004337 -0.008181  0.049004  0.026595 -0.044924  0.074895  0.009845  0.060858 -0.019436  0.068924  0.010928  0.002579  0.010506  0.047682 -0.020770  0.044593  0.017637 -0.039327 -0.028829 -0.015196  0.025572  0.017846 -0.021445  0.036358  0.050771 -0.020593 -0.035496  0.011470 -0.065424  0.063719 -0.054271  0.032851  0.028792 -0.000063  0.013526  0.029719 -0.044451  0.014399  0.245867  0.073511  0.009029  0.003520  0.038824 -0.020015 -0.036546  0.011296 -0.018885 -0.030987 -0.017351 -0.037827 -0.059293 -0.007414  0.047752 -0.038985 -0.011051  0.171788 -0.002318 -0.010284  0.043155  0.037117  0.030925 -0.038471 -0.013875 -0.070968 -0.003252  0.005956  0.021636  0.015831  0.023877  0.047711 -0.063557  0.075657  \n",
       "awful          0.013989 -0.035505  0.009986 -0.009116  0.027473 -0.037504 -0.014115  0.043986  0.024755  0.047453 -0.011431  0.020090  0.024479 -0.018380 -0.026233  0.050278 -0.001687 -0.022463 -0.007484  0.020594  0.044974 -0.029187  0.001596  0.031080  0.048563  0.007506 -0.025279  0.025394  0.014345 -0.006770  0.016871  0.020040  0.088223  0.004570  0.005807 -0.028890 -0.015818  0.011849  0.054793  0.025766 -0.038911 -0.064071 -0.013066 -0.001554  0.019293  0.029381  0.102294  0.030933  0.068220  0.204491 -0.041758 -0.008564  0.061152  0.039016  0.069776 -0.041614  0.050936 -0.042269 -0.000002  0.043146 -0.048839 -0.050978  0.022706 -0.015931 -0.023049 -0.003325  0.017269  0.018861 -0.007192  0.065646  0.003827  0.026161  0.010411 -0.001105 -0.028084 -0.043691 -0.019756 -0.031428  0.018163  0.086441 -0.012338  0.027901  0.242784  0.011662 -0.045923  0.036216 -0.040422  0.006995  0.060533 -0.117955 -0.068407 -0.055641  0.000393 -0.039048  0.000108 -0.018131 -0.021651  0.064353 -0.116046  0.137493  0.017328 -0.028578  0.066309 -0.010129 -0.013693  0.036508 -0.052221 -0.052194 -0.018417 -0.035257  0.007977  0.013068 -0.020170  0.054807 -0.033252 -0.039256  \n",
       "decorator      0.104602 -0.023656  0.043339 -0.059699 -0.070714 -0.014626 -0.020609 -0.017734 -0.031290 -0.019365 -0.025496  0.021478 -0.020899 -0.089511  0.085749  0.031181  0.005333 -0.020131 -0.010428  0.005679 -0.036753  0.023591 -0.012101  0.018789 -0.083450 -0.055797  0.012806 -0.007653 -0.002296 -0.022206 -0.041705  0.002967  0.016144  0.019465 -0.004817  0.017198 -0.042148  0.067943  0.001053 -0.030570 -0.008952  0.046371 -0.038564 -0.025021 -0.098026  0.016978  0.022750  0.055851  0.005480  0.006505 -0.032474  0.024204  0.051205  0.019378 -0.030461 -0.021943  0.059405  0.050042  0.035683 -0.026165 -0.012636  0.016736  0.058192 -0.027293  0.009611  0.045623 -0.014549  0.026107  0.015931  0.016268 -0.047099  0.079324 -0.038588 -0.017416 -0.022368  0.003860  0.012133  0.079430  0.063534  0.045920  0.060749  0.077861  0.143558 -0.036330 -0.036515 -0.015118 -0.017046  0.021518 -0.034655  0.015253  0.040507  0.020399  0.011264 -0.025316 -0.010028 -0.013929  0.047816  0.020784 -0.000164  0.178284 -0.020195  0.000324 -0.020133  0.041894  0.024398 -0.003661  0.022667 -0.032577 -0.036059 -0.024736 -0.034689  0.064175 -0.005290  0.068678 -0.057283  0.007582  \n",
       "misconception -0.018658 -0.043591  0.014441 -0.102390 -0.058495 -0.042832  0.019683  0.049219 -0.039676  0.013467 -0.021884 -0.026002 -0.013556 -0.004607 -0.026727  0.042340 -0.037656  0.020119 -0.002884 -0.038545 -0.006801  0.015269  0.035728  0.060773 -0.001170 -0.058063 -0.005184  0.021659  0.073308  0.050862 -0.003366  0.001381  0.026352  0.021566 -0.048172 -0.006590  0.015406  0.028573 -0.011892 -0.065901  0.000675 -0.001671 -0.027445  0.025454 -0.001399  0.002638  0.004629 -0.005780 -0.028694  0.167404  0.030653  0.017681  0.040271  0.034876 -0.056195 -0.011404  0.002961 -0.027944  0.035596 -0.039411 -0.062866  0.008502  0.017942  0.035756  0.029723 -0.020595 -0.017910  0.034796 -0.044080  0.065484 -0.014943  0.034587 -0.090077  0.057515 -0.053632  0.005629 -0.047457 -0.017616  0.035693  0.052416  0.022840  0.012177  0.204242 -0.030940  0.046657  0.045007 -0.008737 -0.019579  0.050593 -0.076919  0.013397 -0.032011 -0.004139 -0.055836  0.018894 -0.016503  0.029644  0.028216 -0.058165  0.197101  0.029622  0.061143  0.051334  0.039764 -0.007018  0.005230 -0.029945 -0.015367  0.005080 -0.029541 -0.002237  0.011822 -0.040193 -0.022842 -0.016331 -0.014565  \n",
       "crab           0.030708  0.052069 -0.010211 -0.013635 -0.046867 -0.073165  0.084948  0.032888 -0.035646  0.037773 -0.053069 -0.000339 -0.006851  0.075989  0.036542 -0.029959  0.036448  0.036332 -0.013668  0.031791 -0.007692  0.013750  0.054376  0.039661  0.000116 -0.073005  0.005157  0.083316  0.043276  0.004194 -0.010597  0.000929  0.072647  0.019844 -0.009845 -0.031211 -0.063922 -0.024647  0.094604  0.042762 -0.084613 -0.048249 -0.098375 -0.010282 -0.021175  0.015244  0.041014  0.030476 -0.020051 -0.015027 -0.070308  0.035357  0.008635 -0.021839  0.027600  0.020969  0.009418  0.062353  0.088083  0.014872  0.015538  0.028116 -0.019790  0.008888  0.025835 -0.019032  0.042676  0.020973 -0.002846  0.035398  0.051869 -0.025151 -0.014537  0.003352 -0.036576 -0.008851 -0.049537  0.023088 -0.040467  0.115723  0.004328  0.069223  0.140815  0.007436 -0.028549 -0.028862 -0.031053 -0.079870  0.005516  0.052036 -0.054518 -0.024591  0.011395  0.030934 -0.054522  0.019123  0.077553 -0.004958 -0.028895  0.115096  0.043067  0.002244 -0.040573  0.046142  0.000325 -0.016274 -0.059426  0.001933 -0.029121 -0.035760  0.030744  0.017002 -0.010853  0.061508 -0.028334  0.020263  \n",
       "...                 ...       ...       ...       ...       ...       ...       ...       ...       ...       ...       ...       ...       ...       ...       ...       ...       ...       ...       ...       ...       ...       ...       ...       ...       ...       ...       ...       ...       ...       ...       ...       ...       ...       ...       ...       ...       ...       ...       ...       ...       ...       ...       ...       ...       ...       ...       ...       ...       ...       ...       ...       ...       ...       ...       ...       ...       ...       ...       ...       ...       ...       ...       ...       ...       ...       ...       ...       ...       ...       ...       ...       ...       ...       ...       ...       ...       ...       ...       ...       ...       ...       ...       ...       ...       ...       ...       ...       ...       ...       ...       ...       ...       ...       ...       ...       ...       ...       ...       ...       ...       ...       ...       ...       ...       ...       ...       ...       ...       ...       ...       ...       ...       ...       ...       ...       ...  \n",
       "vaccinate      0.055566 -0.011300  0.057105 -0.052858  0.045962 -0.020470 -0.000189 -0.030196 -0.016457  0.018272 -0.001821 -0.005042  0.018443  0.021673 -0.001822 -0.013253  0.004525 -0.000161  0.019151 -0.039240 -0.074216  0.020827  0.028530 -0.025852  0.039976 -0.071466  0.000074  0.050450  0.078272  0.042439 -0.065034  0.063385  0.001497 -0.020000 -0.085082 -0.017045  0.036792 -0.028827  0.007959 -0.058976  0.037314 -0.063744  0.021911  0.095707 -0.070129 -0.006251  0.049219  0.019666 -0.038028  0.072105 -0.081174  0.006280 -0.043384  0.005142 -0.023449  0.028110 -0.013663  0.003619 -0.000790 -0.020785  0.023095  0.058947 -0.026988 -0.004328  0.013476 -0.032567  0.059173  0.057178 -0.037116  0.094095  0.017837 -0.032203  0.055226  0.040112 -0.027389  0.037445  0.016789  0.042886  0.014989  0.081078 -0.033704  0.038590  0.115601 -0.039135  0.043488 -0.047956 -0.031391  0.011976 -0.001479  0.025252  0.027826  0.021414  0.010496  0.042969 -0.015130  0.051444  0.060367  0.064192 -0.025623  0.119277 -0.031831  0.066155  0.033845 -0.016525  0.008972 -0.038890 -0.005454  0.013192 -0.009316 -0.000751 -0.021361  0.005412  0.019531  0.019365 -0.000408  0.022468  \n",
       "boastful       0.069571 -0.005903  0.081156 -0.021215 -0.053401 -0.014014  0.071768  0.044162 -0.001052  0.002423 -0.040683  0.023525 -0.018400 -0.043960 -0.079991 -0.023481 -0.050103 -0.037227 -0.037890 -0.069324  0.012661 -0.026899  0.035687  0.030884 -0.049208 -0.051994  0.000854  0.043943  0.093851  0.064736 -0.052860 -0.023531  0.080256  0.008760  0.005182  0.010996 -0.084760  0.010517  0.002839  0.028523 -0.008092 -0.018446  0.074432 -0.017187  0.020271 -0.034357  0.010760  0.020533 -0.028674  0.056149 -0.031791 -0.002711  0.099564  0.050883  0.003158  0.020939  0.033169 -0.028066  0.045399  0.001880 -0.035072  0.026681  0.037922  0.011587  0.006886 -0.020961  0.018029  0.078361  0.002040  0.019554  0.017028 -0.033588 -0.073720  0.046121 -0.052128  0.092784  0.048110  0.036085  0.017367  0.068189 -0.002608  0.031693  0.144429 -0.050009 -0.020675  0.028418  0.035298 -0.007077 -0.074961 -0.011768  0.002609 -0.052619 -0.003499 -0.026776 -0.004766 -0.032794  0.060407  0.039198 -0.095160  0.155760 -0.001979 -0.001395 -0.022317  0.028448  0.029504 -0.056538  0.030101 -0.014380 -0.064370 -0.017066 -0.008721  0.033386 -0.040513  0.002530 -0.039945 -0.017599  \n",
       "phrase        -0.058528  0.065285 -0.087513 -0.115825  0.022499 -0.024774  0.036103 -0.004231  0.024775  0.004198 -0.019795 -0.064633 -0.001469 -0.016869  0.032951 -0.003615  0.019269 -0.050498  0.069507 -0.036685  0.082373 -0.008827 -0.008564  0.063305  0.010689 -0.038379 -0.025952 -0.003131 -0.014177  0.033302  0.019657  0.011582  0.018964 -0.015687 -0.058643  0.007014 -0.011122 -0.021585  0.020099 -0.009868 -0.019699  0.029781 -0.020384 -0.020782  0.045475 -0.009865  0.002705  0.110405 -0.055338  0.021207 -0.030944  0.030240  0.012771  0.012619 -0.022920  0.070540 -0.007604  0.024985  0.039168 -0.003487 -0.036396  0.015911 -0.036793  0.023608  0.041129  0.008881 -0.003117  0.045816  0.020293 -0.037319  0.020574 -0.001703 -0.003296  0.004238 -0.031665  0.022885  0.006250  0.055551 -0.007252 -0.003325  0.050446 -0.000391  0.191838  0.008870  0.000128  0.089973  0.063521  0.013788  0.016724  0.050653 -0.086953  0.000160  0.043096  0.002811 -0.021237 -0.043554 -0.012026  0.027210 -0.022110  0.194198  0.061368  0.005018  0.004373  0.032080 -0.011544  0.013090 -0.017780  0.015126  0.031574  0.023818 -0.013850 -0.041078  0.043683  0.046820 -0.055000 -0.029599  \n",
       "ski            0.008382  0.021332 -0.007910 -0.039261 -0.004204 -0.034923  0.066608  0.001291  0.086285  0.034908 -0.015330  0.012493 -0.051689 -0.003796 -0.012618  0.021196 -0.013685 -0.061384 -0.013458  0.028805  0.007108  0.051880  0.088713 -0.001977 -0.000163 -0.029829  0.018122  0.005632  0.029128  0.041451  0.010574 -0.046687  0.053688 -0.039152 -0.066719 -0.007831  0.020553  0.027382 -0.001871 -0.087949 -0.014861 -0.013398  0.074896 -0.009438 -0.018144  0.018258  0.045537  0.063243  0.007885 -0.006380 -0.017508  0.033997 -0.003291 -0.007959  0.042868  0.026891 -0.049395  0.076976  0.051432 -0.007085 -0.049923  0.058704 -0.037711  0.047211  0.057437 -0.110134 -0.013813  0.033519 -0.058675  0.060776 -0.020869  0.049130 -0.012893  0.104267 -0.016780  0.033721 -0.040861  0.018616 -0.002271  0.082381 -0.041468  0.070440  0.158369  0.008520 -0.031631 -0.011209 -0.038880 -0.016970  0.009048 -0.050233 -0.002430  0.074730 -0.040414 -0.076996 -0.029274  0.024709  0.062133  0.046645 -0.007068  0.159120  0.006680  0.036902  0.049515  0.050404  0.039928 -0.002150 -0.034333 -0.093344 -0.002908 -0.009015 -0.003638 -0.008709  0.021233  0.045306  0.001246 -0.020212  \n",
       "commanding     0.074690 -0.032016  0.092839 -0.057402 -0.025372  0.003580  0.023142  0.016646 -0.068957 -0.026307 -0.038541  0.019094 -0.042331 -0.030626 -0.002468 -0.004777 -0.029731 -0.003758 -0.038012 -0.019335 -0.002208 -0.001038  0.051712  0.098190 -0.016959 -0.008214 -0.018745 -0.002664  0.052733  0.029994  0.022315  0.000283  0.030527  0.008958 -0.049916  0.025179 -0.063281 -0.019849 -0.046519  0.022930  0.024403  0.036987 -0.013033  0.056618 -0.010980  0.030294  0.023763  0.021788  0.003277 -0.025218  0.013912  0.032784  0.007840  0.015517  0.085115  0.021464  0.061205 -0.009277  0.031175  0.026020  0.004564  0.050285 -0.025953  0.016942 -0.017027  0.037061  0.048977  0.028693  0.002752  0.068661 -0.062564  0.048838  0.011330  0.027440 -0.058865  0.012646  0.017443  0.050385 -0.003471  0.044069 -0.052522  0.083973  0.163117 -0.027056 -0.031732  0.024662 -0.002531 -0.036302 -0.077429  0.073330  0.051751 -0.077653 -0.036914  0.001558  0.018197 -0.016465  0.025043 -0.010481 -0.060741  0.184386 -0.019070  0.036520 -0.003032  0.044082  0.018076  0.028879 -0.041535  0.009056  0.019925 -0.036146 -0.019906 -0.011048 -0.014468  0.003710 -0.038677  0.002350  \n",
       "\n",
       "[3512 rows x 512 columns]"
      ]
     },
     "execution_count": 207,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X_train_A"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 208,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>F_Objectivity</th>\n",
       "      <th>F_Subjectivity</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>argument</th>\n",
       "      <td>0.365722</td>\n",
       "      <td>0.647198</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>awful</th>\n",
       "      <td>0.201122</td>\n",
       "      <td>0.507171</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>decorator</th>\n",
       "      <td>0.774472</td>\n",
       "      <td>0.291107</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>misconception</th>\n",
       "      <td>0.013464</td>\n",
       "      <td>0.474901</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>crab</th>\n",
       "      <td>0.816438</td>\n",
       "      <td>0.227631</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>vaccinate</th>\n",
       "      <td>0.678125</td>\n",
       "      <td>0.443751</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>boastful</th>\n",
       "      <td>0.437815</td>\n",
       "      <td>0.575248</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>phrase</th>\n",
       "      <td>0.265757</td>\n",
       "      <td>0.144886</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>ski</th>\n",
       "      <td>0.900524</td>\n",
       "      <td>0.370531</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>commanding</th>\n",
       "      <td>0.316342</td>\n",
       "      <td>0.538581</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>3512 rows × 2 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "               F_Objectivity  F_Subjectivity\n",
       "argument            0.365722        0.647198\n",
       "awful               0.201122        0.507171\n",
       "decorator           0.774472        0.291107\n",
       "misconception       0.013464        0.474901\n",
       "crab                0.816438        0.227631\n",
       "...                      ...             ...\n",
       "vaccinate           0.678125        0.443751\n",
       "boastful            0.437815        0.575248\n",
       "phrase              0.265757        0.144886\n",
       "ski                 0.900524        0.370531\n",
       "commanding          0.316342        0.538581\n",
       "\n",
       "[3512 rows x 2 columns]"
      ]
     },
     "execution_count": 208,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "Y_train_A"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### B) Multilabel classification: splitting train and test data\n",
    "- The model predicts multiple classes for a single instance.  \n",
    "- In this case, it predicts high or low objectivity and subjectivity (i.e., four classes)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 209,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Generating data (the same process for dataset A):\n",
    "X_train_B, Y_train_B = generateData(train_df)\n",
    "X_test_B, Y_test_B = generateData(test_df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 210,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # Categorical labeling through list comprehension accordingly to SCA median criteria:\n",
    "# Y_train_B['F_Objectivity'] = ['high' if f_objectivity >= 0.565 else 'low' for f_objectivity in Y_train_B['F_Objectivity']]\n",
    "# Y_train_B['F_Subjectivity'] = ['high' if f_subjectivity >= 0.392 else 'low' for f_subjectivity in Y_train_B['F_Subjectivity']]\n",
    "\n",
    "# Y_test_B['F_Objectivity'] = ['high' if f_objectivity >= 0.565 else 'low' for f_objectivity in Y_test_B['F_Objectivity']]\n",
    "# Y_test_B['F_Subjectivity'] = ['high' if f_subjectivity >= 0.392 else 'low' for f_subjectivity in Y_test_B['F_Subjectivity']]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 211,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Categorical binary labeling through list comprehension accordingly to SCA median criteria:\n",
    "Y_train_B['F_Objectivity'] = [1 if f_objectivity >= 0.565 else 0 for f_objectivity in Y_train_B['F_Objectivity']]\n",
    "Y_train_B['F_Subjectivity'] = [1 if f_subjectivity >= 0.392 else 0 for f_subjectivity in Y_train_B['F_Subjectivity']]\n",
    "\n",
    "Y_test_B['F_Objectivity'] = [1 if f_objectivity >= 0.565 else 0 for f_objectivity in Y_test_B['F_Objectivity']]\n",
    "Y_test_B['F_Subjectivity'] = [1 if f_subjectivity >= 0.392 else 0 for f_subjectivity in Y_test_B['F_Subjectivity']]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 212,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train data dimension:\n",
      "X_train: (3512, 512)\n",
      "Y_train: (3512, 2)\n",
      "\n",
      "Test data dimension:\n",
      "X_test: (1171, 512)\n",
      "Y_test: (1171, 2)\n"
     ]
    }
   ],
   "source": [
    "## Checking the generated data dimension:\n",
    "print(\"Train data dimension:\")\n",
    "print(\"X_train:\", X_train_B.shape)\n",
    "print(\"Y_train:\", Y_train_B.shape)\n",
    "\n",
    "print(\"\\nTest data dimension:\")\n",
    "print(\"X_test:\", X_test_B.shape)\n",
    "print(\"Y_test:\", Y_test_B.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 213,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>F_Objectivity</th>\n",
       "      <th>F_Subjectivity</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>over</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>regulation</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>high</th>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>register</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>loose</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "            F_Objectivity  F_Subjectivity\n",
       "over                    0               0\n",
       "regulation              0               0\n",
       "high                    1               1\n",
       "register                0               0\n",
       "loose                   0               0"
      ]
     },
     "execution_count": 213,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "Y_train_B.sample(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### C) Multiclass classification: splitting train and test data\n",
    "- The model predicts between more than two mutually exclusive classes.  \n",
    "- In this case, it will predict between its semantic content: latent, manifest, contextual, or perceptual.  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 214,
   "metadata": {},
   "outputs": [],
   "source": [
    "## It makes use of Y_train_B and Y_test_B data.\n",
    "Y_train_C = Y_train_B.copy()\n",
    "Y_test_C = Y_test_B.copy()\n",
    "\n",
    "## It also makes a copy of X datasets:\n",
    "X_train_C = X_train_B.copy()\n",
    "X_test_C = X_test_B.copy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 215,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Defines a method to map values from columns to the respective labels according to SCA analysis.\n",
    "def map_labels(row):\n",
    "    if row['F_Objectivity'] == 0 and row['F_Subjectivity'] == 1:\n",
    "        return 'Latent'\n",
    "    elif row['F_Objectivity'] == 0 and row['F_Subjectivity'] == 0:\n",
    "        return 'Contextual'\n",
    "    elif row['F_Objectivity'] == 1 and row['F_Subjectivity'] == 0:\n",
    "        return 'Manifest'\n",
    "    elif row['F_Objectivity'] == 1 and row['F_Subjectivity'] == 1:\n",
    "        return 'Perceptual'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 216,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Maps Y_train_C, Y_test_C values into a target column:\n",
    "Y_train_C['target'] = Y_train_C.apply(map_labels, axis=1)\n",
    "Y_test_C['target'] = Y_test_C.apply(map_labels, axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 217,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Dropping unnecessariy columns:\n",
    "Y_train_C.drop(['F_Objectivity','F_Subjectivity'], axis=1, inplace=True)\n",
    "Y_test_C.drop(['F_Objectivity','F_Subjectivity'], axis=1, inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 218,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>target</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>bad</th>\n",
       "      <td>Latent</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>furniture</th>\n",
       "      <td>Manifest</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>repent</th>\n",
       "      <td>Contextual</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>olive</th>\n",
       "      <td>Manifest</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>vaccinate</th>\n",
       "      <td>Perceptual</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "               target\n",
       "bad            Latent\n",
       "furniture    Manifest\n",
       "repent     Contextual\n",
       "olive        Manifest\n",
       "vaccinate  Perceptual"
      ]
     },
     "execution_count": 218,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "Y_train_C.sample(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 219,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 219,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "## All training data, except for train_D are equals:\n",
    "X_train_B.equals(X_train_C) & X_train_A.equals(X_train_B)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### D) MAD selection data\n",
    "Since SCA uses the median as a measure of centrality instead of the mean, we will consider the median absolute deviation (MAD) as an equivalent to standard deviation. MAD is a robust measure of variability that is less sensitive to outliers compared to the standard deviation.  \n",
    "\n",
    "The purpose of using MAD is to focus on the distant data points rather than the central tendency of the data. The central data points may potentially confuse the algorithm, especially if they represent a region of overlap or ambiguity. By considering the median absolute deviation instead of the mean and standard deviation, we aim to identify data points that are far from the central measurement, thereby capturing well-separated regions of the data. This approach helps ensure that our classifier is trained on data points that are more distinct and representative of different classes or categories, leading to more effective classification performance.  \n",
    "\n",
    "$\\text{MAD} = \\text{median}(|x_i - \\text{median}(X)|)$, where $x_i$ represents each data point in the dataset X."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 220,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Median of F_Obj: 0.5649972252148387\n",
      "Median of F_Subj: 0.3925501677449325\n"
     ]
    }
   ],
   "source": [
    "med_fObj = df_factors['F_Objectivity'].median()\n",
    "med_fSubj = df_factors['F_Subjectivity'].median()\n",
    "print(f\"Median of F_Obj: {med_fObj}\")\n",
    "print(f\"Median of F_Subj: {med_fSubj}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 221,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "MAD for F_Obj: 0.24804732650935357\n",
      "MAD for F_Subj: 0.1399289370570712\n"
     ]
    }
   ],
   "source": [
    "# Calculating MAD\n",
    "mad_fObj = np.median(np.abs(df_factors['F_Objectivity'] - med_fObj))\n",
    "mad_fSubj = np.median(np.abs(df_factors['F_Subjectivity'] - med_fSubj))\n",
    "print(f\"MAD for F_Obj: {mad_fObj}\")\n",
    "print(f\"MAD for F_Subj: {mad_fSubj}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 222,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Defining thresholds as MAD:\n",
    "threshold_fObj = mad_fObj\n",
    "threshold_fSubj = mad_fSubj\n",
    "\n",
    "# ## Defining thresholds as TWICE MAD:\n",
    "# threshold_fObj = 2*mad_fObj\n",
    "# threshold_fSubj = 2*mad_fSubj"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 223,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Selecting from the original dataframe (df: SCA) only those words distant from the median (see threshold above):\n",
    "df_MAD = df[\n",
    "    (abs(df['F_Objectivity'] - med_fObj) > threshold_fObj) |\n",
    "    (abs(df['F_Subjectivity'] - med_fSubj) > threshold_fSubj)\n",
    "]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 224,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'pandas.core.frame.DataFrame'>\n",
      "Int64Index: 3484 entries, 1 to 5551\n",
      "Data columns (total 3 columns):\n",
      " #   Column          Non-Null Count  Dtype  \n",
      "---  ------          --------------  -----  \n",
      " 0   word            3484 non-null   object \n",
      " 1   F_Objectivity   3484 non-null   float64\n",
      " 2   F_Subjectivity  3484 non-null   float64\n",
      "dtypes: float64(2), object(1)\n",
      "memory usage: 108.9+ KB\n",
      "None\n",
      "\n",
      "           word  F_Objectivity  F_Subjectivity\n",
      "1445   disband       0.182454        0.269548\n",
      "4792    suburb       0.647577        0.245176\n",
      "3215  namesake       0.068715        0.162642\n"
     ]
    }
   ],
   "source": [
    "print(df_MAD.info())\n",
    "print('\\n',df_MAD.sample(3))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 225,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<matplotlib.collections.PathCollection at 0x21209099510>"
      ]
     },
     "execution_count": 225,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 640x480 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "plt.scatter(df_MAD['F_Objectivity'], df_MAD['F_Subjectivity'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 226,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Repeating the splitting data in train (75%) and test (25%)\n",
    "train_MAD, test_MAD = train_test_split(df_MAD, test_size=0.25, random_state=42)\n",
    "\n",
    "## Generating data based on MAD selected points:\n",
    "X_train_D, Y_train_D = generateData(train_MAD)\n",
    "X_test_D, Y_test_D = generateData(test_MAD)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### E) MAD data for Multiclass\n",
    "This data puts together the manipulation applied for (C) and (D) datasets."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 227,
   "metadata": {},
   "outputs": [],
   "source": [
    "## It makes use of Y_train_D and Y_test_D data.\n",
    "Y_train_E = Y_train_D.copy()\n",
    "Y_test_E = Y_test_D.copy()\n",
    "\n",
    "## It also makes a copy of X datasets:\n",
    "X_train_E = X_train_D.copy()\n",
    "X_test_E = X_test_D.copy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 228,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Categorical binary labeling through list comprehension accordingly to SCA median criteria:\n",
    "Y_train_E['F_Objectivity'] = [1 if f_objectivity >= 0.565 else 0 for f_objectivity in Y_train_E['F_Objectivity']]\n",
    "Y_train_E['F_Subjectivity'] = [1 if f_subjectivity >= 0.392 else 0 for f_subjectivity in Y_train_E['F_Subjectivity']]\n",
    "\n",
    "Y_test_E['F_Objectivity'] = [1 if f_objectivity >= 0.565 else 0 for f_objectivity in Y_test_E['F_Objectivity']]\n",
    "Y_test_E['F_Subjectivity'] = [1 if f_subjectivity >= 0.392 else 0 for f_subjectivity in Y_test_E['F_Subjectivity']]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 229,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Maps Y_train_C, Y_test_C values into a target column:\n",
    "Y_train_E['target'] = Y_train_E.apply(map_labels, axis=1)\n",
    "Y_test_E['target'] = Y_test_E.apply(map_labels, axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 230,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Dropping unnecessariy columns:\n",
    "Y_train_E.drop(['F_Objectivity','F_Subjectivity'], axis=1, inplace=True)\n",
    "Y_test_E.drop(['F_Objectivity','F_Subjectivity'], axis=1, inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 233,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>target</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>rumour</th>\n",
       "      <td>Latent</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>grape</th>\n",
       "      <td>Manifest</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>hurricane</th>\n",
       "      <td>Perceptual</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>humour</th>\n",
       "      <td>Latent</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>naive</th>\n",
       "      <td>Latent</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "               target\n",
       "rumour         Latent\n",
       "grape        Manifest\n",
       "hurricane  Perceptual\n",
       "humour         Latent\n",
       "naive          Latent"
      ]
     },
     "execution_count": 233,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "Y_train_E.sample(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "## Model 01: MLP Classifier for word semantic content\n",
    "- Multilayer perceptron with two continuous outputs between 0 and 1 (sigmoid)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 234,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from tensorflow.keras.models import Sequential\n",
    "from tensorflow.keras.layers import Dense"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Training with data (A)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 235,
   "metadata": {},
   "outputs": [],
   "source": [
    "### Model 01: MLP architecture for continuous output:\n",
    "model_01_A = Sequential([\n",
    "    Dense(256, activation='relu', input_shape=(512,)),\n",
    "    Dense(128, activation='relu'),\n",
    "    Dense(64, activation='relu'),\n",
    "    Dense(32, activation='relu'),\n",
    "    # Dense(2, activation='linear')  # 2 neurons for continuous output between 0 and 1\n",
    "    Dense(2, activation='sigmoid')  # 2 neurons for continuous output between 0 and 1\n",
    "], name='Model_01_A')\n",
    "\n",
    "# Compile the model\n",
    "model_01_A.compile(optimizer='adam',\n",
    "              loss='mean_squared_error',\n",
    "              metrics=['accuracy'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 236,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"Model_01_A\"\n",
      "_________________________________________________________________\n",
      " Layer (type)                Output Shape              Param #   \n",
      "=================================================================\n",
      " dense_35 (Dense)            (None, 256)               131328    \n",
      "                                                                 \n",
      " dense_36 (Dense)            (None, 128)               32896     \n",
      "                                                                 \n",
      " dense_37 (Dense)            (None, 64)                8256      \n",
      "                                                                 \n",
      " dense_38 (Dense)            (None, 32)                2080      \n",
      "                                                                 \n",
      " dense_39 (Dense)            (None, 2)                 66        \n",
      "                                                                 \n",
      "=================================================================\n",
      "Total params: 174626 (682.13 KB)\n",
      "Trainable params: 174626 (682.13 KB)\n",
      "Non-trainable params: 0 (0.00 Byte)\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "# Print model summary\n",
    "model_01_A.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 237,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[0.36572243 0.64719766]\n",
      " [0.20112169 0.50717064]\n",
      " [0.77447234 0.29110657]\n",
      " [0.01346386 0.47490132]\n",
      " [0.81643835 0.2276313 ]]\n"
     ]
    }
   ],
   "source": [
    "## Transforming data into numpy array:\n",
    "Y_train_A_array = Y_train_A.to_numpy()\n",
    "Y_test_A_array = Y_test_A.to_numpy()\n",
    "\n",
    "X_train_A_array = X_train_A.to_numpy()\n",
    "X_test_A_array = X_test_A.to_numpy()\n",
    "\n",
    "# Print the first few elements to verify\n",
    "print(Y_train_A_array[:5])  # Print the first 5 elements\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 238,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/50\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "176/176 [==============================] - 2s 3ms/step - loss: 0.0231 - accuracy: 0.8291 - val_loss: 0.0152 - val_accuracy: 0.8748\n",
      "Epoch 2/50\n",
      "176/176 [==============================] - 0s 2ms/step - loss: 0.0124 - accuracy: 0.8896 - val_loss: 0.0139 - val_accuracy: 0.8834\n",
      "Epoch 3/50\n",
      "176/176 [==============================] - 0s 2ms/step - loss: 0.0102 - accuracy: 0.9007 - val_loss: 0.0130 - val_accuracy: 0.8933\n",
      "Epoch 4/50\n",
      "176/176 [==============================] - 0s 2ms/step - loss: 0.0082 - accuracy: 0.9178 - val_loss: 0.0134 - val_accuracy: 0.8962\n",
      "Epoch 5/50\n",
      "176/176 [==============================] - 0s 2ms/step - loss: 0.0062 - accuracy: 0.9306 - val_loss: 0.0131 - val_accuracy: 0.9004\n",
      "Epoch 6/50\n",
      "176/176 [==============================] - 0s 2ms/step - loss: 0.0048 - accuracy: 0.9470 - val_loss: 0.0132 - val_accuracy: 0.8933\n",
      "Epoch 7/50\n",
      "176/176 [==============================] - 0s 2ms/step - loss: 0.0037 - accuracy: 0.9487 - val_loss: 0.0132 - val_accuracy: 0.8990\n",
      "Epoch 8/50\n",
      "176/176 [==============================] - 0s 2ms/step - loss: 0.0027 - accuracy: 0.9580 - val_loss: 0.0135 - val_accuracy: 0.9033\n",
      "Epoch 9/50\n",
      "176/176 [==============================] - 0s 2ms/step - loss: 0.0022 - accuracy: 0.9619 - val_loss: 0.0132 - val_accuracy: 0.9004\n",
      "Epoch 10/50\n",
      "176/176 [==============================] - 0s 2ms/step - loss: 0.0017 - accuracy: 0.9658 - val_loss: 0.0142 - val_accuracy: 0.8962\n",
      "Epoch 11/50\n",
      "176/176 [==============================] - 0s 2ms/step - loss: 0.0014 - accuracy: 0.9697 - val_loss: 0.0136 - val_accuracy: 0.8819\n",
      "Epoch 12/50\n",
      "176/176 [==============================] - 0s 2ms/step - loss: 0.0012 - accuracy: 0.9715 - val_loss: 0.0136 - val_accuracy: 0.8890\n",
      "Epoch 13/50\n",
      "176/176 [==============================] - 0s 2ms/step - loss: 0.0010 - accuracy: 0.9765 - val_loss: 0.0137 - val_accuracy: 0.8947\n",
      "Epoch 14/50\n",
      "176/176 [==============================] - 0s 2ms/step - loss: 0.0010 - accuracy: 0.9751 - val_loss: 0.0133 - val_accuracy: 0.8976\n",
      "Epoch 15/50\n",
      "176/176 [==============================] - 0s 2ms/step - loss: 9.2020e-04 - accuracy: 0.9740 - val_loss: 0.0138 - val_accuracy: 0.8933\n",
      "Epoch 16/50\n",
      "176/176 [==============================] - 0s 2ms/step - loss: 8.4906e-04 - accuracy: 0.9761 - val_loss: 0.0137 - val_accuracy: 0.8848\n",
      "Epoch 17/50\n",
      "176/176 [==============================] - 0s 2ms/step - loss: 8.3804e-04 - accuracy: 0.9708 - val_loss: 0.0137 - val_accuracy: 0.9018\n",
      "Epoch 18/50\n",
      "176/176 [==============================] - 0s 2ms/step - loss: 7.7386e-04 - accuracy: 0.9772 - val_loss: 0.0138 - val_accuracy: 0.8962\n",
      "Epoch 19/50\n",
      "176/176 [==============================] - 0s 2ms/step - loss: 7.6879e-04 - accuracy: 0.9733 - val_loss: 0.0135 - val_accuracy: 0.8890\n",
      "Epoch 20/50\n",
      "176/176 [==============================] - 0s 2ms/step - loss: 7.1179e-04 - accuracy: 0.9804 - val_loss: 0.0133 - val_accuracy: 0.8933\n",
      "Epoch 21/50\n",
      "176/176 [==============================] - 0s 2ms/step - loss: 7.0047e-04 - accuracy: 0.9712 - val_loss: 0.0133 - val_accuracy: 0.8990\n",
      "Epoch 22/50\n",
      "176/176 [==============================] - 0s 2ms/step - loss: 7.2620e-04 - accuracy: 0.9761 - val_loss: 0.0139 - val_accuracy: 0.8962\n",
      "Epoch 23/50\n",
      "176/176 [==============================] - 0s 2ms/step - loss: 6.6268e-04 - accuracy: 0.9761 - val_loss: 0.0135 - val_accuracy: 0.9004\n",
      "Epoch 24/50\n",
      "176/176 [==============================] - 0s 2ms/step - loss: 5.8961e-04 - accuracy: 0.9779 - val_loss: 0.0134 - val_accuracy: 0.8947\n",
      "Epoch 25/50\n",
      "176/176 [==============================] - 0s 2ms/step - loss: 5.3621e-04 - accuracy: 0.9776 - val_loss: 0.0133 - val_accuracy: 0.9004\n",
      "Epoch 26/50\n",
      "176/176 [==============================] - 0s 2ms/step - loss: 5.7987e-04 - accuracy: 0.9797 - val_loss: 0.0135 - val_accuracy: 0.8876\n",
      "Epoch 27/50\n",
      "176/176 [==============================] - 0s 2ms/step - loss: 5.5626e-04 - accuracy: 0.9811 - val_loss: 0.0134 - val_accuracy: 0.9018\n",
      "Epoch 28/50\n",
      "176/176 [==============================] - 0s 2ms/step - loss: 5.1116e-04 - accuracy: 0.9769 - val_loss: 0.0133 - val_accuracy: 0.8919\n",
      "Epoch 29/50\n",
      "176/176 [==============================] - 0s 2ms/step - loss: 5.2134e-04 - accuracy: 0.9808 - val_loss: 0.0136 - val_accuracy: 0.8962\n",
      "Epoch 30/50\n",
      "176/176 [==============================] - 0s 2ms/step - loss: 5.6299e-04 - accuracy: 0.9754 - val_loss: 0.0133 - val_accuracy: 0.8976\n",
      "Epoch 31/50\n",
      "176/176 [==============================] - 0s 2ms/step - loss: 5.3926e-04 - accuracy: 0.9769 - val_loss: 0.0133 - val_accuracy: 0.8862\n",
      "Epoch 32/50\n",
      "176/176 [==============================] - 0s 2ms/step - loss: 5.7350e-04 - accuracy: 0.9786 - val_loss: 0.0133 - val_accuracy: 0.8962\n",
      "Epoch 33/50\n",
      "176/176 [==============================] - 0s 2ms/step - loss: 5.6690e-04 - accuracy: 0.9790 - val_loss: 0.0134 - val_accuracy: 0.8905\n",
      "Epoch 34/50\n",
      "176/176 [==============================] - 0s 2ms/step - loss: 5.2872e-04 - accuracy: 0.9794 - val_loss: 0.0135 - val_accuracy: 0.8962\n",
      "Epoch 35/50\n",
      "176/176 [==============================] - 0s 2ms/step - loss: 4.4576e-04 - accuracy: 0.9804 - val_loss: 0.0135 - val_accuracy: 0.8933\n",
      "Epoch 36/50\n",
      "176/176 [==============================] - 0s 2ms/step - loss: 3.6631e-04 - accuracy: 0.9779 - val_loss: 0.0134 - val_accuracy: 0.9004\n",
      "Epoch 37/50\n",
      "176/176 [==============================] - 0s 2ms/step - loss: 3.5290e-04 - accuracy: 0.9847 - val_loss: 0.0134 - val_accuracy: 0.8990\n",
      "Epoch 38/50\n",
      "176/176 [==============================] - 0s 2ms/step - loss: 3.7714e-04 - accuracy: 0.9826 - val_loss: 0.0134 - val_accuracy: 0.9033\n",
      "Epoch 39/50\n",
      "176/176 [==============================] - 0s 2ms/step - loss: 4.3283e-04 - accuracy: 0.9779 - val_loss: 0.0132 - val_accuracy: 0.8947\n",
      "Epoch 40/50\n",
      "176/176 [==============================] - 0s 2ms/step - loss: 4.7571e-04 - accuracy: 0.9790 - val_loss: 0.0137 - val_accuracy: 0.8933\n",
      "Epoch 41/50\n",
      "176/176 [==============================] - 0s 2ms/step - loss: 5.2749e-04 - accuracy: 0.9815 - val_loss: 0.0135 - val_accuracy: 0.8933\n",
      "Epoch 42/50\n",
      "176/176 [==============================] - 0s 2ms/step - loss: 4.1736e-04 - accuracy: 0.9765 - val_loss: 0.0133 - val_accuracy: 0.9047\n",
      "Epoch 43/50\n",
      "176/176 [==============================] - 0s 2ms/step - loss: 3.9250e-04 - accuracy: 0.9822 - val_loss: 0.0134 - val_accuracy: 0.8962\n",
      "Epoch 44/50\n",
      "176/176 [==============================] - 0s 2ms/step - loss: 3.5132e-04 - accuracy: 0.9833 - val_loss: 0.0134 - val_accuracy: 0.8976\n",
      "Epoch 45/50\n",
      "176/176 [==============================] - 0s 2ms/step - loss: 3.5055e-04 - accuracy: 0.9843 - val_loss: 0.0132 - val_accuracy: 0.8947\n",
      "Epoch 46/50\n",
      "176/176 [==============================] - 0s 2ms/step - loss: 3.7271e-04 - accuracy: 0.9854 - val_loss: 0.0132 - val_accuracy: 0.8976\n",
      "Epoch 47/50\n",
      "176/176 [==============================] - 0s 2ms/step - loss: 3.4376e-04 - accuracy: 0.9829 - val_loss: 0.0132 - val_accuracy: 0.8962\n",
      "Epoch 48/50\n",
      "176/176 [==============================] - 0s 2ms/step - loss: 3.4169e-04 - accuracy: 0.9808 - val_loss: 0.0132 - val_accuracy: 0.8933\n",
      "Epoch 49/50\n",
      "176/176 [==============================] - 0s 2ms/step - loss: 3.6167e-04 - accuracy: 0.9836 - val_loss: 0.0129 - val_accuracy: 0.9004\n",
      "Epoch 50/50\n",
      "176/176 [==============================] - 0s 2ms/step - loss: 3.7894e-04 - accuracy: 0.9815 - val_loss: 0.0131 - val_accuracy: 0.9018\n"
     ]
    }
   ],
   "source": [
    "# Train the model\n",
    "history_01_A = model_01_A.fit(X_train_A_array, Y_train_A_array, epochs=50, batch_size=16, validation_split=0.2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 239,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " 1/37 [..............................] - ETA: 0s - loss: 0.0172 - accuracy: 0.8438"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "37/37 [==============================] - 0s 1ms/step - loss: 0.0135 - accuracy: 0.8770\n",
      "Model_01_A: Test Accuracy: 87.70%\n"
     ]
    }
   ],
   "source": [
    "# Evaluate the model\n",
    "loss_01_A, accuracy_01_A = model_01_A.evaluate(X_test_A_array, Y_test_A_array)\n",
    "print(f\"{model_01_A.name}: Test Accuracy: {accuracy_01_A * 100:.2f}%\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Applying the trained model to predict SCA for a distinct word:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 240,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Choosing an arbitrary word:\n",
    "entry = 'monster'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 241,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Getting its vector representation (from word embedding) and preparing it to the model input format:\n",
    "new_entry = nlp_getVector(entry)[1]\n",
    "new_entry = np.expand_dims(new_entry, axis=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 242,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[ 0.02616017,  0.02746304,  0.02975387,  0.02968044, -0.06519616,\n",
       "         0.05031857,  0.07530344,  0.06635127, -0.03761565,  0.01005177,\n",
       "        -0.03585095,  0.11727351, -0.02776198, -0.02028794, -0.02000328,\n",
       "         0.04791656, -0.00827752, -0.03620078,  0.09290259,  0.05407454,\n",
       "         0.05711543,  0.06903831,  0.01469001, -0.0448224 ,  0.05293355,\n",
       "        -0.08088628, -0.03772315,  0.04423808,  0.03396869,  0.02243079,\n",
       "         0.00976525,  0.03510152,  0.01447926,  0.10433114, -0.04124927,\n",
       "         0.02331381, -0.06140069,  0.02953744, -0.01834786, -0.06772301,\n",
       "        -0.05290498, -0.04946691, -0.04175916, -0.013333  , -0.05204307,\n",
       "        -0.00421893, -0.01953493,  0.06516182,  0.05313764, -0.02124265,\n",
       "         0.00785025,  0.02248924, -0.03288206, -0.03005493,  0.01187884,\n",
       "        -0.04833665,  0.03483825, -0.0316888 ,  0.00338035, -0.04364322,\n",
       "        -0.00883494, -0.002749  ,  0.02929058,  0.02681011, -0.09624288,\n",
       "        -0.05824707,  0.01651053,  0.08139316,  0.04821713, -0.06382051,\n",
       "        -0.04930201,  0.03535724, -0.05312945, -0.04104573, -0.02868386,\n",
       "        -0.03089989,  0.00187427, -0.02498265,  0.01724515,  0.07929537,\n",
       "        -0.01320603,  0.00260522,  0.00407656, -0.01304799, -0.00382354,\n",
       "        -0.07395516, -0.0086352 , -0.02130007, -0.01722309,  0.05836106,\n",
       "        -0.04884252, -0.0030703 , -0.01253885,  0.03585022, -0.09414739,\n",
       "        -0.01116303,  0.01394786,  0.10080981, -0.08182135,  0.00868037,\n",
       "        -0.02785373,  0.07279103, -0.01789642, -0.03907359, -0.06632361,\n",
       "        -0.00939589, -0.00641592,  0.01800223,  0.06364213,  0.00841707,\n",
       "         0.00059615,  0.06484434, -0.00030752,  0.03348479, -0.03000917,\n",
       "        -0.00360729, -0.02709279,  0.00849134, -0.0273727 ,  0.05751572,\n",
       "        -0.04909122, -0.03227394, -0.03903138, -0.02146861,  0.04037344,\n",
       "        -0.05616824,  0.00631871, -0.01987244, -0.01651146, -0.00084838,\n",
       "         0.00167314, -0.03111224,  0.02470026,  0.00148216, -0.0322372 ,\n",
       "        -0.07253987, -0.03370963,  0.05730028,  0.02042861,  0.00591441,\n",
       "         0.02227441, -0.00373792,  0.00549982, -0.08887859, -0.03402558,\n",
       "         0.0241922 , -0.04218455, -0.04259763,  0.02328282, -0.04820952,\n",
       "         0.00818241,  0.07910366,  0.00718682,  0.00658789,  0.00704222,\n",
       "         0.00528281, -0.0438139 ,  0.01796803, -0.03619868,  0.02242085,\n",
       "        -0.01461233, -0.04899503, -0.02615237,  0.0023899 ,  0.03669584,\n",
       "        -0.04240734,  0.03778355, -0.0105043 , -0.03798708,  0.01822468,\n",
       "        -0.07832558,  0.01818028,  0.01214078,  0.02512539,  0.03780848,\n",
       "         0.03035197, -0.01199416,  0.02659081, -0.04737743, -0.01433276,\n",
       "        -0.03095917, -0.01546133, -0.00978195,  0.0435396 , -0.04705709,\n",
       "        -0.09141356, -0.05244523, -0.04537497,  0.03828824, -0.03825903,\n",
       "         0.01629528, -0.02270238, -0.06587634, -0.03584672, -0.01773358,\n",
       "         0.03807328, -0.01534419, -0.0049579 ,  0.05169719, -0.07176393,\n",
       "         0.09796622,  0.00995899,  0.00035635,  0.05694641, -0.0659828 ,\n",
       "         0.01659811,  0.02651493,  0.02326465,  0.02120788,  0.0070889 ,\n",
       "        -0.09142935, -0.03164714, -0.02683462,  0.01509128, -0.02859432,\n",
       "         0.02376694,  0.01570604, -0.06358638,  0.02298617, -0.0425426 ,\n",
       "         0.07400051,  0.07087404,  0.01191897,  0.04601989, -0.01078488,\n",
       "        -0.0372625 ,  0.04282942,  0.18640321,  0.0826555 , -0.07033835,\n",
       "         0.02197191, -0.00591671, -0.01877033, -0.0041578 , -0.06552045,\n",
       "         0.001296  ,  0.01478547, -0.00717069,  0.04096445,  0.07744963,\n",
       "         0.02928667,  0.09483336, -0.03844086, -0.03784677, -0.02083626,\n",
       "        -0.04067976, -0.02238746,  0.04553591,  0.02323353, -0.04107577,\n",
       "         0.0333186 , -0.01019645,  0.00663608,  0.01591937, -0.05990623,\n",
       "         0.02030769, -0.01908056, -0.04721133,  0.04856892, -0.04843105,\n",
       "         0.01962948,  0.00221706,  0.00298925, -0.01373891, -0.01458527,\n",
       "        -0.0037221 , -0.07867242, -0.01167845,  0.12016355,  0.02974412,\n",
       "         0.0263094 , -0.00402054,  0.01898913, -0.03690138, -0.05642686,\n",
       "         0.06498873,  0.04297642,  0.02556395,  0.03607331,  0.01945323,\n",
       "        -0.02338767, -0.09778085,  0.070166  , -0.05414013, -0.00324598,\n",
       "        -0.05612069, -0.06284943,  0.03362205, -0.03536946,  0.00670616,\n",
       "        -0.08194531, -0.01064768,  0.02151619,  0.03064108,  0.00862726,\n",
       "         0.02733004,  0.02130437, -0.03652912, -0.00094634, -0.07733596,\n",
       "         0.0331368 , -0.03672154,  0.00615876, -0.00049174,  0.08978222,\n",
       "         0.08343001,  0.02195845,  0.06623264,  0.005671  ,  0.05079815,\n",
       "         0.05371142, -0.05732922,  0.09159793, -0.00233585,  0.04885763,\n",
       "        -0.01545884, -0.00150933,  0.04364094, -0.02959683, -0.01308569,\n",
       "        -0.03879916,  0.04083583,  0.02592724, -0.04589811,  0.1013727 ,\n",
       "         0.0454776 ,  0.011873  , -0.00349831,  0.04938577, -0.01286274,\n",
       "         0.03306261, -0.01736229, -0.00655097, -0.02259692, -0.05725698,\n",
       "        -0.03813796,  0.01650469, -0.01858263,  0.05328818,  0.01347943,\n",
       "         0.03761604,  0.01017367, -0.04816389, -0.05839949,  0.0015225 ,\n",
       "         0.04213766, -0.04785685, -0.03081435, -0.02609217, -0.00390939,\n",
       "         0.03988957,  0.0099185 ,  0.00587545, -0.04320128,  0.03007979,\n",
       "         0.04293549, -0.02258611, -0.0128389 ,  0.0198455 , -0.02783798,\n",
       "         0.02482433, -0.05363581,  0.03923104, -0.00727121,  0.01977192,\n",
       "         0.03386253, -0.0403032 ,  0.06063236, -0.08248831, -0.01510584,\n",
       "         0.01583808,  0.07757128,  0.04717242, -0.03547653, -0.00550577,\n",
       "         0.04908841, -0.04805858, -0.0140687 ,  0.07499868,  0.05153849,\n",
       "        -0.07109765, -0.01891489,  0.04645902,  0.02801032,  0.0500031 ,\n",
       "        -0.0414384 ,  0.03922373,  0.02698903,  0.01267302, -0.0056995 ,\n",
       "         0.02828316, -0.07795611, -0.01032993, -0.04861812,  0.0754107 ,\n",
       "         0.00093322, -0.04406828, -0.0250081 , -0.05561627,  0.00767843,\n",
       "         0.01088726, -0.06652123,  0.06318115,  0.00464474,  0.04728194,\n",
       "         0.0101736 ,  0.01769287,  0.0213813 , -0.0112538 ,  0.00377563,\n",
       "        -0.01737802,  0.00654433,  0.02180075,  0.0468727 , -0.00213924,\n",
       "         0.01105526, -0.00029138,  0.03280043, -0.02498908,  0.03531532,\n",
       "         0.00531877, -0.00925587,  0.00769949,  0.00868726,  0.03529778,\n",
       "        -0.0043042 , -0.01559221,  0.05453917,  0.0824726 , -0.03524772,\n",
       "        -0.01488748, -0.00729786, -0.03208988,  0.01470322,  0.08166479,\n",
       "         0.02233562, -0.04431649, -0.00728507, -0.04104038, -0.04687423,\n",
       "         0.03516594, -0.01240898,  0.0513411 ,  0.05902164,  0.02447273,\n",
       "         0.00778074,  0.04353337,  0.01948024,  0.06831728,  0.00193419,\n",
       "         0.04700363, -0.03875988,  0.06664998,  0.04026352,  0.07540856,\n",
       "         0.06179707, -0.00593187,  0.01669571,  0.05621864,  0.01264611,\n",
       "        -0.02683505,  0.05133523,  0.05757952, -0.00073585,  0.0795398 ,\n",
       "         0.05393324, -0.02720661, -0.04584645, -0.01695406, -0.04826979,\n",
       "        -0.0089346 , -0.03904664, -0.03325689, -0.06387562,  0.08267367,\n",
       "         0.04400442, -0.03960017,  0.02812945,  0.19545352,  0.01510061,\n",
       "        -0.03863275, -0.0223638 ,  0.09225757, -0.07492823,  0.00912657,\n",
       "        -0.06611212, -0.03524949, -0.01722816, -0.02583798,  0.00525994,\n",
       "         0.00802204, -0.02036377,  0.02637088,  0.0261551 , -0.06066755,\n",
       "         0.1667621 , -0.01240299, -0.0119879 , -0.01261856, -0.00070402,\n",
       "        -0.03275418,  0.02457217, -0.08414488,  0.03149269, -0.05566578,\n",
       "        -0.01205674, -0.06905434, -0.01145665, -0.07527621,  0.00419669,\n",
       "        -0.02303474,  0.05359574]], dtype=float32)"
      ]
     },
     "execution_count": 242,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "## Showing how this entry is laid out:\n",
    "new_entry"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 243,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1/1 [==============================] - 0s 68ms/step\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "array([[0.72251976, 0.72126365]], dtype=float32)"
      ]
     },
     "execution_count": 243,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "## Predicting the objective and subjective loads for the 'entry' word:\n",
    "result = model_01_A.predict(new_entry)\n",
    "result"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 244,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Generalizing the process above for any word through a python method:\n",
    "def wordClassifier_regression(word, model=model_01_A):\n",
    "    '''Given a word vector, shows the probability for objective and subjective semantic content, respectively.'''\n",
    "    new_entry = nlp_getVector(word)\n",
    "    if new_entry:\n",
    "        vector = nlp_getVector(word)[1]\n",
    "        vector = np.expand_dims(vector, axis=0)\n",
    "        result = model.predict(vector)\n",
    "    else:\n",
    "        print('Word not existent in database.')\n",
    "        return\n",
    "    print(f'--- {word}:\\n{result[0][0]*100:.2f} of objectivity\\n{result[0][1]*100:.2f} of subjectivity')\n",
    "    print(f'Model used: {model.name}')  # Print the name of the model\n",
    "    return\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 245,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1/1 [==============================] - 0s 22ms/step\n",
      "--- hippopotamus:\n",
      "97.07 of objectivity\n",
      "36.32 of subjectivity\n",
      "Model used: Model_01_A\n"
     ]
    }
   ],
   "source": [
    "## Experimenting with a trial word:\n",
    "trial_word = 'hippopotamus'\n",
    "wordClassifier_regression(trial_word)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 246,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1/1 [==============================] - 0s 33ms/step\n",
      "--- I dreamed with a hippopotamus:\n",
      "88.17 of objectivity\n",
      "48.70 of subjectivity\n",
      "Model used: Model_01_A\n"
     ]
    }
   ],
   "source": [
    "## Experimenting with a trial word:\n",
    "trial_phrase = 'I dreamed with a hippopotamus'\n",
    "wordClassifier_regression(trial_phrase)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 247,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>words</th>\n",
       "      <th>F_Objectivity</th>\n",
       "      <th>F_Subjectivity</th>\n",
       "      <th>F_Context</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>2387</th>\n",
       "      <td>hippopotamus</td>\n",
       "      <td>0.973593</td>\n",
       "      <td>0.377959</td>\n",
       "      <td>0.309328</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "             words  F_Objectivity  F_Subjectivity  F_Context\n",
       "2387  hippopotamus       0.973593        0.377959   0.309328"
      ]
     },
     "execution_count": 247,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "## Checking the same trial word in the original SCA dataset (if available)\n",
    "df_factors[df_factors['words']==trial_word]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Training with data (D)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 248,
   "metadata": {},
   "outputs": [],
   "source": [
    "### Model 01: MLP architecture for continuous output:\n",
    "model_01_D = Sequential([\n",
    "    Dense(256, activation='relu', input_shape=(512,)),\n",
    "    Dense(128, activation='relu'),\n",
    "    Dense(64, activation='relu'),\n",
    "    Dense(32, activation='relu'),\n",
    "    # Dense(2, activation='linear')  # 2 neurons for continuous output between 0 and 1\n",
    "    Dense(2, activation='sigmoid')  # 2 neurons for continuous output between 0 and 1\n",
    "], name='Model_01_D')\n",
    "\n",
    "# Compile the model\n",
    "model_01_D.compile(optimizer='adam',\n",
    "              loss='mean_squared_error',\n",
    "              metrics=['accuracy'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 249,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"Model_01_D\"\n",
      "_________________________________________________________________\n",
      " Layer (type)                Output Shape              Param #   \n",
      "=================================================================\n",
      " dense_40 (Dense)            (None, 256)               131328    \n",
      "                                                                 \n",
      " dense_41 (Dense)            (None, 128)               32896     \n",
      "                                                                 \n",
      " dense_42 (Dense)            (None, 64)                8256      \n",
      "                                                                 \n",
      " dense_43 (Dense)            (None, 32)                2080      \n",
      "                                                                 \n",
      " dense_44 (Dense)            (None, 2)                 66        \n",
      "                                                                 \n",
      "=================================================================\n",
      "Total params: 174626 (682.13 KB)\n",
      "Trainable params: 174626 (682.13 KB)\n",
      "Non-trainable params: 0 (0.00 Byte)\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "# Print model summary\n",
    "model_01_D.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 250,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[0.15859087 0.58544867]\n",
      " [0.14240247 0.50945274]\n",
      " [0.95035619 0.17403995]\n",
      " [0.21539557 0.31870239]\n",
      " [0.25452163 0.66739412]]\n"
     ]
    }
   ],
   "source": [
    "## Transforming data into numpy array:\n",
    "Y_train_D_array = Y_train_D.to_numpy()\n",
    "Y_test_D_array = Y_test_D.to_numpy()\n",
    "\n",
    "X_train_D_array = X_train_D.to_numpy()\n",
    "X_test_D_array = X_test_D.to_numpy()\n",
    "\n",
    "# Print the first few elements to verify\n",
    "print(Y_train_D_array[:5])  # Print the first 5 elements\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 251,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/50\n",
      "131/131 [==============================] - 2s 4ms/step - loss: 0.0294 - accuracy: 0.8062 - val_loss: 0.0167 - val_accuracy: 0.8929\n",
      "Epoch 2/50\n",
      "131/131 [==============================] - 0s 2ms/step - loss: 0.0128 - accuracy: 0.9163 - val_loss: 0.0133 - val_accuracy: 0.9235\n",
      "Epoch 3/50\n",
      "131/131 [==============================] - 0s 2ms/step - loss: 0.0105 - accuracy: 0.9234 - val_loss: 0.0141 - val_accuracy: 0.9140\n",
      "Epoch 4/50\n",
      "131/131 [==============================] - 0s 2ms/step - loss: 0.0084 - accuracy: 0.9373 - val_loss: 0.0133 - val_accuracy: 0.9197\n",
      "Epoch 5/50\n",
      "131/131 [==============================] - 0s 2ms/step - loss: 0.0064 - accuracy: 0.9493 - val_loss: 0.0129 - val_accuracy: 0.9120\n",
      "Epoch 6/50\n",
      "131/131 [==============================] - 0s 2ms/step - loss: 0.0047 - accuracy: 0.9617 - val_loss: 0.0138 - val_accuracy: 0.9178\n",
      "Epoch 7/50\n",
      "131/131 [==============================] - 0s 2ms/step - loss: 0.0039 - accuracy: 0.9617 - val_loss: 0.0135 - val_accuracy: 0.9044\n",
      "Epoch 8/50\n",
      "131/131 [==============================] - 0s 2ms/step - loss: 0.0029 - accuracy: 0.9722 - val_loss: 0.0128 - val_accuracy: 0.9140\n",
      "Epoch 9/50\n",
      "131/131 [==============================] - 0s 2ms/step - loss: 0.0023 - accuracy: 0.9746 - val_loss: 0.0134 - val_accuracy: 0.9082\n",
      "Epoch 10/50\n",
      "131/131 [==============================] - 0s 2ms/step - loss: 0.0019 - accuracy: 0.9761 - val_loss: 0.0136 - val_accuracy: 0.9120\n",
      "Epoch 11/50\n",
      "131/131 [==============================] - 0s 2ms/step - loss: 0.0017 - accuracy: 0.9761 - val_loss: 0.0133 - val_accuracy: 0.9120\n",
      "Epoch 12/50\n",
      "131/131 [==============================] - 0s 3ms/step - loss: 0.0014 - accuracy: 0.9828 - val_loss: 0.0130 - val_accuracy: 0.9082\n",
      "Epoch 13/50\n",
      "131/131 [==============================] - 0s 2ms/step - loss: 0.0013 - accuracy: 0.9770 - val_loss: 0.0135 - val_accuracy: 0.9082\n",
      "Epoch 14/50\n",
      "131/131 [==============================] - 0s 2ms/step - loss: 0.0012 - accuracy: 0.9823 - val_loss: 0.0131 - val_accuracy: 0.9044\n",
      "Epoch 15/50\n",
      "131/131 [==============================] - 0s 2ms/step - loss: 0.0011 - accuracy: 0.9804 - val_loss: 0.0130 - val_accuracy: 0.9044\n",
      "Epoch 16/50\n",
      "131/131 [==============================] - 0s 2ms/step - loss: 9.3650e-04 - accuracy: 0.9837 - val_loss: 0.0132 - val_accuracy: 0.9140\n",
      "Epoch 17/50\n",
      "131/131 [==============================] - 0s 2ms/step - loss: 8.0676e-04 - accuracy: 0.9804 - val_loss: 0.0127 - val_accuracy: 0.9044\n",
      "Epoch 18/50\n",
      "131/131 [==============================] - 0s 3ms/step - loss: 6.6790e-04 - accuracy: 0.9871 - val_loss: 0.0133 - val_accuracy: 0.9101\n",
      "Epoch 19/50\n",
      "131/131 [==============================] - 0s 2ms/step - loss: 6.1451e-04 - accuracy: 0.9828 - val_loss: 0.0133 - val_accuracy: 0.9006\n",
      "Epoch 20/50\n",
      "131/131 [==============================] - 0s 2ms/step - loss: 6.3398e-04 - accuracy: 0.9804 - val_loss: 0.0132 - val_accuracy: 0.9044\n",
      "Epoch 21/50\n",
      "131/131 [==============================] - 0s 2ms/step - loss: 6.1349e-04 - accuracy: 0.9876 - val_loss: 0.0134 - val_accuracy: 0.9120\n",
      "Epoch 22/50\n",
      "131/131 [==============================] - 0s 2ms/step - loss: 5.8037e-04 - accuracy: 0.9794 - val_loss: 0.0132 - val_accuracy: 0.9140\n",
      "Epoch 23/50\n",
      "131/131 [==============================] - 0s 2ms/step - loss: 5.2205e-04 - accuracy: 0.9885 - val_loss: 0.0133 - val_accuracy: 0.9006\n",
      "Epoch 24/50\n",
      "131/131 [==============================] - 0s 2ms/step - loss: 5.7121e-04 - accuracy: 0.9852 - val_loss: 0.0127 - val_accuracy: 0.9101\n",
      "Epoch 25/50\n",
      "131/131 [==============================] - 0s 2ms/step - loss: 6.2452e-04 - accuracy: 0.9789 - val_loss: 0.0132 - val_accuracy: 0.9063\n",
      "Epoch 26/50\n",
      "131/131 [==============================] - 0s 2ms/step - loss: 5.4037e-04 - accuracy: 0.9813 - val_loss: 0.0131 - val_accuracy: 0.9140\n",
      "Epoch 27/50\n",
      "131/131 [==============================] - 0s 2ms/step - loss: 5.1181e-04 - accuracy: 0.9885 - val_loss: 0.0132 - val_accuracy: 0.9082\n",
      "Epoch 28/50\n",
      "131/131 [==============================] - 0s 2ms/step - loss: 4.6013e-04 - accuracy: 0.9861 - val_loss: 0.0128 - val_accuracy: 0.9082\n",
      "Epoch 29/50\n",
      "131/131 [==============================] - 0s 2ms/step - loss: 3.8978e-04 - accuracy: 0.9885 - val_loss: 0.0128 - val_accuracy: 0.9101\n",
      "Epoch 30/50\n",
      "131/131 [==============================] - 0s 2ms/step - loss: 3.7622e-04 - accuracy: 0.9876 - val_loss: 0.0130 - val_accuracy: 0.9101\n",
      "Epoch 31/50\n",
      "131/131 [==============================] - 0s 2ms/step - loss: 4.0393e-04 - accuracy: 0.9837 - val_loss: 0.0130 - val_accuracy: 0.9082\n",
      "Epoch 32/50\n",
      "131/131 [==============================] - 0s 2ms/step - loss: 5.0111e-04 - accuracy: 0.9852 - val_loss: 0.0128 - val_accuracy: 0.9120\n",
      "Epoch 33/50\n",
      "131/131 [==============================] - 0s 2ms/step - loss: 5.5450e-04 - accuracy: 0.9847 - val_loss: 0.0129 - val_accuracy: 0.9082\n",
      "Epoch 34/50\n",
      "131/131 [==============================] - 0s 2ms/step - loss: 5.3807e-04 - accuracy: 0.9823 - val_loss: 0.0130 - val_accuracy: 0.9140\n",
      "Epoch 35/50\n",
      "131/131 [==============================] - 0s 2ms/step - loss: 4.9987e-04 - accuracy: 0.9866 - val_loss: 0.0138 - val_accuracy: 0.9120\n",
      "Epoch 36/50\n",
      "131/131 [==============================] - 0s 2ms/step - loss: 5.2746e-04 - accuracy: 0.9861 - val_loss: 0.0132 - val_accuracy: 0.9120\n",
      "Epoch 37/50\n",
      "131/131 [==============================] - 0s 2ms/step - loss: 4.7279e-04 - accuracy: 0.9861 - val_loss: 0.0129 - val_accuracy: 0.9025\n",
      "Epoch 38/50\n",
      "131/131 [==============================] - 0s 2ms/step - loss: 3.4616e-04 - accuracy: 0.9847 - val_loss: 0.0129 - val_accuracy: 0.9178\n",
      "Epoch 39/50\n",
      "131/131 [==============================] - 0s 2ms/step - loss: 3.1951e-04 - accuracy: 0.9876 - val_loss: 0.0129 - val_accuracy: 0.9063\n",
      "Epoch 40/50\n",
      "131/131 [==============================] - 0s 2ms/step - loss: 2.7239e-04 - accuracy: 0.9895 - val_loss: 0.0126 - val_accuracy: 0.9140\n",
      "Epoch 41/50\n",
      "131/131 [==============================] - 0s 2ms/step - loss: 2.8287e-04 - accuracy: 0.9880 - val_loss: 0.0128 - val_accuracy: 0.9063\n",
      "Epoch 42/50\n",
      "131/131 [==============================] - 0s 3ms/step - loss: 3.1571e-04 - accuracy: 0.9852 - val_loss: 0.0130 - val_accuracy: 0.9063\n",
      "Epoch 43/50\n",
      "131/131 [==============================] - 0s 2ms/step - loss: 3.5755e-04 - accuracy: 0.9871 - val_loss: 0.0129 - val_accuracy: 0.9120\n",
      "Epoch 44/50\n",
      "131/131 [==============================] - 0s 2ms/step - loss: 3.7296e-04 - accuracy: 0.9852 - val_loss: 0.0125 - val_accuracy: 0.9120\n",
      "Epoch 45/50\n",
      "131/131 [==============================] - 0s 2ms/step - loss: 3.4335e-04 - accuracy: 0.9890 - val_loss: 0.0127 - val_accuracy: 0.9120\n",
      "Epoch 46/50\n",
      "131/131 [==============================] - 0s 2ms/step - loss: 3.6552e-04 - accuracy: 0.9871 - val_loss: 0.0130 - val_accuracy: 0.9063\n",
      "Epoch 47/50\n",
      "131/131 [==============================] - 0s 3ms/step - loss: 3.8305e-04 - accuracy: 0.9895 - val_loss: 0.0129 - val_accuracy: 0.9006\n",
      "Epoch 48/50\n",
      "131/131 [==============================] - 0s 2ms/step - loss: 3.8543e-04 - accuracy: 0.9885 - val_loss: 0.0127 - val_accuracy: 0.9178\n",
      "Epoch 49/50\n",
      "131/131 [==============================] - 0s 2ms/step - loss: 3.4670e-04 - accuracy: 0.9809 - val_loss: 0.0128 - val_accuracy: 0.9082\n",
      "Epoch 50/50\n",
      "131/131 [==============================] - 0s 2ms/step - loss: 3.1296e-04 - accuracy: 0.9909 - val_loss: 0.0129 - val_accuracy: 0.9120\n"
     ]
    }
   ],
   "source": [
    "# Train the model\n",
    "history_01_D = model_01_D.fit(X_train_D_array, Y_train_D_array, epochs=50, batch_size=16, validation_split=0.2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 252,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " 1/28 [>.............................] - ETA: 0s - loss: 0.0118 - accuracy: 0.9375"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "28/28 [==============================] - 0s 1ms/step - loss: 0.0126 - accuracy: 0.9173\n",
      "Model_01_D: Test Accuracy: 91.73%\n"
     ]
    }
   ],
   "source": [
    "# Evaluate the model\n",
    "loss_01_D, accuracy_01_D = model_01_D.evaluate(X_test_D_array, Y_test_D_array)\n",
    "print(f\"{model_01_D.name}: Test Accuracy: {accuracy_01_D * 100:.2f}%\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Applying Model_01_D to predict SCA for a distinct word:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 253,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1/1 [==============================] - 0s 24ms/step\n",
      "--- child:\n",
      "89.19 of objectivity\n",
      "35.50 of subjectivity\n",
      "Model used: Model_01_A\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1/1 [==============================] - 0s 78ms/step\n",
      "--- child:\n",
      "89.17 of objectivity\n",
      "38.36 of subjectivity\n",
      "Model used: Model_01_D\n"
     ]
    }
   ],
   "source": [
    "## Experimenting with a trial word:\n",
    "trial_word = 'child'\n",
    "wordClassifier_regression(trial_word, model=model_01_A)\n",
    "wordClassifier_regression(trial_word, model=model_01_D)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 254,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>words</th>\n",
       "      <th>F_Objectivity</th>\n",
       "      <th>F_Subjectivity</th>\n",
       "      <th>F_Context</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>859</th>\n",
       "      <td>child</td>\n",
       "      <td>0.90202</td>\n",
       "      <td>0.380464</td>\n",
       "      <td>0.233721</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "     words  F_Objectivity  F_Subjectivity  F_Context\n",
       "859  child        0.90202        0.380464   0.233721"
      ]
     },
     "execution_count": 254,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "## Checking the same trial word in the original SCA dataset (if available)\n",
    "df_factors[df_factors['words']==trial_word]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Optimizing model_01_D by adding dropout layers and regularization\n",
    "- Model_01_D2  \n",
    "\n",
    "> Dropout: Introduce dropout layers to reduce overfitting. Dropout randomly sets a fraction of input units to 0 at each update during training, which helps in preventing overfitting.\n",
    "\n",
    "> L2 Regularization: Add L2 regularization to the layers to penalize large weights, which can help in controlling overfitting.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 268,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Model architecture improvement:\n",
    "from tensorflow.keras.layers import Dropout\n",
    "from tensorflow.keras.layers import Dense\n",
    "from tensorflow.keras.regularizers import l2\n",
    "from tensorflow.keras.optimizers import Adam\n",
    "\n",
    "## Model training improvement:\n",
    "from tensorflow.keras.callbacks import EarlyStopping, ReduceLROnPlateau"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 271,
   "metadata": {},
   "outputs": [],
   "source": [
    "### Model 01: MLP architecture for continuous output:\n",
    "model_01_D2 = Sequential([\n",
    "    Dense(256, activation='relu', kernel_regularizer=l2(0.0001), input_shape=(512,)),\n",
    "    Dropout(0.05),\n",
    "    Dense(128, activation='relu', kernel_regularizer=l2(0.0001)),\n",
    "    Dropout(0.05),\n",
    "    Dense(64, activation='relu', kernel_regularizer=l2(0.0001)),\n",
    "    Dropout(0.05),\n",
    "    Dense(32, activation='relu', kernel_regularizer=l2(0.0001)),\n",
    "    Dropout(0.05),\n",
    "    # Dense(2, activation='linear')  # 2 neurons for continuous output between 0 and 1\n",
    "    Dense(2, activation='sigmoid')  # 2 neurons for continuous output between 0 and 1\n",
    "], name='Model_01_D2_regularized')\n",
    "\n",
    "# Compile the model\n",
    "optimizer = Adam(learning_rate=0.0005) #Adjustable value\n",
    "\n",
    "model_01_D2.compile(optimizer=optimizer,\n",
    "              loss='mean_squared_error',\n",
    "              metrics=['accuracy'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 272,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"Model_01_D2_regularized\"\n",
      "_________________________________________________________________\n",
      " Layer (type)                Output Shape              Param #   \n",
      "=================================================================\n",
      " dense_57 (Dense)            (None, 256)               131328    \n",
      "                                                                 \n",
      " dropout_4 (Dropout)         (None, 256)               0         \n",
      "                                                                 \n",
      " dense_58 (Dense)            (None, 128)               32896     \n",
      "                                                                 \n",
      " dropout_5 (Dropout)         (None, 128)               0         \n",
      "                                                                 \n",
      " dense_59 (Dense)            (None, 64)                8256      \n",
      "                                                                 \n",
      " dropout_6 (Dropout)         (None, 64)                0         \n",
      "                                                                 \n",
      " dense_60 (Dense)            (None, 32)                2080      \n",
      "                                                                 \n",
      " dropout_7 (Dropout)         (None, 32)                0         \n",
      "                                                                 \n",
      " dense_61 (Dense)            (None, 2)                 66        \n",
      "                                                                 \n",
      "=================================================================\n",
      "Total params: 174626 (682.13 KB)\n",
      "Trainable params: 174626 (682.13 KB)\n",
      "Non-trainable params: 0 (0.00 Byte)\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "# Print model summary\n",
    "model_01_D2.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 273,
   "metadata": {},
   "outputs": [],
   "source": [
    "## This model will be trained over the same Y_train_D_array previously defined..."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 276,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/100\n",
      "  1/131 [..............................] - ETA: 0s - loss: 0.0088 - accuracy: 0.9375"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "131/131 [==============================] - 0s 3ms/step - loss: 0.0084 - accuracy: 0.9603 - val_loss: 0.0178 - val_accuracy: 0.9216 - lr: 5.0000e-04\n",
      "Epoch 2/100\n",
      "131/131 [==============================] - 0s 3ms/step - loss: 0.0087 - accuracy: 0.9574 - val_loss: 0.0182 - val_accuracy: 0.9101 - lr: 5.0000e-04\n",
      "Epoch 3/100\n",
      "131/131 [==============================] - 0s 3ms/step - loss: 0.0084 - accuracy: 0.9565 - val_loss: 0.0188 - val_accuracy: 0.9101 - lr: 5.0000e-04\n",
      "Epoch 4/100\n",
      "131/131 [==============================] - 0s 3ms/step - loss: 0.0085 - accuracy: 0.9622 - val_loss: 0.0185 - val_accuracy: 0.9006 - lr: 5.0000e-04\n",
      "Epoch 5/100\n",
      "131/131 [==============================] - 0s 3ms/step - loss: 0.0083 - accuracy: 0.9665 - val_loss: 0.0183 - val_accuracy: 0.8891 - lr: 5.0000e-04\n",
      "Epoch 6/100\n",
      "131/131 [==============================] - 0s 3ms/step - loss: 0.0083 - accuracy: 0.9574 - val_loss: 0.0181 - val_accuracy: 0.9178 - lr: 5.0000e-04\n",
      "Epoch 7/100\n",
      "131/131 [==============================] - 0s 3ms/step - loss: 0.0081 - accuracy: 0.9617 - val_loss: 0.0183 - val_accuracy: 0.9273 - lr: 5.0000e-04\n",
      "Epoch 8/100\n",
      "131/131 [==============================] - 0s 3ms/step - loss: 0.0082 - accuracy: 0.9608 - val_loss: 0.0179 - val_accuracy: 0.9178 - lr: 5.0000e-04\n",
      "Epoch 9/100\n",
      "131/131 [==============================] - 0s 3ms/step - loss: 0.0079 - accuracy: 0.9612 - val_loss: 0.0187 - val_accuracy: 0.9159 - lr: 5.0000e-04\n",
      "Epoch 10/100\n",
      "131/131 [==============================] - 0s 3ms/step - loss: 0.0084 - accuracy: 0.9617 - val_loss: 0.0180 - val_accuracy: 0.9063 - lr: 5.0000e-04\n",
      "Epoch 11/100\n",
      "131/131 [==============================] - 0s 3ms/step - loss: 0.0081 - accuracy: 0.9569 - val_loss: 0.0177 - val_accuracy: 0.9101 - lr: 5.0000e-04\n",
      "Epoch 12/100\n",
      "131/131 [==============================] - 0s 2ms/step - loss: 0.0080 - accuracy: 0.9603 - val_loss: 0.0177 - val_accuracy: 0.9101 - lr: 5.0000e-04\n",
      "Epoch 13/100\n",
      "131/131 [==============================] - 0s 3ms/step - loss: 0.0079 - accuracy: 0.9560 - val_loss: 0.0180 - val_accuracy: 0.9101 - lr: 5.0000e-04\n",
      "Epoch 14/100\n",
      "131/131 [==============================] - 0s 2ms/step - loss: 0.0081 - accuracy: 0.9622 - val_loss: 0.0183 - val_accuracy: 0.9159 - lr: 5.0000e-04\n",
      "Epoch 15/100\n",
      "131/131 [==============================] - 0s 3ms/step - loss: 0.0080 - accuracy: 0.9612 - val_loss: 0.0179 - val_accuracy: 0.9082 - lr: 5.0000e-04\n",
      "Epoch 16/100\n",
      "131/131 [==============================] - 0s 3ms/step - loss: 0.0079 - accuracy: 0.9617 - val_loss: 0.0181 - val_accuracy: 0.9101 - lr: 5.0000e-04\n",
      "Epoch 17/100\n",
      "131/131 [==============================] - 0s 3ms/step - loss: 0.0080 - accuracy: 0.9584 - val_loss: 0.0180 - val_accuracy: 0.9101 - lr: 5.0000e-04\n",
      "Epoch 18/100\n",
      "131/131 [==============================] - 0s 3ms/step - loss: 0.0080 - accuracy: 0.9603 - val_loss: 0.0189 - val_accuracy: 0.9120 - lr: 5.0000e-04\n",
      "Epoch 19/100\n",
      "131/131 [==============================] - 0s 3ms/step - loss: 0.0080 - accuracy: 0.9579 - val_loss: 0.0174 - val_accuracy: 0.9063 - lr: 5.0000e-04\n",
      "Epoch 20/100\n",
      "131/131 [==============================] - 0s 3ms/step - loss: 0.0078 - accuracy: 0.9598 - val_loss: 0.0178 - val_accuracy: 0.9178 - lr: 5.0000e-04\n",
      "Epoch 21/100\n",
      "131/131 [==============================] - 0s 3ms/step - loss: 0.0081 - accuracy: 0.9569 - val_loss: 0.0174 - val_accuracy: 0.9140 - lr: 5.0000e-04\n",
      "Epoch 22/100\n",
      "131/131 [==============================] - 0s 3ms/step - loss: 0.0079 - accuracy: 0.9603 - val_loss: 0.0181 - val_accuracy: 0.8987 - lr: 5.0000e-04\n",
      "Epoch 23/100\n",
      "131/131 [==============================] - 0s 3ms/step - loss: 0.0077 - accuracy: 0.9612 - val_loss: 0.0176 - val_accuracy: 0.9120 - lr: 5.0000e-04\n",
      "Epoch 24/100\n",
      "131/131 [==============================] - 0s 3ms/step - loss: 0.0078 - accuracy: 0.9627 - val_loss: 0.0183 - val_accuracy: 0.9312 - lr: 5.0000e-04\n",
      "Epoch 25/100\n",
      "131/131 [==============================] - 0s 3ms/step - loss: 0.0077 - accuracy: 0.9612 - val_loss: 0.0184 - val_accuracy: 0.9120 - lr: 5.0000e-04\n",
      "Epoch 26/100\n",
      "131/131 [==============================] - 0s 3ms/step - loss: 0.0078 - accuracy: 0.9584 - val_loss: 0.0185 - val_accuracy: 0.9120 - lr: 5.0000e-04\n",
      "Epoch 27/100\n",
      "131/131 [==============================] - 0s 3ms/step - loss: 0.0076 - accuracy: 0.9679 - val_loss: 0.0181 - val_accuracy: 0.9082 - lr: 5.0000e-04\n",
      "Epoch 28/100\n",
      "131/131 [==============================] - 0s 3ms/step - loss: 0.0076 - accuracy: 0.9670 - val_loss: 0.0178 - val_accuracy: 0.9120 - lr: 5.0000e-04\n",
      "Epoch 29/100\n",
      "105/131 [=======================>......] - ETA: 0s - loss: 0.0076 - accuracy: 0.9565Restoring model weights from the end of the best epoch: 19.\n",
      "131/131 [==============================] - 0s 3ms/step - loss: 0.0076 - accuracy: 0.9545 - val_loss: 0.0180 - val_accuracy: 0.9101 - lr: 5.0000e-04\n",
      "Epoch 29: early stopping\n"
     ]
    }
   ],
   "source": [
    "# Train the model\n",
    "early_stopping = EarlyStopping(monitor='val_loss', patience=10, verbose=1, mode='min', restore_best_weights=True)\n",
    "reduce_lr = ReduceLROnPlateau(monitor='val_loss', factor=0.2, patience=5, min_lr=0.001)\n",
    "\n",
    "history_01_D2 = model_01_D2.fit(\n",
    "    X_train_D_array, \n",
    "    Y_train_D_array, \n",
    "    epochs=100, \n",
    "    batch_size=16, \n",
    "    validation_split=0.2,\n",
    "    callbacks=[early_stopping, reduce_lr])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 277,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "28/28 [==============================] - 0s 1ms/step - loss: 0.0162 - accuracy: 0.9127\n",
      "Model_01_D2_regularized: Test Accuracy: 91.27%\n"
     ]
    }
   ],
   "source": [
    "# Evaluate the model\n",
    "loss_01_D2, accuracy_01_D2 = model_01_D2.evaluate(X_test_D_array, Y_test_D_array)\n",
    "print(f\"{model_01_D2.name}: Test Accuracy: {accuracy_01_D2 * 100:.2f}%\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Applying Model_01_D2 to predict SCA for a distinct word:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 278,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1/1 [==============================] - 0s 30ms/step\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "--- child:\n",
      "89.19 of objectivity\n",
      "35.50 of subjectivity\n",
      "Model used: Model_01_A\n",
      "1/1 [==============================] - 0s 32ms/step\n",
      "--- child:\n",
      "89.17 of objectivity\n",
      "38.36 of subjectivity\n",
      "Model used: Model_01_D\n",
      "1/1 [==============================] - 0s 66ms/step\n",
      "--- child:\n",
      "89.16 of objectivity\n",
      "38.32 of subjectivity\n",
      "Model used: Model_01_D2_regularized\n"
     ]
    }
   ],
   "source": [
    "## Experimenting with a trial word:\n",
    "trial_word = 'child'\n",
    "wordClassifier_regression(trial_word, model=model_01_A)\n",
    "wordClassifier_regression(trial_word, model=model_01_D)\n",
    "wordClassifier_regression(trial_word, model=model_01_D2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 279,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>words</th>\n",
       "      <th>F_Objectivity</th>\n",
       "      <th>F_Subjectivity</th>\n",
       "      <th>F_Context</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>859</th>\n",
       "      <td>child</td>\n",
       "      <td>0.90202</td>\n",
       "      <td>0.380464</td>\n",
       "      <td>0.233721</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "     words  F_Objectivity  F_Subjectivity  F_Context\n",
       "859  child        0.90202        0.380464   0.233721"
      ]
     },
     "execution_count": 279,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "## Checking the same trial word in the original SCA dataset (if available)\n",
    "df_factors[df_factors['words']==trial_word]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 285,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1/1 [==============================] - 0s 22ms/step\n",
      "--- The child is having fun in the park:\n",
      "95.21 of objectivity\n",
      "84.46 of subjectivity\n",
      "Model used: Model_01_A\n",
      "1/1 [==============================] - 0s 22ms/step\n",
      "--- The child is having fun in the park:\n",
      "83.52 of objectivity\n",
      "91.22 of subjectivity\n",
      "Model used: Model_01_D\n",
      "1/1 [==============================] - 0s 42ms/step\n",
      "--- The child is having fun in the park:\n",
      "70.95 of objectivity\n",
      "79.92 of subjectivity\n",
      "Model used: Model_01_D2_regularized\n"
     ]
    }
   ],
   "source": [
    "## Now comparing the results for a sentence:\n",
    "trial_sentence = 'The child is having fun in the park'\n",
    "wordClassifier_regression(trial_sentence, model=model_01_A)\n",
    "wordClassifier_regression(trial_sentence, model=model_01_D)\n",
    "wordClassifier_regression(trial_sentence, model=model_01_D2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "----\n",
    "## Model 02: MLP for multilabel classification:\n",
    "- Multilayer perceptron with a single multilabel output (softmax activation) based on SCA\n",
    "- Output: Objective (0) or Subjective (1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 286,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.preprocessing import OneHotEncoder"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Training with data (C)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 287,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Preparing data:\n",
    "encoder_oneHot_C = OneHotEncoder()\n",
    "Y_train_C_encoded = encoder_oneHot_C.fit_transform(Y_train_C[['target']]).toarray()\n",
    "Y_test_C_encoded = encoder_oneHot_C.fit_transform(Y_test_C[['target']]).toarray()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 288,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Transforming data into numpy array -- Y data was transformed above:\n",
    "X_train_C_array = X_train_C.to_numpy()\n",
    "X_test_C_array = X_test_C.to_numpy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 294,
   "metadata": {},
   "outputs": [],
   "source": [
    "# model_02_C = Sequential([\n",
    "#     Dense(480, activation='relu', input_shape=(X_train_C.shape[1],)), ## Equivalent to input_shape=(300,)\n",
    "#     Dense(300, activation='relu'),\n",
    "#     Dense(150, activation='relu'),\n",
    "#     Dense(100, activation='relu'),\n",
    "#     Dense(64, activation='relu'),\n",
    "#     Dense(32, activation='relu'),\n",
    "#     Dense(Y_train_C_encoded.shape[1], activation='softmax')  # Output layer with softmax activation for multiclass classification\n",
    "# ], name='Model_02_C')\n",
    "\n",
    "# # Compile the model\n",
    "# model_02_C.compile(optimizer='adam',\n",
    "#                  loss='categorical_crossentropy', \n",
    "#                  metrics=['accuracy'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 311,
   "metadata": {},
   "outputs": [],
   "source": [
    "### Model 02: MLP architecture for multiclass output:\n",
    "model_02_C = Sequential([\n",
    "    Dense(512, activation='relu', kernel_regularizer=l2(0.0005), input_shape=(512,)),\n",
    "    Dropout(0.1),\n",
    "    Dense(256, activation='relu', kernel_regularizer=l2(0.0005)),\n",
    "    Dropout(0.1),\n",
    "    Dense(128, activation='relu', kernel_regularizer=l2(0.0005)),\n",
    "    Dropout(0.1),\n",
    "    Dense(64, activation='relu', kernel_regularizer=l2(0.0005)),\n",
    "    Dropout(0.1),\n",
    "    Dense(32, activation='relu', kernel_regularizer=l2(0.0005)),\n",
    "    Dropout(0.1),\n",
    "    Dense(Y_train_C_encoded.shape[1], activation='softmax')  # Output layer with softmax activation for multiclass classification\n",
    "], name='Model_02_C_regularized')\n",
    "\n",
    "# Compile the model\n",
    "optimizer = Adam(learning_rate=0.0005) #Adjustable value\n",
    "\n",
    "model_02_C.compile(optimizer=optimizer,\n",
    "              loss='categorical_crossentropy',\n",
    "              metrics=['accuracy'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 312,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"Model_02_C_regularized\"\n",
      "_________________________________________________________________\n",
      " Layer (type)                Output Shape              Param #   \n",
      "=================================================================\n",
      " dense_93 (Dense)            (None, 512)               262656    \n",
      "                                                                 \n",
      " dropout_28 (Dropout)        (None, 512)               0         \n",
      "                                                                 \n",
      " dense_94 (Dense)            (None, 256)               131328    \n",
      "                                                                 \n",
      " dropout_29 (Dropout)        (None, 256)               0         \n",
      "                                                                 \n",
      " dense_95 (Dense)            (None, 128)               32896     \n",
      "                                                                 \n",
      " dropout_30 (Dropout)        (None, 128)               0         \n",
      "                                                                 \n",
      " dense_96 (Dense)            (None, 64)                8256      \n",
      "                                                                 \n",
      " dropout_31 (Dropout)        (None, 64)                0         \n",
      "                                                                 \n",
      " dense_97 (Dense)            (None, 32)                2080      \n",
      "                                                                 \n",
      " dropout_32 (Dropout)        (None, 32)                0         \n",
      "                                                                 \n",
      " dense_98 (Dense)            (None, 4)                 132       \n",
      "                                                                 \n",
      "=================================================================\n",
      "Total params: 437348 (1.67 MB)\n",
      "Trainable params: 437348 (1.67 MB)\n",
      "Non-trainable params: 0 (0.00 Byte)\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "model_02_C.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 315,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/100\n",
      "  1/176 [..............................] - ETA: 1s - loss: 0.7310 - accuracy: 0.8125"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "176/176 [==============================] - 1s 6ms/step - loss: 0.8068 - accuracy: 0.7889 - val_loss: 0.9804 - val_accuracy: 0.6999 - lr: 1.0000e-04\n",
      "Epoch 2/100\n",
      "176/176 [==============================] - 1s 5ms/step - loss: 0.7609 - accuracy: 0.7989 - val_loss: 0.9844 - val_accuracy: 0.7055 - lr: 1.0000e-04\n",
      "Epoch 3/100\n",
      "176/176 [==============================] - 1s 5ms/step - loss: 0.7271 - accuracy: 0.8142 - val_loss: 1.0053 - val_accuracy: 0.7027 - lr: 1.0000e-04\n",
      "Epoch 4/100\n",
      "176/176 [==============================] - 1s 5ms/step - loss: 0.7053 - accuracy: 0.8238 - val_loss: 1.0230 - val_accuracy: 0.7041 - lr: 1.0000e-04\n",
      "Epoch 5/100\n",
      "176/176 [==============================] - 1s 5ms/step - loss: 0.6843 - accuracy: 0.8327 - val_loss: 1.0315 - val_accuracy: 0.6899 - lr: 1.0000e-04\n",
      "Epoch 6/100\n",
      "176/176 [==============================] - 1s 5ms/step - loss: 0.6570 - accuracy: 0.8466 - val_loss: 1.0582 - val_accuracy: 0.6842 - lr: 1.0000e-04\n",
      "Epoch 7/100\n",
      "176/176 [==============================] - 1s 6ms/step - loss: 0.6328 - accuracy: 0.8537 - val_loss: 1.0533 - val_accuracy: 0.6999 - lr: 1.0000e-04\n",
      "Epoch 8/100\n",
      "176/176 [==============================] - 1s 5ms/step - loss: 0.6168 - accuracy: 0.8619 - val_loss: 1.0762 - val_accuracy: 0.7013 - lr: 1.0000e-04\n",
      "Epoch 9/100\n",
      "176/176 [==============================] - 1s 5ms/step - loss: 0.5622 - accuracy: 0.8815 - val_loss: 1.1141 - val_accuracy: 0.6999 - lr: 1.0000e-04\n",
      "Epoch 10/100\n",
      "176/176 [==============================] - 1s 5ms/step - loss: 0.5412 - accuracy: 0.8928 - val_loss: 1.1416 - val_accuracy: 0.6942 - lr: 1.0000e-04\n",
      "Epoch 11/100\n",
      "176/176 [==============================] - 1s 5ms/step - loss: 0.5189 - accuracy: 0.9025 - val_loss: 1.1716 - val_accuracy: 0.6842 - lr: 1.0000e-04\n",
      "Epoch 12/100\n",
      "176/176 [==============================] - 1s 5ms/step - loss: 0.4872 - accuracy: 0.9167 - val_loss: 1.1935 - val_accuracy: 0.6799 - lr: 1.0000e-04\n",
      "Epoch 13/100\n",
      "176/176 [==============================] - 1s 5ms/step - loss: 0.4507 - accuracy: 0.9295 - val_loss: 1.2481 - val_accuracy: 0.6885 - lr: 1.0000e-04\n",
      "Epoch 14/100\n",
      "176/176 [==============================] - 1s 5ms/step - loss: 0.4223 - accuracy: 0.9405 - val_loss: 1.2818 - val_accuracy: 0.6899 - lr: 1.0000e-04\n",
      "Epoch 15/100\n",
      "176/176 [==============================] - 1s 5ms/step - loss: 0.3991 - accuracy: 0.9438 - val_loss: 1.3269 - val_accuracy: 0.6828 - lr: 1.0000e-04\n",
      "Epoch 16/100\n",
      "176/176 [==============================] - 1s 5ms/step - loss: 0.3774 - accuracy: 0.9537 - val_loss: 1.3880 - val_accuracy: 0.6757 - lr: 1.0000e-04\n",
      "Epoch 17/100\n",
      "176/176 [==============================] - 1s 5ms/step - loss: 0.3611 - accuracy: 0.9601 - val_loss: 1.4064 - val_accuracy: 0.6814 - lr: 1.0000e-04\n",
      "Epoch 18/100\n",
      "176/176 [==============================] - 1s 5ms/step - loss: 0.3397 - accuracy: 0.9687 - val_loss: 1.4775 - val_accuracy: 0.6871 - lr: 1.0000e-04\n",
      "Epoch 19/100\n",
      "176/176 [==============================] - 1s 5ms/step - loss: 0.3232 - accuracy: 0.9697 - val_loss: 1.5043 - val_accuracy: 0.6842 - lr: 1.0000e-04\n",
      "Epoch 20/100\n",
      "176/176 [==============================] - 1s 5ms/step - loss: 0.3128 - accuracy: 0.9744 - val_loss: 1.5410 - val_accuracy: 0.6828 - lr: 1.0000e-04\n",
      "Epoch 21/100\n",
      "176/176 [==============================] - 1s 5ms/step - loss: 0.2880 - accuracy: 0.9854 - val_loss: 1.6179 - val_accuracy: 0.6814 - lr: 1.0000e-04\n",
      "Epoch 22/100\n",
      "176/176 [==============================] - 1s 5ms/step - loss: 0.2850 - accuracy: 0.9815 - val_loss: 1.6623 - val_accuracy: 0.6714 - lr: 1.0000e-04\n",
      "Epoch 23/100\n",
      "176/176 [==============================] - 1s 5ms/step - loss: 0.2683 - accuracy: 0.9858 - val_loss: 1.6985 - val_accuracy: 0.6657 - lr: 1.0000e-04\n",
      "Epoch 24/100\n",
      "176/176 [==============================] - 1s 5ms/step - loss: 0.2664 - accuracy: 0.9868 - val_loss: 1.7684 - val_accuracy: 0.6686 - lr: 1.0000e-04\n",
      "Epoch 25/100\n",
      "176/176 [==============================] - 1s 5ms/step - loss: 0.2624 - accuracy: 0.9879 - val_loss: 1.7676 - val_accuracy: 0.6728 - lr: 1.0000e-04\n",
      "Epoch 26/100\n",
      "176/176 [==============================] - 1s 5ms/step - loss: 0.2514 - accuracy: 0.9915 - val_loss: 1.7487 - val_accuracy: 0.6828 - lr: 1.0000e-04\n",
      "Epoch 27/100\n",
      "176/176 [==============================] - 1s 5ms/step - loss: 0.2464 - accuracy: 0.9915 - val_loss: 1.7909 - val_accuracy: 0.6828 - lr: 1.0000e-04\n",
      "Epoch 28/100\n",
      "176/176 [==============================] - 1s 5ms/step - loss: 0.2374 - accuracy: 0.9936 - val_loss: 1.8357 - val_accuracy: 0.6714 - lr: 1.0000e-04\n",
      "Epoch 29/100\n",
      "176/176 [==============================] - 1s 5ms/step - loss: 0.2347 - accuracy: 0.9936 - val_loss: 1.8821 - val_accuracy: 0.6771 - lr: 1.0000e-04\n",
      "Epoch 30/100\n",
      "176/176 [==============================] - 1s 5ms/step - loss: 0.2280 - accuracy: 0.9957 - val_loss: 1.9043 - val_accuracy: 0.6714 - lr: 1.0000e-04\n",
      "Epoch 31/100\n",
      "165/176 [===========================>..] - ETA: 0s - loss: 0.2216 - accuracy: 0.9973Restoring model weights from the end of the best epoch: 1.\n",
      "176/176 [==============================] - 1s 5ms/step - loss: 0.2213 - accuracy: 0.9975 - val_loss: 1.9026 - val_accuracy: 0.6728 - lr: 1.0000e-04\n",
      "Epoch 31: early stopping\n"
     ]
    }
   ],
   "source": [
    "# Train the model\n",
    "early_stopping = EarlyStopping(monitor='val_loss', patience=30, verbose=1, mode='min', restore_best_weights=True)\n",
    "reduce_lr = ReduceLROnPlateau(monitor='val_loss', factor=0.001, patience=10, min_lr=0.0001)\n",
    "\n",
    "history_02_C = model_02_C.fit(\n",
    "    X_train_C_array, \n",
    "    Y_train_C_encoded, \n",
    "    epochs=100, \n",
    "    batch_size=16, \n",
    "    validation_split=0.2,\n",
    "    callbacks=[early_stopping, reduce_lr])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 316,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " 1/37 [..............................] - ETA: 1s - loss: 1.2221 - accuracy: 0.6875"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "37/37 [==============================] - 0s 2ms/step - loss: 0.9517 - accuracy: 0.7054\n",
      "Model_02_C_regularized: Test Accuracy: 70.54%\n"
     ]
    }
   ],
   "source": [
    "# Evaluate the model\n",
    "loss_02_C, accuracy_02_C = model_02_C.evaluate(X_test_C_array, Y_test_C_encoded)\n",
    "print(f\"{model_02_C.name}: Test Accuracy: {accuracy_02_C * 100:.2f}%\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Training with data (E)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 317,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Preparing data:\n",
    "encoder_oneHot_E = OneHotEncoder()\n",
    "Y_train_E_encoded = encoder_oneHot_E.fit_transform(Y_train_E[['target']]).toarray()\n",
    "Y_test_E_encoded = encoder_oneHot_E.fit_transform(Y_test_E[['target']]).toarray()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 318,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Transforming data into numpy array -- Y data was transformed above:\n",
    "X_train_E_array = X_train_E.to_numpy()\n",
    "X_test_E_array = X_test_E.to_numpy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 319,
   "metadata": {},
   "outputs": [],
   "source": [
    "# model_02_E = Sequential([\n",
    "#     Dense(300, activation='relu', input_shape=(X_train_E.shape[1],)), ## Equivalent to input_shape=(300,)\n",
    "#     Dense(150, activation='relu'),\n",
    "#     Dense(100, activation='relu'),\n",
    "#     Dense(64, activation='relu'),\n",
    "#     Dense(32, activation='relu'),\n",
    "#     Dense(Y_train_E_encoded.shape[1], activation='softmax')  # Output layer with softmax activation for multiclass classification\n",
    "# ], name='Model_02_E')\n",
    "\n",
    "# # Compile the model\n",
    "# model_02_E.compile(optimizer='adam',\n",
    "#                  loss='categorical_crossentropy', \n",
    "#                  metrics=['accuracy'])\n",
    "\n",
    "# model_02_E.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 320,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"Model_02_E_regularized\"\n",
      "_________________________________________________________________\n",
      " Layer (type)                Output Shape              Param #   \n",
      "=================================================================\n",
      " dense_99 (Dense)            (None, 512)               262656    \n",
      "                                                                 \n",
      " dropout_33 (Dropout)        (None, 512)               0         \n",
      "                                                                 \n",
      " dense_100 (Dense)           (None, 256)               131328    \n",
      "                                                                 \n",
      " dropout_34 (Dropout)        (None, 256)               0         \n",
      "                                                                 \n",
      " dense_101 (Dense)           (None, 128)               32896     \n",
      "                                                                 \n",
      " dropout_35 (Dropout)        (None, 128)               0         \n",
      "                                                                 \n",
      " dense_102 (Dense)           (None, 64)                8256      \n",
      "                                                                 \n",
      " dropout_36 (Dropout)        (None, 64)                0         \n",
      "                                                                 \n",
      " dense_103 (Dense)           (None, 32)                2080      \n",
      "                                                                 \n",
      " dropout_37 (Dropout)        (None, 32)                0         \n",
      "                                                                 \n",
      " dense_104 (Dense)           (None, 4)                 132       \n",
      "                                                                 \n",
      "=================================================================\n",
      "Total params: 437348 (1.67 MB)\n",
      "Trainable params: 437348 (1.67 MB)\n",
      "Non-trainable params: 0 (0.00 Byte)\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "model_02_E = Sequential([\n",
    "    Dense(512, activation='relu', kernel_regularizer=l2(0.0005), input_shape=(X_train_E.shape[1],)),\n",
    "    Dropout(0.1),\n",
    "    Dense(256, activation='relu', kernel_regularizer=l2(0.0005)),\n",
    "    Dropout(0.1),\n",
    "    Dense(128, activation='relu', kernel_regularizer=l2(0.0005)),\n",
    "    Dropout(0.1),\n",
    "    Dense(64, activation='relu', kernel_regularizer=l2(0.0005)),\n",
    "    Dropout(0.1),\n",
    "    Dense(32, activation='relu', kernel_regularizer=l2(0.0005)),\n",
    "    Dropout(0.1),\n",
    "    Dense(Y_train_C_encoded.shape[1], activation='softmax')  # Output layer with softmax activation for multiclass classification\n",
    "], name='Model_02_E_regularized')\n",
    "\n",
    "# Compile the model\n",
    "optimizer = Adam(learning_rate=0.0005) #Adjustable value\n",
    "\n",
    "model_02_E.compile(optimizer=optimizer,\n",
    "              loss='categorical_crossentropy',\n",
    "              metrics=['accuracy'])\n",
    "\n",
    "model_02_E.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 321,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/100\n",
      "131/131 [==============================] - 3s 6ms/step - loss: 1.5029 - accuracy: 0.5675 - val_loss: 1.1091 - val_accuracy: 0.6998 - lr: 5.0000e-04\n",
      "Epoch 2/100\n",
      "131/131 [==============================] - 1s 5ms/step - loss: 1.0150 - accuracy: 0.7225 - val_loss: 0.9178 - val_accuracy: 0.7591 - lr: 5.0000e-04\n",
      "Epoch 3/100\n",
      "131/131 [==============================] - 1s 4ms/step - loss: 0.8563 - accuracy: 0.7699 - val_loss: 0.9676 - val_accuracy: 0.7419 - lr: 5.0000e-04\n",
      "Epoch 4/100\n",
      "131/131 [==============================] - 1s 4ms/step - loss: 0.7629 - accuracy: 0.8144 - val_loss: 0.8969 - val_accuracy: 0.7553 - lr: 5.0000e-04\n",
      "Epoch 5/100\n",
      "131/131 [==============================] - 1s 4ms/step - loss: 0.6992 - accuracy: 0.8321 - val_loss: 0.8915 - val_accuracy: 0.7591 - lr: 5.0000e-04\n",
      "Epoch 6/100\n",
      "131/131 [==============================] - 1s 4ms/step - loss: 0.6180 - accuracy: 0.8617 - val_loss: 0.9175 - val_accuracy: 0.7457 - lr: 5.0000e-04\n",
      "Epoch 7/100\n",
      "131/131 [==============================] - 1s 4ms/step - loss: 0.5831 - accuracy: 0.8708 - val_loss: 0.9268 - val_accuracy: 0.7476 - lr: 5.0000e-04\n",
      "Epoch 8/100\n",
      "131/131 [==============================] - 1s 4ms/step - loss: 0.5319 - accuracy: 0.9019 - val_loss: 0.9712 - val_accuracy: 0.7400 - lr: 5.0000e-04\n",
      "Epoch 9/100\n",
      "131/131 [==============================] - 1s 5ms/step - loss: 0.4708 - accuracy: 0.9201 - val_loss: 1.0143 - val_accuracy: 0.7419 - lr: 5.0000e-04\n",
      "Epoch 10/100\n",
      "131/131 [==============================] - 1s 4ms/step - loss: 0.4048 - accuracy: 0.9402 - val_loss: 1.0538 - val_accuracy: 0.7476 - lr: 5.0000e-04\n",
      "Epoch 11/100\n",
      "131/131 [==============================] - 1s 4ms/step - loss: 0.3769 - accuracy: 0.9569 - val_loss: 1.1224 - val_accuracy: 0.7247 - lr: 5.0000e-04\n",
      "Epoch 12/100\n",
      "131/131 [==============================] - 1s 4ms/step - loss: 0.3506 - accuracy: 0.9636 - val_loss: 1.1462 - val_accuracy: 0.7572 - lr: 5.0000e-04\n",
      "Epoch 13/100\n",
      "131/131 [==============================] - 1s 4ms/step - loss: 0.3390 - accuracy: 0.9694 - val_loss: 1.1858 - val_accuracy: 0.7533 - lr: 5.0000e-04\n",
      "Epoch 14/100\n",
      "131/131 [==============================] - 1s 4ms/step - loss: 0.3132 - accuracy: 0.9770 - val_loss: 1.3224 - val_accuracy: 0.7553 - lr: 5.0000e-04\n",
      "Epoch 15/100\n",
      "131/131 [==============================] - 1s 5ms/step - loss: 0.2806 - accuracy: 0.9837 - val_loss: 1.3771 - val_accuracy: 0.7572 - lr: 5.0000e-04\n",
      "Epoch 16/100\n",
      "131/131 [==============================] - 1s 5ms/step - loss: 0.2635 - accuracy: 0.9933 - val_loss: 1.3859 - val_accuracy: 0.7553 - lr: 1.0000e-04\n",
      "Epoch 17/100\n",
      "131/131 [==============================] - 1s 4ms/step - loss: 0.2472 - accuracy: 0.9976 - val_loss: 1.4182 - val_accuracy: 0.7572 - lr: 1.0000e-04\n",
      "Epoch 18/100\n",
      "131/131 [==============================] - 1s 5ms/step - loss: 0.2435 - accuracy: 0.9976 - val_loss: 1.4276 - val_accuracy: 0.7476 - lr: 1.0000e-04\n",
      "Epoch 19/100\n",
      "131/131 [==============================] - 1s 4ms/step - loss: 0.2388 - accuracy: 0.9981 - val_loss: 1.4698 - val_accuracy: 0.7572 - lr: 1.0000e-04\n",
      "Epoch 20/100\n",
      "131/131 [==============================] - 1s 4ms/step - loss: 0.2323 - accuracy: 0.9995 - val_loss: 1.4794 - val_accuracy: 0.7572 - lr: 1.0000e-04\n",
      "Epoch 21/100\n",
      "131/131 [==============================] - 1s 4ms/step - loss: 0.2287 - accuracy: 0.9990 - val_loss: 1.4910 - val_accuracy: 0.7533 - lr: 1.0000e-04\n",
      "Epoch 22/100\n",
      "131/131 [==============================] - 1s 5ms/step - loss: 0.2273 - accuracy: 0.9976 - val_loss: 1.4737 - val_accuracy: 0.7495 - lr: 1.0000e-04\n",
      "Epoch 23/100\n",
      "131/131 [==============================] - 1s 4ms/step - loss: 0.2255 - accuracy: 0.9981 - val_loss: 1.4889 - val_accuracy: 0.7572 - lr: 1.0000e-04\n",
      "Epoch 24/100\n",
      "131/131 [==============================] - 1s 4ms/step - loss: 0.2219 - accuracy: 0.9976 - val_loss: 1.5158 - val_accuracy: 0.7495 - lr: 1.0000e-04\n",
      "Epoch 25/100\n",
      "131/131 [==============================] - 1s 4ms/step - loss: 0.2183 - accuracy: 0.9981 - val_loss: 1.4795 - val_accuracy: 0.7514 - lr: 1.0000e-04\n",
      "Epoch 26/100\n",
      "131/131 [==============================] - 1s 4ms/step - loss: 0.2146 - accuracy: 0.9990 - val_loss: 1.4673 - val_accuracy: 0.7591 - lr: 1.0000e-04\n",
      "Epoch 27/100\n",
      "131/131 [==============================] - 1s 4ms/step - loss: 0.2103 - accuracy: 0.9986 - val_loss: 1.4554 - val_accuracy: 0.7667 - lr: 1.0000e-04\n",
      "Epoch 28/100\n",
      "131/131 [==============================] - 1s 4ms/step - loss: 0.2085 - accuracy: 0.9986 - val_loss: 1.4795 - val_accuracy: 0.7553 - lr: 1.0000e-04\n",
      "Epoch 29/100\n",
      "131/131 [==============================] - 1s 4ms/step - loss: 0.2036 - accuracy: 0.9990 - val_loss: 1.4636 - val_accuracy: 0.7533 - lr: 1.0000e-04\n",
      "Epoch 30/100\n",
      "131/131 [==============================] - 1s 5ms/step - loss: 0.2051 - accuracy: 0.9976 - val_loss: 1.4052 - val_accuracy: 0.7629 - lr: 1.0000e-04\n",
      "Epoch 31/100\n",
      "131/131 [==============================] - 1s 4ms/step - loss: 0.1982 - accuracy: 0.9990 - val_loss: 1.4483 - val_accuracy: 0.7553 - lr: 1.0000e-04\n",
      "Epoch 32/100\n",
      "131/131 [==============================] - 1s 5ms/step - loss: 0.1940 - accuracy: 0.9990 - val_loss: 1.4230 - val_accuracy: 0.7591 - lr: 1.0000e-04\n",
      "Epoch 33/100\n",
      "131/131 [==============================] - 1s 5ms/step - loss: 0.1915 - accuracy: 0.9986 - val_loss: 1.4043 - val_accuracy: 0.7629 - lr: 1.0000e-04\n",
      "Epoch 34/100\n",
      "131/131 [==============================] - 1s 4ms/step - loss: 0.1882 - accuracy: 0.9986 - val_loss: 1.4119 - val_accuracy: 0.7610 - lr: 1.0000e-04\n",
      "Epoch 35/100\n",
      "127/131 [============================>.] - ETA: 0s - loss: 0.1845 - accuracy: 0.9980Restoring model weights from the end of the best epoch: 5.\n",
      "131/131 [==============================] - 1s 5ms/step - loss: 0.1845 - accuracy: 0.9981 - val_loss: 1.4107 - val_accuracy: 0.7572 - lr: 1.0000e-04\n",
      "Epoch 35: early stopping\n"
     ]
    }
   ],
   "source": [
    "early_stopping = EarlyStopping(monitor='val_loss', patience=30, verbose=1, mode='min', restore_best_weights=True)\n",
    "reduce_lr = ReduceLROnPlateau(monitor='val_loss', factor=0.001, patience=10, min_lr=0.0001)\n",
    "\n",
    "history_02_E = model_02_E.fit(\n",
    "    X_train_E_array, \n",
    "    Y_train_E_encoded, \n",
    "    epochs=100, \n",
    "    batch_size=16, \n",
    "    validation_split=0.2,\n",
    "    callbacks=[early_stopping, reduce_lr])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 322,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "28/28 [==============================] - 0s 3ms/step - loss: 0.8177 - accuracy: 0.7933\n",
      "Model_02_E_regularized: Test Accuracy: 79.33%\n"
     ]
    }
   ],
   "source": [
    "# Evaluate the model\n",
    "loss_02_E, accuracy_02_E = model_02_E.evaluate(X_test_E_array, Y_test_E_encoded)\n",
    "print(f\"{model_02_E.name}: Test Accuracy: {accuracy_02_E * 100:.2f}%\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Exploring Model 02 results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 323,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Generalizing the process above for any word through a python method:\n",
    "def wordClassifier_multiclass(word, model, encoder):\n",
    "    '''Given a word vector, classifies its content as perceptual, manifest, contextual, or latent, according to SCA.\n",
    "    Usage example: wordClassifier_multiclass(word='trial', model=model_02_C, encoder=encoder_oneHot_C).\n",
    "    '''\n",
    "    new_entry = nlp_getVector(word)\n",
    "    if new_entry:\n",
    "        vector = nlp_getVector(word)[1]\n",
    "        vector = np.expand_dims(vector, axis=0)\n",
    "        result = model.predict(vector)\n",
    "        decoded_result = encoder.inverse_transform(result)\n",
    "    else:\n",
    "        print('Word not existent in database.')\n",
    "        return\n",
    "    print(f'--- SCA: \"{word}\" has {decoded_result[0][0]} content.')\n",
    "    print(f'Model used: {model.name}')  # Print the name of the model\n",
    "    return\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 324,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1/1 [==============================] - 0s 75ms/step\n",
      "--- SCA: \"happyness\" has Latent content.\n",
      "Model used: Model_02_E_regularized\n",
      "1/1 [==============================] - 0s 63ms/step\n",
      "--- SCA: \"happyness\" has Latent content.\n",
      "Model used: Model_02_C_regularized\n"
     ]
    }
   ],
   "source": [
    "## Experiment with a given word:\n",
    "wordClassifier_multiclass('happyness',model=model_02_E, encoder=encoder_oneHot_E)\n",
    "wordClassifier_multiclass('happyness',model=model_02_C, encoder=encoder_oneHot_C)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 325,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1/1 [==============================] - 0s 22ms/step\n",
      "--- SCA: \"imagination\" has Latent content.\n",
      "Model used: Model_02_E_regularized\n",
      "1/1 [==============================] - 0s 18ms/step\n",
      "--- SCA: \"imagination\" has Latent content.\n",
      "Model used: Model_02_C_regularized\n",
      "\n",
      "-------------------------\n",
      "1/1 [==============================] - 0s 21ms/step\n",
      "--- SCA: \"care\" has Latent content.\n",
      "Model used: Model_02_E_regularized\n",
      "1/1 [==============================] - 0s 19ms/step\n",
      "--- SCA: \"care\" has Latent content.\n",
      "Model used: Model_02_C_regularized\n",
      "\n",
      "-------------------------\n",
      "1/1 [==============================] - 0s 22ms/step\n",
      "--- SCA: \"fear\" has Latent content.\n",
      "Model used: Model_02_E_regularized\n",
      "1/1 [==============================] - 0s 18ms/step\n",
      "--- SCA: \"fear\" has Latent content.\n",
      "Model used: Model_02_C_regularized\n",
      "\n",
      "-------------------------\n",
      "1/1 [==============================] - 0s 21ms/step\n",
      "--- SCA: \"play\" has Perceptual content.\n",
      "Model used: Model_02_E_regularized\n",
      "1/1 [==============================] - 0s 21ms/step\n",
      "--- SCA: \"play\" has Perceptual content.\n",
      "Model used: Model_02_C_regularized\n",
      "\n",
      "-------------------------\n",
      "1/1 [==============================] - 0s 25ms/step\n",
      "--- SCA: \"protection\" has Latent content.\n",
      "Model used: Model_02_E_regularized\n",
      "1/1 [==============================] - 0s 26ms/step\n",
      "--- SCA: \"protection\" has Latent content.\n",
      "Model used: Model_02_C_regularized\n",
      "\n",
      "-------------------------\n",
      "1/1 [==============================] - 0s 18ms/step\n",
      "--- SCA: \"monster\" has Perceptual content.\n",
      "Model used: Model_02_E_regularized\n",
      "1/1 [==============================] - 0s 28ms/step\n",
      "--- SCA: \"monster\" has Perceptual content.\n",
      "Model used: Model_02_C_regularized\n",
      "\n",
      "-------------------------\n",
      "1/1 [==============================] - 0s 60ms/step\n",
      "--- SCA: \"Mom\" has Perceptual content.\n",
      "Model used: Model_02_E_regularized\n",
      "1/1 [==============================] - 0s 32ms/step\n",
      "--- SCA: \"Mom\" has Perceptual content.\n",
      "Model used: Model_02_C_regularized\n",
      "\n",
      "-------------------------\n",
      "1/1 [==============================] - 0s 16ms/step\n",
      "--- SCA: \"Dad\" has Perceptual content.\n",
      "Model used: Model_02_E_regularized\n",
      "1/1 [==============================] - 0s 31ms/step\n",
      "--- SCA: \"Dad\" has Perceptual content.\n",
      "Model used: Model_02_C_regularized\n",
      "\n",
      "-------------------------\n",
      "1/1 [==============================] - 0s 16ms/step\n",
      "--- SCA: \"toy\" has Manifest content.\n",
      "Model used: Model_02_E_regularized\n",
      "1/1 [==============================] - 0s 16ms/step\n",
      "--- SCA: \"toy\" has Manifest content.\n",
      "Model used: Model_02_C_regularized\n",
      "\n",
      "-------------------------\n",
      "1/1 [==============================] - 0s 20ms/step\n",
      "--- SCA: \"child\" has Manifest content.\n",
      "Model used: Model_02_E_regularized\n",
      "1/1 [==============================] - 0s 16ms/step\n",
      "--- SCA: \"child\" has Manifest content.\n",
      "Model used: Model_02_C_regularized\n",
      "\n",
      "-------------------------\n",
      "1/1 [==============================] - 0s 26ms/step\n",
      "--- SCA: \"doll\" has Manifest content.\n",
      "Model used: Model_02_E_regularized\n",
      "1/1 [==============================] - 0s 13ms/step\n",
      "--- SCA: \"doll\" has Manifest content.\n",
      "Model used: Model_02_C_regularized\n",
      "\n",
      "-------------------------\n"
     ]
    }
   ],
   "source": [
    "## Checking words classification based on the sequence explored in the PhD Thesis.\n",
    "seq_words_A = ['imagination','care','fear','play','protection','monster','Mom','Dad','toy','child','doll']\n",
    "\n",
    "for word in seq_words_A:\n",
    "    wordClassifier_multiclass(word,model=model_02_E, encoder=encoder_oneHot_E)\n",
    "    # print('\\n')\n",
    "    wordClassifier_multiclass(word,model=model_02_C, encoder=encoder_oneHot_C)\n",
    "    print(f'\\n{25*\"-\"}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 326,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1/1 [==============================] - 0s 19ms/step\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "--- SCA: \"pleasure\" has Latent content.\n",
      "Model used: Model_02_E_regularized\n",
      "1/1 [==============================] - 0s 19ms/step\n",
      "--- SCA: \"pleasure\" has Latent content.\n",
      "Model used: Model_02_C_regularized\n",
      "\n",
      "-------------------------\n",
      "1/1 [==============================] - 0s 21ms/step\n",
      "--- SCA: \"love\" has Latent content.\n",
      "Model used: Model_02_E_regularized\n",
      "1/1 [==============================] - 0s 20ms/step\n",
      "--- SCA: \"love\" has Latent content.\n",
      "Model used: Model_02_C_regularized\n",
      "\n",
      "-------------------------\n",
      "1/1 [==============================] - 0s 20ms/step\n",
      "--- SCA: \"anxiety\" has Latent content.\n",
      "Model used: Model_02_E_regularized\n",
      "1/1 [==============================] - 0s 19ms/step\n",
      "--- SCA: \"anxiety\" has Latent content.\n",
      "Model used: Model_02_C_regularized\n",
      "\n",
      "-------------------------\n",
      "1/1 [==============================] - 0s 21ms/step\n",
      "--- SCA: \"suffering\" has Latent content.\n",
      "Model used: Model_02_E_regularized\n",
      "1/1 [==============================] - 0s 26ms/step\n",
      "--- SCA: \"suffering\" has Latent content.\n",
      "Model used: Model_02_C_regularized\n",
      "\n",
      "-------------------------\n",
      "1/1 [==============================] - 0s 16ms/step\n",
      "--- SCA: \"touch\" has Latent content.\n",
      "Model used: Model_02_E_regularized\n",
      "1/1 [==============================] - 0s 13ms/step\n",
      "--- SCA: \"touch\" has Perceptual content.\n",
      "Model used: Model_02_C_regularized\n",
      "\n",
      "-------------------------\n",
      "1/1 [==============================] - 0s 32ms/step\n",
      "--- SCA: \"hug\" has Perceptual content.\n",
      "Model used: Model_02_E_regularized\n",
      "1/1 [==============================] - 0s 32ms/step\n",
      "--- SCA: \"hug\" has Perceptual content.\n",
      "Model used: Model_02_C_regularized\n",
      "\n",
      "-------------------------\n",
      "1/1 [==============================] - 0s 31ms/step\n",
      "--- SCA: \"cuddle\" has Perceptual content.\n",
      "Model used: Model_02_E_regularized\n",
      "1/1 [==============================] - 0s 26ms/step\n",
      "--- SCA: \"cuddle\" has Perceptual content.\n",
      "Model used: Model_02_C_regularized\n",
      "\n",
      "-------------------------\n",
      "1/1 [==============================] - 0s 18ms/step\n",
      "--- SCA: \"wound\" has Manifest content.\n",
      "Model used: Model_02_E_regularized\n",
      "1/1 [==============================] - 0s 26ms/step\n",
      "--- SCA: \"wound\" has Perceptual content.\n",
      "Model used: Model_02_C_regularized\n",
      "\n",
      "-------------------------\n",
      "1/1 [==============================] - 0s 16ms/step\n",
      "--- SCA: \"arms\" has Perceptual content.\n",
      "Model used: Model_02_E_regularized\n",
      "1/1 [==============================] - 0s 29ms/step\n",
      "--- SCA: \"arms\" has Manifest content.\n",
      "Model used: Model_02_C_regularized\n",
      "\n",
      "-------------------------\n",
      "1/1 [==============================] - 0s 24ms/step\n",
      "--- SCA: \"hand\" has Manifest content.\n",
      "Model used: Model_02_E_regularized\n",
      "1/1 [==============================] - 0s 14ms/step\n",
      "--- SCA: \"hand\" has Manifest content.\n",
      "Model used: Model_02_C_regularized\n",
      "\n",
      "-------------------------\n"
     ]
    }
   ],
   "source": [
    "## Checking words classification based on the sequence explored in the PhD Thesis.\n",
    "seq_words_B = ['pleasure','love','anxiety','suffering','touch','hug','cuddle','wound','arms','hand']\n",
    "\n",
    "for word in seq_words_B:\n",
    "    wordClassifier_multiclass(word,model=model_02_E, encoder=encoder_oneHot_E)\n",
    "    # print('\\n')\n",
    "    wordClassifier_multiclass(word,model=model_02_C, encoder=encoder_oneHot_C)\n",
    "    print(f'\\n{25*\"-\"}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 330,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1/1 [==============================] - 0s 23ms/step\n",
      "--- SCA: \"A cigar is just a cigar\" has Manifest content.\n",
      "Model used: Model_02_C_regularized\n",
      "1/1 [==============================] - 0s 14ms/step\n",
      "--- SCA: \"A cigar is just a cigar\" has Manifest content.\n",
      "Model used: Model_02_E_regularized\n"
     ]
    }
   ],
   "source": [
    "phrase = 'A cigar is just a cigar'\n",
    "wordClassifier_multiclass(phrase,model=model_02_C, encoder=encoder_oneHot_C)\n",
    "wordClassifier_multiclass(phrase,model=model_02_E, encoder=encoder_oneHot_E)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 331,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1/1 [==============================] - 0s 25ms/step\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "--- SCA: \"Did you dream again?\" has Perceptual content.\n",
      "Model used: Model_02_C_regularized\n",
      "1/1 [==============================] - 0s 21ms/step\n",
      "--- SCA: \"Did you dream again?\" has Latent content.\n",
      "Model used: Model_02_E_regularized\n"
     ]
    }
   ],
   "source": [
    "phrase = 'Did you dream again?'\n",
    "wordClassifier_multiclass(phrase,model=model_02_C, encoder=encoder_oneHot_C)\n",
    "wordClassifier_multiclass(phrase,model=model_02_E, encoder=encoder_oneHot_E)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Calculating similarities:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 332,
   "metadata": {},
   "outputs": [],
   "source": [
    "# phrase_1 = 'cake'\n",
    "# phrase_2 = 'This is a good cake'\n",
    "# phrase_3 = 'This is a horrible cake'\n",
    "\n",
    "phrase_1 = 'thesis'\n",
    "phrase_2 = 'This is a good thesis'\n",
    "phrase_3 = 'This is an awful thesis'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 334,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1/1 [==============================] - 0s 20ms/step\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "--- thesis:\n",
      "39.74 of objectivity\n",
      "35.98 of subjectivity\n",
      "Model used: Model_01_A\n",
      "1/1 [==============================] - 0s 21ms/step\n",
      "--- thesis:\n",
      "34.19 of objectivity\n",
      "34.81 of subjectivity\n",
      "Model used: Model_01_D\n",
      "\n",
      "\n",
      "1/1 [==============================] - 0s 22ms/step\n",
      "--- This is a good thesis:\n",
      "33.72 of objectivity\n",
      "51.45 of subjectivity\n",
      "Model used: Model_01_A\n",
      "1/1 [==============================] - 0s 42ms/step\n",
      "--- This is a good thesis:\n",
      "42.43 of objectivity\n",
      "53.71 of subjectivity\n",
      "Model used: Model_01_D\n",
      "\n",
      "\n",
      "1/1 [==============================] - 0s 6ms/step\n",
      "--- This is an awful thesis:\n",
      "31.74 of objectivity\n",
      "45.91 of subjectivity\n",
      "Model used: Model_01_A\n",
      "1/1 [==============================] - 0s 12ms/step\n",
      "--- This is an awful thesis:\n",
      "36.09 of objectivity\n",
      "48.22 of subjectivity\n",
      "Model used: Model_01_D\n",
      "\n",
      "\n"
     ]
    }
   ],
   "source": [
    "for phrase in (phrase_1, phrase_2, phrase_3):\n",
    "    wordClassifier_regression(phrase, model=model_01_A)\n",
    "    wordClassifier_regression(phrase, model=model_01_D)\n",
    "    print('\\n')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 160,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1/1 [==============================] - 0s 22ms/step\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "--- SCA: \"thesis\" has Latent content.\n",
      "Model used: Model_02_E\n",
      "\n",
      "\n",
      "1/1 [==============================] - 0s 21ms/step\n",
      "--- SCA: \"This is a good thesis\" has Latent content.\n",
      "Model used: Model_02_E\n",
      "\n",
      "\n",
      "1/1 [==============================] - 0s 25ms/step\n",
      "--- SCA: \"This is an awful thesis\" has Latent content.\n",
      "Model used: Model_02_E\n",
      "\n",
      "\n"
     ]
    }
   ],
   "source": [
    "for phrase in (phrase_1, phrase_2, phrase_3):\n",
    "    wordClassifier_multiclass(phrase, model=model_02_E, encoder=encoder_oneHot_E)\n",
    "    print('\\n')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 164,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.6041243931483511"
      ]
     },
     "execution_count": 164,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "nlp(phrase_1).similarity(nlp(phrase_3))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 335,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1/1 [==============================] - 0s 27ms/step\n",
      "--- SCA: \"This is a huge text, I wonder if it is not an extremely huge text. And how would it be if I write more, and more, and more words. Maybe a paragraph. Maybe a whole page. Maybe a book? Who kmows. What is the point is that I am writing too much. And expecting what would result from this.\" has Manifest content.\n",
      "Model used: Model_02_E_regularized\n"
     ]
    }
   ],
   "source": [
    "phrase = 'This is a huge text, I wonder if it is not an extremely huge text. And how would it be if I write more, and more, and more words. Maybe a paragraph. Maybe a whole page. Maybe a book? Who kmows. What is the point is that I am writing too much. And expecting what would result from this.'\n",
    "wordClassifier_multiclass(phrase,model=model_02_E, encoder=encoder_oneHot_E)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 339,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1/1 [==============================] - 0s 21ms/step\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "--- This is a huge text, I wonder if it is not an extremely huge text. And how would it be if I write more, and more, and more words. Maybe a paragraph. Maybe a whole page. Maybe a book? Who kmows. What is the point is that I am writing too much. And expecting what would result from this.:\n",
      "72.54 of objectivity\n",
      "37.75 of subjectivity\n",
      "Model used: Model_01_D2_regularized\n"
     ]
    }
   ],
   "source": [
    "wordClassifier_regression(phrase, model=model_01_D2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### NEXT STEPS:  \n",
    "Calcular a objetividade/subjetividade para cada frase da SICK database. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 169,
   "metadata": {},
   "outputs": [],
   "source": [
    "def most_similar(word, topn=5):\n",
    "    word = nlp.vocab[str(word)]\n",
    "    queries = [\n",
    "        w for w in word.vocab \n",
    "        if w.is_lower == word.is_lower and w.prob >= -15 and np.count_nonzero(w.vector)\n",
    "    ]\n",
    "\n",
    "    by_similarity = sorted(queries, key=lambda w: word.similarity(w), reverse=True)\n",
    "    return [(w.lower_,w.similarity(word)) for w in by_similarity[:topn+1] if w.lower_ != word.lower_]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 170,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[]"
      ]
     },
     "execution_count": 170,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\n",
    "most_similar(\"dog\", topn=3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 175,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "spacy.vocab.Vocab"
      ]
     },
     "execution_count": 175,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "type(nlp.vocab[str('word')].vocab)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 191,
   "metadata": {},
   "outputs": [],
   "source": [
    "tmp = nlp.vocab['bad']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 192,
   "metadata": {},
   "outputs": [],
   "source": [
    "tmp2 = list(tmp.vocab)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 193,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "nuthin\n",
      "endure\n",
      "softly\n",
      "negotiation\n",
      "strength\n",
      "spa\n",
      "full\n",
      "blubber\n",
      "cheese\n",
      "hero\n"
     ]
    }
   ],
   "source": [
    "for i in range(0,10):\n",
    "    print(tmp2[i].text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Defining a method to get vector and text information for a word:\n",
    "def nlp_getVector(word, verbose=False):\n",
    "    '''\n",
    "    Obtains the vector representation of a given word from SpaCy word embedding.\n",
    "    Usage:  nlp_getVector(word)[0] to get the text; nlp_getVector(word)[1] to get the vector.\n",
    "            var_text, var_vector = nlp_getVector(word)\n",
    "    '''\n",
    "    ## Generates the word hash:\n",
    "    # hash = nlp.vocab.strings[word]\n",
    "    try:\n",
    "        word_vector = nlp(word).vector\n",
    "        word_text = nlp(word).text\n",
    "    except:\n",
    "        if verbose:\n",
    "            print('Error: word vector not available.')\n",
    "        return None\n",
    "    return (word_text, word_vector)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Generalizing the process above for any word through a python method:\n",
    "def wordClassifier_multiclass(word, model, encoder):\n",
    "    '''Given a word vector, classifies its content as perceptual, manifest, contextual, or latent, according to SCA.\n",
    "    Usage example: wordClassifier_multiclass(word='trial', model=model_02_C, encoder=encoder_oneHot_C).\n",
    "    '''\n",
    "    new_entry = nlp_getVector(word)\n",
    "    if new_entry:\n",
    "        vector = nlp_getVector(word)[1]\n",
    "        vector = np.expand_dims(vector, axis=0)\n",
    "        result = model.predict(vector)\n",
    "        decoded_result = encoder.inverse_transform(result)\n",
    "    else:\n",
    "        print('Word not existent in database.')\n",
    "        return\n",
    "    print(f'--- SCA: \"{word}\" has {decoded_result[0][0]} content.')\n",
    "    print(f'Model used: {model.name}')  # Print the name of the model\n",
    "    return\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "----\n",
    "## Saving and loading Keras trained models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 119,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ## Saving Keras models for posterior use:\n",
    "# model_01_A.save('../models/model_01_A.h5')\n",
    "# model_01_D.save('../models/model_01_D.h5')\n",
    "# model_02_C.save('../models/model_02_C.h5')\n",
    "# model_02_E.save('../models/model_02_E.h5')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 458,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Loading saved models:\n",
    "from keras.models import load_model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 462,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = load_model('../models/model_02_C.h5')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 467,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Saving and loading Scklearn models (e.g., OneHot Encoders)\n",
    "with open('../models/encoder_oneHot_C.pickle', 'wb') as f:\n",
    "    pickle.dump(encoder_oneHot_C, f)\n",
    "\n",
    "with open('../models/encoder_oneHot_E.pickle', 'wb') as f:\n",
    "    pickle.dump(encoder_oneHot_E, f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 469,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Loading saved encoder:\n",
    "with open('../models/encoder_oneHot_C.pickle', 'rb') as f:\n",
    "    encoder = pickle.load(f)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Trying loaded model and encoder:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 470,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1/1 [==============================] - 0s 75ms/step\n",
      "--- SCA: \"debug\" has Latent content.\n",
      "Model used: Model_02_C\n"
     ]
    }
   ],
   "source": [
    "wordClassifier_multiclass('debug',model=model, encoder=encoder)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "## Pycaret: exploring outperforming models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 302,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pycaret.classification"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 303,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>0</th>\n",
       "      <th>1</th>\n",
       "      <th>2</th>\n",
       "      <th>3</th>\n",
       "      <th>4</th>\n",
       "      <th>5</th>\n",
       "      <th>6</th>\n",
       "      <th>7</th>\n",
       "      <th>8</th>\n",
       "      <th>9</th>\n",
       "      <th>...</th>\n",
       "      <th>291</th>\n",
       "      <th>292</th>\n",
       "      <th>293</th>\n",
       "      <th>294</th>\n",
       "      <th>295</th>\n",
       "      <th>296</th>\n",
       "      <th>297</th>\n",
       "      <th>298</th>\n",
       "      <th>299</th>\n",
       "      <th>target</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>garment</th>\n",
       "      <td>-2.44960</td>\n",
       "      <td>-0.709460</td>\n",
       "      <td>-1.4459</td>\n",
       "      <td>2.7438</td>\n",
       "      <td>2.19640</td>\n",
       "      <td>0.62562</td>\n",
       "      <td>1.3016</td>\n",
       "      <td>2.0838</td>\n",
       "      <td>-2.2201</td>\n",
       "      <td>-2.2038</td>\n",
       "      <td>...</td>\n",
       "      <td>3.413000</td>\n",
       "      <td>0.53852</td>\n",
       "      <td>-0.15337</td>\n",
       "      <td>-4.65310</td>\n",
       "      <td>0.54157</td>\n",
       "      <td>1.41030</td>\n",
       "      <td>-4.592600</td>\n",
       "      <td>0.43055</td>\n",
       "      <td>0.35458</td>\n",
       "      <td>Manifest</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>oven</th>\n",
       "      <td>-2.32870</td>\n",
       "      <td>1.487700</td>\n",
       "      <td>-1.1173</td>\n",
       "      <td>4.4829</td>\n",
       "      <td>1.85580</td>\n",
       "      <td>1.05840</td>\n",
       "      <td>1.8389</td>\n",
       "      <td>-2.3667</td>\n",
       "      <td>-6.2502</td>\n",
       "      <td>3.5786</td>\n",
       "      <td>...</td>\n",
       "      <td>0.578500</td>\n",
       "      <td>-2.85220</td>\n",
       "      <td>-4.87860</td>\n",
       "      <td>-0.81042</td>\n",
       "      <td>-1.06000</td>\n",
       "      <td>1.10440</td>\n",
       "      <td>0.489910</td>\n",
       "      <td>2.73810</td>\n",
       "      <td>3.74540</td>\n",
       "      <td>Manifest</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>landscape</th>\n",
       "      <td>-0.63003</td>\n",
       "      <td>0.041465</td>\n",
       "      <td>-2.3347</td>\n",
       "      <td>-4.7172</td>\n",
       "      <td>0.10905</td>\n",
       "      <td>-1.18150</td>\n",
       "      <td>3.3926</td>\n",
       "      <td>3.1072</td>\n",
       "      <td>-2.1309</td>\n",
       "      <td>1.3538</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.072589</td>\n",
       "      <td>1.17210</td>\n",
       "      <td>0.39403</td>\n",
       "      <td>0.82975</td>\n",
       "      <td>-0.17167</td>\n",
       "      <td>-0.54614</td>\n",
       "      <td>0.039231</td>\n",
       "      <td>-4.14750</td>\n",
       "      <td>0.87988</td>\n",
       "      <td>Perceptual</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>3 rows × 301 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                 0         1       2       3        4        5       6       7       8       9  ...       291      292      293      294      295      296       297      298      299      target\n",
       "garment   -2.44960 -0.709460 -1.4459  2.7438  2.19640  0.62562  1.3016  2.0838 -2.2201 -2.2038  ...  3.413000  0.53852 -0.15337 -4.65310  0.54157  1.41030 -4.592600  0.43055  0.35458    Manifest\n",
       "oven      -2.32870  1.487700 -1.1173  4.4829  1.85580  1.05840  1.8389 -2.3667 -6.2502  3.5786  ...  0.578500 -2.85220 -4.87860 -0.81042 -1.06000  1.10440  0.489910  2.73810  3.74540    Manifest\n",
       "landscape -0.63003  0.041465 -2.3347 -4.7172  0.10905 -1.18150  3.3926  3.1072 -2.1309  1.3538  ... -0.072589  1.17210  0.39403  0.82975 -0.17167 -0.54614  0.039231 -4.14750  0.87988  Perceptual\n",
       "\n",
       "[3 rows x 301 columns]"
      ]
     },
     "execution_count": 303,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_pycaret = pd.concat([X_train_C, Y_train_C], axis=1)\n",
    "df_pycaret.sample(3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 304,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<style type=\"text/css\">\n",
       "#T_f2a58_row9_col1 {\n",
       "  background-color: lightgreen;\n",
       "}\n",
       "</style>\n",
       "<table id=\"T_f2a58\">\n",
       "  <thead>\n",
       "    <tr>\n",
       "      <th class=\"blank level0\" >&nbsp;</th>\n",
       "      <th id=\"T_f2a58_level0_col0\" class=\"col_heading level0 col0\" >Description</th>\n",
       "      <th id=\"T_f2a58_level0_col1\" class=\"col_heading level0 col1\" >Value</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th id=\"T_f2a58_level0_row0\" class=\"row_heading level0 row0\" >0</th>\n",
       "      <td id=\"T_f2a58_row0_col0\" class=\"data row0 col0\" >Session id</td>\n",
       "      <td id=\"T_f2a58_row0_col1\" class=\"data row0 col1\" >9088</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th id=\"T_f2a58_level0_row1\" class=\"row_heading level0 row1\" >1</th>\n",
       "      <td id=\"T_f2a58_row1_col0\" class=\"data row1 col0\" >Target</td>\n",
       "      <td id=\"T_f2a58_row1_col1\" class=\"data row1 col1\" >target</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th id=\"T_f2a58_level0_row2\" class=\"row_heading level0 row2\" >2</th>\n",
       "      <td id=\"T_f2a58_row2_col0\" class=\"data row2 col0\" >Target type</td>\n",
       "      <td id=\"T_f2a58_row2_col1\" class=\"data row2 col1\" >Multiclass</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th id=\"T_f2a58_level0_row3\" class=\"row_heading level0 row3\" >3</th>\n",
       "      <td id=\"T_f2a58_row3_col0\" class=\"data row3 col0\" >Target mapping</td>\n",
       "      <td id=\"T_f2a58_row3_col1\" class=\"data row3 col1\" >Contextual: 0, Latent: 1, Manifest: 2, Perceptual: 3</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th id=\"T_f2a58_level0_row4\" class=\"row_heading level0 row4\" >4</th>\n",
       "      <td id=\"T_f2a58_row4_col0\" class=\"data row4 col0\" >Original data shape</td>\n",
       "      <td id=\"T_f2a58_row4_col1\" class=\"data row4 col1\" >(3511, 301)</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th id=\"T_f2a58_level0_row5\" class=\"row_heading level0 row5\" >5</th>\n",
       "      <td id=\"T_f2a58_row5_col0\" class=\"data row5 col0\" >Transformed data shape</td>\n",
       "      <td id=\"T_f2a58_row5_col1\" class=\"data row5 col1\" >(3511, 301)</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th id=\"T_f2a58_level0_row6\" class=\"row_heading level0 row6\" >6</th>\n",
       "      <td id=\"T_f2a58_row6_col0\" class=\"data row6 col0\" >Transformed train set shape</td>\n",
       "      <td id=\"T_f2a58_row6_col1\" class=\"data row6 col1\" >(2457, 301)</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th id=\"T_f2a58_level0_row7\" class=\"row_heading level0 row7\" >7</th>\n",
       "      <td id=\"T_f2a58_row7_col0\" class=\"data row7 col0\" >Transformed test set shape</td>\n",
       "      <td id=\"T_f2a58_row7_col1\" class=\"data row7 col1\" >(1054, 301)</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th id=\"T_f2a58_level0_row8\" class=\"row_heading level0 row8\" >8</th>\n",
       "      <td id=\"T_f2a58_row8_col0\" class=\"data row8 col0\" >Numeric features</td>\n",
       "      <td id=\"T_f2a58_row8_col1\" class=\"data row8 col1\" >300</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th id=\"T_f2a58_level0_row9\" class=\"row_heading level0 row9\" >9</th>\n",
       "      <td id=\"T_f2a58_row9_col0\" class=\"data row9 col0\" >Preprocess</td>\n",
       "      <td id=\"T_f2a58_row9_col1\" class=\"data row9 col1\" >True</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th id=\"T_f2a58_level0_row10\" class=\"row_heading level0 row10\" >10</th>\n",
       "      <td id=\"T_f2a58_row10_col0\" class=\"data row10 col0\" >Imputation type</td>\n",
       "      <td id=\"T_f2a58_row10_col1\" class=\"data row10 col1\" >simple</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th id=\"T_f2a58_level0_row11\" class=\"row_heading level0 row11\" >11</th>\n",
       "      <td id=\"T_f2a58_row11_col0\" class=\"data row11 col0\" >Numeric imputation</td>\n",
       "      <td id=\"T_f2a58_row11_col1\" class=\"data row11 col1\" >mean</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th id=\"T_f2a58_level0_row12\" class=\"row_heading level0 row12\" >12</th>\n",
       "      <td id=\"T_f2a58_row12_col0\" class=\"data row12 col0\" >Categorical imputation</td>\n",
       "      <td id=\"T_f2a58_row12_col1\" class=\"data row12 col1\" >mode</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th id=\"T_f2a58_level0_row13\" class=\"row_heading level0 row13\" >13</th>\n",
       "      <td id=\"T_f2a58_row13_col0\" class=\"data row13 col0\" >Fold Generator</td>\n",
       "      <td id=\"T_f2a58_row13_col1\" class=\"data row13 col1\" >StratifiedKFold</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th id=\"T_f2a58_level0_row14\" class=\"row_heading level0 row14\" >14</th>\n",
       "      <td id=\"T_f2a58_row14_col0\" class=\"data row14 col0\" >Fold Number</td>\n",
       "      <td id=\"T_f2a58_row14_col1\" class=\"data row14 col1\" >10</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th id=\"T_f2a58_level0_row15\" class=\"row_heading level0 row15\" >15</th>\n",
       "      <td id=\"T_f2a58_row15_col0\" class=\"data row15 col0\" >CPU Jobs</td>\n",
       "      <td id=\"T_f2a58_row15_col1\" class=\"data row15 col1\" >-1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th id=\"T_f2a58_level0_row16\" class=\"row_heading level0 row16\" >16</th>\n",
       "      <td id=\"T_f2a58_row16_col0\" class=\"data row16 col0\" >Use GPU</td>\n",
       "      <td id=\"T_f2a58_row16_col1\" class=\"data row16 col1\" >False</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th id=\"T_f2a58_level0_row17\" class=\"row_heading level0 row17\" >17</th>\n",
       "      <td id=\"T_f2a58_row17_col0\" class=\"data row17 col0\" >Log Experiment</td>\n",
       "      <td id=\"T_f2a58_row17_col1\" class=\"data row17 col1\" >False</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th id=\"T_f2a58_level0_row18\" class=\"row_heading level0 row18\" >18</th>\n",
       "      <td id=\"T_f2a58_row18_col0\" class=\"data row18 col0\" >Experiment Name</td>\n",
       "      <td id=\"T_f2a58_row18_col1\" class=\"data row18 col1\" >clf-default-name</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th id=\"T_f2a58_level0_row19\" class=\"row_heading level0 row19\" >19</th>\n",
       "      <td id=\"T_f2a58_row19_col0\" class=\"data row19 col0\" >USI</td>\n",
       "      <td id=\"T_f2a58_row19_col1\" class=\"data row19 col1\" >ae3f</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n"
      ],
      "text/plain": [
       "<pandas.io.formats.style.Styler at 0x20e4a392a90>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "## Fazendo um experimento:\n",
    "exp_class = pycaret.classification.setup(df_pycaret, target='target', session_id=9088)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 305,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>Initiated</th>\n",
       "      <td>. . . . . . . . . . . . . . . . . .</td>\n",
       "      <td>21:55:05</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Status</th>\n",
       "      <td>. . . . . . . . . . . . . . . . . .</td>\n",
       "      <td>Fitting 10 Folds</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Estimator</th>\n",
       "      <td>. . . . . . . . . . . . . . . . . .</td>\n",
       "      <td>Logistic Regression</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                                   \n",
       "                                                                   \n",
       "Initiated  . . . . . . . . . . . . . . . . . .             21:55:05\n",
       "Status     . . . . . . . . . . . . . . . . . .     Fitting 10 Folds\n",
       "Estimator  . . . . . . . . . . . . . . . . . .  Logistic Regression"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<style type=\"text/css\">\n",
       "#T_e8219 th {\n",
       "  text-align: left;\n",
       "}\n",
       "#T_e8219_row0_col0, #T_e8219_row0_col2, #T_e8219_row0_col4, #T_e8219_row0_col5, #T_e8219_row0_col6, #T_e8219_row0_col7, #T_e8219_row1_col0, #T_e8219_row1_col1, #T_e8219_row1_col2, #T_e8219_row1_col3, #T_e8219_row2_col0, #T_e8219_row2_col1, #T_e8219_row2_col3, #T_e8219_row2_col4, #T_e8219_row2_col5, #T_e8219_row2_col6, #T_e8219_row2_col7, #T_e8219_row3_col0, #T_e8219_row3_col1, #T_e8219_row3_col2, #T_e8219_row3_col3, #T_e8219_row3_col4, #T_e8219_row3_col5, #T_e8219_row3_col6, #T_e8219_row3_col7, #T_e8219_row4_col0, #T_e8219_row4_col1, #T_e8219_row4_col2, #T_e8219_row4_col3, #T_e8219_row4_col4, #T_e8219_row4_col5, #T_e8219_row4_col6, #T_e8219_row4_col7, #T_e8219_row5_col0, #T_e8219_row5_col1, #T_e8219_row5_col2, #T_e8219_row5_col3, #T_e8219_row5_col4, #T_e8219_row5_col5, #T_e8219_row5_col6, #T_e8219_row5_col7, #T_e8219_row6_col0, #T_e8219_row6_col1, #T_e8219_row6_col2, #T_e8219_row6_col3, #T_e8219_row6_col4, #T_e8219_row6_col5, #T_e8219_row6_col6, #T_e8219_row6_col7, #T_e8219_row7_col0, #T_e8219_row7_col1, #T_e8219_row7_col2, #T_e8219_row7_col3, #T_e8219_row7_col4, #T_e8219_row7_col5, #T_e8219_row7_col6, #T_e8219_row7_col7, #T_e8219_row8_col0, #T_e8219_row8_col1, #T_e8219_row8_col2, #T_e8219_row8_col3, #T_e8219_row8_col4, #T_e8219_row8_col5, #T_e8219_row8_col6, #T_e8219_row8_col7, #T_e8219_row9_col0, #T_e8219_row9_col1, #T_e8219_row9_col2, #T_e8219_row9_col3, #T_e8219_row9_col4, #T_e8219_row9_col5, #T_e8219_row9_col6, #T_e8219_row9_col7, #T_e8219_row10_col0, #T_e8219_row10_col1, #T_e8219_row10_col2, #T_e8219_row10_col3, #T_e8219_row10_col4, #T_e8219_row10_col5, #T_e8219_row10_col6, #T_e8219_row10_col7, #T_e8219_row11_col0, #T_e8219_row11_col1, #T_e8219_row11_col2, #T_e8219_row11_col3, #T_e8219_row11_col4, #T_e8219_row11_col5, #T_e8219_row11_col6, #T_e8219_row11_col7, #T_e8219_row12_col0, #T_e8219_row12_col1, #T_e8219_row12_col2, #T_e8219_row12_col3, #T_e8219_row12_col4, #T_e8219_row12_col5, #T_e8219_row12_col6, #T_e8219_row12_col7, #T_e8219_row13_col0, #T_e8219_row13_col1, #T_e8219_row13_col2, #T_e8219_row13_col3, #T_e8219_row13_col4, #T_e8219_row13_col5, #T_e8219_row13_col6, #T_e8219_row13_col7 {\n",
       "  text-align: left;\n",
       "}\n",
       "#T_e8219_row0_col1, #T_e8219_row0_col3, #T_e8219_row1_col4, #T_e8219_row1_col5, #T_e8219_row1_col6, #T_e8219_row1_col7, #T_e8219_row2_col2 {\n",
       "  text-align: left;\n",
       "  background-color: yellow;\n",
       "}\n",
       "#T_e8219_row0_col8, #T_e8219_row13_col8 {\n",
       "  text-align: left;\n",
       "  background-color: yellow;\n",
       "  background-color: lightgrey;\n",
       "}\n",
       "#T_e8219_row1_col8, #T_e8219_row2_col8, #T_e8219_row3_col8, #T_e8219_row4_col8, #T_e8219_row5_col8, #T_e8219_row6_col8, #T_e8219_row7_col8, #T_e8219_row8_col8, #T_e8219_row9_col8, #T_e8219_row10_col8, #T_e8219_row11_col8, #T_e8219_row12_col8 {\n",
       "  text-align: left;\n",
       "  background-color: lightgrey;\n",
       "}\n",
       "</style>\n",
       "<table id=\"T_e8219\">\n",
       "  <thead>\n",
       "    <tr>\n",
       "      <th class=\"blank level0\" >&nbsp;</th>\n",
       "      <th id=\"T_e8219_level0_col0\" class=\"col_heading level0 col0\" >Model</th>\n",
       "      <th id=\"T_e8219_level0_col1\" class=\"col_heading level0 col1\" >Accuracy</th>\n",
       "      <th id=\"T_e8219_level0_col2\" class=\"col_heading level0 col2\" >AUC</th>\n",
       "      <th id=\"T_e8219_level0_col3\" class=\"col_heading level0 col3\" >Recall</th>\n",
       "      <th id=\"T_e8219_level0_col4\" class=\"col_heading level0 col4\" >Prec.</th>\n",
       "      <th id=\"T_e8219_level0_col5\" class=\"col_heading level0 col5\" >F1</th>\n",
       "      <th id=\"T_e8219_level0_col6\" class=\"col_heading level0 col6\" >Kappa</th>\n",
       "      <th id=\"T_e8219_level0_col7\" class=\"col_heading level0 col7\" >MCC</th>\n",
       "      <th id=\"T_e8219_level0_col8\" class=\"col_heading level0 col8\" >TT (Sec)</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th id=\"T_e8219_level0_row0\" class=\"row_heading level0 row0\" >ridge</th>\n",
       "      <td id=\"T_e8219_row0_col0\" class=\"data row0 col0\" >Ridge Classifier</td>\n",
       "      <td id=\"T_e8219_row0_col1\" class=\"data row0 col1\" >0.6789</td>\n",
       "      <td id=\"T_e8219_row0_col2\" class=\"data row0 col2\" >0.0000</td>\n",
       "      <td id=\"T_e8219_row0_col3\" class=\"data row0 col3\" >0.6789</td>\n",
       "      <td id=\"T_e8219_row0_col4\" class=\"data row0 col4\" >0.6689</td>\n",
       "      <td id=\"T_e8219_row0_col5\" class=\"data row0 col5\" >0.6675</td>\n",
       "      <td id=\"T_e8219_row0_col6\" class=\"data row0 col6\" >0.5516</td>\n",
       "      <td id=\"T_e8219_row0_col7\" class=\"data row0 col7\" >0.5552</td>\n",
       "      <td id=\"T_e8219_row0_col8\" class=\"data row0 col8\" >0.0220</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th id=\"T_e8219_level0_row1\" class=\"row_heading level0 row1\" >lda</th>\n",
       "      <td id=\"T_e8219_row1_col0\" class=\"data row1 col0\" >Linear Discriminant Analysis</td>\n",
       "      <td id=\"T_e8219_row1_col1\" class=\"data row1 col1\" >0.6777</td>\n",
       "      <td id=\"T_e8219_row1_col2\" class=\"data row1 col2\" >0.8846</td>\n",
       "      <td id=\"T_e8219_row1_col3\" class=\"data row1 col3\" >0.6777</td>\n",
       "      <td id=\"T_e8219_row1_col4\" class=\"data row1 col4\" >0.6770</td>\n",
       "      <td id=\"T_e8219_row1_col5\" class=\"data row1 col5\" >0.6758</td>\n",
       "      <td id=\"T_e8219_row1_col6\" class=\"data row1 col6\" >0.5564</td>\n",
       "      <td id=\"T_e8219_row1_col7\" class=\"data row1 col7\" >0.5573</td>\n",
       "      <td id=\"T_e8219_row1_col8\" class=\"data row1 col8\" >0.0560</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th id=\"T_e8219_level0_row2\" class=\"row_heading level0 row2\" >lightgbm</th>\n",
       "      <td id=\"T_e8219_row2_col0\" class=\"data row2 col0\" >Light Gradient Boosting Machine</td>\n",
       "      <td id=\"T_e8219_row2_col1\" class=\"data row2 col1\" >0.6712</td>\n",
       "      <td id=\"T_e8219_row2_col2\" class=\"data row2 col2\" >0.8857</td>\n",
       "      <td id=\"T_e8219_row2_col3\" class=\"data row2 col3\" >0.6712</td>\n",
       "      <td id=\"T_e8219_row2_col4\" class=\"data row2 col4\" >0.6594</td>\n",
       "      <td id=\"T_e8219_row2_col5\" class=\"data row2 col5\" >0.6541</td>\n",
       "      <td id=\"T_e8219_row2_col6\" class=\"data row2 col6\" >0.5381</td>\n",
       "      <td id=\"T_e8219_row2_col7\" class=\"data row2 col7\" >0.5441</td>\n",
       "      <td id=\"T_e8219_row2_col8\" class=\"data row2 col8\" >2.0130</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th id=\"T_e8219_level0_row3\" class=\"row_heading level0 row3\" >gbc</th>\n",
       "      <td id=\"T_e8219_row3_col0\" class=\"data row3 col0\" >Gradient Boosting Classifier</td>\n",
       "      <td id=\"T_e8219_row3_col1\" class=\"data row3 col1\" >0.6590</td>\n",
       "      <td id=\"T_e8219_row3_col2\" class=\"data row3 col2\" >0.8830</td>\n",
       "      <td id=\"T_e8219_row3_col3\" class=\"data row3 col3\" >0.6590</td>\n",
       "      <td id=\"T_e8219_row3_col4\" class=\"data row3 col4\" >0.6475</td>\n",
       "      <td id=\"T_e8219_row3_col5\" class=\"data row3 col5\" >0.6451</td>\n",
       "      <td id=\"T_e8219_row3_col6\" class=\"data row3 col6\" >0.5227</td>\n",
       "      <td id=\"T_e8219_row3_col7\" class=\"data row3 col7\" >0.5270</td>\n",
       "      <td id=\"T_e8219_row3_col8\" class=\"data row3 col8\" >13.9600</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th id=\"T_e8219_level0_row4\" class=\"row_heading level0 row4\" >lr</th>\n",
       "      <td id=\"T_e8219_row4_col0\" class=\"data row4 col0\" >Logistic Regression</td>\n",
       "      <td id=\"T_e8219_row4_col1\" class=\"data row4 col1\" >0.6443</td>\n",
       "      <td id=\"T_e8219_row4_col2\" class=\"data row4 col2\" >0.8568</td>\n",
       "      <td id=\"T_e8219_row4_col3\" class=\"data row4 col3\" >0.6443</td>\n",
       "      <td id=\"T_e8219_row4_col4\" class=\"data row4 col4\" >0.6458</td>\n",
       "      <td id=\"T_e8219_row4_col5\" class=\"data row4 col5\" >0.6440</td>\n",
       "      <td id=\"T_e8219_row4_col6\" class=\"data row4 col6\" >0.5121</td>\n",
       "      <td id=\"T_e8219_row4_col7\" class=\"data row4 col7\" >0.5128</td>\n",
       "      <td id=\"T_e8219_row4_col8\" class=\"data row4 col8\" >0.6520</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th id=\"T_e8219_level0_row5\" class=\"row_heading level0 row5\" >svm</th>\n",
       "      <td id=\"T_e8219_row5_col0\" class=\"data row5 col0\" >SVM - Linear Kernel</td>\n",
       "      <td id=\"T_e8219_row5_col1\" class=\"data row5 col1\" >0.6325</td>\n",
       "      <td id=\"T_e8219_row5_col2\" class=\"data row5 col2\" >0.0000</td>\n",
       "      <td id=\"T_e8219_row5_col3\" class=\"data row5 col3\" >0.6325</td>\n",
       "      <td id=\"T_e8219_row5_col4\" class=\"data row5 col4\" >0.6294</td>\n",
       "      <td id=\"T_e8219_row5_col5\" class=\"data row5 col5\" >0.6254</td>\n",
       "      <td id=\"T_e8219_row5_col6\" class=\"data row5 col6\" >0.4924</td>\n",
       "      <td id=\"T_e8219_row5_col7\" class=\"data row5 col7\" >0.4956</td>\n",
       "      <td id=\"T_e8219_row5_col8\" class=\"data row5 col8\" >0.0560</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th id=\"T_e8219_level0_row6\" class=\"row_heading level0 row6\" >rf</th>\n",
       "      <td id=\"T_e8219_row6_col0\" class=\"data row6 col0\" >Random Forest Classifier</td>\n",
       "      <td id=\"T_e8219_row6_col1\" class=\"data row6 col1\" >0.6305</td>\n",
       "      <td id=\"T_e8219_row6_col2\" class=\"data row6 col2\" >0.8571</td>\n",
       "      <td id=\"T_e8219_row6_col3\" class=\"data row6 col3\" >0.6305</td>\n",
       "      <td id=\"T_e8219_row6_col4\" class=\"data row6 col4\" >0.6097</td>\n",
       "      <td id=\"T_e8219_row6_col5\" class=\"data row6 col5\" >0.5810</td>\n",
       "      <td id=\"T_e8219_row6_col6\" class=\"data row6 col6\" >0.4691</td>\n",
       "      <td id=\"T_e8219_row6_col7\" class=\"data row6 col7\" >0.4888</td>\n",
       "      <td id=\"T_e8219_row6_col8\" class=\"data row6 col8\" >0.3810</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th id=\"T_e8219_level0_row7\" class=\"row_heading level0 row7\" >et</th>\n",
       "      <td id=\"T_e8219_row7_col0\" class=\"data row7 col0\" >Extra Trees Classifier</td>\n",
       "      <td id=\"T_e8219_row7_col1\" class=\"data row7 col1\" >0.6260</td>\n",
       "      <td id=\"T_e8219_row7_col2\" class=\"data row7 col2\" >0.8647</td>\n",
       "      <td id=\"T_e8219_row7_col3\" class=\"data row7 col3\" >0.6260</td>\n",
       "      <td id=\"T_e8219_row7_col4\" class=\"data row7 col4\" >0.6127</td>\n",
       "      <td id=\"T_e8219_row7_col5\" class=\"data row7 col5\" >0.5636</td>\n",
       "      <td id=\"T_e8219_row7_col6\" class=\"data row7 col6\" >0.4588</td>\n",
       "      <td id=\"T_e8219_row7_col7\" class=\"data row7 col7\" >0.4857</td>\n",
       "      <td id=\"T_e8219_row7_col8\" class=\"data row7 col8\" >0.1060</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th id=\"T_e8219_level0_row8\" class=\"row_heading level0 row8\" >knn</th>\n",
       "      <td id=\"T_e8219_row8_col0\" class=\"data row8 col0\" >K Neighbors Classifier</td>\n",
       "      <td id=\"T_e8219_row8_col1\" class=\"data row8 col1\" >0.6215</td>\n",
       "      <td id=\"T_e8219_row8_col2\" class=\"data row8 col2\" >0.8314</td>\n",
       "      <td id=\"T_e8219_row8_col3\" class=\"data row8 col3\" >0.6215</td>\n",
       "      <td id=\"T_e8219_row8_col4\" class=\"data row8 col4\" >0.6134</td>\n",
       "      <td id=\"T_e8219_row8_col5\" class=\"data row8 col5\" >0.6043</td>\n",
       "      <td id=\"T_e8219_row8_col6\" class=\"data row8 col6\" >0.4684</td>\n",
       "      <td id=\"T_e8219_row8_col7\" class=\"data row8 col7\" >0.4747</td>\n",
       "      <td id=\"T_e8219_row8_col8\" class=\"data row8 col8\" >0.2140</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th id=\"T_e8219_level0_row9\" class=\"row_heading level0 row9\" >qda</th>\n",
       "      <td id=\"T_e8219_row9_col0\" class=\"data row9 col0\" >Quadratic Discriminant Analysis</td>\n",
       "      <td id=\"T_e8219_row9_col1\" class=\"data row9 col1\" >0.6125</td>\n",
       "      <td id=\"T_e8219_row9_col2\" class=\"data row9 col2\" >0.7806</td>\n",
       "      <td id=\"T_e8219_row9_col3\" class=\"data row9 col3\" >0.6125</td>\n",
       "      <td id=\"T_e8219_row9_col4\" class=\"data row9 col4\" >0.5969</td>\n",
       "      <td id=\"T_e8219_row9_col5\" class=\"data row9 col5\" >0.4932</td>\n",
       "      <td id=\"T_e8219_row9_col6\" class=\"data row9 col6\" >0.4298</td>\n",
       "      <td id=\"T_e8219_row9_col7\" class=\"data row9 col7\" >0.4790</td>\n",
       "      <td id=\"T_e8219_row9_col8\" class=\"data row9 col8\" >0.0360</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th id=\"T_e8219_level0_row10\" class=\"row_heading level0 row10\" >ada</th>\n",
       "      <td id=\"T_e8219_row10_col0\" class=\"data row10 col0\" >Ada Boost Classifier</td>\n",
       "      <td id=\"T_e8219_row10_col1\" class=\"data row10 col1\" >0.5967</td>\n",
       "      <td id=\"T_e8219_row10_col2\" class=\"data row10 col2\" >0.8112</td>\n",
       "      <td id=\"T_e8219_row10_col3\" class=\"data row10 col3\" >0.5967</td>\n",
       "      <td id=\"T_e8219_row10_col4\" class=\"data row10 col4\" >0.5852</td>\n",
       "      <td id=\"T_e8219_row10_col5\" class=\"data row10 col5\" >0.5890</td>\n",
       "      <td id=\"T_e8219_row10_col6\" class=\"data row10 col6\" >0.4413</td>\n",
       "      <td id=\"T_e8219_row10_col7\" class=\"data row10 col7\" >0.4425</td>\n",
       "      <td id=\"T_e8219_row10_col8\" class=\"data row10 col8\" >0.6600</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th id=\"T_e8219_level0_row11\" class=\"row_heading level0 row11\" >nb</th>\n",
       "      <td id=\"T_e8219_row11_col0\" class=\"data row11 col0\" >Naive Bayes</td>\n",
       "      <td id=\"T_e8219_row11_col1\" class=\"data row11 col1\" >0.5934</td>\n",
       "      <td id=\"T_e8219_row11_col2\" class=\"data row11 col2\" >0.8249</td>\n",
       "      <td id=\"T_e8219_row11_col3\" class=\"data row11 col3\" >0.5934</td>\n",
       "      <td id=\"T_e8219_row11_col4\" class=\"data row11 col4\" >0.5949</td>\n",
       "      <td id=\"T_e8219_row11_col5\" class=\"data row11 col5\" >0.5933</td>\n",
       "      <td id=\"T_e8219_row11_col6\" class=\"data row11 col6\" >0.4423</td>\n",
       "      <td id=\"T_e8219_row11_col7\" class=\"data row11 col7\" >0.4428</td>\n",
       "      <td id=\"T_e8219_row11_col8\" class=\"data row11 col8\" >0.0270</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th id=\"T_e8219_level0_row12\" class=\"row_heading level0 row12\" >dt</th>\n",
       "      <td id=\"T_e8219_row12_col0\" class=\"data row12 col0\" >Decision Tree Classifier</td>\n",
       "      <td id=\"T_e8219_row12_col1\" class=\"data row12 col1\" >0.4587</td>\n",
       "      <td id=\"T_e8219_row12_col2\" class=\"data row12 col2\" >0.6353</td>\n",
       "      <td id=\"T_e8219_row12_col3\" class=\"data row12 col3\" >0.4587</td>\n",
       "      <td id=\"T_e8219_row12_col4\" class=\"data row12 col4\" >0.4639</td>\n",
       "      <td id=\"T_e8219_row12_col5\" class=\"data row12 col5\" >0.4602</td>\n",
       "      <td id=\"T_e8219_row12_col6\" class=\"data row12 col6\" >0.2590</td>\n",
       "      <td id=\"T_e8219_row12_col7\" class=\"data row12 col7\" >0.2595</td>\n",
       "      <td id=\"T_e8219_row12_col8\" class=\"data row12 col8\" >0.1000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th id=\"T_e8219_level0_row13\" class=\"row_heading level0 row13\" >dummy</th>\n",
       "      <td id=\"T_e8219_row13_col0\" class=\"data row13 col0\" >Dummy Classifier</td>\n",
       "      <td id=\"T_e8219_row13_col1\" class=\"data row13 col1\" >0.3407</td>\n",
       "      <td id=\"T_e8219_row13_col2\" class=\"data row13 col2\" >0.5000</td>\n",
       "      <td id=\"T_e8219_row13_col3\" class=\"data row13 col3\" >0.3407</td>\n",
       "      <td id=\"T_e8219_row13_col4\" class=\"data row13 col4\" >0.1161</td>\n",
       "      <td id=\"T_e8219_row13_col5\" class=\"data row13 col5\" >0.1731</td>\n",
       "      <td id=\"T_e8219_row13_col6\" class=\"data row13 col6\" >0.0000</td>\n",
       "      <td id=\"T_e8219_row13_col7\" class=\"data row13 col7\" >0.0000</td>\n",
       "      <td id=\"T_e8219_row13_col8\" class=\"data row13 col8\" >0.0220</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n"
      ],
      "text/plain": [
       "<pandas.io.formats.style.Styler at 0x20e3fb57b10>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<style>#sk-container-id-1 {color: black;background-color: white;}#sk-container-id-1 pre{padding: 0;}#sk-container-id-1 div.sk-toggleable {background-color: white;}#sk-container-id-1 label.sk-toggleable__label {cursor: pointer;display: block;width: 100%;margin-bottom: 0;padding: 0.3em;box-sizing: border-box;text-align: center;}#sk-container-id-1 label.sk-toggleable__label-arrow:before {content: \"▸\";float: left;margin-right: 0.25em;color: #696969;}#sk-container-id-1 label.sk-toggleable__label-arrow:hover:before {color: black;}#sk-container-id-1 div.sk-estimator:hover label.sk-toggleable__label-arrow:before {color: black;}#sk-container-id-1 div.sk-toggleable__content {max-height: 0;max-width: 0;overflow: hidden;text-align: left;background-color: #f0f8ff;}#sk-container-id-1 div.sk-toggleable__content pre {margin: 0.2em;color: black;border-radius: 0.25em;background-color: #f0f8ff;}#sk-container-id-1 input.sk-toggleable__control:checked~div.sk-toggleable__content {max-height: 200px;max-width: 100%;overflow: auto;}#sk-container-id-1 input.sk-toggleable__control:checked~label.sk-toggleable__label-arrow:before {content: \"▾\";}#sk-container-id-1 div.sk-estimator input.sk-toggleable__control:checked~label.sk-toggleable__label {background-color: #d4ebff;}#sk-container-id-1 div.sk-label input.sk-toggleable__control:checked~label.sk-toggleable__label {background-color: #d4ebff;}#sk-container-id-1 input.sk-hidden--visually {border: 0;clip: rect(1px 1px 1px 1px);clip: rect(1px, 1px, 1px, 1px);height: 1px;margin: -1px;overflow: hidden;padding: 0;position: absolute;width: 1px;}#sk-container-id-1 div.sk-estimator {font-family: monospace;background-color: #f0f8ff;border: 1px dotted black;border-radius: 0.25em;box-sizing: border-box;margin-bottom: 0.5em;}#sk-container-id-1 div.sk-estimator:hover {background-color: #d4ebff;}#sk-container-id-1 div.sk-parallel-item::after {content: \"\";width: 100%;border-bottom: 1px solid gray;flex-grow: 1;}#sk-container-id-1 div.sk-label:hover label.sk-toggleable__label {background-color: #d4ebff;}#sk-container-id-1 div.sk-serial::before {content: \"\";position: absolute;border-left: 1px solid gray;box-sizing: border-box;top: 0;bottom: 0;left: 50%;z-index: 0;}#sk-container-id-1 div.sk-serial {display: flex;flex-direction: column;align-items: center;background-color: white;padding-right: 0.2em;padding-left: 0.2em;position: relative;}#sk-container-id-1 div.sk-item {position: relative;z-index: 1;}#sk-container-id-1 div.sk-parallel {display: flex;align-items: stretch;justify-content: center;background-color: white;position: relative;}#sk-container-id-1 div.sk-item::before, #sk-container-id-1 div.sk-parallel-item::before {content: \"\";position: absolute;border-left: 1px solid gray;box-sizing: border-box;top: 0;bottom: 0;left: 50%;z-index: -1;}#sk-container-id-1 div.sk-parallel-item {display: flex;flex-direction: column;z-index: 1;position: relative;background-color: white;}#sk-container-id-1 div.sk-parallel-item:first-child::after {align-self: flex-end;width: 50%;}#sk-container-id-1 div.sk-parallel-item:last-child::after {align-self: flex-start;width: 50%;}#sk-container-id-1 div.sk-parallel-item:only-child::after {width: 0;}#sk-container-id-1 div.sk-dashed-wrapped {border: 1px dashed gray;margin: 0 0.4em 0.5em 0.4em;box-sizing: border-box;padding-bottom: 0.4em;background-color: white;}#sk-container-id-1 div.sk-label label {font-family: monospace;font-weight: bold;display: inline-block;line-height: 1.2em;}#sk-container-id-1 div.sk-label-container {text-align: center;}#sk-container-id-1 div.sk-container {/* jupyter's `normalize.less` sets `[hidden] { display: none; }` but bootstrap.min.css set `[hidden] { display: none !important; }` so we also need the `!important` here to be able to override the default hidden behavior on the sphinx rendered scikit-learn.org. See: https://github.com/scikit-learn/scikit-learn/issues/21755 */display: inline-block !important;position: relative;}#sk-container-id-1 div.sk-text-repr-fallback {display: none;}</style><div id=\"sk-container-id-1\" class=\"sk-top-container\"><div class=\"sk-text-repr-fallback\"><pre>RidgeClassifier(alpha=1.0, class_weight=None, copy_X=True, fit_intercept=True,\n",
       "                max_iter=None, positive=False, random_state=9088, solver=&#x27;auto&#x27;,\n",
       "                tol=0.0001)</pre><b>In a Jupyter environment, please rerun this cell to show the HTML representation or trust the notebook. <br />On GitHub, the HTML representation is unable to render, please try loading this page with nbviewer.org.</b></div><div class=\"sk-container\" hidden><div class=\"sk-item\"><div class=\"sk-estimator sk-toggleable\"><input class=\"sk-toggleable__control sk-hidden--visually\" id=\"sk-estimator-id-1\" type=\"checkbox\" checked><label for=\"sk-estimator-id-1\" class=\"sk-toggleable__label sk-toggleable__label-arrow\">RidgeClassifier</label><div class=\"sk-toggleable__content\"><pre>RidgeClassifier(alpha=1.0, class_weight=None, copy_X=True, fit_intercept=True,\n",
       "                max_iter=None, positive=False, random_state=9088, solver=&#x27;auto&#x27;,\n",
       "                tol=0.0001)</pre></div></div></div></div></div>"
      ],
      "text/plain": [
       "RidgeClassifier(alpha=1.0, class_weight=None, copy_X=True, fit_intercept=True,\n",
       "                max_iter=None, positive=False, random_state=9088, solver='auto',\n",
       "                tol=0.0001)"
      ]
     },
     "execution_count": 305,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "exp_class.compare_models()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "hide_input": false,
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.7"
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {},
   "number_sections": true,
   "sideBar": true,
   "skip_h1_title": true,
   "title_cell": "Summary",
   "title_sidebar": "Contents",
   "toc_cell": true,
   "toc_position": {
    "height": "calc(100% - 180px)",
    "left": "10px",
    "top": "150px",
    "width": "300.15px"
   },
   "toc_section_display": true,
   "toc_window_display": false
  },
  "varInspector": {
   "cols": {
    "lenName": 16,
    "lenType": 16,
    "lenVar": 40
   },
   "kernels_config": {
    "python": {
     "delete_cmd_postfix": "",
     "delete_cmd_prefix": "del ",
     "library": "var_list.py",
     "varRefreshCmd": "print(var_dic_list())"
    },
    "r": {
     "delete_cmd_postfix": ") ",
     "delete_cmd_prefix": "rm(",
     "library": "var_list.r",
     "varRefreshCmd": "cat(var_dic_list()) "
    }
   },
   "position": {
    "height": "498.85px",
    "left": "651.8px",
    "right": "20px",
    "top": "56px",
    "width": "715px"
   },
   "types_to_exclude": [
    "module",
    "function",
    "builtin_function_or_method",
    "instance",
    "_Feature"
   ],
   "window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
