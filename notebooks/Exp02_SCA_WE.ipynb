{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Semantic Similarity - Experiment 02\n",
    "The objective of this trial is to expand the SCA_index (i.e., Semantic Content Analysis Index) to a full word embedding, setting a subjective or objective load for each word.\n",
    "\n",
    "version 2: using SpaCy Universal Sentence Encoder, which means vectors with dimension 512 instead of 300."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Introduction"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-11-12T22:37:14.251208Z",
     "start_time": "2022-11-12T22:37:14.073574Z"
    }
   },
   "outputs": [],
   "source": [
    "## Data analysis packages:\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "#from math import isnan  #Verifies if a given value is numerical.\n",
    "#import re  # Regular Expression"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-11-12T22:37:15.470110Z",
     "start_time": "2022-11-12T22:37:14.895488Z"
    }
   },
   "outputs": [],
   "source": [
    "## Visualization packages:\n",
    "# import seaborn as sns\n",
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "## MLOps:\n",
    "import pickle"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Definitions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-04-28T22:15:12.656866Z",
     "start_time": "2022-04-28T22:15:12.612272Z"
    }
   },
   "outputs": [],
   "source": [
    "## Forcing Pandas to display any number of elements\n",
    "pd.set_option('display.max_columns', None)\n",
    "pd.set_option('display.max_rows', 50)\n",
    "pd.set_option('display.max_seq_items', None)\n",
    "pd.set_option('display.width', 2000)\n",
    "pd.set_option('display.max_colwidth', None)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## SpaCy Word Embeddings (WE): \n",
    "Also using Spacy library: https://spacy.io/\n",
    "> !pip install -U spacy  \n",
    "> !python -m spacy download en_core_web_sm  \n",
    "> !python -m spacy download en_core_web_lg\n",
    "\n",
    "Some instructions on how to use it:  \n",
    "https://spacy.io/usage/spacy-101"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-04-28T22:38:26.997959Z",
     "start_time": "2022-04-28T22:38:25.715707Z"
    }
   },
   "outputs": [],
   "source": [
    "## Importing SpaCy library:\n",
    "import spacy\n",
    "\n",
    "# Load English tokenizer, tagger, parser and NER\n",
    "nlp = spacy.load(\"en_core_web_lg\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'width': 300,\n",
       " 'vectors': 514157,\n",
       " 'keys': 514157,\n",
       " 'name': 'en_vectors',\n",
       " 'mode': 'default'}"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "## We check below that this model has 514.157 keys and vectors, respectively.\n",
    "nlp.meta['vectors']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "514157"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "## Again, checking the number of keys.\n",
    "nlp.vocab.vectors.n_keys"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "4473158105997569131"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "## Finding the SpaCy index for a given word:\n",
    "nlp.vocab.strings['problem']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Getting the text and vector for a given index:\n",
    "tmp_Idx = 4473158105997569131\n",
    "\n",
    "tmp_vector = nlp.vocab[tmp_Idx].vector\n",
    "tmp_text = nlp.vocab[tmp_Idx].text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "4473158105997569131: problem:\n",
      "[ 3.3611  4.0891 -2.1247]...\n"
     ]
    }
   ],
   "source": [
    "print(f'{tmp_Idx}: {tmp_text}:\\n{tmp_vector[0:3]}...')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "import spacy_universal_sentence_encoder"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "nlp = spacy_universal_sentence_encoder.load_model('en_use_md')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Defining a method to get vector and text information for a word:\n",
    "def nlp_getVector(word, verbose=False):\n",
    "    '''\n",
    "    Obtains the vector representation of a given word from SpaCy word embedding.\n",
    "    Usage:  nlp_getVector(word)[0] to get the text; nlp_getVector(word)[1] to get the vector.\n",
    "            var_text, var_vector = nlp_getVector(word)\n",
    "    '''\n",
    "    ## Generates the word hash:\n",
    "    # hash = nlp.vocab.strings[word]\n",
    "    try:\n",
    "        word_vector = nlp(word).vector\n",
    "        word_text = nlp(word).text\n",
    "    except:\n",
    "        if verbose:\n",
    "            print('Error: word vector not available.')\n",
    "        return None\n",
    "    return (word_text, word_vector)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Word: study <-> Vector 3rd elements: [-0.00239888 -0.06967633  0.0222383 ]\n"
     ]
    }
   ],
   "source": [
    "## Testing the method:\n",
    "print(f'Word: {nlp_getVector(\"study\")[0]} <-> Vector 3rd elements: {nlp_getVector(\"study\")[1][:3]}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "## Semantic Content Analysis (SCA)\n",
    "* Read the SCA obtained from Glasgow Norms data;  \n",
    "* Import F_s and F_o from the previous study;  \n",
    "* Generate datasets for training classifiers."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>words</th>\n",
       "      <th>F_Objectivity</th>\n",
       "      <th>F_Subjectivity</th>\n",
       "      <th>F_Context</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>abattoir</td>\n",
       "      <td>0.512527</td>\n",
       "      <td>0.380603</td>\n",
       "      <td>0.960466</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>abbey</td>\n",
       "      <td>0.714765</td>\n",
       "      <td>0.240456</td>\n",
       "      <td>0.696198</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>abbreviate</td>\n",
       "      <td>0.286952</td>\n",
       "      <td>0.171052</td>\n",
       "      <td>0.767043</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>abdicate</td>\n",
       "      <td>0.144736</td>\n",
       "      <td>0.384300</td>\n",
       "      <td>0.863127</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>abdication</td>\n",
       "      <td>0.167654</td>\n",
       "      <td>0.334086</td>\n",
       "      <td>0.896733</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "        words  F_Objectivity  F_Subjectivity  F_Context\n",
       "0    abattoir       0.512527        0.380603   0.960466\n",
       "1       abbey       0.714765        0.240456   0.696198\n",
       "2  abbreviate       0.286952        0.171052   0.767043\n",
       "3    abdicate       0.144736        0.384300   0.863127\n",
       "4  abdication       0.167654        0.334086   0.896733"
      ]
     },
     "execution_count": 27,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_factors = pd.read_csv('../data/df_factors.csv', sep=';')\n",
    "df_factors.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### De-Duplicating words  \n",
    "There are words in the Glasgow Norms that were differentiated from their homonymous, such as 'case'. In this section, we first select those words and then input a mean value for them."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>words</th>\n",
       "      <th>F_Objectivity</th>\n",
       "      <th>F_Subjectivity</th>\n",
       "      <th>F_Context</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>533</th>\n",
       "      <td>bookcase</td>\n",
       "      <td>0.926393</td>\n",
       "      <td>0.374441</td>\n",
       "      <td>0.335542</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>756</th>\n",
       "      <td>case</td>\n",
       "      <td>0.715863</td>\n",
       "      <td>0.164100</td>\n",
       "      <td>0.409611</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>757</th>\n",
       "      <td>case (container)</td>\n",
       "      <td>0.821136</td>\n",
       "      <td>0.079956</td>\n",
       "      <td>0.400335</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>758</th>\n",
       "      <td>case (instance)</td>\n",
       "      <td>0.233820</td>\n",
       "      <td>0.213528</td>\n",
       "      <td>0.651273</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>759</th>\n",
       "      <td>case (legal)</td>\n",
       "      <td>0.456260</td>\n",
       "      <td>0.369828</td>\n",
       "      <td>0.733642</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4808</th>\n",
       "      <td>suitcase</td>\n",
       "      <td>0.961068</td>\n",
       "      <td>0.256584</td>\n",
       "      <td>0.356338</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                 words  F_Objectivity  F_Subjectivity  F_Context\n",
       "533           bookcase       0.926393        0.374441   0.335542\n",
       "756               case       0.715863        0.164100   0.409611\n",
       "757   case (container)       0.821136        0.079956   0.400335\n",
       "758    case (instance)       0.233820        0.213528   0.651273\n",
       "759       case (legal)       0.456260        0.369828   0.733642\n",
       "4808          suitcase       0.961068        0.256584   0.356338"
      ]
     },
     "execution_count": 28,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "## Exemplifying the need for de-duplication:\n",
    "df_factors[df_factors['words'].str.contains('case')]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>words</th>\n",
       "      <th>F_Objectivity</th>\n",
       "      <th>F_Subjectivity</th>\n",
       "      <th>F_Context</th>\n",
       "      <th>word</th>\n",
       "      <th>distinction</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>abattoir</td>\n",
       "      <td>0.512527</td>\n",
       "      <td>0.380603</td>\n",
       "      <td>0.960466</td>\n",
       "      <td>abattoir</td>\n",
       "      <td>None</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>abbey</td>\n",
       "      <td>0.714765</td>\n",
       "      <td>0.240456</td>\n",
       "      <td>0.696198</td>\n",
       "      <td>abbey</td>\n",
       "      <td>None</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>abbreviate</td>\n",
       "      <td>0.286952</td>\n",
       "      <td>0.171052</td>\n",
       "      <td>0.767043</td>\n",
       "      <td>abbreviate</td>\n",
       "      <td>None</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>abdicate</td>\n",
       "      <td>0.144736</td>\n",
       "      <td>0.384300</td>\n",
       "      <td>0.863127</td>\n",
       "      <td>abdicate</td>\n",
       "      <td>None</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>abdication</td>\n",
       "      <td>0.167654</td>\n",
       "      <td>0.334086</td>\n",
       "      <td>0.896733</td>\n",
       "      <td>abdication</td>\n",
       "      <td>None</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "        words  F_Objectivity  F_Subjectivity  F_Context        word distinction\n",
       "0    abattoir       0.512527        0.380603   0.960466    abattoir        None\n",
       "1       abbey       0.714765        0.240456   0.696198       abbey        None\n",
       "2  abbreviate       0.286952        0.171052   0.767043  abbreviate        None\n",
       "3    abdicate       0.144736        0.384300   0.863127    abdicate        None\n",
       "4  abdication       0.167654        0.334086   0.896733  abdication        None"
      ]
     },
     "execution_count": 29,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "## Creating a new dataframe by splitting the 'words' column into two columns:\n",
    "df_homonym = df_factors.copy()\n",
    "df_homonym[['word','distinction']] = df_homonym['words'].str.split('(', expand=True)\n",
    "## Renaming the columns of the new dataframe:\n",
    "# df_homonym.columns = ['word','distinction']\n",
    "\n",
    "# Stripping whitespace from the new columns\n",
    "df_homonym['word'] = df_homonym['word'].str.strip()\n",
    "df_homonym['distinction'] = df_homonym['distinction'].str.strip().str.rstrip(')')\n",
    "\n",
    "## Showing dataframe:\n",
    "df_homonym.head(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>n_distinction</th>\n",
       "      <th>n_words</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0</td>\n",
       "      <td>4303</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>2</td>\n",
       "      <td>288</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>3</td>\n",
       "      <td>69</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>4</td>\n",
       "      <td>19</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>5</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   n_distinction  n_words\n",
       "0              0     4303\n",
       "1              1        2\n",
       "2              2      288\n",
       "3              3       69\n",
       "4              4       19\n",
       "5              5        2"
      ]
     },
     "execution_count": 30,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Calculating the number of distinct elements in the 'distinction' column for each word in the 'word' column\n",
    "distinct_count = df_homonym.groupby('word')['distinction'].nunique().reset_index()\n",
    "\n",
    "# Counting the frequency of different numbers of distinct elements\n",
    "pivot_table = distinct_count.groupby('distinction')['word'].count().reset_index()\n",
    "\n",
    "# Renaming the columns\n",
    "pivot_table.columns = ['n_distinction', 'n_words']\n",
    "pivot_table"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Analyzing the words with 5 distinct meaning:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>word</th>\n",
       "      <th>distinction</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>721</th>\n",
       "      <td>charge</td>\n",
       "      <td>5</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>822</th>\n",
       "      <td>club</td>\n",
       "      <td>5</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "       word  distinction\n",
       "721  charge            5\n",
       "822    club            5"
      ]
     },
     "execution_count": 31,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "## Getting the word with the highest number of distinct meanings: \n",
    "distinct_count[distinct_count['distinction'] == 5]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>words</th>\n",
       "      <th>F_Objectivity</th>\n",
       "      <th>F_Subjectivity</th>\n",
       "      <th>F_Context</th>\n",
       "      <th>word</th>\n",
       "      <th>distinction</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>819</th>\n",
       "      <td>charge</td>\n",
       "      <td>0.340881</td>\n",
       "      <td>0.330534</td>\n",
       "      <td>0.438418</td>\n",
       "      <td>charge</td>\n",
       "      <td>None</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>820</th>\n",
       "      <td>charge (card)</td>\n",
       "      <td>0.477083</td>\n",
       "      <td>0.293352</td>\n",
       "      <td>0.671798</td>\n",
       "      <td>charge</td>\n",
       "      <td>card</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>821</th>\n",
       "      <td>charge (dependent)</td>\n",
       "      <td>0.256759</td>\n",
       "      <td>0.164572</td>\n",
       "      <td>0.791042</td>\n",
       "      <td>charge</td>\n",
       "      <td>dependent</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>822</th>\n",
       "      <td>charge (electric)</td>\n",
       "      <td>0.391504</td>\n",
       "      <td>0.413080</td>\n",
       "      <td>0.697287</td>\n",
       "      <td>charge</td>\n",
       "      <td>electric</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>823</th>\n",
       "      <td>charge (price)</td>\n",
       "      <td>0.428509</td>\n",
       "      <td>0.353942</td>\n",
       "      <td>0.498770</td>\n",
       "      <td>charge</td>\n",
       "      <td>price</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>824</th>\n",
       "      <td>charge (rush)</td>\n",
       "      <td>0.551191</td>\n",
       "      <td>0.557549</td>\n",
       "      <td>0.591914</td>\n",
       "      <td>charge</td>\n",
       "      <td>rush</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>942</th>\n",
       "      <td>club</td>\n",
       "      <td>0.774815</td>\n",
       "      <td>0.476734</td>\n",
       "      <td>0.467275</td>\n",
       "      <td>club</td>\n",
       "      <td>None</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>943</th>\n",
       "      <td>club (card suit)</td>\n",
       "      <td>0.773094</td>\n",
       "      <td>0.187918</td>\n",
       "      <td>0.492067</td>\n",
       "      <td>club</td>\n",
       "      <td>card suit</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>944</th>\n",
       "      <td>club (disco)</td>\n",
       "      <td>0.863845</td>\n",
       "      <td>0.547812</td>\n",
       "      <td>0.658553</td>\n",
       "      <td>club</td>\n",
       "      <td>disco</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>945</th>\n",
       "      <td>club (hit)</td>\n",
       "      <td>0.725571</td>\n",
       "      <td>0.435103</td>\n",
       "      <td>0.607930</td>\n",
       "      <td>club</td>\n",
       "      <td>hit</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>946</th>\n",
       "      <td>club (organisation)</td>\n",
       "      <td>0.699817</td>\n",
       "      <td>0.428406</td>\n",
       "      <td>0.356641</td>\n",
       "      <td>club</td>\n",
       "      <td>organisation</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>947</th>\n",
       "      <td>club (tool)</td>\n",
       "      <td>0.860703</td>\n",
       "      <td>0.289625</td>\n",
       "      <td>0.537132</td>\n",
       "      <td>club</td>\n",
       "      <td>tool</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                   words  F_Objectivity  F_Subjectivity  F_Context    word   distinction\n",
       "819               charge       0.340881        0.330534   0.438418  charge          None\n",
       "820        charge (card)       0.477083        0.293352   0.671798  charge          card\n",
       "821   charge (dependent)       0.256759        0.164572   0.791042  charge     dependent\n",
       "822    charge (electric)       0.391504        0.413080   0.697287  charge      electric\n",
       "823       charge (price)       0.428509        0.353942   0.498770  charge         price\n",
       "824        charge (rush)       0.551191        0.557549   0.591914  charge          rush\n",
       "942                 club       0.774815        0.476734   0.467275    club          None\n",
       "943     club (card suit)       0.773094        0.187918   0.492067    club     card suit\n",
       "944         club (disco)       0.863845        0.547812   0.658553    club         disco\n",
       "945           club (hit)       0.725571        0.435103   0.607930    club           hit\n",
       "946  club (organisation)       0.699817        0.428406   0.356641    club  organisation\n",
       "947          club (tool)       0.860703        0.289625   0.537132    club          tool"
      ]
     },
     "execution_count": 32,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_homonym[(df_homonym['word']=='club') | (df_homonym['word']=='charge')]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Replacing the distinct values for the average (column 'word'):"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Calculating the mean of F_Objectivity and F_Subjectivity for each group of \"word\"\n",
    "mean_values = df_homonym.groupby('word')[['F_Objectivity', 'F_Subjectivity']].mean().reset_index()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>word</th>\n",
       "      <th>F_Objectivity</th>\n",
       "      <th>F_Subjectivity</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Apple</td>\n",
       "      <td>0.940620</td>\n",
       "      <td>0.524376</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>Christmas</td>\n",
       "      <td>0.850793</td>\n",
       "      <td>0.833898</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>Dad</td>\n",
       "      <td>0.856533</td>\n",
       "      <td>0.493834</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>Dame</td>\n",
       "      <td>0.626968</td>\n",
       "      <td>0.300580</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>FALSE</td>\n",
       "      <td>0.156905</td>\n",
       "      <td>0.473624</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4678</th>\n",
       "      <td>zeppelin</td>\n",
       "      <td>0.864760</td>\n",
       "      <td>0.396531</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4679</th>\n",
       "      <td>zero</td>\n",
       "      <td>0.379392</td>\n",
       "      <td>0.315118</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4680</th>\n",
       "      <td>zest</td>\n",
       "      <td>0.402894</td>\n",
       "      <td>0.476800</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4681</th>\n",
       "      <td>zoo</td>\n",
       "      <td>0.867152</td>\n",
       "      <td>0.507390</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4682</th>\n",
       "      <td>zoology</td>\n",
       "      <td>0.420591</td>\n",
       "      <td>0.345092</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>4683 rows × 3 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "           word  F_Objectivity  F_Subjectivity\n",
       "0         Apple       0.940620        0.524376\n",
       "1     Christmas       0.850793        0.833898\n",
       "2           Dad       0.856533        0.493834\n",
       "3          Dame       0.626968        0.300580\n",
       "4         FALSE       0.156905        0.473624\n",
       "...         ...            ...             ...\n",
       "4678   zeppelin       0.864760        0.396531\n",
       "4679       zero       0.379392        0.315118\n",
       "4680       zest       0.402894        0.476800\n",
       "4681        zoo       0.867152        0.507390\n",
       "4682    zoology       0.420591        0.345092\n",
       "\n",
       "[4683 rows x 3 columns]"
      ]
     },
     "execution_count": 34,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "mean_values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>words</th>\n",
       "      <th>F_Objectivity</th>\n",
       "      <th>F_Subjectivity</th>\n",
       "      <th>F_Context</th>\n",
       "      <th>word</th>\n",
       "      <th>distinction</th>\n",
       "      <th>F_Objectivity_mean</th>\n",
       "      <th>F_Subjectivity_mean</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>819</th>\n",
       "      <td>charge</td>\n",
       "      <td>0.340881</td>\n",
       "      <td>0.330534</td>\n",
       "      <td>0.438418</td>\n",
       "      <td>charge</td>\n",
       "      <td>None</td>\n",
       "      <td>0.407655</td>\n",
       "      <td>0.352172</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>820</th>\n",
       "      <td>charge (card)</td>\n",
       "      <td>0.477083</td>\n",
       "      <td>0.293352</td>\n",
       "      <td>0.671798</td>\n",
       "      <td>charge</td>\n",
       "      <td>card</td>\n",
       "      <td>0.407655</td>\n",
       "      <td>0.352172</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>821</th>\n",
       "      <td>charge (dependent)</td>\n",
       "      <td>0.256759</td>\n",
       "      <td>0.164572</td>\n",
       "      <td>0.791042</td>\n",
       "      <td>charge</td>\n",
       "      <td>dependent</td>\n",
       "      <td>0.407655</td>\n",
       "      <td>0.352172</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>822</th>\n",
       "      <td>charge (electric)</td>\n",
       "      <td>0.391504</td>\n",
       "      <td>0.413080</td>\n",
       "      <td>0.697287</td>\n",
       "      <td>charge</td>\n",
       "      <td>electric</td>\n",
       "      <td>0.407655</td>\n",
       "      <td>0.352172</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>823</th>\n",
       "      <td>charge (price)</td>\n",
       "      <td>0.428509</td>\n",
       "      <td>0.353942</td>\n",
       "      <td>0.498770</td>\n",
       "      <td>charge</td>\n",
       "      <td>price</td>\n",
       "      <td>0.407655</td>\n",
       "      <td>0.352172</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>824</th>\n",
       "      <td>charge (rush)</td>\n",
       "      <td>0.551191</td>\n",
       "      <td>0.557549</td>\n",
       "      <td>0.591914</td>\n",
       "      <td>charge</td>\n",
       "      <td>rush</td>\n",
       "      <td>0.407655</td>\n",
       "      <td>0.352172</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                  words  F_Objectivity  F_Subjectivity  F_Context    word distinction  F_Objectivity_mean  F_Subjectivity_mean\n",
       "819              charge       0.340881        0.330534   0.438418  charge        None            0.407655             0.352172\n",
       "820       charge (card)       0.477083        0.293352   0.671798  charge        card            0.407655             0.352172\n",
       "821  charge (dependent)       0.256759        0.164572   0.791042  charge   dependent            0.407655             0.352172\n",
       "822   charge (electric)       0.391504        0.413080   0.697287  charge    electric            0.407655             0.352172\n",
       "823      charge (price)       0.428509        0.353942   0.498770  charge       price            0.407655             0.352172\n",
       "824       charge (rush)       0.551191        0.557549   0.591914  charge        rush            0.407655             0.352172"
      ]
     },
     "execution_count": 35,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "## Merging the mean values back into the original DataFrame\n",
    "df = pd.merge(df_homonym, mean_values, on='word', suffixes=('', '_mean'))\n",
    "\n",
    "## Checking an example (is the \"_mean\" values equal for all instances?): \n",
    "df[df['word'] == 'charge']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Dropping duplicated words (by column)\n",
    "df.drop_duplicates(subset=['word'], inplace=True)\n",
    "\n",
    "## Selecting only the columns of interest:\n",
    "df = df[['word', 'F_Objectivity_mean', 'F_Subjectivity_mean']]\n",
    "\n",
    "## Renaming the columns to remove the \"_mean\" suffix\n",
    "df.rename(columns={'F_Objectivity_mean': 'F_Objectivity', 'F_Subjectivity_mean': 'F_Subjectivity'}, inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'pandas.core.frame.DataFrame'>\n",
      "Int64Index: 4683 entries, 0 to 5552\n",
      "Data columns (total 3 columns):\n",
      " #   Column          Non-Null Count  Dtype  \n",
      "---  ------          --------------  -----  \n",
      " 0   word            4683 non-null   object \n",
      " 1   F_Objectivity   4683 non-null   float64\n",
      " 2   F_Subjectivity  4683 non-null   float64\n",
      "dtypes: float64(2), object(1)\n",
      "memory usage: 146.3+ KB\n",
      "None\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>word</th>\n",
       "      <th>F_Objectivity</th>\n",
       "      <th>F_Subjectivity</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>abattoir</td>\n",
       "      <td>0.512527</td>\n",
       "      <td>0.380603</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>abbey</td>\n",
       "      <td>0.714765</td>\n",
       "      <td>0.240456</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>abbreviate</td>\n",
       "      <td>0.286952</td>\n",
       "      <td>0.171052</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>abdicate</td>\n",
       "      <td>0.144736</td>\n",
       "      <td>0.384300</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>abdication</td>\n",
       "      <td>0.167654</td>\n",
       "      <td>0.334086</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "         word  F_Objectivity  F_Subjectivity\n",
       "0    abattoir       0.512527        0.380603\n",
       "1       abbey       0.714765        0.240456\n",
       "2  abbreviate       0.286952        0.171052\n",
       "3    abdicate       0.144736        0.384300\n",
       "4  abdication       0.167654        0.334086"
      ]
     },
     "execution_count": 37,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "## Showing dataframe:\n",
    "print(df.info())\n",
    "df.head(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Saving the prepared data:\n",
    "# df.to_csv('../data/df_factors_prepared.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ### Selecionando apenas as palavras no df_factors que atendam aos critérios:\n",
    "# df_selected = df.loc[((df['F_Subjectivity'] > 0.75) | (df['F_Subjectivity'] < 0.3)) & ((df['F_Objectivity'] > 0.75) | (df['F_Objectivity'] < 0.3))]\n",
    "# df_factors = df_selected.copy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "4683"
      ]
     },
     "execution_count": 40,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "## Generating a list of words from SCA for training.\n",
    "SCA_words = [word for word in df.word]\n",
    "len(SCA_words)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Preparing dataframe for training:\n",
    "# df.set_index('word', inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>word</th>\n",
       "      <th>F_Objectivity</th>\n",
       "      <th>F_Subjectivity</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>abattoir</td>\n",
       "      <td>0.512527</td>\n",
       "      <td>0.380603</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>abbey</td>\n",
       "      <td>0.714765</td>\n",
       "      <td>0.240456</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>abbreviate</td>\n",
       "      <td>0.286952</td>\n",
       "      <td>0.171052</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>abdicate</td>\n",
       "      <td>0.144736</td>\n",
       "      <td>0.384300</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>abdication</td>\n",
       "      <td>0.167654</td>\n",
       "      <td>0.334086</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5548</th>\n",
       "      <td>zeppelin</td>\n",
       "      <td>0.864760</td>\n",
       "      <td>0.396531</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5549</th>\n",
       "      <td>zero</td>\n",
       "      <td>0.379392</td>\n",
       "      <td>0.315118</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5550</th>\n",
       "      <td>zest</td>\n",
       "      <td>0.402894</td>\n",
       "      <td>0.476800</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5551</th>\n",
       "      <td>zoo</td>\n",
       "      <td>0.867152</td>\n",
       "      <td>0.507390</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5552</th>\n",
       "      <td>zoology</td>\n",
       "      <td>0.420591</td>\n",
       "      <td>0.345092</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>4683 rows × 3 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "            word  F_Objectivity  F_Subjectivity\n",
       "0       abattoir       0.512527        0.380603\n",
       "1          abbey       0.714765        0.240456\n",
       "2     abbreviate       0.286952        0.171052\n",
       "3       abdicate       0.144736        0.384300\n",
       "4     abdication       0.167654        0.334086\n",
       "...          ...            ...             ...\n",
       "5548    zeppelin       0.864760        0.396531\n",
       "5549        zero       0.379392        0.315118\n",
       "5550        zest       0.402894        0.476800\n",
       "5551         zoo       0.867152        0.507390\n",
       "5552     zoology       0.420591        0.345092\n",
       "\n",
       "[4683 rows x 3 columns]"
      ]
     },
     "execution_count": 42,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## DataPrep for machine learning classifiers\n",
    "This section prepares data from Semantic Content Analysis (SCA) and Word Embedding (WE) sources for machine learning classification tasks. Steps include data integration, cleaning, feature engineering, transformation, and splitting into training and test sets for linear and multilabel classification. The goal is to optimize data quality and model performance."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "# Splitting data in train (75%) and test (25%)\n",
    "train_df, test_df = train_test_split(df, test_size=0.25, random_state=42)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'pandas.core.frame.DataFrame'>\n",
      "Int64Index: 3512 entries, 213 to 1012\n",
      "Data columns (total 3 columns):\n",
      " #   Column          Non-Null Count  Dtype  \n",
      "---  ------          --------------  -----  \n",
      " 0   word            3512 non-null   object \n",
      " 1   F_Objectivity   3512 non-null   float64\n",
      " 2   F_Subjectivity  3512 non-null   float64\n",
      "dtypes: float64(2), object(1)\n",
      "memory usage: 109.8+ KB\n",
      "None\n",
      "<class 'pandas.core.frame.DataFrame'>\n",
      "Int64Index: 1171 entries, 513 to 3371\n",
      "Data columns (total 3 columns):\n",
      " #   Column          Non-Null Count  Dtype  \n",
      "---  ------          --------------  -----  \n",
      " 0   word            1171 non-null   object \n",
      " 1   F_Objectivity   1171 non-null   float64\n",
      " 2   F_Subjectivity  1171 non-null   float64\n",
      "dtypes: float64(2), object(1)\n",
      "memory usage: 36.6+ KB\n",
      "None\n"
     ]
    }
   ],
   "source": [
    "print(train_df.info())\n",
    "print(test_df.info())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [],
   "source": [
    "def generateData(df_sca:pd.DataFrame):\n",
    "    '''Creates a dataset with explanatory variables from word embedding and dependent variables based on the SCA target data.\n",
    "    Usage example: \"X_train, Y_train = generateData(df)\"\n",
    "    Input: Pandas.DataFrame with SCA words.\n",
    "    Output: X: dataframe with explanatory variables; Y: dataframe with dependent variables.\n",
    "    '''\n",
    "    X = {}\n",
    "    Y = {}\n",
    "    \n",
    "    for i, row in df_sca.iterrows():\n",
    "        word = row['word']\n",
    "        f_objectivity = row['F_Objectivity']\n",
    "        f_subjectivity = row['F_Subjectivity']\n",
    "\n",
    "        try:\n",
    "            X[word] = nlp_getVector(word)[1]  # Stores the word vector\n",
    "            Y[word] = {'F_Objectivity': f_objectivity, 'F_Subjectivity': f_subjectivity}\n",
    "            # print(f'Debug: {word} <=> {nlp_getVector(word)[0]}')\n",
    "        except:\n",
    "            pass\n",
    "\n",
    "    return pd.DataFrame.from_dict(X, orient='index'), pd.DataFrame.from_dict(Y, orient='index')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### A) Regression models: splitting train and test data\n",
    "- Multi output (F_obj, F_sub), with continuous values."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Generating data:\n",
    "X_train_A, Y_train_A = generateData(train_df)\n",
    "X_test_A, Y_test_A = generateData(test_df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train data dimension:\n",
      "X_train: (3512, 512)\n",
      "Y_train: (3512, 2)\n",
      "\n",
      "Test data dimension:\n",
      "X_test: (1171, 512)\n",
      "Y_test: (1171, 2)\n"
     ]
    }
   ],
   "source": [
    "## Checking the generated data dimension:\n",
    "print(\"Train data dimension:\")\n",
    "print(\"X_train:\", X_train_A.shape)\n",
    "print(\"Y_train:\", Y_train_A.shape)\n",
    "\n",
    "print(\"\\nTest data dimension:\")\n",
    "print(\"X_test:\", X_test_A.shape)\n",
    "print(\"Y_test:\", Y_test_A.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>0</th>\n",
       "      <th>1</th>\n",
       "      <th>2</th>\n",
       "      <th>3</th>\n",
       "      <th>4</th>\n",
       "      <th>5</th>\n",
       "      <th>6</th>\n",
       "      <th>7</th>\n",
       "      <th>8</th>\n",
       "      <th>9</th>\n",
       "      <th>10</th>\n",
       "      <th>11</th>\n",
       "      <th>12</th>\n",
       "      <th>13</th>\n",
       "      <th>14</th>\n",
       "      <th>15</th>\n",
       "      <th>16</th>\n",
       "      <th>17</th>\n",
       "      <th>18</th>\n",
       "      <th>19</th>\n",
       "      <th>20</th>\n",
       "      <th>21</th>\n",
       "      <th>22</th>\n",
       "      <th>23</th>\n",
       "      <th>24</th>\n",
       "      <th>25</th>\n",
       "      <th>26</th>\n",
       "      <th>27</th>\n",
       "      <th>28</th>\n",
       "      <th>29</th>\n",
       "      <th>30</th>\n",
       "      <th>31</th>\n",
       "      <th>32</th>\n",
       "      <th>33</th>\n",
       "      <th>34</th>\n",
       "      <th>35</th>\n",
       "      <th>36</th>\n",
       "      <th>37</th>\n",
       "      <th>38</th>\n",
       "      <th>39</th>\n",
       "      <th>40</th>\n",
       "      <th>41</th>\n",
       "      <th>42</th>\n",
       "      <th>43</th>\n",
       "      <th>44</th>\n",
       "      <th>45</th>\n",
       "      <th>46</th>\n",
       "      <th>47</th>\n",
       "      <th>48</th>\n",
       "      <th>49</th>\n",
       "      <th>50</th>\n",
       "      <th>51</th>\n",
       "      <th>52</th>\n",
       "      <th>53</th>\n",
       "      <th>54</th>\n",
       "      <th>55</th>\n",
       "      <th>56</th>\n",
       "      <th>57</th>\n",
       "      <th>58</th>\n",
       "      <th>59</th>\n",
       "      <th>60</th>\n",
       "      <th>61</th>\n",
       "      <th>62</th>\n",
       "      <th>63</th>\n",
       "      <th>64</th>\n",
       "      <th>65</th>\n",
       "      <th>66</th>\n",
       "      <th>67</th>\n",
       "      <th>68</th>\n",
       "      <th>69</th>\n",
       "      <th>70</th>\n",
       "      <th>71</th>\n",
       "      <th>72</th>\n",
       "      <th>73</th>\n",
       "      <th>74</th>\n",
       "      <th>75</th>\n",
       "      <th>76</th>\n",
       "      <th>77</th>\n",
       "      <th>78</th>\n",
       "      <th>79</th>\n",
       "      <th>80</th>\n",
       "      <th>81</th>\n",
       "      <th>82</th>\n",
       "      <th>83</th>\n",
       "      <th>84</th>\n",
       "      <th>85</th>\n",
       "      <th>86</th>\n",
       "      <th>87</th>\n",
       "      <th>88</th>\n",
       "      <th>89</th>\n",
       "      <th>90</th>\n",
       "      <th>91</th>\n",
       "      <th>92</th>\n",
       "      <th>93</th>\n",
       "      <th>94</th>\n",
       "      <th>95</th>\n",
       "      <th>96</th>\n",
       "      <th>97</th>\n",
       "      <th>98</th>\n",
       "      <th>99</th>\n",
       "      <th>100</th>\n",
       "      <th>101</th>\n",
       "      <th>102</th>\n",
       "      <th>103</th>\n",
       "      <th>104</th>\n",
       "      <th>105</th>\n",
       "      <th>106</th>\n",
       "      <th>107</th>\n",
       "      <th>108</th>\n",
       "      <th>109</th>\n",
       "      <th>110</th>\n",
       "      <th>111</th>\n",
       "      <th>112</th>\n",
       "      <th>113</th>\n",
       "      <th>114</th>\n",
       "      <th>115</th>\n",
       "      <th>116</th>\n",
       "      <th>117</th>\n",
       "      <th>118</th>\n",
       "      <th>119</th>\n",
       "      <th>120</th>\n",
       "      <th>121</th>\n",
       "      <th>122</th>\n",
       "      <th>123</th>\n",
       "      <th>124</th>\n",
       "      <th>125</th>\n",
       "      <th>126</th>\n",
       "      <th>127</th>\n",
       "      <th>128</th>\n",
       "      <th>129</th>\n",
       "      <th>130</th>\n",
       "      <th>131</th>\n",
       "      <th>132</th>\n",
       "      <th>133</th>\n",
       "      <th>134</th>\n",
       "      <th>135</th>\n",
       "      <th>136</th>\n",
       "      <th>137</th>\n",
       "      <th>138</th>\n",
       "      <th>139</th>\n",
       "      <th>140</th>\n",
       "      <th>141</th>\n",
       "      <th>142</th>\n",
       "      <th>143</th>\n",
       "      <th>144</th>\n",
       "      <th>145</th>\n",
       "      <th>146</th>\n",
       "      <th>147</th>\n",
       "      <th>148</th>\n",
       "      <th>149</th>\n",
       "      <th>150</th>\n",
       "      <th>151</th>\n",
       "      <th>152</th>\n",
       "      <th>153</th>\n",
       "      <th>154</th>\n",
       "      <th>155</th>\n",
       "      <th>156</th>\n",
       "      <th>157</th>\n",
       "      <th>158</th>\n",
       "      <th>159</th>\n",
       "      <th>160</th>\n",
       "      <th>161</th>\n",
       "      <th>162</th>\n",
       "      <th>163</th>\n",
       "      <th>164</th>\n",
       "      <th>165</th>\n",
       "      <th>166</th>\n",
       "      <th>167</th>\n",
       "      <th>168</th>\n",
       "      <th>169</th>\n",
       "      <th>170</th>\n",
       "      <th>171</th>\n",
       "      <th>172</th>\n",
       "      <th>173</th>\n",
       "      <th>174</th>\n",
       "      <th>175</th>\n",
       "      <th>176</th>\n",
       "      <th>177</th>\n",
       "      <th>178</th>\n",
       "      <th>179</th>\n",
       "      <th>180</th>\n",
       "      <th>181</th>\n",
       "      <th>182</th>\n",
       "      <th>183</th>\n",
       "      <th>184</th>\n",
       "      <th>185</th>\n",
       "      <th>186</th>\n",
       "      <th>187</th>\n",
       "      <th>188</th>\n",
       "      <th>189</th>\n",
       "      <th>190</th>\n",
       "      <th>191</th>\n",
       "      <th>192</th>\n",
       "      <th>193</th>\n",
       "      <th>194</th>\n",
       "      <th>195</th>\n",
       "      <th>196</th>\n",
       "      <th>197</th>\n",
       "      <th>198</th>\n",
       "      <th>199</th>\n",
       "      <th>200</th>\n",
       "      <th>201</th>\n",
       "      <th>202</th>\n",
       "      <th>203</th>\n",
       "      <th>204</th>\n",
       "      <th>205</th>\n",
       "      <th>206</th>\n",
       "      <th>207</th>\n",
       "      <th>208</th>\n",
       "      <th>209</th>\n",
       "      <th>210</th>\n",
       "      <th>211</th>\n",
       "      <th>212</th>\n",
       "      <th>213</th>\n",
       "      <th>214</th>\n",
       "      <th>215</th>\n",
       "      <th>216</th>\n",
       "      <th>217</th>\n",
       "      <th>218</th>\n",
       "      <th>219</th>\n",
       "      <th>220</th>\n",
       "      <th>221</th>\n",
       "      <th>222</th>\n",
       "      <th>223</th>\n",
       "      <th>224</th>\n",
       "      <th>225</th>\n",
       "      <th>226</th>\n",
       "      <th>227</th>\n",
       "      <th>228</th>\n",
       "      <th>229</th>\n",
       "      <th>230</th>\n",
       "      <th>231</th>\n",
       "      <th>232</th>\n",
       "      <th>233</th>\n",
       "      <th>234</th>\n",
       "      <th>235</th>\n",
       "      <th>236</th>\n",
       "      <th>237</th>\n",
       "      <th>238</th>\n",
       "      <th>239</th>\n",
       "      <th>240</th>\n",
       "      <th>241</th>\n",
       "      <th>242</th>\n",
       "      <th>243</th>\n",
       "      <th>244</th>\n",
       "      <th>245</th>\n",
       "      <th>246</th>\n",
       "      <th>247</th>\n",
       "      <th>248</th>\n",
       "      <th>249</th>\n",
       "      <th>250</th>\n",
       "      <th>251</th>\n",
       "      <th>252</th>\n",
       "      <th>253</th>\n",
       "      <th>254</th>\n",
       "      <th>255</th>\n",
       "      <th>256</th>\n",
       "      <th>257</th>\n",
       "      <th>258</th>\n",
       "      <th>259</th>\n",
       "      <th>260</th>\n",
       "      <th>261</th>\n",
       "      <th>262</th>\n",
       "      <th>263</th>\n",
       "      <th>264</th>\n",
       "      <th>265</th>\n",
       "      <th>266</th>\n",
       "      <th>267</th>\n",
       "      <th>268</th>\n",
       "      <th>269</th>\n",
       "      <th>270</th>\n",
       "      <th>271</th>\n",
       "      <th>272</th>\n",
       "      <th>273</th>\n",
       "      <th>274</th>\n",
       "      <th>275</th>\n",
       "      <th>276</th>\n",
       "      <th>277</th>\n",
       "      <th>278</th>\n",
       "      <th>279</th>\n",
       "      <th>280</th>\n",
       "      <th>281</th>\n",
       "      <th>282</th>\n",
       "      <th>283</th>\n",
       "      <th>284</th>\n",
       "      <th>285</th>\n",
       "      <th>286</th>\n",
       "      <th>287</th>\n",
       "      <th>288</th>\n",
       "      <th>289</th>\n",
       "      <th>290</th>\n",
       "      <th>291</th>\n",
       "      <th>292</th>\n",
       "      <th>293</th>\n",
       "      <th>294</th>\n",
       "      <th>295</th>\n",
       "      <th>296</th>\n",
       "      <th>297</th>\n",
       "      <th>298</th>\n",
       "      <th>299</th>\n",
       "      <th>300</th>\n",
       "      <th>301</th>\n",
       "      <th>302</th>\n",
       "      <th>303</th>\n",
       "      <th>304</th>\n",
       "      <th>305</th>\n",
       "      <th>306</th>\n",
       "      <th>307</th>\n",
       "      <th>308</th>\n",
       "      <th>309</th>\n",
       "      <th>310</th>\n",
       "      <th>311</th>\n",
       "      <th>312</th>\n",
       "      <th>313</th>\n",
       "      <th>314</th>\n",
       "      <th>315</th>\n",
       "      <th>316</th>\n",
       "      <th>317</th>\n",
       "      <th>318</th>\n",
       "      <th>319</th>\n",
       "      <th>320</th>\n",
       "      <th>321</th>\n",
       "      <th>322</th>\n",
       "      <th>323</th>\n",
       "      <th>324</th>\n",
       "      <th>325</th>\n",
       "      <th>326</th>\n",
       "      <th>327</th>\n",
       "      <th>328</th>\n",
       "      <th>329</th>\n",
       "      <th>330</th>\n",
       "      <th>331</th>\n",
       "      <th>332</th>\n",
       "      <th>333</th>\n",
       "      <th>334</th>\n",
       "      <th>335</th>\n",
       "      <th>336</th>\n",
       "      <th>337</th>\n",
       "      <th>338</th>\n",
       "      <th>339</th>\n",
       "      <th>340</th>\n",
       "      <th>341</th>\n",
       "      <th>342</th>\n",
       "      <th>343</th>\n",
       "      <th>344</th>\n",
       "      <th>345</th>\n",
       "      <th>346</th>\n",
       "      <th>347</th>\n",
       "      <th>348</th>\n",
       "      <th>349</th>\n",
       "      <th>350</th>\n",
       "      <th>351</th>\n",
       "      <th>352</th>\n",
       "      <th>353</th>\n",
       "      <th>354</th>\n",
       "      <th>355</th>\n",
       "      <th>356</th>\n",
       "      <th>357</th>\n",
       "      <th>358</th>\n",
       "      <th>359</th>\n",
       "      <th>360</th>\n",
       "      <th>361</th>\n",
       "      <th>362</th>\n",
       "      <th>363</th>\n",
       "      <th>364</th>\n",
       "      <th>365</th>\n",
       "      <th>366</th>\n",
       "      <th>367</th>\n",
       "      <th>368</th>\n",
       "      <th>369</th>\n",
       "      <th>370</th>\n",
       "      <th>371</th>\n",
       "      <th>372</th>\n",
       "      <th>373</th>\n",
       "      <th>374</th>\n",
       "      <th>375</th>\n",
       "      <th>376</th>\n",
       "      <th>377</th>\n",
       "      <th>378</th>\n",
       "      <th>379</th>\n",
       "      <th>380</th>\n",
       "      <th>381</th>\n",
       "      <th>382</th>\n",
       "      <th>383</th>\n",
       "      <th>384</th>\n",
       "      <th>385</th>\n",
       "      <th>386</th>\n",
       "      <th>387</th>\n",
       "      <th>388</th>\n",
       "      <th>389</th>\n",
       "      <th>390</th>\n",
       "      <th>391</th>\n",
       "      <th>392</th>\n",
       "      <th>393</th>\n",
       "      <th>394</th>\n",
       "      <th>395</th>\n",
       "      <th>396</th>\n",
       "      <th>397</th>\n",
       "      <th>398</th>\n",
       "      <th>399</th>\n",
       "      <th>400</th>\n",
       "      <th>401</th>\n",
       "      <th>402</th>\n",
       "      <th>403</th>\n",
       "      <th>404</th>\n",
       "      <th>405</th>\n",
       "      <th>406</th>\n",
       "      <th>407</th>\n",
       "      <th>408</th>\n",
       "      <th>409</th>\n",
       "      <th>410</th>\n",
       "      <th>411</th>\n",
       "      <th>412</th>\n",
       "      <th>413</th>\n",
       "      <th>414</th>\n",
       "      <th>415</th>\n",
       "      <th>416</th>\n",
       "      <th>417</th>\n",
       "      <th>418</th>\n",
       "      <th>419</th>\n",
       "      <th>420</th>\n",
       "      <th>421</th>\n",
       "      <th>422</th>\n",
       "      <th>423</th>\n",
       "      <th>424</th>\n",
       "      <th>425</th>\n",
       "      <th>426</th>\n",
       "      <th>427</th>\n",
       "      <th>428</th>\n",
       "      <th>429</th>\n",
       "      <th>430</th>\n",
       "      <th>431</th>\n",
       "      <th>432</th>\n",
       "      <th>433</th>\n",
       "      <th>434</th>\n",
       "      <th>435</th>\n",
       "      <th>436</th>\n",
       "      <th>437</th>\n",
       "      <th>438</th>\n",
       "      <th>439</th>\n",
       "      <th>440</th>\n",
       "      <th>441</th>\n",
       "      <th>442</th>\n",
       "      <th>443</th>\n",
       "      <th>444</th>\n",
       "      <th>445</th>\n",
       "      <th>446</th>\n",
       "      <th>447</th>\n",
       "      <th>448</th>\n",
       "      <th>449</th>\n",
       "      <th>450</th>\n",
       "      <th>451</th>\n",
       "      <th>452</th>\n",
       "      <th>453</th>\n",
       "      <th>454</th>\n",
       "      <th>455</th>\n",
       "      <th>456</th>\n",
       "      <th>457</th>\n",
       "      <th>458</th>\n",
       "      <th>459</th>\n",
       "      <th>460</th>\n",
       "      <th>461</th>\n",
       "      <th>462</th>\n",
       "      <th>463</th>\n",
       "      <th>464</th>\n",
       "      <th>465</th>\n",
       "      <th>466</th>\n",
       "      <th>467</th>\n",
       "      <th>468</th>\n",
       "      <th>469</th>\n",
       "      <th>470</th>\n",
       "      <th>471</th>\n",
       "      <th>472</th>\n",
       "      <th>473</th>\n",
       "      <th>474</th>\n",
       "      <th>475</th>\n",
       "      <th>476</th>\n",
       "      <th>477</th>\n",
       "      <th>478</th>\n",
       "      <th>479</th>\n",
       "      <th>480</th>\n",
       "      <th>481</th>\n",
       "      <th>482</th>\n",
       "      <th>483</th>\n",
       "      <th>484</th>\n",
       "      <th>485</th>\n",
       "      <th>486</th>\n",
       "      <th>487</th>\n",
       "      <th>488</th>\n",
       "      <th>489</th>\n",
       "      <th>490</th>\n",
       "      <th>491</th>\n",
       "      <th>492</th>\n",
       "      <th>493</th>\n",
       "      <th>494</th>\n",
       "      <th>495</th>\n",
       "      <th>496</th>\n",
       "      <th>497</th>\n",
       "      <th>498</th>\n",
       "      <th>499</th>\n",
       "      <th>500</th>\n",
       "      <th>501</th>\n",
       "      <th>502</th>\n",
       "      <th>503</th>\n",
       "      <th>504</th>\n",
       "      <th>505</th>\n",
       "      <th>506</th>\n",
       "      <th>507</th>\n",
       "      <th>508</th>\n",
       "      <th>509</th>\n",
       "      <th>510</th>\n",
       "      <th>511</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>argument</th>\n",
       "      <td>-0.057584</td>\n",
       "      <td>0.000001</td>\n",
       "      <td>0.055926</td>\n",
       "      <td>0.025750</td>\n",
       "      <td>-0.052444</td>\n",
       "      <td>0.034983</td>\n",
       "      <td>0.072001</td>\n",
       "      <td>-0.062773</td>\n",
       "      <td>0.030634</td>\n",
       "      <td>0.023789</td>\n",
       "      <td>-0.030285</td>\n",
       "      <td>-0.016392</td>\n",
       "      <td>0.016137</td>\n",
       "      <td>0.018487</td>\n",
       "      <td>-0.045962</td>\n",
       "      <td>-0.081218</td>\n",
       "      <td>0.008702</td>\n",
       "      <td>0.087169</td>\n",
       "      <td>-0.019983</td>\n",
       "      <td>0.016165</td>\n",
       "      <td>-0.043445</td>\n",
       "      <td>0.015750</td>\n",
       "      <td>0.040670</td>\n",
       "      <td>0.019039</td>\n",
       "      <td>-0.030091</td>\n",
       "      <td>0.067343</td>\n",
       "      <td>-0.035152</td>\n",
       "      <td>-0.050932</td>\n",
       "      <td>-0.050478</td>\n",
       "      <td>0.007857</td>\n",
       "      <td>-0.023909</td>\n",
       "      <td>-0.036066</td>\n",
       "      <td>-0.063819</td>\n",
       "      <td>0.013660</td>\n",
       "      <td>-0.014213</td>\n",
       "      <td>0.032831</td>\n",
       "      <td>0.000128</td>\n",
       "      <td>0.022295</td>\n",
       "      <td>0.008915</td>\n",
       "      <td>0.029342</td>\n",
       "      <td>-0.039961</td>\n",
       "      <td>-0.011504</td>\n",
       "      <td>-0.005429</td>\n",
       "      <td>-0.018344</td>\n",
       "      <td>0.014334</td>\n",
       "      <td>-0.029569</td>\n",
       "      <td>-0.039182</td>\n",
       "      <td>-0.017273</td>\n",
       "      <td>0.018854</td>\n",
       "      <td>-0.044364</td>\n",
       "      <td>-0.009508</td>\n",
       "      <td>-0.038428</td>\n",
       "      <td>0.079336</td>\n",
       "      <td>0.033354</td>\n",
       "      <td>-0.017089</td>\n",
       "      <td>0.014819</td>\n",
       "      <td>-0.034245</td>\n",
       "      <td>-0.020829</td>\n",
       "      <td>0.073357</td>\n",
       "      <td>0.018781</td>\n",
       "      <td>0.017274</td>\n",
       "      <td>0.027938</td>\n",
       "      <td>-0.058285</td>\n",
       "      <td>0.033429</td>\n",
       "      <td>0.038148</td>\n",
       "      <td>0.050036</td>\n",
       "      <td>-0.042139</td>\n",
       "      <td>-0.039587</td>\n",
       "      <td>0.062817</td>\n",
       "      <td>0.018287</td>\n",
       "      <td>-0.049389</td>\n",
       "      <td>-0.006416</td>\n",
       "      <td>0.011627</td>\n",
       "      <td>0.043233</td>\n",
       "      <td>-0.030799</td>\n",
       "      <td>0.045815</td>\n",
       "      <td>-0.031661</td>\n",
       "      <td>0.017208</td>\n",
       "      <td>0.046690</td>\n",
       "      <td>0.001913</td>\n",
       "      <td>0.039871</td>\n",
       "      <td>0.023903</td>\n",
       "      <td>-0.036332</td>\n",
       "      <td>-0.020771</td>\n",
       "      <td>-0.007836</td>\n",
       "      <td>0.065198</td>\n",
       "      <td>-0.034911</td>\n",
       "      <td>0.015282</td>\n",
       "      <td>-0.012333</td>\n",
       "      <td>0.028438</td>\n",
       "      <td>0.031518</td>\n",
       "      <td>-0.019205</td>\n",
       "      <td>-0.062143</td>\n",
       "      <td>-0.015026</td>\n",
       "      <td>0.054232</td>\n",
       "      <td>0.005134</td>\n",
       "      <td>0.049093</td>\n",
       "      <td>-0.014332</td>\n",
       "      <td>0.045847</td>\n",
       "      <td>-0.008020</td>\n",
       "      <td>0.004010</td>\n",
       "      <td>-0.046933</td>\n",
       "      <td>0.040072</td>\n",
       "      <td>-0.004521</td>\n",
       "      <td>0.005188</td>\n",
       "      <td>-0.010432</td>\n",
       "      <td>-0.014471</td>\n",
       "      <td>0.024448</td>\n",
       "      <td>-0.023686</td>\n",
       "      <td>0.004382</td>\n",
       "      <td>0.004548</td>\n",
       "      <td>-0.079834</td>\n",
       "      <td>-0.032111</td>\n",
       "      <td>0.024725</td>\n",
       "      <td>-0.007961</td>\n",
       "      <td>-0.015412</td>\n",
       "      <td>0.007960</td>\n",
       "      <td>0.043120</td>\n",
       "      <td>-0.000988</td>\n",
       "      <td>0.085556</td>\n",
       "      <td>-0.087542</td>\n",
       "      <td>0.027295</td>\n",
       "      <td>0.024891</td>\n",
       "      <td>-0.052877</td>\n",
       "      <td>0.050715</td>\n",
       "      <td>0.041792</td>\n",
       "      <td>-0.029795</td>\n",
       "      <td>-0.058189</td>\n",
       "      <td>-0.063624</td>\n",
       "      <td>0.034922</td>\n",
       "      <td>-0.059336</td>\n",
       "      <td>0.023104</td>\n",
       "      <td>0.046315</td>\n",
       "      <td>0.058178</td>\n",
       "      <td>0.033994</td>\n",
       "      <td>0.054983</td>\n",
       "      <td>0.086336</td>\n",
       "      <td>-0.060357</td>\n",
       "      <td>-0.057711</td>\n",
       "      <td>0.042543</td>\n",
       "      <td>-0.040400</td>\n",
       "      <td>-0.031473</td>\n",
       "      <td>0.030364</td>\n",
       "      <td>-0.046617</td>\n",
       "      <td>0.000323</td>\n",
       "      <td>-0.003256</td>\n",
       "      <td>-0.023044</td>\n",
       "      <td>-0.019966</td>\n",
       "      <td>0.065511</td>\n",
       "      <td>-0.036345</td>\n",
       "      <td>-0.021797</td>\n",
       "      <td>-0.000303</td>\n",
       "      <td>-0.054354</td>\n",
       "      <td>0.008926</td>\n",
       "      <td>-0.061933</td>\n",
       "      <td>-0.069910</td>\n",
       "      <td>-0.071256</td>\n",
       "      <td>-0.071903</td>\n",
       "      <td>-0.064597</td>\n",
       "      <td>0.018952</td>\n",
       "      <td>0.009328</td>\n",
       "      <td>-0.000932</td>\n",
       "      <td>0.050384</td>\n",
       "      <td>-0.028000</td>\n",
       "      <td>0.038387</td>\n",
       "      <td>0.045326</td>\n",
       "      <td>-0.062275</td>\n",
       "      <td>-0.051920</td>\n",
       "      <td>0.024715</td>\n",
       "      <td>0.028655</td>\n",
       "      <td>-0.041929</td>\n",
       "      <td>-0.075593</td>\n",
       "      <td>-0.051104</td>\n",
       "      <td>-0.061172</td>\n",
       "      <td>0.004018</td>\n",
       "      <td>0.023573</td>\n",
       "      <td>0.047923</td>\n",
       "      <td>-0.012488</td>\n",
       "      <td>0.018772</td>\n",
       "      <td>0.066610</td>\n",
       "      <td>-0.045729</td>\n",
       "      <td>0.020685</td>\n",
       "      <td>-0.059596</td>\n",
       "      <td>0.059906</td>\n",
       "      <td>-0.006771</td>\n",
       "      <td>-0.010002</td>\n",
       "      <td>-0.010421</td>\n",
       "      <td>0.034321</td>\n",
       "      <td>-0.067768</td>\n",
       "      <td>-0.058326</td>\n",
       "      <td>0.077001</td>\n",
       "      <td>0.012309</td>\n",
       "      <td>0.013914</td>\n",
       "      <td>0.038823</td>\n",
       "      <td>0.062681</td>\n",
       "      <td>-0.090058</td>\n",
       "      <td>0.053686</td>\n",
       "      <td>-0.003382</td>\n",
       "      <td>0.015303</td>\n",
       "      <td>-0.051602</td>\n",
       "      <td>-0.041136</td>\n",
       "      <td>-0.052476</td>\n",
       "      <td>0.023577</td>\n",
       "      <td>0.081222</td>\n",
       "      <td>-0.057633</td>\n",
       "      <td>0.024575</td>\n",
       "      <td>-0.070626</td>\n",
       "      <td>-0.058188</td>\n",
       "      <td>-0.040819</td>\n",
       "      <td>-0.009062</td>\n",
       "      <td>-0.082470</td>\n",
       "      <td>-0.013702</td>\n",
       "      <td>0.042660</td>\n",
       "      <td>-0.021233</td>\n",
       "      <td>-0.024118</td>\n",
       "      <td>-0.027460</td>\n",
       "      <td>-0.021096</td>\n",
       "      <td>-0.062413</td>\n",
       "      <td>0.007989</td>\n",
       "      <td>0.001720</td>\n",
       "      <td>0.045124</td>\n",
       "      <td>-0.059831</td>\n",
       "      <td>0.017545</td>\n",
       "      <td>-0.015112</td>\n",
       "      <td>0.010197</td>\n",
       "      <td>0.009643</td>\n",
       "      <td>-0.029225</td>\n",
       "      <td>0.043299</td>\n",
       "      <td>0.012116</td>\n",
       "      <td>-0.029709</td>\n",
       "      <td>-0.012300</td>\n",
       "      <td>-0.005173</td>\n",
       "      <td>0.044378</td>\n",
       "      <td>0.096184</td>\n",
       "      <td>-0.037672</td>\n",
       "      <td>0.044596</td>\n",
       "      <td>0.027317</td>\n",
       "      <td>-0.028190</td>\n",
       "      <td>-0.005828</td>\n",
       "      <td>0.009645</td>\n",
       "      <td>0.042948</td>\n",
       "      <td>-0.063338</td>\n",
       "      <td>-0.078461</td>\n",
       "      <td>-0.032988</td>\n",
       "      <td>0.058270</td>\n",
       "      <td>0.071955</td>\n",
       "      <td>-0.037694</td>\n",
       "      <td>0.059929</td>\n",
       "      <td>0.071091</td>\n",
       "      <td>-0.009234</td>\n",
       "      <td>0.019456</td>\n",
       "      <td>-0.029141</td>\n",
       "      <td>0.027986</td>\n",
       "      <td>-0.049155</td>\n",
       "      <td>0.010092</td>\n",
       "      <td>-0.001152</td>\n",
       "      <td>0.074553</td>\n",
       "      <td>-0.005894</td>\n",
       "      <td>0.013841</td>\n",
       "      <td>-0.020429</td>\n",
       "      <td>-0.038924</td>\n",
       "      <td>0.063172</td>\n",
       "      <td>-0.024724</td>\n",
       "      <td>-0.063695</td>\n",
       "      <td>0.022492</td>\n",
       "      <td>0.007181</td>\n",
       "      <td>-0.007627</td>\n",
       "      <td>0.002298</td>\n",
       "      <td>0.064664</td>\n",
       "      <td>0.010802</td>\n",
       "      <td>0.072767</td>\n",
       "      <td>0.026134</td>\n",
       "      <td>-0.017315</td>\n",
       "      <td>0.067959</td>\n",
       "      <td>-0.006620</td>\n",
       "      <td>-0.068015</td>\n",
       "      <td>0.014293</td>\n",
       "      <td>-0.062352</td>\n",
       "      <td>0.075880</td>\n",
       "      <td>0.007846</td>\n",
       "      <td>0.054692</td>\n",
       "      <td>-0.005531</td>\n",
       "      <td>0.050767</td>\n",
       "      <td>-0.030855</td>\n",
       "      <td>-0.013326</td>\n",
       "      <td>0.072889</td>\n",
       "      <td>0.049295</td>\n",
       "      <td>0.035683</td>\n",
       "      <td>-0.039169</td>\n",
       "      <td>-0.050700</td>\n",
       "      <td>-0.014010</td>\n",
       "      <td>0.038789</td>\n",
       "      <td>-0.071818</td>\n",
       "      <td>-0.042616</td>\n",
       "      <td>0.022737</td>\n",
       "      <td>0.056538</td>\n",
       "      <td>-0.010876</td>\n",
       "      <td>-0.051784</td>\n",
       "      <td>0.024468</td>\n",
       "      <td>-0.069797</td>\n",
       "      <td>0.028796</td>\n",
       "      <td>-0.008048</td>\n",
       "      <td>-0.090181</td>\n",
       "      <td>-0.066201</td>\n",
       "      <td>0.019653</td>\n",
       "      <td>0.079588</td>\n",
       "      <td>0.044652</td>\n",
       "      <td>-0.063610</td>\n",
       "      <td>-0.032376</td>\n",
       "      <td>-0.014145</td>\n",
       "      <td>0.030291</td>\n",
       "      <td>-0.024217</td>\n",
       "      <td>-0.022981</td>\n",
       "      <td>-0.025790</td>\n",
       "      <td>-0.022492</td>\n",
       "      <td>0.004969</td>\n",
       "      <td>-0.004608</td>\n",
       "      <td>0.036844</td>\n",
       "      <td>-0.084480</td>\n",
       "      <td>-0.032229</td>\n",
       "      <td>0.007105</td>\n",
       "      <td>0.024804</td>\n",
       "      <td>-0.016186</td>\n",
       "      <td>0.043481</td>\n",
       "      <td>-0.031115</td>\n",
       "      <td>-0.047650</td>\n",
       "      <td>0.027854</td>\n",
       "      <td>0.077159</td>\n",
       "      <td>-0.042288</td>\n",
       "      <td>-0.035300</td>\n",
       "      <td>0.013603</td>\n",
       "      <td>0.042379</td>\n",
       "      <td>-0.039202</td>\n",
       "      <td>0.076130</td>\n",
       "      <td>-0.043431</td>\n",
       "      <td>-0.029254</td>\n",
       "      <td>-0.072606</td>\n",
       "      <td>-0.052107</td>\n",
       "      <td>0.017179</td>\n",
       "      <td>0.003263</td>\n",
       "      <td>0.027721</td>\n",
       "      <td>0.014097</td>\n",
       "      <td>0.070146</td>\n",
       "      <td>-0.066576</td>\n",
       "      <td>-0.077218</td>\n",
       "      <td>0.037960</td>\n",
       "      <td>0.059802</td>\n",
       "      <td>0.019573</td>\n",
       "      <td>-0.025816</td>\n",
       "      <td>0.027238</td>\n",
       "      <td>0.079769</td>\n",
       "      <td>-0.029708</td>\n",
       "      <td>-0.006219</td>\n",
       "      <td>-0.014115</td>\n",
       "      <td>-0.030580</td>\n",
       "      <td>0.013314</td>\n",
       "      <td>-0.089792</td>\n",
       "      <td>-0.052086</td>\n",
       "      <td>0.045907</td>\n",
       "      <td>-0.069082</td>\n",
       "      <td>0.054834</td>\n",
       "      <td>-0.027700</td>\n",
       "      <td>0.084317</td>\n",
       "      <td>-0.063174</td>\n",
       "      <td>0.001991</td>\n",
       "      <td>-0.033803</td>\n",
       "      <td>-0.004145</td>\n",
       "      <td>0.018333</td>\n",
       "      <td>0.088532</td>\n",
       "      <td>-0.016738</td>\n",
       "      <td>-0.034501</td>\n",
       "      <td>0.030454</td>\n",
       "      <td>0.006947</td>\n",
       "      <td>0.046070</td>\n",
       "      <td>0.002859</td>\n",
       "      <td>-0.084782</td>\n",
       "      <td>0.067521</td>\n",
       "      <td>-0.079398</td>\n",
       "      <td>-0.051415</td>\n",
       "      <td>-0.085916</td>\n",
       "      <td>-0.061763</td>\n",
       "      <td>-0.065935</td>\n",
       "      <td>-0.090115</td>\n",
       "      <td>-0.021330</td>\n",
       "      <td>0.061905</td>\n",
       "      <td>-0.079906</td>\n",
       "      <td>0.039929</td>\n",
       "      <td>0.054689</td>\n",
       "      <td>-0.004971</td>\n",
       "      <td>0.012030</td>\n",
       "      <td>0.062726</td>\n",
       "      <td>0.046365</td>\n",
       "      <td>0.073986</td>\n",
       "      <td>-0.075979</td>\n",
       "      <td>-0.053222</td>\n",
       "      <td>-0.027387</td>\n",
       "      <td>0.018516</td>\n",
       "      <td>-0.071379</td>\n",
       "      <td>-0.062263</td>\n",
       "      <td>-0.064034</td>\n",
       "      <td>0.023971</td>\n",
       "      <td>0.018099</td>\n",
       "      <td>0.021977</td>\n",
       "      <td>0.061723</td>\n",
       "      <td>-0.013675</td>\n",
       "      <td>-0.035256</td>\n",
       "      <td>-0.034237</td>\n",
       "      <td>0.047126</td>\n",
       "      <td>-0.092807</td>\n",
       "      <td>0.056644</td>\n",
       "      <td>0.023457</td>\n",
       "      <td>-0.060108</td>\n",
       "      <td>0.086088</td>\n",
       "      <td>-0.059559</td>\n",
       "      <td>-0.015015</td>\n",
       "      <td>-0.035430</td>\n",
       "      <td>-0.036492</td>\n",
       "      <td>-0.031708</td>\n",
       "      <td>-0.032735</td>\n",
       "      <td>-0.040144</td>\n",
       "      <td>0.011704</td>\n",
       "      <td>-0.033966</td>\n",
       "      <td>-0.010243</td>\n",
       "      <td>-0.001060</td>\n",
       "      <td>-0.024761</td>\n",
       "      <td>0.021965</td>\n",
       "      <td>0.057084</td>\n",
       "      <td>0.078910</td>\n",
       "      <td>-0.002770</td>\n",
       "      <td>0.005780</td>\n",
       "      <td>0.005478</td>\n",
       "      <td>0.047648</td>\n",
       "      <td>-0.048318</td>\n",
       "      <td>-0.041998</td>\n",
       "      <td>0.071814</td>\n",
       "      <td>0.008772</td>\n",
       "      <td>0.011096</td>\n",
       "      <td>-0.002644</td>\n",
       "      <td>0.046806</td>\n",
       "      <td>0.080080</td>\n",
       "      <td>0.038559</td>\n",
       "      <td>-0.018047</td>\n",
       "      <td>0.045859</td>\n",
       "      <td>0.007169</td>\n",
       "      <td>-0.027391</td>\n",
       "      <td>0.007768</td>\n",
       "      <td>-0.004566</td>\n",
       "      <td>0.021875</td>\n",
       "      <td>0.085753</td>\n",
       "      <td>-0.022324</td>\n",
       "      <td>0.006330</td>\n",
       "      <td>-0.064182</td>\n",
       "      <td>0.021552</td>\n",
       "      <td>0.045393</td>\n",
       "      <td>0.060638</td>\n",
       "      <td>0.017268</td>\n",
       "      <td>0.045636</td>\n",
       "      <td>-0.006496</td>\n",
       "      <td>-0.080592</td>\n",
       "      <td>0.043981</td>\n",
       "      <td>0.027675</td>\n",
       "      <td>0.002706</td>\n",
       "      <td>-0.007369</td>\n",
       "      <td>0.039583</td>\n",
       "      <td>0.031410</td>\n",
       "      <td>-0.042385</td>\n",
       "      <td>-0.030802</td>\n",
       "      <td>-0.015994</td>\n",
       "      <td>-0.018830</td>\n",
       "      <td>0.076203</td>\n",
       "      <td>0.002100</td>\n",
       "      <td>-0.085850</td>\n",
       "      <td>-0.028647</td>\n",
       "      <td>0.000867</td>\n",
       "      <td>0.063751</td>\n",
       "      <td>-0.013786</td>\n",
       "      <td>-0.050527</td>\n",
       "      <td>0.066184</td>\n",
       "      <td>-0.001355</td>\n",
       "      <td>0.084783</td>\n",
       "      <td>-0.007728</td>\n",
       "      <td>-0.064325</td>\n",
       "      <td>0.037782</td>\n",
       "      <td>-0.066981</td>\n",
       "      <td>0.044540</td>\n",
       "      <td>-0.040387</td>\n",
       "      <td>-0.046770</td>\n",
       "      <td>-0.038879</td>\n",
       "      <td>0.042562</td>\n",
       "      <td>-0.004817</td>\n",
       "      <td>-0.062256</td>\n",
       "      <td>0.014689</td>\n",
       "      <td>-0.072014</td>\n",
       "      <td>0.001126</td>\n",
       "      <td>0.057677</td>\n",
       "      <td>-0.089631</td>\n",
       "      <td>0.071474</td>\n",
       "      <td>-0.068246</td>\n",
       "      <td>0.066406</td>\n",
       "      <td>-0.005436</td>\n",
       "      <td>0.044631</td>\n",
       "      <td>0.000822</td>\n",
       "      <td>-0.031578</td>\n",
       "      <td>-0.095102</td>\n",
       "      <td>0.047018</td>\n",
       "      <td>0.025369</td>\n",
       "      <td>0.019591</td>\n",
       "      <td>0.004513</td>\n",
       "      <td>-0.025299</td>\n",
       "      <td>-0.011916</td>\n",
       "      <td>0.010102</td>\n",
       "      <td>0.011102</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>awful</th>\n",
       "      <td>-0.076146</td>\n",
       "      <td>0.023700</td>\n",
       "      <td>0.018652</td>\n",
       "      <td>0.065849</td>\n",
       "      <td>0.031395</td>\n",
       "      <td>0.003467</td>\n",
       "      <td>0.067020</td>\n",
       "      <td>-0.051645</td>\n",
       "      <td>-0.035994</td>\n",
       "      <td>0.032914</td>\n",
       "      <td>0.019309</td>\n",
       "      <td>0.045878</td>\n",
       "      <td>-0.051007</td>\n",
       "      <td>0.016304</td>\n",
       "      <td>0.051063</td>\n",
       "      <td>0.041033</td>\n",
       "      <td>-0.020460</td>\n",
       "      <td>0.006427</td>\n",
       "      <td>0.002416</td>\n",
       "      <td>-0.065817</td>\n",
       "      <td>0.036337</td>\n",
       "      <td>0.047262</td>\n",
       "      <td>-0.011263</td>\n",
       "      <td>-0.024436</td>\n",
       "      <td>-0.038048</td>\n",
       "      <td>0.034624</td>\n",
       "      <td>0.050856</td>\n",
       "      <td>0.000241</td>\n",
       "      <td>-0.024685</td>\n",
       "      <td>0.019189</td>\n",
       "      <td>0.068494</td>\n",
       "      <td>0.011564</td>\n",
       "      <td>0.034836</td>\n",
       "      <td>-0.020168</td>\n",
       "      <td>-0.034911</td>\n",
       "      <td>0.002077</td>\n",
       "      <td>0.073511</td>\n",
       "      <td>-0.030884</td>\n",
       "      <td>-0.010971</td>\n",
       "      <td>0.042874</td>\n",
       "      <td>0.036954</td>\n",
       "      <td>0.007873</td>\n",
       "      <td>-0.039832</td>\n",
       "      <td>-0.014288</td>\n",
       "      <td>0.005792</td>\n",
       "      <td>-0.057307</td>\n",
       "      <td>-0.037924</td>\n",
       "      <td>0.042260</td>\n",
       "      <td>-0.008010</td>\n",
       "      <td>0.072971</td>\n",
       "      <td>0.068897</td>\n",
       "      <td>-0.083284</td>\n",
       "      <td>0.082698</td>\n",
       "      <td>0.083827</td>\n",
       "      <td>0.029966</td>\n",
       "      <td>-0.017867</td>\n",
       "      <td>0.056651</td>\n",
       "      <td>-0.027699</td>\n",
       "      <td>0.010190</td>\n",
       "      <td>-0.048413</td>\n",
       "      <td>0.019944</td>\n",
       "      <td>-0.030172</td>\n",
       "      <td>-0.041436</td>\n",
       "      <td>0.011228</td>\n",
       "      <td>0.030549</td>\n",
       "      <td>0.073059</td>\n",
       "      <td>0.066297</td>\n",
       "      <td>-0.078503</td>\n",
       "      <td>0.036721</td>\n",
       "      <td>0.008179</td>\n",
       "      <td>-0.029867</td>\n",
       "      <td>0.067916</td>\n",
       "      <td>0.038127</td>\n",
       "      <td>-0.062772</td>\n",
       "      <td>0.037611</td>\n",
       "      <td>0.035925</td>\n",
       "      <td>-0.052314</td>\n",
       "      <td>0.042182</td>\n",
       "      <td>0.023702</td>\n",
       "      <td>0.065532</td>\n",
       "      <td>-0.025242</td>\n",
       "      <td>-0.013086</td>\n",
       "      <td>-0.031029</td>\n",
       "      <td>-0.014235</td>\n",
       "      <td>-0.076955</td>\n",
       "      <td>-0.013248</td>\n",
       "      <td>0.044102</td>\n",
       "      <td>-0.031263</td>\n",
       "      <td>0.005484</td>\n",
       "      <td>0.005979</td>\n",
       "      <td>0.050421</td>\n",
       "      <td>0.063695</td>\n",
       "      <td>0.032133</td>\n",
       "      <td>0.016594</td>\n",
       "      <td>-0.046370</td>\n",
       "      <td>0.002882</td>\n",
       "      <td>-0.060153</td>\n",
       "      <td>0.046705</td>\n",
       "      <td>-0.047750</td>\n",
       "      <td>-0.043766</td>\n",
       "      <td>-0.006674</td>\n",
       "      <td>-0.062031</td>\n",
       "      <td>0.043071</td>\n",
       "      <td>-0.003301</td>\n",
       "      <td>-0.018319</td>\n",
       "      <td>-0.045059</td>\n",
       "      <td>-0.034041</td>\n",
       "      <td>0.012005</td>\n",
       "      <td>-0.067164</td>\n",
       "      <td>0.057331</td>\n",
       "      <td>-0.028019</td>\n",
       "      <td>-0.093891</td>\n",
       "      <td>-0.029152</td>\n",
       "      <td>-0.067930</td>\n",
       "      <td>0.044273</td>\n",
       "      <td>0.085499</td>\n",
       "      <td>-0.024145</td>\n",
       "      <td>-0.062524</td>\n",
       "      <td>-0.004840</td>\n",
       "      <td>-0.004902</td>\n",
       "      <td>-0.010149</td>\n",
       "      <td>0.052071</td>\n",
       "      <td>-0.039184</td>\n",
       "      <td>-0.003743</td>\n",
       "      <td>0.037911</td>\n",
       "      <td>-0.026832</td>\n",
       "      <td>-0.018096</td>\n",
       "      <td>0.017694</td>\n",
       "      <td>0.065242</td>\n",
       "      <td>-0.030636</td>\n",
       "      <td>-0.027106</td>\n",
       "      <td>-0.007936</td>\n",
       "      <td>0.038585</td>\n",
       "      <td>0.026767</td>\n",
       "      <td>0.075463</td>\n",
       "      <td>-0.061721</td>\n",
       "      <td>0.047792</td>\n",
       "      <td>-0.001146</td>\n",
       "      <td>-0.035391</td>\n",
       "      <td>-0.026613</td>\n",
       "      <td>0.058524</td>\n",
       "      <td>0.012451</td>\n",
       "      <td>0.009211</td>\n",
       "      <td>-0.045514</td>\n",
       "      <td>0.070426</td>\n",
       "      <td>0.016005</td>\n",
       "      <td>-0.008694</td>\n",
       "      <td>0.001654</td>\n",
       "      <td>-0.060517</td>\n",
       "      <td>-0.031460</td>\n",
       "      <td>-0.016749</td>\n",
       "      <td>0.024138</td>\n",
       "      <td>-0.059874</td>\n",
       "      <td>-0.050092</td>\n",
       "      <td>-0.041756</td>\n",
       "      <td>-0.000136</td>\n",
       "      <td>0.024711</td>\n",
       "      <td>-0.061333</td>\n",
       "      <td>-0.048236</td>\n",
       "      <td>0.020967</td>\n",
       "      <td>0.025715</td>\n",
       "      <td>-0.034998</td>\n",
       "      <td>0.080872</td>\n",
       "      <td>-0.070894</td>\n",
       "      <td>0.078175</td>\n",
       "      <td>-0.006992</td>\n",
       "      <td>-0.034740</td>\n",
       "      <td>-0.049206</td>\n",
       "      <td>0.072300</td>\n",
       "      <td>-0.058618</td>\n",
       "      <td>0.001261</td>\n",
       "      <td>-0.093785</td>\n",
       "      <td>0.035789</td>\n",
       "      <td>-0.065793</td>\n",
       "      <td>-0.046774</td>\n",
       "      <td>0.015142</td>\n",
       "      <td>0.047012</td>\n",
       "      <td>-0.000676</td>\n",
       "      <td>-0.009646</td>\n",
       "      <td>-0.088854</td>\n",
       "      <td>0.043970</td>\n",
       "      <td>0.032780</td>\n",
       "      <td>-0.067831</td>\n",
       "      <td>-0.029021</td>\n",
       "      <td>0.039883</td>\n",
       "      <td>-0.040123</td>\n",
       "      <td>-0.024022</td>\n",
       "      <td>-0.060763</td>\n",
       "      <td>0.030862</td>\n",
       "      <td>0.012750</td>\n",
       "      <td>-0.015573</td>\n",
       "      <td>0.064420</td>\n",
       "      <td>0.079513</td>\n",
       "      <td>0.032668</td>\n",
       "      <td>-0.031097</td>\n",
       "      <td>-0.087684</td>\n",
       "      <td>0.024130</td>\n",
       "      <td>0.037422</td>\n",
       "      <td>0.010016</td>\n",
       "      <td>0.063889</td>\n",
       "      <td>-0.063342</td>\n",
       "      <td>0.049380</td>\n",
       "      <td>0.027706</td>\n",
       "      <td>0.000200</td>\n",
       "      <td>-0.069464</td>\n",
       "      <td>0.019735</td>\n",
       "      <td>-0.055534</td>\n",
       "      <td>0.020007</td>\n",
       "      <td>-0.048360</td>\n",
       "      <td>-0.012899</td>\n",
       "      <td>-0.039110</td>\n",
       "      <td>0.003028</td>\n",
       "      <td>-0.022496</td>\n",
       "      <td>-0.026989</td>\n",
       "      <td>-0.079784</td>\n",
       "      <td>0.040686</td>\n",
       "      <td>0.039482</td>\n",
       "      <td>-0.069832</td>\n",
       "      <td>-0.004059</td>\n",
       "      <td>0.006077</td>\n",
       "      <td>-0.027229</td>\n",
       "      <td>-0.004972</td>\n",
       "      <td>-0.012753</td>\n",
       "      <td>0.039188</td>\n",
       "      <td>-0.077668</td>\n",
       "      <td>0.091806</td>\n",
       "      <td>0.031729</td>\n",
       "      <td>0.056743</td>\n",
       "      <td>0.002037</td>\n",
       "      <td>-0.026558</td>\n",
       "      <td>0.032215</td>\n",
       "      <td>-0.050665</td>\n",
       "      <td>0.017084</td>\n",
       "      <td>0.094173</td>\n",
       "      <td>0.066535</td>\n",
       "      <td>0.016531</td>\n",
       "      <td>-0.046306</td>\n",
       "      <td>-0.052921</td>\n",
       "      <td>-0.092610</td>\n",
       "      <td>0.033968</td>\n",
       "      <td>0.009872</td>\n",
       "      <td>-0.017931</td>\n",
       "      <td>-0.000709</td>\n",
       "      <td>-0.041260</td>\n",
       "      <td>0.028534</td>\n",
       "      <td>0.090091</td>\n",
       "      <td>-0.019826</td>\n",
       "      <td>-0.041295</td>\n",
       "      <td>0.022478</td>\n",
       "      <td>-0.081318</td>\n",
       "      <td>0.048759</td>\n",
       "      <td>0.085806</td>\n",
       "      <td>0.026717</td>\n",
       "      <td>-0.009958</td>\n",
       "      <td>0.012046</td>\n",
       "      <td>-0.013678</td>\n",
       "      <td>0.018020</td>\n",
       "      <td>-0.033809</td>\n",
       "      <td>-0.009707</td>\n",
       "      <td>-0.003284</td>\n",
       "      <td>-0.036073</td>\n",
       "      <td>0.023484</td>\n",
       "      <td>-0.073450</td>\n",
       "      <td>0.033753</td>\n",
       "      <td>0.035960</td>\n",
       "      <td>-0.045858</td>\n",
       "      <td>0.009989</td>\n",
       "      <td>-0.058249</td>\n",
       "      <td>0.073809</td>\n",
       "      <td>0.005028</td>\n",
       "      <td>0.082788</td>\n",
       "      <td>0.085546</td>\n",
       "      <td>0.036497</td>\n",
       "      <td>0.032613</td>\n",
       "      <td>0.041649</td>\n",
       "      <td>-0.042294</td>\n",
       "      <td>-0.049808</td>\n",
       "      <td>0.002953</td>\n",
       "      <td>0.087391</td>\n",
       "      <td>0.082777</td>\n",
       "      <td>-0.057213</td>\n",
       "      <td>-0.031808</td>\n",
       "      <td>0.011665</td>\n",
       "      <td>0.004209</td>\n",
       "      <td>0.037820</td>\n",
       "      <td>0.083069</td>\n",
       "      <td>0.056791</td>\n",
       "      <td>0.005112</td>\n",
       "      <td>-0.050093</td>\n",
       "      <td>0.047322</td>\n",
       "      <td>0.028997</td>\n",
       "      <td>0.092261</td>\n",
       "      <td>-0.079828</td>\n",
       "      <td>-0.034951</td>\n",
       "      <td>0.053753</td>\n",
       "      <td>-0.030682</td>\n",
       "      <td>0.026563</td>\n",
       "      <td>0.045139</td>\n",
       "      <td>0.066788</td>\n",
       "      <td>0.021184</td>\n",
       "      <td>0.084609</td>\n",
       "      <td>-0.035686</td>\n",
       "      <td>0.041314</td>\n",
       "      <td>-0.004158</td>\n",
       "      <td>0.034212</td>\n",
       "      <td>0.016547</td>\n",
       "      <td>-0.003487</td>\n",
       "      <td>-0.000089</td>\n",
       "      <td>-0.020263</td>\n",
       "      <td>-0.059818</td>\n",
       "      <td>-0.040742</td>\n",
       "      <td>-0.075174</td>\n",
       "      <td>0.001203</td>\n",
       "      <td>0.001711</td>\n",
       "      <td>0.047951</td>\n",
       "      <td>-0.005222</td>\n",
       "      <td>0.018197</td>\n",
       "      <td>0.064222</td>\n",
       "      <td>-0.022703</td>\n",
       "      <td>0.039388</td>\n",
       "      <td>0.021222</td>\n",
       "      <td>-0.075000</td>\n",
       "      <td>0.060255</td>\n",
       "      <td>0.006474</td>\n",
       "      <td>-0.007914</td>\n",
       "      <td>0.060536</td>\n",
       "      <td>0.004895</td>\n",
       "      <td>-0.042214</td>\n",
       "      <td>-0.039466</td>\n",
       "      <td>0.023853</td>\n",
       "      <td>-0.002251</td>\n",
       "      <td>-0.038294</td>\n",
       "      <td>0.022688</td>\n",
       "      <td>0.055275</td>\n",
       "      <td>-0.040939</td>\n",
       "      <td>0.011868</td>\n",
       "      <td>-0.008202</td>\n",
       "      <td>0.028161</td>\n",
       "      <td>-0.041423</td>\n",
       "      <td>0.018889</td>\n",
       "      <td>-0.000651</td>\n",
       "      <td>0.009558</td>\n",
       "      <td>-0.018608</td>\n",
       "      <td>-0.037636</td>\n",
       "      <td>0.042375</td>\n",
       "      <td>-0.025393</td>\n",
       "      <td>0.043634</td>\n",
       "      <td>0.002574</td>\n",
       "      <td>-0.048387</td>\n",
       "      <td>-0.004234</td>\n",
       "      <td>-0.006132</td>\n",
       "      <td>-0.066571</td>\n",
       "      <td>-0.019887</td>\n",
       "      <td>0.010871</td>\n",
       "      <td>-0.016940</td>\n",
       "      <td>-0.034968</td>\n",
       "      <td>-0.088767</td>\n",
       "      <td>0.046514</td>\n",
       "      <td>-0.026038</td>\n",
       "      <td>-0.007387</td>\n",
       "      <td>0.045358</td>\n",
       "      <td>-0.032535</td>\n",
       "      <td>0.093300</td>\n",
       "      <td>0.051422</td>\n",
       "      <td>-0.025148</td>\n",
       "      <td>-0.040950</td>\n",
       "      <td>0.003125</td>\n",
       "      <td>0.053990</td>\n",
       "      <td>0.075959</td>\n",
       "      <td>-0.031078</td>\n",
       "      <td>0.002818</td>\n",
       "      <td>-0.042940</td>\n",
       "      <td>-0.033236</td>\n",
       "      <td>-0.036373</td>\n",
       "      <td>-0.061697</td>\n",
       "      <td>-0.069972</td>\n",
       "      <td>0.031146</td>\n",
       "      <td>0.042176</td>\n",
       "      <td>-0.017943</td>\n",
       "      <td>-0.054916</td>\n",
       "      <td>-0.004784</td>\n",
       "      <td>-0.042544</td>\n",
       "      <td>0.011832</td>\n",
       "      <td>-0.007422</td>\n",
       "      <td>0.093396</td>\n",
       "      <td>-0.061648</td>\n",
       "      <td>-0.017691</td>\n",
       "      <td>-0.023652</td>\n",
       "      <td>0.010603</td>\n",
       "      <td>-0.002925</td>\n",
       "      <td>0.068141</td>\n",
       "      <td>-0.048737</td>\n",
       "      <td>0.031473</td>\n",
       "      <td>0.025743</td>\n",
       "      <td>-0.003023</td>\n",
       "      <td>-0.029445</td>\n",
       "      <td>-0.036457</td>\n",
       "      <td>0.003909</td>\n",
       "      <td>0.000115</td>\n",
       "      <td>-0.037345</td>\n",
       "      <td>-0.045354</td>\n",
       "      <td>0.056018</td>\n",
       "      <td>-0.058831</td>\n",
       "      <td>-0.063883</td>\n",
       "      <td>0.008590</td>\n",
       "      <td>0.053133</td>\n",
       "      <td>0.027540</td>\n",
       "      <td>0.069558</td>\n",
       "      <td>-0.034867</td>\n",
       "      <td>-0.037002</td>\n",
       "      <td>-0.070502</td>\n",
       "      <td>0.063650</td>\n",
       "      <td>-0.040577</td>\n",
       "      <td>0.015556</td>\n",
       "      <td>-0.069908</td>\n",
       "      <td>0.053437</td>\n",
       "      <td>-0.003592</td>\n",
       "      <td>-0.032247</td>\n",
       "      <td>-0.011380</td>\n",
       "      <td>-0.018528</td>\n",
       "      <td>0.021873</td>\n",
       "      <td>0.032956</td>\n",
       "      <td>0.010046</td>\n",
       "      <td>-0.053758</td>\n",
       "      <td>0.053524</td>\n",
       "      <td>0.076512</td>\n",
       "      <td>0.062618</td>\n",
       "      <td>0.077849</td>\n",
       "      <td>-0.045502</td>\n",
       "      <td>0.028627</td>\n",
       "      <td>-0.038741</td>\n",
       "      <td>0.021568</td>\n",
       "      <td>-0.033849</td>\n",
       "      <td>-0.057351</td>\n",
       "      <td>-0.049024</td>\n",
       "      <td>-0.049809</td>\n",
       "      <td>0.002694</td>\n",
       "      <td>-0.001142</td>\n",
       "      <td>-0.017030</td>\n",
       "      <td>0.047223</td>\n",
       "      <td>0.037956</td>\n",
       "      <td>-0.066853</td>\n",
       "      <td>-0.000362</td>\n",
       "      <td>-0.000358</td>\n",
       "      <td>0.025561</td>\n",
       "      <td>-0.001205</td>\n",
       "      <td>0.081358</td>\n",
       "      <td>-0.066752</td>\n",
       "      <td>-0.062665</td>\n",
       "      <td>-0.054362</td>\n",
       "      <td>-0.009323</td>\n",
       "      <td>-0.015458</td>\n",
       "      <td>0.048016</td>\n",
       "      <td>-0.033786</td>\n",
       "      <td>0.056153</td>\n",
       "      <td>0.036444</td>\n",
       "      <td>-0.035550</td>\n",
       "      <td>0.024998</td>\n",
       "      <td>-0.027910</td>\n",
       "      <td>0.048046</td>\n",
       "      <td>-0.025117</td>\n",
       "      <td>-0.014647</td>\n",
       "      <td>0.055247</td>\n",
       "      <td>-0.044303</td>\n",
       "      <td>0.007595</td>\n",
       "      <td>-0.017439</td>\n",
       "      <td>0.001466</td>\n",
       "      <td>-0.031234</td>\n",
       "      <td>0.002822</td>\n",
       "      <td>-0.068441</td>\n",
       "      <td>0.063972</td>\n",
       "      <td>-0.035880</td>\n",
       "      <td>-0.012636</td>\n",
       "      <td>0.024031</td>\n",
       "      <td>0.048829</td>\n",
       "      <td>-0.020855</td>\n",
       "      <td>-0.068279</td>\n",
       "      <td>0.039981</td>\n",
       "      <td>0.093936</td>\n",
       "      <td>0.080030</td>\n",
       "      <td>0.004546</td>\n",
       "      <td>-0.048764</td>\n",
       "      <td>0.071922</td>\n",
       "      <td>0.001301</td>\n",
       "      <td>-0.021302</td>\n",
       "      <td>0.025027</td>\n",
       "      <td>0.053854</td>\n",
       "      <td>-0.015680</td>\n",
       "      <td>0.009696</td>\n",
       "      <td>0.034640</td>\n",
       "      <td>-0.046887</td>\n",
       "      <td>-0.015079</td>\n",
       "      <td>-0.013106</td>\n",
       "      <td>-0.057902</td>\n",
       "      <td>-0.024479</td>\n",
       "      <td>0.006301</td>\n",
       "      <td>0.081928</td>\n",
       "      <td>-0.078714</td>\n",
       "      <td>0.009384</td>\n",
       "      <td>0.011722</td>\n",
       "      <td>-0.037346</td>\n",
       "      <td>-0.003848</td>\n",
       "      <td>-0.004141</td>\n",
       "      <td>-0.084734</td>\n",
       "      <td>0.061091</td>\n",
       "      <td>0.035161</td>\n",
       "      <td>0.002606</td>\n",
       "      <td>-0.011290</td>\n",
       "      <td>-0.054125</td>\n",
       "      <td>-0.057468</td>\n",
       "      <td>-0.003473</td>\n",
       "      <td>0.002821</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>decorator</th>\n",
       "      <td>0.015437</td>\n",
       "      <td>0.065674</td>\n",
       "      <td>0.031793</td>\n",
       "      <td>-0.002947</td>\n",
       "      <td>-0.019215</td>\n",
       "      <td>0.056831</td>\n",
       "      <td>0.002746</td>\n",
       "      <td>0.029131</td>\n",
       "      <td>0.064465</td>\n",
       "      <td>-0.044334</td>\n",
       "      <td>0.026005</td>\n",
       "      <td>0.010356</td>\n",
       "      <td>0.065691</td>\n",
       "      <td>-0.075613</td>\n",
       "      <td>0.039199</td>\n",
       "      <td>0.059256</td>\n",
       "      <td>0.004944</td>\n",
       "      <td>0.069531</td>\n",
       "      <td>0.020214</td>\n",
       "      <td>-0.019658</td>\n",
       "      <td>0.055134</td>\n",
       "      <td>0.068949</td>\n",
       "      <td>0.020131</td>\n",
       "      <td>-0.074199</td>\n",
       "      <td>0.039254</td>\n",
       "      <td>-0.050819</td>\n",
       "      <td>-0.018112</td>\n",
       "      <td>-0.069238</td>\n",
       "      <td>0.023453</td>\n",
       "      <td>-0.009353</td>\n",
       "      <td>-0.037261</td>\n",
       "      <td>0.036599</td>\n",
       "      <td>0.019584</td>\n",
       "      <td>0.038329</td>\n",
       "      <td>0.002487</td>\n",
       "      <td>-0.039646</td>\n",
       "      <td>0.042414</td>\n",
       "      <td>-0.033027</td>\n",
       "      <td>-0.031804</td>\n",
       "      <td>0.033814</td>\n",
       "      <td>0.023004</td>\n",
       "      <td>-0.024751</td>\n",
       "      <td>-0.038306</td>\n",
       "      <td>-0.041249</td>\n",
       "      <td>-0.072157</td>\n",
       "      <td>0.052488</td>\n",
       "      <td>-0.006171</td>\n",
       "      <td>0.046659</td>\n",
       "      <td>0.076298</td>\n",
       "      <td>-0.006558</td>\n",
       "      <td>0.037305</td>\n",
       "      <td>0.004132</td>\n",
       "      <td>0.060841</td>\n",
       "      <td>-0.055396</td>\n",
       "      <td>0.038871</td>\n",
       "      <td>-0.080222</td>\n",
       "      <td>-0.020556</td>\n",
       "      <td>-0.037200</td>\n",
       "      <td>0.069074</td>\n",
       "      <td>-0.055814</td>\n",
       "      <td>0.054261</td>\n",
       "      <td>0.021472</td>\n",
       "      <td>-0.069792</td>\n",
       "      <td>0.042697</td>\n",
       "      <td>0.063749</td>\n",
       "      <td>0.029979</td>\n",
       "      <td>-0.074923</td>\n",
       "      <td>-0.034402</td>\n",
       "      <td>-0.005997</td>\n",
       "      <td>0.028390</td>\n",
       "      <td>-0.067814</td>\n",
       "      <td>-0.045679</td>\n",
       "      <td>-0.003550</td>\n",
       "      <td>-0.030747</td>\n",
       "      <td>0.016075</td>\n",
       "      <td>-0.056769</td>\n",
       "      <td>-0.065840</td>\n",
       "      <td>-0.009134</td>\n",
       "      <td>0.041753</td>\n",
       "      <td>0.026851</td>\n",
       "      <td>-0.043075</td>\n",
       "      <td>0.025882</td>\n",
       "      <td>-0.079814</td>\n",
       "      <td>-0.038577</td>\n",
       "      <td>0.073663</td>\n",
       "      <td>-0.002741</td>\n",
       "      <td>-0.005279</td>\n",
       "      <td>-0.044670</td>\n",
       "      <td>0.044587</td>\n",
       "      <td>-0.050850</td>\n",
       "      <td>0.019114</td>\n",
       "      <td>-0.043696</td>\n",
       "      <td>0.075536</td>\n",
       "      <td>0.047575</td>\n",
       "      <td>-0.060941</td>\n",
       "      <td>-0.017738</td>\n",
       "      <td>-0.038199</td>\n",
       "      <td>-0.077056</td>\n",
       "      <td>-0.021951</td>\n",
       "      <td>-0.078204</td>\n",
       "      <td>0.015555</td>\n",
       "      <td>-0.011308</td>\n",
       "      <td>-0.032753</td>\n",
       "      <td>0.040401</td>\n",
       "      <td>0.007392</td>\n",
       "      <td>-0.059576</td>\n",
       "      <td>0.024170</td>\n",
       "      <td>-0.005230</td>\n",
       "      <td>-0.025785</td>\n",
       "      <td>0.004306</td>\n",
       "      <td>0.035214</td>\n",
       "      <td>0.029278</td>\n",
       "      <td>0.018957</td>\n",
       "      <td>-0.047952</td>\n",
       "      <td>0.016966</td>\n",
       "      <td>-0.040710</td>\n",
       "      <td>-0.042458</td>\n",
       "      <td>0.028166</td>\n",
       "      <td>-0.011035</td>\n",
       "      <td>0.033820</td>\n",
       "      <td>-0.044929</td>\n",
       "      <td>0.012219</td>\n",
       "      <td>0.051090</td>\n",
       "      <td>-0.054629</td>\n",
       "      <td>-0.061649</td>\n",
       "      <td>0.072111</td>\n",
       "      <td>-0.082227</td>\n",
       "      <td>0.071163</td>\n",
       "      <td>-0.069223</td>\n",
       "      <td>-0.007345</td>\n",
       "      <td>0.024400</td>\n",
       "      <td>0.068508</td>\n",
       "      <td>0.063566</td>\n",
       "      <td>0.020393</td>\n",
       "      <td>-0.003837</td>\n",
       "      <td>0.009066</td>\n",
       "      <td>0.027641</td>\n",
       "      <td>-0.054281</td>\n",
       "      <td>-0.003913</td>\n",
       "      <td>0.058263</td>\n",
       "      <td>-0.086442</td>\n",
       "      <td>0.009382</td>\n",
       "      <td>0.045873</td>\n",
       "      <td>-0.076577</td>\n",
       "      <td>0.041925</td>\n",
       "      <td>-0.069554</td>\n",
       "      <td>0.032639</td>\n",
       "      <td>-0.030950</td>\n",
       "      <td>-0.016383</td>\n",
       "      <td>-0.048456</td>\n",
       "      <td>-0.001511</td>\n",
       "      <td>0.028026</td>\n",
       "      <td>-0.065152</td>\n",
       "      <td>0.052603</td>\n",
       "      <td>-0.055091</td>\n",
       "      <td>-0.008152</td>\n",
       "      <td>0.015010</td>\n",
       "      <td>-0.013831</td>\n",
       "      <td>0.055248</td>\n",
       "      <td>0.008181</td>\n",
       "      <td>0.028924</td>\n",
       "      <td>-0.044680</td>\n",
       "      <td>0.048349</td>\n",
       "      <td>0.011497</td>\n",
       "      <td>-0.005242</td>\n",
       "      <td>0.010580</td>\n",
       "      <td>0.034806</td>\n",
       "      <td>-0.055018</td>\n",
       "      <td>0.025380</td>\n",
       "      <td>-0.025614</td>\n",
       "      <td>-0.028189</td>\n",
       "      <td>0.020514</td>\n",
       "      <td>0.008817</td>\n",
       "      <td>-0.064089</td>\n",
       "      <td>-0.014137</td>\n",
       "      <td>0.054572</td>\n",
       "      <td>0.065426</td>\n",
       "      <td>-0.051268</td>\n",
       "      <td>0.010069</td>\n",
       "      <td>-0.005362</td>\n",
       "      <td>-0.079989</td>\n",
       "      <td>0.068649</td>\n",
       "      <td>0.034645</td>\n",
       "      <td>0.049123</td>\n",
       "      <td>0.005019</td>\n",
       "      <td>0.026484</td>\n",
       "      <td>-0.069465</td>\n",
       "      <td>0.053890</td>\n",
       "      <td>0.039131</td>\n",
       "      <td>0.069294</td>\n",
       "      <td>0.064780</td>\n",
       "      <td>-0.044164</td>\n",
       "      <td>0.064746</td>\n",
       "      <td>-0.048647</td>\n",
       "      <td>-0.041526</td>\n",
       "      <td>-0.015924</td>\n",
       "      <td>0.018057</td>\n",
       "      <td>0.041440</td>\n",
       "      <td>-0.048777</td>\n",
       "      <td>-0.000328</td>\n",
       "      <td>-0.036432</td>\n",
       "      <td>0.066196</td>\n",
       "      <td>-0.078564</td>\n",
       "      <td>-0.014947</td>\n",
       "      <td>-0.031295</td>\n",
       "      <td>0.020629</td>\n",
       "      <td>0.035372</td>\n",
       "      <td>0.052295</td>\n",
       "      <td>-0.029933</td>\n",
       "      <td>-0.005077</td>\n",
       "      <td>-0.050017</td>\n",
       "      <td>0.081178</td>\n",
       "      <td>-0.068810</td>\n",
       "      <td>-0.061950</td>\n",
       "      <td>-0.026568</td>\n",
       "      <td>-0.065405</td>\n",
       "      <td>-0.020080</td>\n",
       "      <td>0.049664</td>\n",
       "      <td>-0.066542</td>\n",
       "      <td>0.016401</td>\n",
       "      <td>-0.001722</td>\n",
       "      <td>-0.017019</td>\n",
       "      <td>-0.075020</td>\n",
       "      <td>-0.020728</td>\n",
       "      <td>0.041361</td>\n",
       "      <td>-0.000341</td>\n",
       "      <td>-0.043822</td>\n",
       "      <td>-0.030990</td>\n",
       "      <td>-0.052294</td>\n",
       "      <td>-0.019539</td>\n",
       "      <td>0.058909</td>\n",
       "      <td>0.002056</td>\n",
       "      <td>0.018513</td>\n",
       "      <td>0.090231</td>\n",
       "      <td>0.048884</td>\n",
       "      <td>0.053404</td>\n",
       "      <td>0.007118</td>\n",
       "      <td>-0.073676</td>\n",
       "      <td>-0.060466</td>\n",
       "      <td>0.054085</td>\n",
       "      <td>0.019149</td>\n",
       "      <td>-0.075739</td>\n",
       "      <td>0.067402</td>\n",
       "      <td>0.014897</td>\n",
       "      <td>0.041953</td>\n",
       "      <td>-0.032033</td>\n",
       "      <td>-0.016655</td>\n",
       "      <td>0.047979</td>\n",
       "      <td>-0.067920</td>\n",
       "      <td>0.069193</td>\n",
       "      <td>0.006897</td>\n",
       "      <td>-0.069890</td>\n",
       "      <td>0.025095</td>\n",
       "      <td>0.051158</td>\n",
       "      <td>-0.034050</td>\n",
       "      <td>0.015111</td>\n",
       "      <td>0.034174</td>\n",
       "      <td>0.014129</td>\n",
       "      <td>-0.020497</td>\n",
       "      <td>0.030682</td>\n",
       "      <td>-0.069037</td>\n",
       "      <td>-0.000781</td>\n",
       "      <td>0.075952</td>\n",
       "      <td>-0.032652</td>\n",
       "      <td>0.043115</td>\n",
       "      <td>0.054799</td>\n",
       "      <td>0.029659</td>\n",
       "      <td>0.046451</td>\n",
       "      <td>0.068551</td>\n",
       "      <td>-0.049734</td>\n",
       "      <td>0.013996</td>\n",
       "      <td>-0.071325</td>\n",
       "      <td>-0.029056</td>\n",
       "      <td>0.008925</td>\n",
       "      <td>-0.062857</td>\n",
       "      <td>-0.065307</td>\n",
       "      <td>-0.066391</td>\n",
       "      <td>-0.052747</td>\n",
       "      <td>0.077071</td>\n",
       "      <td>0.049022</td>\n",
       "      <td>0.066229</td>\n",
       "      <td>-0.014651</td>\n",
       "      <td>-0.051110</td>\n",
       "      <td>-0.014207</td>\n",
       "      <td>-0.042188</td>\n",
       "      <td>-0.011078</td>\n",
       "      <td>0.048341</td>\n",
       "      <td>0.033682</td>\n",
       "      <td>0.000767</td>\n",
       "      <td>-0.029926</td>\n",
       "      <td>-0.030757</td>\n",
       "      <td>-0.043596</td>\n",
       "      <td>0.048538</td>\n",
       "      <td>-0.027174</td>\n",
       "      <td>0.018813</td>\n",
       "      <td>0.039662</td>\n",
       "      <td>-0.018775</td>\n",
       "      <td>-0.036942</td>\n",
       "      <td>-0.056748</td>\n",
       "      <td>0.005569</td>\n",
       "      <td>0.046507</td>\n",
       "      <td>0.019279</td>\n",
       "      <td>0.038329</td>\n",
       "      <td>-0.005456</td>\n",
       "      <td>-0.010993</td>\n",
       "      <td>0.014443</td>\n",
       "      <td>-0.059218</td>\n",
       "      <td>0.037260</td>\n",
       "      <td>0.003454</td>\n",
       "      <td>-0.013390</td>\n",
       "      <td>-0.036825</td>\n",
       "      <td>0.029628</td>\n",
       "      <td>0.054219</td>\n",
       "      <td>0.038619</td>\n",
       "      <td>0.065447</td>\n",
       "      <td>0.071682</td>\n",
       "      <td>0.044861</td>\n",
       "      <td>0.001633</td>\n",
       "      <td>-0.070580</td>\n",
       "      <td>0.054042</td>\n",
       "      <td>-0.055432</td>\n",
       "      <td>0.009267</td>\n",
       "      <td>-0.012783</td>\n",
       "      <td>0.026901</td>\n",
       "      <td>-0.049928</td>\n",
       "      <td>-0.070214</td>\n",
       "      <td>0.041941</td>\n",
       "      <td>0.042372</td>\n",
       "      <td>-0.028134</td>\n",
       "      <td>0.023039</td>\n",
       "      <td>0.009851</td>\n",
       "      <td>-0.051295</td>\n",
       "      <td>0.061595</td>\n",
       "      <td>0.071139</td>\n",
       "      <td>-0.044223</td>\n",
       "      <td>0.024194</td>\n",
       "      <td>-0.014387</td>\n",
       "      <td>0.025996</td>\n",
       "      <td>0.000863</td>\n",
       "      <td>0.025795</td>\n",
       "      <td>0.003621</td>\n",
       "      <td>0.020872</td>\n",
       "      <td>0.076698</td>\n",
       "      <td>-0.028938</td>\n",
       "      <td>-0.043122</td>\n",
       "      <td>-0.045511</td>\n",
       "      <td>0.033978</td>\n",
       "      <td>-0.042647</td>\n",
       "      <td>-0.069417</td>\n",
       "      <td>0.007300</td>\n",
       "      <td>-0.004394</td>\n",
       "      <td>0.048451</td>\n",
       "      <td>-0.057236</td>\n",
       "      <td>0.016411</td>\n",
       "      <td>-0.067716</td>\n",
       "      <td>-0.065516</td>\n",
       "      <td>-0.083176</td>\n",
       "      <td>0.082927</td>\n",
       "      <td>-0.021818</td>\n",
       "      <td>-0.075622</td>\n",
       "      <td>-0.001700</td>\n",
       "      <td>0.054432</td>\n",
       "      <td>0.023115</td>\n",
       "      <td>0.013354</td>\n",
       "      <td>-0.017812</td>\n",
       "      <td>-0.057256</td>\n",
       "      <td>0.050663</td>\n",
       "      <td>-0.007543</td>\n",
       "      <td>0.006424</td>\n",
       "      <td>-0.039833</td>\n",
       "      <td>0.031752</td>\n",
       "      <td>0.018204</td>\n",
       "      <td>0.022471</td>\n",
       "      <td>0.054691</td>\n",
       "      <td>-0.070720</td>\n",
       "      <td>0.004839</td>\n",
       "      <td>0.008086</td>\n",
       "      <td>-0.062865</td>\n",
       "      <td>-0.052130</td>\n",
       "      <td>0.008152</td>\n",
       "      <td>-0.079034</td>\n",
       "      <td>-0.017766</td>\n",
       "      <td>-0.048258</td>\n",
       "      <td>-0.034965</td>\n",
       "      <td>-0.041423</td>\n",
       "      <td>-0.060240</td>\n",
       "      <td>-0.061309</td>\n",
       "      <td>0.079033</td>\n",
       "      <td>0.042056</td>\n",
       "      <td>-0.022823</td>\n",
       "      <td>-0.050511</td>\n",
       "      <td>0.030578</td>\n",
       "      <td>0.014960</td>\n",
       "      <td>-0.033145</td>\n",
       "      <td>0.046618</td>\n",
       "      <td>0.041888</td>\n",
       "      <td>-0.014559</td>\n",
       "      <td>0.025480</td>\n",
       "      <td>-0.070325</td>\n",
       "      <td>0.028939</td>\n",
       "      <td>-0.048110</td>\n",
       "      <td>-0.036248</td>\n",
       "      <td>0.019347</td>\n",
       "      <td>0.046270</td>\n",
       "      <td>0.022403</td>\n",
       "      <td>0.009652</td>\n",
       "      <td>-0.010524</td>\n",
       "      <td>-0.010203</td>\n",
       "      <td>-0.064965</td>\n",
       "      <td>-0.053609</td>\n",
       "      <td>-0.007922</td>\n",
       "      <td>-0.087992</td>\n",
       "      <td>0.038238</td>\n",
       "      <td>0.042409</td>\n",
       "      <td>-0.015223</td>\n",
       "      <td>0.015186</td>\n",
       "      <td>-0.020519</td>\n",
       "      <td>0.013376</td>\n",
       "      <td>0.016078</td>\n",
       "      <td>-0.031622</td>\n",
       "      <td>0.045399</td>\n",
       "      <td>-0.051746</td>\n",
       "      <td>-0.049487</td>\n",
       "      <td>0.015081</td>\n",
       "      <td>0.056282</td>\n",
       "      <td>-0.059757</td>\n",
       "      <td>0.050467</td>\n",
       "      <td>0.045041</td>\n",
       "      <td>-0.023176</td>\n",
       "      <td>-0.047181</td>\n",
       "      <td>0.000647</td>\n",
       "      <td>0.057048</td>\n",
       "      <td>0.006192</td>\n",
       "      <td>-0.077530</td>\n",
       "      <td>0.032721</td>\n",
       "      <td>-0.016440</td>\n",
       "      <td>0.033782</td>\n",
       "      <td>-0.019212</td>\n",
       "      <td>-0.031443</td>\n",
       "      <td>0.058002</td>\n",
       "      <td>-0.053286</td>\n",
       "      <td>-0.004739</td>\n",
       "      <td>0.025995</td>\n",
       "      <td>0.020117</td>\n",
       "      <td>0.040129</td>\n",
       "      <td>0.060031</td>\n",
       "      <td>-0.012824</td>\n",
       "      <td>-0.009807</td>\n",
       "      <td>0.021923</td>\n",
       "      <td>-0.047796</td>\n",
       "      <td>-0.030240</td>\n",
       "      <td>-0.016530</td>\n",
       "      <td>0.074136</td>\n",
       "      <td>0.041001</td>\n",
       "      <td>0.052631</td>\n",
       "      <td>0.025878</td>\n",
       "      <td>-0.008818</td>\n",
       "      <td>-0.003311</td>\n",
       "      <td>0.029389</td>\n",
       "      <td>-0.027237</td>\n",
       "      <td>-0.005571</td>\n",
       "      <td>0.073993</td>\n",
       "      <td>0.044530</td>\n",
       "      <td>0.047586</td>\n",
       "      <td>0.062708</td>\n",
       "      <td>-0.006184</td>\n",
       "      <td>-0.029530</td>\n",
       "      <td>0.009271</td>\n",
       "      <td>0.041086</td>\n",
       "      <td>0.043945</td>\n",
       "      <td>-0.000306</td>\n",
       "      <td>0.061877</td>\n",
       "      <td>-0.041166</td>\n",
       "      <td>-0.055623</td>\n",
       "      <td>-0.056213</td>\n",
       "      <td>-0.054792</td>\n",
       "      <td>-0.049162</td>\n",
       "      <td>0.039579</td>\n",
       "      <td>0.048976</td>\n",
       "      <td>-0.016334</td>\n",
       "      <td>0.014645</td>\n",
       "      <td>-0.027659</td>\n",
       "      <td>0.052344</td>\n",
       "      <td>0.024724</td>\n",
       "      <td>0.056533</td>\n",
       "      <td>-0.030742</td>\n",
       "      <td>0.004794</td>\n",
       "      <td>0.067283</td>\n",
       "      <td>0.030255</td>\n",
       "      <td>-0.036236</td>\n",
       "      <td>-0.068788</td>\n",
       "      <td>-0.016268</td>\n",
       "      <td>-0.064952</td>\n",
       "      <td>0.029606</td>\n",
       "      <td>0.022271</td>\n",
       "      <td>0.025709</td>\n",
       "      <td>0.062627</td>\n",
       "      <td>-0.056331</td>\n",
       "      <td>-0.004910</td>\n",
       "      <td>0.046135</td>\n",
       "      <td>0.033801</td>\n",
       "      <td>0.079989</td>\n",
       "      <td>0.050995</td>\n",
       "      <td>-0.090093</td>\n",
       "      <td>0.069056</td>\n",
       "      <td>0.060365</td>\n",
       "      <td>-0.053866</td>\n",
       "      <td>0.013783</td>\n",
       "      <td>-0.011048</td>\n",
       "      <td>-0.023909</td>\n",
       "      <td>0.044623</td>\n",
       "      <td>-0.006743</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>misconception</th>\n",
       "      <td>-0.034154</td>\n",
       "      <td>0.056789</td>\n",
       "      <td>0.030497</td>\n",
       "      <td>-0.078713</td>\n",
       "      <td>-0.026613</td>\n",
       "      <td>0.067809</td>\n",
       "      <td>-0.016281</td>\n",
       "      <td>-0.076655</td>\n",
       "      <td>-0.037526</td>\n",
       "      <td>0.081971</td>\n",
       "      <td>0.027423</td>\n",
       "      <td>0.002251</td>\n",
       "      <td>-0.006066</td>\n",
       "      <td>0.003941</td>\n",
       "      <td>-0.001951</td>\n",
       "      <td>-0.027527</td>\n",
       "      <td>0.034397</td>\n",
       "      <td>0.047273</td>\n",
       "      <td>-0.004808</td>\n",
       "      <td>-0.053303</td>\n",
       "      <td>-0.028236</td>\n",
       "      <td>0.042453</td>\n",
       "      <td>0.009182</td>\n",
       "      <td>-0.030765</td>\n",
       "      <td>0.036382</td>\n",
       "      <td>0.043928</td>\n",
       "      <td>-0.009203</td>\n",
       "      <td>-0.008126</td>\n",
       "      <td>0.074062</td>\n",
       "      <td>0.040288</td>\n",
       "      <td>0.045802</td>\n",
       "      <td>0.002905</td>\n",
       "      <td>-0.044374</td>\n",
       "      <td>0.015171</td>\n",
       "      <td>-0.066765</td>\n",
       "      <td>-0.001422</td>\n",
       "      <td>-0.061134</td>\n",
       "      <td>0.046754</td>\n",
       "      <td>-0.013790</td>\n",
       "      <td>0.070782</td>\n",
       "      <td>-0.029035</td>\n",
       "      <td>-0.004827</td>\n",
       "      <td>-0.002575</td>\n",
       "      <td>-0.029278</td>\n",
       "      <td>-0.054538</td>\n",
       "      <td>-0.070414</td>\n",
       "      <td>-0.086206</td>\n",
       "      <td>0.051062</td>\n",
       "      <td>0.011531</td>\n",
       "      <td>0.029132</td>\n",
       "      <td>0.054043</td>\n",
       "      <td>-0.050168</td>\n",
       "      <td>0.079337</td>\n",
       "      <td>-0.017831</td>\n",
       "      <td>0.027501</td>\n",
       "      <td>-0.049313</td>\n",
       "      <td>0.044035</td>\n",
       "      <td>-0.054718</td>\n",
       "      <td>0.081314</td>\n",
       "      <td>-0.004593</td>\n",
       "      <td>-0.060233</td>\n",
       "      <td>0.005465</td>\n",
       "      <td>0.078469</td>\n",
       "      <td>-0.029092</td>\n",
       "      <td>-0.064353</td>\n",
       "      <td>-0.027618</td>\n",
       "      <td>0.031682</td>\n",
       "      <td>-0.017906</td>\n",
       "      <td>0.038917</td>\n",
       "      <td>0.079529</td>\n",
       "      <td>0.046470</td>\n",
       "      <td>-0.033299</td>\n",
       "      <td>-0.045568</td>\n",
       "      <td>-0.070669</td>\n",
       "      <td>0.023175</td>\n",
       "      <td>-0.013373</td>\n",
       "      <td>0.002263</td>\n",
       "      <td>0.041046</td>\n",
       "      <td>0.005996</td>\n",
       "      <td>-0.073019</td>\n",
       "      <td>0.025884</td>\n",
       "      <td>0.065306</td>\n",
       "      <td>-0.046600</td>\n",
       "      <td>0.002057</td>\n",
       "      <td>-0.003763</td>\n",
       "      <td>0.022454</td>\n",
       "      <td>0.074203</td>\n",
       "      <td>0.001770</td>\n",
       "      <td>-0.041545</td>\n",
       "      <td>0.046130</td>\n",
       "      <td>0.034818</td>\n",
       "      <td>-0.028614</td>\n",
       "      <td>0.026789</td>\n",
       "      <td>0.013958</td>\n",
       "      <td>-0.020135</td>\n",
       "      <td>0.057529</td>\n",
       "      <td>-0.081354</td>\n",
       "      <td>0.057267</td>\n",
       "      <td>0.011461</td>\n",
       "      <td>-0.052128</td>\n",
       "      <td>0.025705</td>\n",
       "      <td>-0.059955</td>\n",
       "      <td>0.027711</td>\n",
       "      <td>-0.013011</td>\n",
       "      <td>0.007698</td>\n",
       "      <td>-0.010456</td>\n",
       "      <td>-0.031352</td>\n",
       "      <td>-0.038039</td>\n",
       "      <td>-0.074499</td>\n",
       "      <td>0.017237</td>\n",
       "      <td>0.057172</td>\n",
       "      <td>-0.082175</td>\n",
       "      <td>-0.086558</td>\n",
       "      <td>-0.034316</td>\n",
       "      <td>0.013059</td>\n",
       "      <td>-0.004027</td>\n",
       "      <td>0.004249</td>\n",
       "      <td>0.054917</td>\n",
       "      <td>0.071742</td>\n",
       "      <td>-0.011617</td>\n",
       "      <td>-0.076143</td>\n",
       "      <td>0.044475</td>\n",
       "      <td>0.008609</td>\n",
       "      <td>0.010895</td>\n",
       "      <td>0.017432</td>\n",
       "      <td>0.036195</td>\n",
       "      <td>0.015971</td>\n",
       "      <td>-0.019197</td>\n",
       "      <td>-0.066839</td>\n",
       "      <td>-0.009980</td>\n",
       "      <td>-0.065841</td>\n",
       "      <td>-0.006096</td>\n",
       "      <td>0.058108</td>\n",
       "      <td>0.013299</td>\n",
       "      <td>-0.055490</td>\n",
       "      <td>0.041030</td>\n",
       "      <td>0.075199</td>\n",
       "      <td>0.045329</td>\n",
       "      <td>-0.002646</td>\n",
       "      <td>0.017002</td>\n",
       "      <td>-0.052113</td>\n",
       "      <td>-0.013220</td>\n",
       "      <td>0.030502</td>\n",
       "      <td>-0.046650</td>\n",
       "      <td>0.012063</td>\n",
       "      <td>-0.071703</td>\n",
       "      <td>-0.076647</td>\n",
       "      <td>-0.002664</td>\n",
       "      <td>0.042771</td>\n",
       "      <td>-0.043813</td>\n",
       "      <td>0.015949</td>\n",
       "      <td>0.042224</td>\n",
       "      <td>-0.067835</td>\n",
       "      <td>0.001561</td>\n",
       "      <td>-0.067934</td>\n",
       "      <td>-0.030295</td>\n",
       "      <td>-0.070409</td>\n",
       "      <td>-0.079169</td>\n",
       "      <td>-0.026560</td>\n",
       "      <td>0.027638</td>\n",
       "      <td>0.026364</td>\n",
       "      <td>-0.047205</td>\n",
       "      <td>0.018246</td>\n",
       "      <td>-0.020594</td>\n",
       "      <td>0.049088</td>\n",
       "      <td>-0.010302</td>\n",
       "      <td>-0.057182</td>\n",
       "      <td>-0.005701</td>\n",
       "      <td>0.009216</td>\n",
       "      <td>0.009676</td>\n",
       "      <td>-0.033836</td>\n",
       "      <td>-0.082241</td>\n",
       "      <td>0.057516</td>\n",
       "      <td>-0.017868</td>\n",
       "      <td>-0.031307</td>\n",
       "      <td>0.018975</td>\n",
       "      <td>0.068723</td>\n",
       "      <td>-0.000034</td>\n",
       "      <td>0.054391</td>\n",
       "      <td>0.017167</td>\n",
       "      <td>-0.027566</td>\n",
       "      <td>0.058595</td>\n",
       "      <td>-0.070371</td>\n",
       "      <td>-0.046508</td>\n",
       "      <td>-0.029141</td>\n",
       "      <td>0.023633</td>\n",
       "      <td>-0.014438</td>\n",
       "      <td>0.005827</td>\n",
       "      <td>0.021120</td>\n",
       "      <td>-0.061500</td>\n",
       "      <td>0.009427</td>\n",
       "      <td>0.022469</td>\n",
       "      <td>-0.024213</td>\n",
       "      <td>0.051502</td>\n",
       "      <td>0.009010</td>\n",
       "      <td>-0.050925</td>\n",
       "      <td>0.063563</td>\n",
       "      <td>-0.016744</td>\n",
       "      <td>-0.028757</td>\n",
       "      <td>-0.014449</td>\n",
       "      <td>-0.076551</td>\n",
       "      <td>0.010053</td>\n",
       "      <td>-0.037417</td>\n",
       "      <td>0.055246</td>\n",
       "      <td>-0.070086</td>\n",
       "      <td>0.027179</td>\n",
       "      <td>-0.076198</td>\n",
       "      <td>-0.006539</td>\n",
       "      <td>0.005825</td>\n",
       "      <td>0.064117</td>\n",
       "      <td>-0.063292</td>\n",
       "      <td>0.006542</td>\n",
       "      <td>-0.028721</td>\n",
       "      <td>0.025230</td>\n",
       "      <td>-0.071278</td>\n",
       "      <td>0.020629</td>\n",
       "      <td>-0.046245</td>\n",
       "      <td>-0.058321</td>\n",
       "      <td>0.000976</td>\n",
       "      <td>0.032937</td>\n",
       "      <td>-0.011987</td>\n",
       "      <td>-0.060293</td>\n",
       "      <td>-0.008482</td>\n",
       "      <td>0.039845</td>\n",
       "      <td>0.056724</td>\n",
       "      <td>0.003717</td>\n",
       "      <td>-0.008027</td>\n",
       "      <td>-0.062912</td>\n",
       "      <td>-0.022392</td>\n",
       "      <td>-0.004539</td>\n",
       "      <td>-0.053931</td>\n",
       "      <td>-0.002279</td>\n",
       "      <td>0.041470</td>\n",
       "      <td>0.091689</td>\n",
       "      <td>0.047256</td>\n",
       "      <td>-0.032948</td>\n",
       "      <td>0.027204</td>\n",
       "      <td>-0.035020</td>\n",
       "      <td>-0.034575</td>\n",
       "      <td>-0.069146</td>\n",
       "      <td>0.068564</td>\n",
       "      <td>-0.064134</td>\n",
       "      <td>-0.016592</td>\n",
       "      <td>-0.019586</td>\n",
       "      <td>0.019255</td>\n",
       "      <td>0.054613</td>\n",
       "      <td>-0.018804</td>\n",
       "      <td>0.076465</td>\n",
       "      <td>0.031736</td>\n",
       "      <td>-0.053508</td>\n",
       "      <td>0.021507</td>\n",
       "      <td>0.013832</td>\n",
       "      <td>0.004261</td>\n",
       "      <td>-0.033279</td>\n",
       "      <td>-0.007607</td>\n",
       "      <td>0.004471</td>\n",
       "      <td>-0.047477</td>\n",
       "      <td>-0.015780</td>\n",
       "      <td>-0.024102</td>\n",
       "      <td>0.034589</td>\n",
       "      <td>-0.058397</td>\n",
       "      <td>-0.060095</td>\n",
       "      <td>-0.086189</td>\n",
       "      <td>-0.022586</td>\n",
       "      <td>-0.003368</td>\n",
       "      <td>0.053644</td>\n",
       "      <td>-0.004562</td>\n",
       "      <td>-0.049887</td>\n",
       "      <td>0.041428</td>\n",
       "      <td>0.046805</td>\n",
       "      <td>0.032228</td>\n",
       "      <td>-0.042200</td>\n",
       "      <td>-0.075073</td>\n",
       "      <td>-0.025621</td>\n",
       "      <td>-0.020568</td>\n",
       "      <td>-0.070207</td>\n",
       "      <td>0.049527</td>\n",
       "      <td>0.021357</td>\n",
       "      <td>0.028856</td>\n",
       "      <td>0.075859</td>\n",
       "      <td>-0.078842</td>\n",
       "      <td>-0.045703</td>\n",
       "      <td>0.084661</td>\n",
       "      <td>-0.006199</td>\n",
       "      <td>-0.004000</td>\n",
       "      <td>0.079369</td>\n",
       "      <td>-0.017606</td>\n",
       "      <td>0.060691</td>\n",
       "      <td>-0.061375</td>\n",
       "      <td>0.019518</td>\n",
       "      <td>0.017140</td>\n",
       "      <td>-0.004531</td>\n",
       "      <td>-0.057377</td>\n",
       "      <td>-0.011032</td>\n",
       "      <td>-0.057604</td>\n",
       "      <td>0.041226</td>\n",
       "      <td>-0.003819</td>\n",
       "      <td>-0.010758</td>\n",
       "      <td>0.043441</td>\n",
       "      <td>0.000082</td>\n",
       "      <td>0.041475</td>\n",
       "      <td>0.020987</td>\n",
       "      <td>-0.046404</td>\n",
       "      <td>-0.052170</td>\n",
       "      <td>0.066946</td>\n",
       "      <td>-0.035612</td>\n",
       "      <td>0.036729</td>\n",
       "      <td>-0.014877</td>\n",
       "      <td>-0.022250</td>\n",
       "      <td>-0.006169</td>\n",
       "      <td>-0.042886</td>\n",
       "      <td>0.011059</td>\n",
       "      <td>0.015460</td>\n",
       "      <td>0.030279</td>\n",
       "      <td>-0.012471</td>\n",
       "      <td>-0.036970</td>\n",
       "      <td>0.033666</td>\n",
       "      <td>0.035401</td>\n",
       "      <td>-0.034424</td>\n",
       "      <td>-0.022334</td>\n",
       "      <td>-0.020666</td>\n",
       "      <td>-0.056259</td>\n",
       "      <td>-0.014859</td>\n",
       "      <td>0.035520</td>\n",
       "      <td>-0.068057</td>\n",
       "      <td>-0.046371</td>\n",
       "      <td>0.005644</td>\n",
       "      <td>0.008288</td>\n",
       "      <td>0.007837</td>\n",
       "      <td>-0.043657</td>\n",
       "      <td>0.021454</td>\n",
       "      <td>0.063920</td>\n",
       "      <td>0.053817</td>\n",
       "      <td>0.083319</td>\n",
       "      <td>-0.040147</td>\n",
       "      <td>0.057409</td>\n",
       "      <td>-0.042668</td>\n",
       "      <td>-0.012305</td>\n",
       "      <td>0.013440</td>\n",
       "      <td>0.068528</td>\n",
       "      <td>-0.044585</td>\n",
       "      <td>0.008306</td>\n",
       "      <td>0.019217</td>\n",
       "      <td>-0.046726</td>\n",
       "      <td>-0.041486</td>\n",
       "      <td>-0.000523</td>\n",
       "      <td>0.070361</td>\n",
       "      <td>-0.054341</td>\n",
       "      <td>-0.042647</td>\n",
       "      <td>0.028851</td>\n",
       "      <td>0.056877</td>\n",
       "      <td>-0.053782</td>\n",
       "      <td>0.032337</td>\n",
       "      <td>0.038638</td>\n",
       "      <td>-0.009494</td>\n",
       "      <td>-0.006674</td>\n",
       "      <td>-0.081569</td>\n",
       "      <td>0.059548</td>\n",
       "      <td>0.009932</td>\n",
       "      <td>-0.073700</td>\n",
       "      <td>0.011981</td>\n",
       "      <td>-0.002238</td>\n",
       "      <td>0.080643</td>\n",
       "      <td>0.006766</td>\n",
       "      <td>-0.024293</td>\n",
       "      <td>-0.004638</td>\n",
       "      <td>0.045566</td>\n",
       "      <td>-0.003386</td>\n",
       "      <td>0.029386</td>\n",
       "      <td>-0.039917</td>\n",
       "      <td>-0.030597</td>\n",
       "      <td>-0.021465</td>\n",
       "      <td>0.054359</td>\n",
       "      <td>-0.063675</td>\n",
       "      <td>0.007416</td>\n",
       "      <td>-0.047241</td>\n",
       "      <td>-0.034450</td>\n",
       "      <td>-0.051978</td>\n",
       "      <td>-0.084676</td>\n",
       "      <td>-0.034226</td>\n",
       "      <td>-0.054737</td>\n",
       "      <td>0.000705</td>\n",
       "      <td>-0.034623</td>\n",
       "      <td>-0.021208</td>\n",
       "      <td>0.091257</td>\n",
       "      <td>-0.071899</td>\n",
       "      <td>-0.009954</td>\n",
       "      <td>0.057576</td>\n",
       "      <td>0.075837</td>\n",
       "      <td>-0.017580</td>\n",
       "      <td>0.037935</td>\n",
       "      <td>0.016344</td>\n",
       "      <td>-0.002932</td>\n",
       "      <td>-0.039862</td>\n",
       "      <td>-0.034449</td>\n",
       "      <td>-0.029185</td>\n",
       "      <td>-0.047611</td>\n",
       "      <td>0.047566</td>\n",
       "      <td>-0.000416</td>\n",
       "      <td>-0.011780</td>\n",
       "      <td>0.037712</td>\n",
       "      <td>0.048843</td>\n",
       "      <td>0.024755</td>\n",
       "      <td>-0.062980</td>\n",
       "      <td>0.016671</td>\n",
       "      <td>-0.007598</td>\n",
       "      <td>-0.041735</td>\n",
       "      <td>0.059873</td>\n",
       "      <td>-0.089206</td>\n",
       "      <td>-0.029773</td>\n",
       "      <td>-0.008058</td>\n",
       "      <td>-0.076280</td>\n",
       "      <td>0.073256</td>\n",
       "      <td>-0.046136</td>\n",
       "      <td>0.040195</td>\n",
       "      <td>0.044383</td>\n",
       "      <td>0.030609</td>\n",
       "      <td>-0.070345</td>\n",
       "      <td>0.046401</td>\n",
       "      <td>0.010846</td>\n",
       "      <td>-0.030563</td>\n",
       "      <td>0.036867</td>\n",
       "      <td>0.039984</td>\n",
       "      <td>0.031604</td>\n",
       "      <td>0.055534</td>\n",
       "      <td>0.053524</td>\n",
       "      <td>0.064416</td>\n",
       "      <td>0.070160</td>\n",
       "      <td>-0.010095</td>\n",
       "      <td>-0.039146</td>\n",
       "      <td>0.020533</td>\n",
       "      <td>-0.050308</td>\n",
       "      <td>-0.086189</td>\n",
       "      <td>-0.067358</td>\n",
       "      <td>0.065705</td>\n",
       "      <td>-0.081466</td>\n",
       "      <td>0.024587</td>\n",
       "      <td>-0.027059</td>\n",
       "      <td>-0.060660</td>\n",
       "      <td>-0.049794</td>\n",
       "      <td>0.044823</td>\n",
       "      <td>0.048261</td>\n",
       "      <td>0.026147</td>\n",
       "      <td>0.048554</td>\n",
       "      <td>-0.011370</td>\n",
       "      <td>0.022707</td>\n",
       "      <td>-0.018570</td>\n",
       "      <td>0.035266</td>\n",
       "      <td>0.009890</td>\n",
       "      <td>-0.058766</td>\n",
       "      <td>0.027933</td>\n",
       "      <td>-0.023505</td>\n",
       "      <td>0.070215</td>\n",
       "      <td>-0.031561</td>\n",
       "      <td>0.029631</td>\n",
       "      <td>0.037513</td>\n",
       "      <td>-0.013175</td>\n",
       "      <td>0.048388</td>\n",
       "      <td>-0.056155</td>\n",
       "      <td>0.054404</td>\n",
       "      <td>0.053720</td>\n",
       "      <td>-0.049853</td>\n",
       "      <td>0.070728</td>\n",
       "      <td>0.064865</td>\n",
       "      <td>-0.055803</td>\n",
       "      <td>-0.072805</td>\n",
       "      <td>0.006038</td>\n",
       "      <td>0.040390</td>\n",
       "      <td>-0.004444</td>\n",
       "      <td>0.044455</td>\n",
       "      <td>-0.084494</td>\n",
       "      <td>-0.033479</td>\n",
       "      <td>-0.060080</td>\n",
       "      <td>0.002631</td>\n",
       "      <td>0.009176</td>\n",
       "      <td>-0.016510</td>\n",
       "      <td>-0.075071</td>\n",
       "      <td>0.018011</td>\n",
       "      <td>0.077930</td>\n",
       "      <td>0.066667</td>\n",
       "      <td>0.041971</td>\n",
       "      <td>0.041898</td>\n",
       "      <td>-0.050355</td>\n",
       "      <td>-0.037281</td>\n",
       "      <td>0.035519</td>\n",
       "      <td>-0.010927</td>\n",
       "      <td>-0.063864</td>\n",
       "      <td>-0.024931</td>\n",
       "      <td>0.003871</td>\n",
       "      <td>0.023619</td>\n",
       "      <td>-0.070655</td>\n",
       "      <td>-0.005272</td>\n",
       "      <td>-0.010078</td>\n",
       "      <td>-0.041793</td>\n",
       "      <td>0.026052</td>\n",
       "      <td>-0.058696</td>\n",
       "      <td>0.008776</td>\n",
       "      <td>0.018026</td>\n",
       "      <td>0.048573</td>\n",
       "      <td>-0.001823</td>\n",
       "      <td>-0.030892</td>\n",
       "      <td>-0.008985</td>\n",
       "      <td>-0.071650</td>\n",
       "      <td>-0.091038</td>\n",
       "      <td>0.068272</td>\n",
       "      <td>0.020632</td>\n",
       "      <td>-0.021735</td>\n",
       "      <td>0.002143</td>\n",
       "      <td>-0.053941</td>\n",
       "      <td>0.026607</td>\n",
       "      <td>0.025247</td>\n",
       "      <td>0.013437</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>crab</th>\n",
       "      <td>-0.016708</td>\n",
       "      <td>0.002911</td>\n",
       "      <td>0.007000</td>\n",
       "      <td>0.015318</td>\n",
       "      <td>-0.006244</td>\n",
       "      <td>-0.057960</td>\n",
       "      <td>-0.042535</td>\n",
       "      <td>-0.033015</td>\n",
       "      <td>-0.038489</td>\n",
       "      <td>0.019487</td>\n",
       "      <td>0.072698</td>\n",
       "      <td>-0.065903</td>\n",
       "      <td>-0.000874</td>\n",
       "      <td>-0.012989</td>\n",
       "      <td>-0.061724</td>\n",
       "      <td>-0.036529</td>\n",
       "      <td>-0.036080</td>\n",
       "      <td>0.011276</td>\n",
       "      <td>0.046366</td>\n",
       "      <td>-0.063745</td>\n",
       "      <td>-0.004769</td>\n",
       "      <td>0.045863</td>\n",
       "      <td>0.011048</td>\n",
       "      <td>-0.013588</td>\n",
       "      <td>-0.003097</td>\n",
       "      <td>0.054539</td>\n",
       "      <td>0.052981</td>\n",
       "      <td>0.056658</td>\n",
       "      <td>0.046551</td>\n",
       "      <td>-0.036492</td>\n",
       "      <td>-0.021686</td>\n",
       "      <td>-0.072016</td>\n",
       "      <td>0.061624</td>\n",
       "      <td>-0.065766</td>\n",
       "      <td>-0.059771</td>\n",
       "      <td>0.040640</td>\n",
       "      <td>0.009580</td>\n",
       "      <td>0.052817</td>\n",
       "      <td>-0.021747</td>\n",
       "      <td>-0.018621</td>\n",
       "      <td>0.008989</td>\n",
       "      <td>-0.075750</td>\n",
       "      <td>0.056454</td>\n",
       "      <td>-0.058991</td>\n",
       "      <td>-0.080451</td>\n",
       "      <td>0.044077</td>\n",
       "      <td>-0.056637</td>\n",
       "      <td>-0.022220</td>\n",
       "      <td>0.016940</td>\n",
       "      <td>-0.045077</td>\n",
       "      <td>0.079832</td>\n",
       "      <td>-0.052661</td>\n",
       "      <td>0.071158</td>\n",
       "      <td>0.047757</td>\n",
       "      <td>0.077209</td>\n",
       "      <td>-0.069621</td>\n",
       "      <td>-0.031901</td>\n",
       "      <td>-0.027668</td>\n",
       "      <td>0.039575</td>\n",
       "      <td>-0.027541</td>\n",
       "      <td>-0.000462</td>\n",
       "      <td>-0.013485</td>\n",
       "      <td>-0.004151</td>\n",
       "      <td>0.056259</td>\n",
       "      <td>-0.038508</td>\n",
       "      <td>0.055647</td>\n",
       "      <td>-0.012054</td>\n",
       "      <td>-0.003845</td>\n",
       "      <td>0.039164</td>\n",
       "      <td>0.036182</td>\n",
       "      <td>-0.046892</td>\n",
       "      <td>0.028439</td>\n",
       "      <td>0.062893</td>\n",
       "      <td>-0.051784</td>\n",
       "      <td>-0.000144</td>\n",
       "      <td>-0.005228</td>\n",
       "      <td>-0.005999</td>\n",
       "      <td>0.065260</td>\n",
       "      <td>0.030990</td>\n",
       "      <td>0.021756</td>\n",
       "      <td>-0.029967</td>\n",
       "      <td>0.070186</td>\n",
       "      <td>-0.074550</td>\n",
       "      <td>0.025069</td>\n",
       "      <td>-0.075702</td>\n",
       "      <td>-0.048358</td>\n",
       "      <td>0.007014</td>\n",
       "      <td>0.012645</td>\n",
       "      <td>0.013348</td>\n",
       "      <td>-0.061626</td>\n",
       "      <td>-0.035409</td>\n",
       "      <td>0.036397</td>\n",
       "      <td>0.024200</td>\n",
       "      <td>-0.001777</td>\n",
       "      <td>0.022490</td>\n",
       "      <td>-0.023139</td>\n",
       "      <td>-0.024822</td>\n",
       "      <td>-0.063044</td>\n",
       "      <td>0.032666</td>\n",
       "      <td>0.020477</td>\n",
       "      <td>0.024427</td>\n",
       "      <td>-0.025261</td>\n",
       "      <td>-0.060829</td>\n",
       "      <td>-0.030101</td>\n",
       "      <td>0.054820</td>\n",
       "      <td>0.004361</td>\n",
       "      <td>0.042388</td>\n",
       "      <td>-0.019440</td>\n",
       "      <td>0.025787</td>\n",
       "      <td>0.058766</td>\n",
       "      <td>-0.062082</td>\n",
       "      <td>0.040878</td>\n",
       "      <td>-0.044142</td>\n",
       "      <td>0.042879</td>\n",
       "      <td>0.054031</td>\n",
       "      <td>0.061690</td>\n",
       "      <td>-0.057573</td>\n",
       "      <td>-0.062365</td>\n",
       "      <td>-0.064791</td>\n",
       "      <td>-0.017471</td>\n",
       "      <td>0.079737</td>\n",
       "      <td>-0.060824</td>\n",
       "      <td>0.005194</td>\n",
       "      <td>-0.065564</td>\n",
       "      <td>-0.032914</td>\n",
       "      <td>0.071133</td>\n",
       "      <td>-0.051208</td>\n",
       "      <td>0.036035</td>\n",
       "      <td>-0.049490</td>\n",
       "      <td>-0.043829</td>\n",
       "      <td>-0.045300</td>\n",
       "      <td>0.078517</td>\n",
       "      <td>0.062225</td>\n",
       "      <td>0.072204</td>\n",
       "      <td>-0.014592</td>\n",
       "      <td>0.025537</td>\n",
       "      <td>0.064917</td>\n",
       "      <td>-0.006360</td>\n",
       "      <td>0.004579</td>\n",
       "      <td>-0.077026</td>\n",
       "      <td>0.007326</td>\n",
       "      <td>-0.053800</td>\n",
       "      <td>0.043089</td>\n",
       "      <td>-0.059520</td>\n",
       "      <td>0.022654</td>\n",
       "      <td>0.071411</td>\n",
       "      <td>0.038626</td>\n",
       "      <td>-0.003498</td>\n",
       "      <td>-0.063165</td>\n",
       "      <td>0.059374</td>\n",
       "      <td>0.062276</td>\n",
       "      <td>0.002944</td>\n",
       "      <td>-0.062867</td>\n",
       "      <td>-0.036747</td>\n",
       "      <td>0.033791</td>\n",
       "      <td>-0.001102</td>\n",
       "      <td>0.038814</td>\n",
       "      <td>-0.062954</td>\n",
       "      <td>0.015709</td>\n",
       "      <td>-0.010734</td>\n",
       "      <td>-0.056800</td>\n",
       "      <td>-0.066593</td>\n",
       "      <td>-0.017201</td>\n",
       "      <td>-0.000705</td>\n",
       "      <td>-0.015755</td>\n",
       "      <td>-0.033986</td>\n",
       "      <td>0.012237</td>\n",
       "      <td>-0.029840</td>\n",
       "      <td>0.020716</td>\n",
       "      <td>0.002799</td>\n",
       "      <td>-0.032390</td>\n",
       "      <td>-0.023541</td>\n",
       "      <td>0.034555</td>\n",
       "      <td>-0.022887</td>\n",
       "      <td>-0.048504</td>\n",
       "      <td>-0.033019</td>\n",
       "      <td>0.054788</td>\n",
       "      <td>0.051276</td>\n",
       "      <td>-0.021503</td>\n",
       "      <td>-0.011998</td>\n",
       "      <td>0.043651</td>\n",
       "      <td>0.015641</td>\n",
       "      <td>0.030144</td>\n",
       "      <td>-0.067091</td>\n",
       "      <td>-0.005969</td>\n",
       "      <td>0.068762</td>\n",
       "      <td>-0.077857</td>\n",
       "      <td>0.049406</td>\n",
       "      <td>-0.004070</td>\n",
       "      <td>0.038430</td>\n",
       "      <td>0.047602</td>\n",
       "      <td>0.060042</td>\n",
       "      <td>0.067295</td>\n",
       "      <td>-0.032528</td>\n",
       "      <td>-0.019642</td>\n",
       "      <td>0.035619</td>\n",
       "      <td>0.075774</td>\n",
       "      <td>-0.033484</td>\n",
       "      <td>-0.055975</td>\n",
       "      <td>0.040675</td>\n",
       "      <td>0.034260</td>\n",
       "      <td>-0.059429</td>\n",
       "      <td>-0.015919</td>\n",
       "      <td>-0.005863</td>\n",
       "      <td>0.012188</td>\n",
       "      <td>-0.054521</td>\n",
       "      <td>-0.031349</td>\n",
       "      <td>0.071273</td>\n",
       "      <td>-0.068224</td>\n",
       "      <td>-0.042330</td>\n",
       "      <td>-0.011450</td>\n",
       "      <td>0.068818</td>\n",
       "      <td>-0.039030</td>\n",
       "      <td>-0.062780</td>\n",
       "      <td>-0.006073</td>\n",
       "      <td>-0.065241</td>\n",
       "      <td>0.007722</td>\n",
       "      <td>0.019375</td>\n",
       "      <td>0.037414</td>\n",
       "      <td>0.066684</td>\n",
       "      <td>-0.037973</td>\n",
       "      <td>-0.077074</td>\n",
       "      <td>-0.046535</td>\n",
       "      <td>-0.058743</td>\n",
       "      <td>-0.039353</td>\n",
       "      <td>-0.010794</td>\n",
       "      <td>-0.023444</td>\n",
       "      <td>0.029874</td>\n",
       "      <td>-0.038092</td>\n",
       "      <td>0.031726</td>\n",
       "      <td>0.051370</td>\n",
       "      <td>-0.037315</td>\n",
       "      <td>-0.034746</td>\n",
       "      <td>0.081595</td>\n",
       "      <td>0.055963</td>\n",
       "      <td>0.063254</td>\n",
       "      <td>0.019989</td>\n",
       "      <td>-0.036706</td>\n",
       "      <td>-0.057626</td>\n",
       "      <td>0.034620</td>\n",
       "      <td>0.017276</td>\n",
       "      <td>-0.034026</td>\n",
       "      <td>0.030884</td>\n",
       "      <td>0.011983</td>\n",
       "      <td>-0.039323</td>\n",
       "      <td>-0.064660</td>\n",
       "      <td>0.080648</td>\n",
       "      <td>-0.063890</td>\n",
       "      <td>-0.041449</td>\n",
       "      <td>0.023663</td>\n",
       "      <td>0.064329</td>\n",
       "      <td>0.048727</td>\n",
       "      <td>0.007597</td>\n",
       "      <td>-0.030637</td>\n",
       "      <td>-0.033215</td>\n",
       "      <td>0.061575</td>\n",
       "      <td>-0.044511</td>\n",
       "      <td>0.047595</td>\n",
       "      <td>0.024730</td>\n",
       "      <td>0.050769</td>\n",
       "      <td>-0.045433</td>\n",
       "      <td>-0.029853</td>\n",
       "      <td>-0.001771</td>\n",
       "      <td>-0.030202</td>\n",
       "      <td>0.036024</td>\n",
       "      <td>0.039999</td>\n",
       "      <td>0.080115</td>\n",
       "      <td>0.005320</td>\n",
       "      <td>0.025570</td>\n",
       "      <td>-0.039550</td>\n",
       "      <td>-0.008781</td>\n",
       "      <td>0.014348</td>\n",
       "      <td>0.037793</td>\n",
       "      <td>0.028025</td>\n",
       "      <td>0.079354</td>\n",
       "      <td>-0.035360</td>\n",
       "      <td>0.035859</td>\n",
       "      <td>-0.034297</td>\n",
       "      <td>0.075652</td>\n",
       "      <td>0.032100</td>\n",
       "      <td>0.014291</td>\n",
       "      <td>-0.030578</td>\n",
       "      <td>0.021240</td>\n",
       "      <td>0.038162</td>\n",
       "      <td>-0.011066</td>\n",
       "      <td>0.024013</td>\n",
       "      <td>-0.014658</td>\n",
       "      <td>0.062250</td>\n",
       "      <td>-0.076272</td>\n",
       "      <td>0.005197</td>\n",
       "      <td>0.039206</td>\n",
       "      <td>-0.048179</td>\n",
       "      <td>0.080727</td>\n",
       "      <td>0.014491</td>\n",
       "      <td>0.021994</td>\n",
       "      <td>0.033844</td>\n",
       "      <td>-0.078713</td>\n",
       "      <td>0.054091</td>\n",
       "      <td>-0.057999</td>\n",
       "      <td>-0.029239</td>\n",
       "      <td>0.064402</td>\n",
       "      <td>-0.074698</td>\n",
       "      <td>-0.012669</td>\n",
       "      <td>-0.005822</td>\n",
       "      <td>-0.028326</td>\n",
       "      <td>0.067021</td>\n",
       "      <td>0.040595</td>\n",
       "      <td>-0.047290</td>\n",
       "      <td>-0.053494</td>\n",
       "      <td>-0.077079</td>\n",
       "      <td>-0.016192</td>\n",
       "      <td>-0.057975</td>\n",
       "      <td>0.054447</td>\n",
       "      <td>0.036689</td>\n",
       "      <td>-0.033355</td>\n",
       "      <td>0.044778</td>\n",
       "      <td>-0.011137</td>\n",
       "      <td>0.010875</td>\n",
       "      <td>-0.040631</td>\n",
       "      <td>-0.042331</td>\n",
       "      <td>-0.050699</td>\n",
       "      <td>-0.000015</td>\n",
       "      <td>0.058344</td>\n",
       "      <td>0.023232</td>\n",
       "      <td>0.012743</td>\n",
       "      <td>0.074750</td>\n",
       "      <td>-0.046889</td>\n",
       "      <td>0.037751</td>\n",
       "      <td>-0.020852</td>\n",
       "      <td>-0.031396</td>\n",
       "      <td>-0.021376</td>\n",
       "      <td>-0.067655</td>\n",
       "      <td>0.023899</td>\n",
       "      <td>-0.022183</td>\n",
       "      <td>-0.072389</td>\n",
       "      <td>-0.049675</td>\n",
       "      <td>0.025535</td>\n",
       "      <td>0.037350</td>\n",
       "      <td>0.020273</td>\n",
       "      <td>-0.066463</td>\n",
       "      <td>-0.046169</td>\n",
       "      <td>-0.008139</td>\n",
       "      <td>-0.076265</td>\n",
       "      <td>0.026609</td>\n",
       "      <td>-0.016196</td>\n",
       "      <td>-0.081377</td>\n",
       "      <td>-0.011060</td>\n",
       "      <td>0.047870</td>\n",
       "      <td>0.048441</td>\n",
       "      <td>-0.022399</td>\n",
       "      <td>-0.046059</td>\n",
       "      <td>0.030927</td>\n",
       "      <td>-0.078033</td>\n",
       "      <td>0.036120</td>\n",
       "      <td>-0.034290</td>\n",
       "      <td>0.022602</td>\n",
       "      <td>-0.076372</td>\n",
       "      <td>-0.008197</td>\n",
       "      <td>0.061978</td>\n",
       "      <td>-0.018166</td>\n",
       "      <td>-0.014090</td>\n",
       "      <td>0.002324</td>\n",
       "      <td>0.075844</td>\n",
       "      <td>0.055968</td>\n",
       "      <td>-0.000977</td>\n",
       "      <td>-0.059545</td>\n",
       "      <td>0.058583</td>\n",
       "      <td>-0.011686</td>\n",
       "      <td>0.002117</td>\n",
       "      <td>0.076429</td>\n",
       "      <td>0.078360</td>\n",
       "      <td>0.041819</td>\n",
       "      <td>0.041636</td>\n",
       "      <td>-0.036642</td>\n",
       "      <td>-0.015381</td>\n",
       "      <td>-0.042747</td>\n",
       "      <td>0.039568</td>\n",
       "      <td>0.000638</td>\n",
       "      <td>-0.027965</td>\n",
       "      <td>-0.028481</td>\n",
       "      <td>0.053231</td>\n",
       "      <td>0.054999</td>\n",
       "      <td>0.021657</td>\n",
       "      <td>0.036547</td>\n",
       "      <td>-0.010514</td>\n",
       "      <td>-0.014238</td>\n",
       "      <td>-0.055776</td>\n",
       "      <td>-0.055854</td>\n",
       "      <td>0.021718</td>\n",
       "      <td>0.080040</td>\n",
       "      <td>-0.020494</td>\n",
       "      <td>0.035921</td>\n",
       "      <td>0.020063</td>\n",
       "      <td>-0.045633</td>\n",
       "      <td>-0.004799</td>\n",
       "      <td>-0.018480</td>\n",
       "      <td>-0.062645</td>\n",
       "      <td>-0.008894</td>\n",
       "      <td>-0.016136</td>\n",
       "      <td>0.033499</td>\n",
       "      <td>-0.006930</td>\n",
       "      <td>0.015178</td>\n",
       "      <td>-0.020242</td>\n",
       "      <td>-0.021059</td>\n",
       "      <td>-0.040721</td>\n",
       "      <td>0.047935</td>\n",
       "      <td>-0.026875</td>\n",
       "      <td>0.024810</td>\n",
       "      <td>0.015256</td>\n",
       "      <td>-0.055888</td>\n",
       "      <td>-0.062866</td>\n",
       "      <td>0.035687</td>\n",
       "      <td>0.022228</td>\n",
       "      <td>-0.070362</td>\n",
       "      <td>-0.081347</td>\n",
       "      <td>0.020142</td>\n",
       "      <td>0.064630</td>\n",
       "      <td>-0.030652</td>\n",
       "      <td>-0.003897</td>\n",
       "      <td>-0.062167</td>\n",
       "      <td>0.014456</td>\n",
       "      <td>0.019430</td>\n",
       "      <td>-0.020528</td>\n",
       "      <td>-0.042739</td>\n",
       "      <td>0.031253</td>\n",
       "      <td>-0.043001</td>\n",
       "      <td>-0.003920</td>\n",
       "      <td>0.044265</td>\n",
       "      <td>0.014015</td>\n",
       "      <td>-0.047095</td>\n",
       "      <td>-0.015290</td>\n",
       "      <td>0.046872</td>\n",
       "      <td>0.048980</td>\n",
       "      <td>-0.052740</td>\n",
       "      <td>-0.007654</td>\n",
       "      <td>-0.031185</td>\n",
       "      <td>-0.025726</td>\n",
       "      <td>-0.035936</td>\n",
       "      <td>-0.011776</td>\n",
       "      <td>0.060453</td>\n",
       "      <td>0.060689</td>\n",
       "      <td>-0.007430</td>\n",
       "      <td>0.046921</td>\n",
       "      <td>0.001355</td>\n",
       "      <td>0.051434</td>\n",
       "      <td>-0.015661</td>\n",
       "      <td>-0.013057</td>\n",
       "      <td>-0.071655</td>\n",
       "      <td>0.070370</td>\n",
       "      <td>0.035660</td>\n",
       "      <td>0.068666</td>\n",
       "      <td>0.006782</td>\n",
       "      <td>-0.065619</td>\n",
       "      <td>0.032055</td>\n",
       "      <td>-0.008144</td>\n",
       "      <td>0.040906</td>\n",
       "      <td>0.071529</td>\n",
       "      <td>-0.066488</td>\n",
       "      <td>-0.067596</td>\n",
       "      <td>0.035801</td>\n",
       "      <td>0.066213</td>\n",
       "      <td>0.018814</td>\n",
       "      <td>-0.061897</td>\n",
       "      <td>0.014703</td>\n",
       "      <td>-0.041915</td>\n",
       "      <td>-0.004560</td>\n",
       "      <td>-0.032829</td>\n",
       "      <td>0.039266</td>\n",
       "      <td>0.054741</td>\n",
       "      <td>0.009650</td>\n",
       "      <td>-0.077373</td>\n",
       "      <td>-0.074589</td>\n",
       "      <td>0.081182</td>\n",
       "      <td>0.043701</td>\n",
       "      <td>0.017032</td>\n",
       "      <td>-0.011870</td>\n",
       "      <td>-0.070479</td>\n",
       "      <td>-0.031854</td>\n",
       "      <td>0.065114</td>\n",
       "      <td>0.000744</td>\n",
       "      <td>0.072337</td>\n",
       "      <td>-0.040985</td>\n",
       "      <td>0.025274</td>\n",
       "      <td>0.071217</td>\n",
       "      <td>-0.013274</td>\n",
       "      <td>0.041275</td>\n",
       "      <td>-0.019355</td>\n",
       "      <td>0.014577</td>\n",
       "      <td>-0.052955</td>\n",
       "      <td>-0.014330</td>\n",
       "      <td>-0.049519</td>\n",
       "      <td>-0.028708</td>\n",
       "      <td>-0.047589</td>\n",
       "      <td>0.020427</td>\n",
       "      <td>0.026056</td>\n",
       "      <td>-0.045594</td>\n",
       "      <td>0.064833</td>\n",
       "      <td>-0.052026</td>\n",
       "      <td>-0.029484</td>\n",
       "      <td>0.025574</td>\n",
       "      <td>0.064684</td>\n",
       "      <td>-0.041059</td>\n",
       "      <td>-0.021374</td>\n",
       "      <td>-0.078985</td>\n",
       "      <td>0.036315</td>\n",
       "      <td>0.043513</td>\n",
       "      <td>-0.002334</td>\n",
       "      <td>0.012682</td>\n",
       "      <td>-0.012753</td>\n",
       "      <td>-0.019673</td>\n",
       "      <td>0.001925</td>\n",
       "      <td>-0.033901</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>vaccinate</th>\n",
       "      <td>0.041393</td>\n",
       "      <td>-0.053996</td>\n",
       "      <td>0.006270</td>\n",
       "      <td>-0.031008</td>\n",
       "      <td>-0.043719</td>\n",
       "      <td>0.038320</td>\n",
       "      <td>0.027678</td>\n",
       "      <td>-0.000121</td>\n",
       "      <td>-0.043901</td>\n",
       "      <td>0.006641</td>\n",
       "      <td>0.082772</td>\n",
       "      <td>0.075204</td>\n",
       "      <td>-0.016138</td>\n",
       "      <td>-0.017668</td>\n",
       "      <td>-0.057705</td>\n",
       "      <td>0.048859</td>\n",
       "      <td>-0.082415</td>\n",
       "      <td>-0.060692</td>\n",
       "      <td>0.001952</td>\n",
       "      <td>-0.070204</td>\n",
       "      <td>-0.026326</td>\n",
       "      <td>0.022189</td>\n",
       "      <td>-0.063208</td>\n",
       "      <td>0.075920</td>\n",
       "      <td>0.053428</td>\n",
       "      <td>0.080072</td>\n",
       "      <td>0.020538</td>\n",
       "      <td>-0.025106</td>\n",
       "      <td>0.018793</td>\n",
       "      <td>-0.015768</td>\n",
       "      <td>-0.035460</td>\n",
       "      <td>0.082776</td>\n",
       "      <td>-0.014979</td>\n",
       "      <td>0.029658</td>\n",
       "      <td>0.074225</td>\n",
       "      <td>0.046354</td>\n",
       "      <td>-0.059199</td>\n",
       "      <td>0.014226</td>\n",
       "      <td>-0.033579</td>\n",
       "      <td>-0.012409</td>\n",
       "      <td>0.013193</td>\n",
       "      <td>0.036875</td>\n",
       "      <td>0.004690</td>\n",
       "      <td>-0.054965</td>\n",
       "      <td>-0.067827</td>\n",
       "      <td>0.031122</td>\n",
       "      <td>0.026535</td>\n",
       "      <td>0.033968</td>\n",
       "      <td>-0.051577</td>\n",
       "      <td>0.060606</td>\n",
       "      <td>-0.059317</td>\n",
       "      <td>-0.011460</td>\n",
       "      <td>0.076966</td>\n",
       "      <td>0.047517</td>\n",
       "      <td>-0.078353</td>\n",
       "      <td>-0.010326</td>\n",
       "      <td>0.010784</td>\n",
       "      <td>-0.056559</td>\n",
       "      <td>0.005060</td>\n",
       "      <td>-0.012383</td>\n",
       "      <td>-0.056605</td>\n",
       "      <td>-0.040747</td>\n",
       "      <td>-0.077148</td>\n",
       "      <td>0.011394</td>\n",
       "      <td>0.021280</td>\n",
       "      <td>0.037187</td>\n",
       "      <td>0.021617</td>\n",
       "      <td>-0.033748</td>\n",
       "      <td>0.009538</td>\n",
       "      <td>0.065655</td>\n",
       "      <td>-0.059822</td>\n",
       "      <td>-0.014842</td>\n",
       "      <td>-0.036477</td>\n",
       "      <td>0.060701</td>\n",
       "      <td>0.059079</td>\n",
       "      <td>0.030162</td>\n",
       "      <td>0.030193</td>\n",
       "      <td>-0.033122</td>\n",
       "      <td>-0.077544</td>\n",
       "      <td>-0.019550</td>\n",
       "      <td>0.068423</td>\n",
       "      <td>-0.030209</td>\n",
       "      <td>0.005250</td>\n",
       "      <td>0.029243</td>\n",
       "      <td>-0.059771</td>\n",
       "      <td>-0.035753</td>\n",
       "      <td>-0.003516</td>\n",
       "      <td>0.026230</td>\n",
       "      <td>-0.029676</td>\n",
       "      <td>-0.067693</td>\n",
       "      <td>-0.045914</td>\n",
       "      <td>0.036132</td>\n",
       "      <td>-0.028665</td>\n",
       "      <td>0.059655</td>\n",
       "      <td>-0.046660</td>\n",
       "      <td>0.029455</td>\n",
       "      <td>0.009680</td>\n",
       "      <td>-0.041362</td>\n",
       "      <td>0.061414</td>\n",
       "      <td>0.043852</td>\n",
       "      <td>0.030574</td>\n",
       "      <td>-0.031862</td>\n",
       "      <td>0.035690</td>\n",
       "      <td>0.034221</td>\n",
       "      <td>0.062470</td>\n",
       "      <td>0.060958</td>\n",
       "      <td>0.059123</td>\n",
       "      <td>-0.007372</td>\n",
       "      <td>0.053796</td>\n",
       "      <td>-0.045344</td>\n",
       "      <td>0.037147</td>\n",
       "      <td>-0.021527</td>\n",
       "      <td>0.000291</td>\n",
       "      <td>0.062329</td>\n",
       "      <td>0.041251</td>\n",
       "      <td>-0.003925</td>\n",
       "      <td>-0.057923</td>\n",
       "      <td>-0.015850</td>\n",
       "      <td>-0.022617</td>\n",
       "      <td>0.010191</td>\n",
       "      <td>0.004627</td>\n",
       "      <td>-0.060337</td>\n",
       "      <td>0.023784</td>\n",
       "      <td>-0.044664</td>\n",
       "      <td>0.045218</td>\n",
       "      <td>0.058547</td>\n",
       "      <td>-0.015792</td>\n",
       "      <td>0.013244</td>\n",
       "      <td>-0.023951</td>\n",
       "      <td>0.017615</td>\n",
       "      <td>0.013677</td>\n",
       "      <td>0.082859</td>\n",
       "      <td>0.069877</td>\n",
       "      <td>0.061769</td>\n",
       "      <td>0.028572</td>\n",
       "      <td>0.009356</td>\n",
       "      <td>0.034798</td>\n",
       "      <td>0.033531</td>\n",
       "      <td>0.031690</td>\n",
       "      <td>0.019452</td>\n",
       "      <td>0.005719</td>\n",
       "      <td>-0.018034</td>\n",
       "      <td>-0.036908</td>\n",
       "      <td>0.024841</td>\n",
       "      <td>0.016815</td>\n",
       "      <td>0.062610</td>\n",
       "      <td>0.001348</td>\n",
       "      <td>-0.067835</td>\n",
       "      <td>0.056583</td>\n",
       "      <td>0.043135</td>\n",
       "      <td>0.047916</td>\n",
       "      <td>0.016387</td>\n",
       "      <td>-0.063827</td>\n",
       "      <td>-0.041877</td>\n",
       "      <td>0.000339</td>\n",
       "      <td>-0.004294</td>\n",
       "      <td>-0.029280</td>\n",
       "      <td>-0.037563</td>\n",
       "      <td>0.024057</td>\n",
       "      <td>0.063139</td>\n",
       "      <td>0.029266</td>\n",
       "      <td>0.041019</td>\n",
       "      <td>0.036688</td>\n",
       "      <td>-0.019191</td>\n",
       "      <td>-0.043240</td>\n",
       "      <td>-0.005434</td>\n",
       "      <td>-0.062924</td>\n",
       "      <td>-0.016879</td>\n",
       "      <td>-0.013983</td>\n",
       "      <td>-0.029432</td>\n",
       "      <td>0.011711</td>\n",
       "      <td>-0.070706</td>\n",
       "      <td>0.030219</td>\n",
       "      <td>0.029914</td>\n",
       "      <td>0.055713</td>\n",
       "      <td>0.035761</td>\n",
       "      <td>0.007882</td>\n",
       "      <td>-0.042309</td>\n",
       "      <td>-0.046125</td>\n",
       "      <td>-0.012273</td>\n",
       "      <td>-0.064084</td>\n",
       "      <td>-0.002575</td>\n",
       "      <td>0.068239</td>\n",
       "      <td>-0.047888</td>\n",
       "      <td>0.052742</td>\n",
       "      <td>-0.057032</td>\n",
       "      <td>0.002457</td>\n",
       "      <td>-0.039006</td>\n",
       "      <td>-0.038642</td>\n",
       "      <td>0.039950</td>\n",
       "      <td>-0.044314</td>\n",
       "      <td>0.017698</td>\n",
       "      <td>0.047405</td>\n",
       "      <td>-0.004988</td>\n",
       "      <td>0.075874</td>\n",
       "      <td>-0.016020</td>\n",
       "      <td>0.050875</td>\n",
       "      <td>-0.048873</td>\n",
       "      <td>-0.002336</td>\n",
       "      <td>0.021357</td>\n",
       "      <td>-0.034928</td>\n",
       "      <td>0.046110</td>\n",
       "      <td>0.039303</td>\n",
       "      <td>0.003042</td>\n",
       "      <td>0.001583</td>\n",
       "      <td>-0.074783</td>\n",
       "      <td>-0.015139</td>\n",
       "      <td>0.081015</td>\n",
       "      <td>-0.063060</td>\n",
       "      <td>0.064020</td>\n",
       "      <td>-0.045401</td>\n",
       "      <td>-0.006835</td>\n",
       "      <td>0.038624</td>\n",
       "      <td>0.066773</td>\n",
       "      <td>-0.060457</td>\n",
       "      <td>-0.038722</td>\n",
       "      <td>0.077861</td>\n",
       "      <td>0.050446</td>\n",
       "      <td>-0.008646</td>\n",
       "      <td>0.060800</td>\n",
       "      <td>-0.039997</td>\n",
       "      <td>-0.034271</td>\n",
       "      <td>0.070857</td>\n",
       "      <td>0.013882</td>\n",
       "      <td>0.027279</td>\n",
       "      <td>-0.055249</td>\n",
       "      <td>0.031210</td>\n",
       "      <td>0.011553</td>\n",
       "      <td>-0.046640</td>\n",
       "      <td>0.035244</td>\n",
       "      <td>0.017364</td>\n",
       "      <td>-0.010522</td>\n",
       "      <td>0.078577</td>\n",
       "      <td>0.082108</td>\n",
       "      <td>0.028731</td>\n",
       "      <td>-0.028056</td>\n",
       "      <td>-0.053339</td>\n",
       "      <td>0.018247</td>\n",
       "      <td>-0.010892</td>\n",
       "      <td>-0.029003</td>\n",
       "      <td>0.023822</td>\n",
       "      <td>-0.040213</td>\n",
       "      <td>0.017374</td>\n",
       "      <td>-0.066965</td>\n",
       "      <td>-0.043656</td>\n",
       "      <td>-0.014981</td>\n",
       "      <td>0.000252</td>\n",
       "      <td>0.021323</td>\n",
       "      <td>-0.017392</td>\n",
       "      <td>-0.055453</td>\n",
       "      <td>0.072211</td>\n",
       "      <td>-0.075839</td>\n",
       "      <td>0.004998</td>\n",
       "      <td>0.039937</td>\n",
       "      <td>-0.024987</td>\n",
       "      <td>0.005549</td>\n",
       "      <td>0.002359</td>\n",
       "      <td>0.004212</td>\n",
       "      <td>0.047502</td>\n",
       "      <td>-0.048678</td>\n",
       "      <td>-0.021398</td>\n",
       "      <td>-0.008389</td>\n",
       "      <td>-0.054579</td>\n",
       "      <td>0.042405</td>\n",
       "      <td>0.047104</td>\n",
       "      <td>-0.012104</td>\n",
       "      <td>0.042331</td>\n",
       "      <td>0.039275</td>\n",
       "      <td>0.070677</td>\n",
       "      <td>0.049368</td>\n",
       "      <td>-0.035363</td>\n",
       "      <td>-0.072097</td>\n",
       "      <td>-0.012421</td>\n",
       "      <td>-0.078613</td>\n",
       "      <td>-0.066277</td>\n",
       "      <td>-0.034878</td>\n",
       "      <td>0.048891</td>\n",
       "      <td>-0.074424</td>\n",
       "      <td>0.072630</td>\n",
       "      <td>0.077667</td>\n",
       "      <td>0.015895</td>\n",
       "      <td>0.020440</td>\n",
       "      <td>-0.036295</td>\n",
       "      <td>0.026898</td>\n",
       "      <td>-0.028113</td>\n",
       "      <td>-0.029463</td>\n",
       "      <td>-0.037961</td>\n",
       "      <td>0.050384</td>\n",
       "      <td>-0.070224</td>\n",
       "      <td>-0.007928</td>\n",
       "      <td>0.056410</td>\n",
       "      <td>-0.037946</td>\n",
       "      <td>0.078800</td>\n",
       "      <td>-0.012110</td>\n",
       "      <td>0.054140</td>\n",
       "      <td>0.068925</td>\n",
       "      <td>0.048381</td>\n",
       "      <td>0.051237</td>\n",
       "      <td>-0.070851</td>\n",
       "      <td>0.052826</td>\n",
       "      <td>-0.024953</td>\n",
       "      <td>-0.065382</td>\n",
       "      <td>-0.019471</td>\n",
       "      <td>-0.043215</td>\n",
       "      <td>0.054353</td>\n",
       "      <td>0.033137</td>\n",
       "      <td>-0.057318</td>\n",
       "      <td>0.055100</td>\n",
       "      <td>-0.015772</td>\n",
       "      <td>0.030700</td>\n",
       "      <td>0.066227</td>\n",
       "      <td>0.045186</td>\n",
       "      <td>-0.055524</td>\n",
       "      <td>0.013598</td>\n",
       "      <td>0.073974</td>\n",
       "      <td>0.012642</td>\n",
       "      <td>-0.026671</td>\n",
       "      <td>0.074775</td>\n",
       "      <td>0.048365</td>\n",
       "      <td>-0.060164</td>\n",
       "      <td>0.000294</td>\n",
       "      <td>0.028768</td>\n",
       "      <td>0.045488</td>\n",
       "      <td>-0.026078</td>\n",
       "      <td>-0.035854</td>\n",
       "      <td>0.073687</td>\n",
       "      <td>0.054658</td>\n",
       "      <td>0.070989</td>\n",
       "      <td>0.031952</td>\n",
       "      <td>-0.009804</td>\n",
       "      <td>0.063213</td>\n",
       "      <td>0.001351</td>\n",
       "      <td>-0.015314</td>\n",
       "      <td>0.076752</td>\n",
       "      <td>0.009618</td>\n",
       "      <td>0.015477</td>\n",
       "      <td>0.003546</td>\n",
       "      <td>0.058678</td>\n",
       "      <td>-0.013885</td>\n",
       "      <td>0.017009</td>\n",
       "      <td>0.035707</td>\n",
       "      <td>0.029293</td>\n",
       "      <td>0.080068</td>\n",
       "      <td>0.006524</td>\n",
       "      <td>-0.026286</td>\n",
       "      <td>-0.081493</td>\n",
       "      <td>0.039460</td>\n",
       "      <td>0.053000</td>\n",
       "      <td>-0.055508</td>\n",
       "      <td>0.051934</td>\n",
       "      <td>0.026078</td>\n",
       "      <td>0.047467</td>\n",
       "      <td>0.039581</td>\n",
       "      <td>0.065588</td>\n",
       "      <td>0.073398</td>\n",
       "      <td>0.009060</td>\n",
       "      <td>-0.070075</td>\n",
       "      <td>-0.037799</td>\n",
       "      <td>-0.011843</td>\n",
       "      <td>-0.068857</td>\n",
       "      <td>0.063748</td>\n",
       "      <td>-0.009012</td>\n",
       "      <td>0.039876</td>\n",
       "      <td>0.030824</td>\n",
       "      <td>-0.042201</td>\n",
       "      <td>-0.077752</td>\n",
       "      <td>0.021205</td>\n",
       "      <td>0.000496</td>\n",
       "      <td>-0.026340</td>\n",
       "      <td>0.072197</td>\n",
       "      <td>-0.078397</td>\n",
       "      <td>0.031649</td>\n",
       "      <td>-0.028109</td>\n",
       "      <td>0.013554</td>\n",
       "      <td>0.030434</td>\n",
       "      <td>-0.039760</td>\n",
       "      <td>0.049478</td>\n",
       "      <td>-0.017531</td>\n",
       "      <td>0.049398</td>\n",
       "      <td>0.006439</td>\n",
       "      <td>0.039892</td>\n",
       "      <td>-0.042380</td>\n",
       "      <td>-0.072161</td>\n",
       "      <td>0.063856</td>\n",
       "      <td>0.052389</td>\n",
       "      <td>-0.058272</td>\n",
       "      <td>-0.006660</td>\n",
       "      <td>-0.072475</td>\n",
       "      <td>0.014216</td>\n",
       "      <td>0.075971</td>\n",
       "      <td>-0.043970</td>\n",
       "      <td>-0.069018</td>\n",
       "      <td>-0.012582</td>\n",
       "      <td>-0.066808</td>\n",
       "      <td>-0.060694</td>\n",
       "      <td>0.026416</td>\n",
       "      <td>-0.041792</td>\n",
       "      <td>-0.033061</td>\n",
       "      <td>-0.062973</td>\n",
       "      <td>0.010151</td>\n",
       "      <td>-0.056118</td>\n",
       "      <td>-0.050802</td>\n",
       "      <td>-0.009570</td>\n",
       "      <td>0.005110</td>\n",
       "      <td>-0.001752</td>\n",
       "      <td>0.051285</td>\n",
       "      <td>-0.017072</td>\n",
       "      <td>0.019568</td>\n",
       "      <td>-0.045850</td>\n",
       "      <td>-0.038270</td>\n",
       "      <td>0.024662</td>\n",
       "      <td>0.004760</td>\n",
       "      <td>-0.047173</td>\n",
       "      <td>0.030418</td>\n",
       "      <td>-0.005538</td>\n",
       "      <td>0.044404</td>\n",
       "      <td>-0.014341</td>\n",
       "      <td>0.050481</td>\n",
       "      <td>0.013253</td>\n",
       "      <td>-0.052401</td>\n",
       "      <td>-0.053548</td>\n",
       "      <td>0.025247</td>\n",
       "      <td>0.052459</td>\n",
       "      <td>0.005304</td>\n",
       "      <td>-0.032342</td>\n",
       "      <td>-0.057506</td>\n",
       "      <td>-0.002381</td>\n",
       "      <td>0.037012</td>\n",
       "      <td>-0.023396</td>\n",
       "      <td>-0.025962</td>\n",
       "      <td>-0.008575</td>\n",
       "      <td>-0.043294</td>\n",
       "      <td>0.044235</td>\n",
       "      <td>-0.014760</td>\n",
       "      <td>0.003989</td>\n",
       "      <td>-0.029687</td>\n",
       "      <td>0.031682</td>\n",
       "      <td>-0.001010</td>\n",
       "      <td>-0.067138</td>\n",
       "      <td>-0.016754</td>\n",
       "      <td>0.070909</td>\n",
       "      <td>-0.013612</td>\n",
       "      <td>-0.003098</td>\n",
       "      <td>0.029299</td>\n",
       "      <td>-0.025628</td>\n",
       "      <td>-0.061713</td>\n",
       "      <td>0.005865</td>\n",
       "      <td>0.067967</td>\n",
       "      <td>0.057875</td>\n",
       "      <td>0.008246</td>\n",
       "      <td>0.035280</td>\n",
       "      <td>-0.063615</td>\n",
       "      <td>-0.056653</td>\n",
       "      <td>-0.032101</td>\n",
       "      <td>0.045081</td>\n",
       "      <td>0.046592</td>\n",
       "      <td>0.022508</td>\n",
       "      <td>-0.032629</td>\n",
       "      <td>-0.043877</td>\n",
       "      <td>-0.009259</td>\n",
       "      <td>-0.077351</td>\n",
       "      <td>0.026070</td>\n",
       "      <td>0.046329</td>\n",
       "      <td>0.001020</td>\n",
       "      <td>-0.031854</td>\n",
       "      <td>-0.040619</td>\n",
       "      <td>0.024622</td>\n",
       "      <td>-0.017670</td>\n",
       "      <td>-0.032785</td>\n",
       "      <td>0.027073</td>\n",
       "      <td>-0.072396</td>\n",
       "      <td>0.012766</td>\n",
       "      <td>0.015221</td>\n",
       "      <td>0.009589</td>\n",
       "      <td>-0.029872</td>\n",
       "      <td>-0.025265</td>\n",
       "      <td>-0.057105</td>\n",
       "      <td>0.081700</td>\n",
       "      <td>-0.047924</td>\n",
       "      <td>0.063655</td>\n",
       "      <td>0.043113</td>\n",
       "      <td>-0.068742</td>\n",
       "      <td>0.038860</td>\n",
       "      <td>0.041552</td>\n",
       "      <td>0.017204</td>\n",
       "      <td>-0.058776</td>\n",
       "      <td>-0.047284</td>\n",
       "      <td>0.006175</td>\n",
       "      <td>0.065167</td>\n",
       "      <td>0.023445</td>\n",
       "      <td>-0.046691</td>\n",
       "      <td>0.036170</td>\n",
       "      <td>0.033127</td>\n",
       "      <td>-0.058263</td>\n",
       "      <td>0.040276</td>\n",
       "      <td>-0.032758</td>\n",
       "      <td>0.026630</td>\n",
       "      <td>-0.049931</td>\n",
       "      <td>-0.032048</td>\n",
       "      <td>-0.063238</td>\n",
       "      <td>0.056260</td>\n",
       "      <td>-0.064717</td>\n",
       "      <td>-0.005196</td>\n",
       "      <td>-0.079546</td>\n",
       "      <td>0.043179</td>\n",
       "      <td>-0.082887</td>\n",
       "      <td>-0.001214</td>\n",
       "      <td>0.029362</td>\n",
       "      <td>0.045764</td>\n",
       "      <td>-0.042659</td>\n",
       "      <td>-0.034748</td>\n",
       "      <td>-0.059509</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>boastful</th>\n",
       "      <td>0.006982</td>\n",
       "      <td>-0.036990</td>\n",
       "      <td>0.001479</td>\n",
       "      <td>0.010849</td>\n",
       "      <td>-0.012228</td>\n",
       "      <td>0.074476</td>\n",
       "      <td>-0.012781</td>\n",
       "      <td>-0.065526</td>\n",
       "      <td>0.006991</td>\n",
       "      <td>0.054437</td>\n",
       "      <td>-0.008508</td>\n",
       "      <td>0.014751</td>\n",
       "      <td>0.010516</td>\n",
       "      <td>-0.001452</td>\n",
       "      <td>-0.050290</td>\n",
       "      <td>-0.088938</td>\n",
       "      <td>0.065113</td>\n",
       "      <td>-0.019750</td>\n",
       "      <td>0.016942</td>\n",
       "      <td>-0.034655</td>\n",
       "      <td>0.025398</td>\n",
       "      <td>0.068590</td>\n",
       "      <td>0.052124</td>\n",
       "      <td>-0.072730</td>\n",
       "      <td>-0.049247</td>\n",
       "      <td>-0.018123</td>\n",
       "      <td>0.027242</td>\n",
       "      <td>0.007443</td>\n",
       "      <td>0.068252</td>\n",
       "      <td>0.021166</td>\n",
       "      <td>0.033197</td>\n",
       "      <td>-0.013984</td>\n",
       "      <td>-0.037331</td>\n",
       "      <td>-0.026236</td>\n",
       "      <td>0.015629</td>\n",
       "      <td>0.088569</td>\n",
       "      <td>0.032182</td>\n",
       "      <td>0.009855</td>\n",
       "      <td>-0.005393</td>\n",
       "      <td>-0.046501</td>\n",
       "      <td>-0.036995</td>\n",
       "      <td>-0.054499</td>\n",
       "      <td>0.031506</td>\n",
       "      <td>-0.032422</td>\n",
       "      <td>-0.038621</td>\n",
       "      <td>0.033319</td>\n",
       "      <td>-0.048318</td>\n",
       "      <td>-0.012861</td>\n",
       "      <td>-0.046348</td>\n",
       "      <td>0.029922</td>\n",
       "      <td>0.021805</td>\n",
       "      <td>-0.022052</td>\n",
       "      <td>0.092005</td>\n",
       "      <td>0.070355</td>\n",
       "      <td>-0.036538</td>\n",
       "      <td>-0.035853</td>\n",
       "      <td>-0.046953</td>\n",
       "      <td>-0.059532</td>\n",
       "      <td>-0.025107</td>\n",
       "      <td>0.005947</td>\n",
       "      <td>-0.057831</td>\n",
       "      <td>0.042697</td>\n",
       "      <td>-0.059668</td>\n",
       "      <td>0.050734</td>\n",
       "      <td>-0.046341</td>\n",
       "      <td>0.050468</td>\n",
       "      <td>0.007135</td>\n",
       "      <td>-0.040002</td>\n",
       "      <td>0.033393</td>\n",
       "      <td>0.030829</td>\n",
       "      <td>0.002263</td>\n",
       "      <td>0.013901</td>\n",
       "      <td>-0.044483</td>\n",
       "      <td>-0.049996</td>\n",
       "      <td>0.086839</td>\n",
       "      <td>0.049652</td>\n",
       "      <td>-0.064902</td>\n",
       "      <td>0.010027</td>\n",
       "      <td>0.045391</td>\n",
       "      <td>0.025019</td>\n",
       "      <td>0.022550</td>\n",
       "      <td>0.074197</td>\n",
       "      <td>-0.066880</td>\n",
       "      <td>-0.046477</td>\n",
       "      <td>-0.015914</td>\n",
       "      <td>-0.017991</td>\n",
       "      <td>-0.039582</td>\n",
       "      <td>0.000153</td>\n",
       "      <td>0.047411</td>\n",
       "      <td>-0.039077</td>\n",
       "      <td>0.026213</td>\n",
       "      <td>0.004772</td>\n",
       "      <td>0.006327</td>\n",
       "      <td>0.078806</td>\n",
       "      <td>-0.038787</td>\n",
       "      <td>0.029122</td>\n",
       "      <td>-0.070279</td>\n",
       "      <td>-0.042052</td>\n",
       "      <td>0.030344</td>\n",
       "      <td>0.020169</td>\n",
       "      <td>-0.045350</td>\n",
       "      <td>-0.052541</td>\n",
       "      <td>-0.014947</td>\n",
       "      <td>-0.057364</td>\n",
       "      <td>0.061809</td>\n",
       "      <td>0.027518</td>\n",
       "      <td>-0.043141</td>\n",
       "      <td>-0.015691</td>\n",
       "      <td>0.049883</td>\n",
       "      <td>0.050189</td>\n",
       "      <td>0.018904</td>\n",
       "      <td>-0.032533</td>\n",
       "      <td>-0.035071</td>\n",
       "      <td>-0.032948</td>\n",
       "      <td>-0.033646</td>\n",
       "      <td>-0.013204</td>\n",
       "      <td>-0.012885</td>\n",
       "      <td>0.035386</td>\n",
       "      <td>-0.026496</td>\n",
       "      <td>0.014493</td>\n",
       "      <td>-0.052952</td>\n",
       "      <td>0.062394</td>\n",
       "      <td>-0.060120</td>\n",
       "      <td>0.007572</td>\n",
       "      <td>-0.054795</td>\n",
       "      <td>0.032192</td>\n",
       "      <td>0.020441</td>\n",
       "      <td>-0.069634</td>\n",
       "      <td>-0.007907</td>\n",
       "      <td>-0.022994</td>\n",
       "      <td>-0.025549</td>\n",
       "      <td>-0.035994</td>\n",
       "      <td>0.110628</td>\n",
       "      <td>0.108779</td>\n",
       "      <td>0.023031</td>\n",
       "      <td>-0.029957</td>\n",
       "      <td>0.054660</td>\n",
       "      <td>0.043424</td>\n",
       "      <td>0.010320</td>\n",
       "      <td>-0.065601</td>\n",
       "      <td>-0.087687</td>\n",
       "      <td>-0.076916</td>\n",
       "      <td>0.033915</td>\n",
       "      <td>-0.063269</td>\n",
       "      <td>0.039957</td>\n",
       "      <td>-0.057189</td>\n",
       "      <td>-0.060155</td>\n",
       "      <td>0.023702</td>\n",
       "      <td>-0.038123</td>\n",
       "      <td>-0.064914</td>\n",
       "      <td>0.054965</td>\n",
       "      <td>0.029572</td>\n",
       "      <td>-0.062713</td>\n",
       "      <td>-0.038532</td>\n",
       "      <td>-0.062120</td>\n",
       "      <td>0.030562</td>\n",
       "      <td>-0.056484</td>\n",
       "      <td>-0.005805</td>\n",
       "      <td>0.028380</td>\n",
       "      <td>-0.020050</td>\n",
       "      <td>0.023675</td>\n",
       "      <td>-0.062436</td>\n",
       "      <td>-0.014559</td>\n",
       "      <td>-0.025000</td>\n",
       "      <td>0.023563</td>\n",
       "      <td>-0.012469</td>\n",
       "      <td>-0.016942</td>\n",
       "      <td>-0.040821</td>\n",
       "      <td>0.008688</td>\n",
       "      <td>-0.073647</td>\n",
       "      <td>-0.064165</td>\n",
       "      <td>0.011827</td>\n",
       "      <td>0.004716</td>\n",
       "      <td>-0.018712</td>\n",
       "      <td>0.040118</td>\n",
       "      <td>0.048953</td>\n",
       "      <td>0.009917</td>\n",
       "      <td>0.030175</td>\n",
       "      <td>0.034624</td>\n",
       "      <td>-0.039287</td>\n",
       "      <td>-0.074250</td>\n",
       "      <td>0.031717</td>\n",
       "      <td>0.047116</td>\n",
       "      <td>-0.015456</td>\n",
       "      <td>0.022567</td>\n",
       "      <td>-0.027981</td>\n",
       "      <td>0.011419</td>\n",
       "      <td>-0.052313</td>\n",
       "      <td>0.029638</td>\n",
       "      <td>-0.024809</td>\n",
       "      <td>0.044479</td>\n",
       "      <td>0.000930</td>\n",
       "      <td>0.078747</td>\n",
       "      <td>-0.020694</td>\n",
       "      <td>0.002874</td>\n",
       "      <td>-0.062905</td>\n",
       "      <td>0.072150</td>\n",
       "      <td>-0.004347</td>\n",
       "      <td>0.005578</td>\n",
       "      <td>0.078008</td>\n",
       "      <td>-0.101961</td>\n",
       "      <td>0.047172</td>\n",
       "      <td>0.016991</td>\n",
       "      <td>0.027549</td>\n",
       "      <td>0.052327</td>\n",
       "      <td>-0.074413</td>\n",
       "      <td>0.031618</td>\n",
       "      <td>-0.062335</td>\n",
       "      <td>0.031865</td>\n",
       "      <td>0.036533</td>\n",
       "      <td>0.003486</td>\n",
       "      <td>-0.013006</td>\n",
       "      <td>0.009951</td>\n",
       "      <td>-0.013115</td>\n",
       "      <td>0.068941</td>\n",
       "      <td>-0.059499</td>\n",
       "      <td>-0.052338</td>\n",
       "      <td>0.037283</td>\n",
       "      <td>-0.040879</td>\n",
       "      <td>0.044461</td>\n",
       "      <td>0.019734</td>\n",
       "      <td>-0.078896</td>\n",
       "      <td>-0.052077</td>\n",
       "      <td>0.024156</td>\n",
       "      <td>-0.014355</td>\n",
       "      <td>0.032629</td>\n",
       "      <td>-0.006387</td>\n",
       "      <td>-0.042155</td>\n",
       "      <td>-0.061048</td>\n",
       "      <td>-0.057423</td>\n",
       "      <td>-0.016177</td>\n",
       "      <td>-0.095286</td>\n",
       "      <td>0.048530</td>\n",
       "      <td>0.117524</td>\n",
       "      <td>0.025619</td>\n",
       "      <td>0.000304</td>\n",
       "      <td>0.028994</td>\n",
       "      <td>-0.012078</td>\n",
       "      <td>-0.034992</td>\n",
       "      <td>-0.040559</td>\n",
       "      <td>0.008688</td>\n",
       "      <td>0.036617</td>\n",
       "      <td>0.057016</td>\n",
       "      <td>-0.061172</td>\n",
       "      <td>0.023232</td>\n",
       "      <td>-0.034713</td>\n",
       "      <td>0.038133</td>\n",
       "      <td>0.002603</td>\n",
       "      <td>-0.021562</td>\n",
       "      <td>-0.033302</td>\n",
       "      <td>0.064885</td>\n",
       "      <td>-0.002064</td>\n",
       "      <td>0.035436</td>\n",
       "      <td>-0.057827</td>\n",
       "      <td>0.035041</td>\n",
       "      <td>-0.071123</td>\n",
       "      <td>0.052781</td>\n",
       "      <td>-0.002846</td>\n",
       "      <td>-0.007422</td>\n",
       "      <td>0.008339</td>\n",
       "      <td>-0.048439</td>\n",
       "      <td>0.015472</td>\n",
       "      <td>0.026307</td>\n",
       "      <td>-0.039965</td>\n",
       "      <td>-0.024364</td>\n",
       "      <td>0.031877</td>\n",
       "      <td>0.021205</td>\n",
       "      <td>-0.052837</td>\n",
       "      <td>0.001476</td>\n",
       "      <td>-0.014058</td>\n",
       "      <td>-0.006741</td>\n",
       "      <td>-0.087335</td>\n",
       "      <td>-0.004949</td>\n",
       "      <td>-0.002404</td>\n",
       "      <td>-0.005332</td>\n",
       "      <td>0.008617</td>\n",
       "      <td>-0.038839</td>\n",
       "      <td>-0.044578</td>\n",
       "      <td>0.043553</td>\n",
       "      <td>0.015349</td>\n",
       "      <td>0.027808</td>\n",
       "      <td>0.020776</td>\n",
       "      <td>0.060347</td>\n",
       "      <td>0.000124</td>\n",
       "      <td>-0.028743</td>\n",
       "      <td>0.075873</td>\n",
       "      <td>0.048450</td>\n",
       "      <td>0.048778</td>\n",
       "      <td>-0.044934</td>\n",
       "      <td>0.050242</td>\n",
       "      <td>-0.055424</td>\n",
       "      <td>0.040995</td>\n",
       "      <td>0.018923</td>\n",
       "      <td>-0.084035</td>\n",
       "      <td>-0.075381</td>\n",
       "      <td>0.002633</td>\n",
       "      <td>-0.014448</td>\n",
       "      <td>0.062219</td>\n",
       "      <td>0.003956</td>\n",
       "      <td>-0.043502</td>\n",
       "      <td>0.082317</td>\n",
       "      <td>0.011435</td>\n",
       "      <td>-0.043307</td>\n",
       "      <td>-0.024926</td>\n",
       "      <td>0.022903</td>\n",
       "      <td>0.042373</td>\n",
       "      <td>-0.042264</td>\n",
       "      <td>0.034753</td>\n",
       "      <td>0.040458</td>\n",
       "      <td>-0.083294</td>\n",
       "      <td>-0.028300</td>\n",
       "      <td>0.018975</td>\n",
       "      <td>0.017199</td>\n",
       "      <td>0.088574</td>\n",
       "      <td>-0.061183</td>\n",
       "      <td>-0.021605</td>\n",
       "      <td>0.023511</td>\n",
       "      <td>0.058749</td>\n",
       "      <td>-0.079480</td>\n",
       "      <td>-0.002113</td>\n",
       "      <td>-0.035712</td>\n",
       "      <td>0.018524</td>\n",
       "      <td>-0.014070</td>\n",
       "      <td>0.002299</td>\n",
       "      <td>-0.024528</td>\n",
       "      <td>-0.026118</td>\n",
       "      <td>0.002997</td>\n",
       "      <td>0.001698</td>\n",
       "      <td>-0.043691</td>\n",
       "      <td>0.014546</td>\n",
       "      <td>-0.011737</td>\n",
       "      <td>0.023094</td>\n",
       "      <td>0.076833</td>\n",
       "      <td>0.011625</td>\n",
       "      <td>-0.051375</td>\n",
       "      <td>0.030305</td>\n",
       "      <td>-0.021082</td>\n",
       "      <td>-0.000816</td>\n",
       "      <td>0.006732</td>\n",
       "      <td>-0.011991</td>\n",
       "      <td>0.042745</td>\n",
       "      <td>-0.057660</td>\n",
       "      <td>0.027031</td>\n",
       "      <td>-0.027659</td>\n",
       "      <td>-0.043771</td>\n",
       "      <td>0.012445</td>\n",
       "      <td>-0.028161</td>\n",
       "      <td>0.006314</td>\n",
       "      <td>-0.017260</td>\n",
       "      <td>-0.006904</td>\n",
       "      <td>0.020979</td>\n",
       "      <td>0.032319</td>\n",
       "      <td>-0.002480</td>\n",
       "      <td>0.045031</td>\n",
       "      <td>-0.036720</td>\n",
       "      <td>-0.053197</td>\n",
       "      <td>-0.097158</td>\n",
       "      <td>-0.034898</td>\n",
       "      <td>-0.025371</td>\n",
       "      <td>-0.019374</td>\n",
       "      <td>0.012664</td>\n",
       "      <td>-0.027051</td>\n",
       "      <td>0.067239</td>\n",
       "      <td>-0.012419</td>\n",
       "      <td>0.037818</td>\n",
       "      <td>0.028464</td>\n",
       "      <td>0.100829</td>\n",
       "      <td>-0.051197</td>\n",
       "      <td>0.065681</td>\n",
       "      <td>0.033457</td>\n",
       "      <td>-0.002663</td>\n",
       "      <td>-0.049526</td>\n",
       "      <td>-0.014274</td>\n",
       "      <td>0.071777</td>\n",
       "      <td>-0.029715</td>\n",
       "      <td>-0.067806</td>\n",
       "      <td>0.038436</td>\n",
       "      <td>-0.069939</td>\n",
       "      <td>-0.099191</td>\n",
       "      <td>-0.039537</td>\n",
       "      <td>0.001066</td>\n",
       "      <td>0.073831</td>\n",
       "      <td>-0.040274</td>\n",
       "      <td>-0.051056</td>\n",
       "      <td>-0.020469</td>\n",
       "      <td>-0.086644</td>\n",
       "      <td>0.019278</td>\n",
       "      <td>-0.000787</td>\n",
       "      <td>-0.048879</td>\n",
       "      <td>0.055948</td>\n",
       "      <td>0.042339</td>\n",
       "      <td>0.024407</td>\n",
       "      <td>-0.026598</td>\n",
       "      <td>0.044540</td>\n",
       "      <td>0.028799</td>\n",
       "      <td>-0.023075</td>\n",
       "      <td>-0.067413</td>\n",
       "      <td>-0.014196</td>\n",
       "      <td>-0.082709</td>\n",
       "      <td>0.004873</td>\n",
       "      <td>0.005131</td>\n",
       "      <td>-0.032619</td>\n",
       "      <td>-0.007272</td>\n",
       "      <td>0.038210</td>\n",
       "      <td>0.049647</td>\n",
       "      <td>0.030512</td>\n",
       "      <td>-0.028824</td>\n",
       "      <td>-0.008388</td>\n",
       "      <td>-0.032613</td>\n",
       "      <td>-0.083119</td>\n",
       "      <td>-0.073640</td>\n",
       "      <td>-0.045124</td>\n",
       "      <td>0.066278</td>\n",
       "      <td>0.034532</td>\n",
       "      <td>-0.057090</td>\n",
       "      <td>0.045727</td>\n",
       "      <td>0.004895</td>\n",
       "      <td>-0.027830</td>\n",
       "      <td>0.034169</td>\n",
       "      <td>-0.026087</td>\n",
       "      <td>-0.019225</td>\n",
       "      <td>-0.041222</td>\n",
       "      <td>0.008392</td>\n",
       "      <td>-0.032168</td>\n",
       "      <td>0.096632</td>\n",
       "      <td>-0.031408</td>\n",
       "      <td>0.059761</td>\n",
       "      <td>0.092302</td>\n",
       "      <td>0.002783</td>\n",
       "      <td>0.068226</td>\n",
       "      <td>0.040781</td>\n",
       "      <td>0.037944</td>\n",
       "      <td>-0.040092</td>\n",
       "      <td>-0.072099</td>\n",
       "      <td>0.075160</td>\n",
       "      <td>-0.019148</td>\n",
       "      <td>-0.024740</td>\n",
       "      <td>-0.012033</td>\n",
       "      <td>-0.046746</td>\n",
       "      <td>0.089455</td>\n",
       "      <td>-0.013998</td>\n",
       "      <td>-0.022423</td>\n",
       "      <td>-0.056901</td>\n",
       "      <td>0.007466</td>\n",
       "      <td>-0.007857</td>\n",
       "      <td>-0.027699</td>\n",
       "      <td>0.014038</td>\n",
       "      <td>-0.054487</td>\n",
       "      <td>0.019062</td>\n",
       "      <td>0.017756</td>\n",
       "      <td>0.028948</td>\n",
       "      <td>0.030280</td>\n",
       "      <td>0.004067</td>\n",
       "      <td>-0.001283</td>\n",
       "      <td>0.076337</td>\n",
       "      <td>-0.011588</td>\n",
       "      <td>0.012464</td>\n",
       "      <td>0.032531</td>\n",
       "      <td>-0.042552</td>\n",
       "      <td>0.049211</td>\n",
       "      <td>0.033130</td>\n",
       "      <td>-0.037410</td>\n",
       "      <td>-0.006063</td>\n",
       "      <td>0.036539</td>\n",
       "      <td>-0.059788</td>\n",
       "      <td>-0.092124</td>\n",
       "      <td>0.017384</td>\n",
       "      <td>-0.031840</td>\n",
       "      <td>-0.036138</td>\n",
       "      <td>0.069098</td>\n",
       "      <td>-0.058350</td>\n",
       "      <td>0.045435</td>\n",
       "      <td>-0.025932</td>\n",
       "      <td>-0.019635</td>\n",
       "      <td>0.053013</td>\n",
       "      <td>-0.036178</td>\n",
       "      <td>-0.020680</td>\n",
       "      <td>0.024972</td>\n",
       "      <td>0.099408</td>\n",
       "      <td>0.062121</td>\n",
       "      <td>0.021462</td>\n",
       "      <td>-0.076078</td>\n",
       "      <td>0.065728</td>\n",
       "      <td>-0.016879</td>\n",
       "      <td>0.006647</td>\n",
       "      <td>-0.023279</td>\n",
       "      <td>-0.023405</td>\n",
       "      <td>0.042554</td>\n",
       "      <td>0.074027</td>\n",
       "      <td>0.008236</td>\n",
       "      <td>-0.051422</td>\n",
       "      <td>0.010519</td>\n",
       "      <td>-0.025413</td>\n",
       "      <td>0.021815</td>\n",
       "      <td>0.043855</td>\n",
       "      <td>0.044764</td>\n",
       "      <td>-0.033540</td>\n",
       "      <td>-0.063108</td>\n",
       "      <td>-0.000659</td>\n",
       "      <td>0.024258</td>\n",
       "      <td>-0.019741</td>\n",
       "      <td>-0.028066</td>\n",
       "      <td>-0.007588</td>\n",
       "      <td>-0.116748</td>\n",
       "      <td>0.082915</td>\n",
       "      <td>-0.026700</td>\n",
       "      <td>-0.042339</td>\n",
       "      <td>-0.027726</td>\n",
       "      <td>-0.024228</td>\n",
       "      <td>0.026245</td>\n",
       "      <td>0.035487</td>\n",
       "      <td>-0.073523</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>phrase</th>\n",
       "      <td>-0.003138</td>\n",
       "      <td>0.030418</td>\n",
       "      <td>-0.006425</td>\n",
       "      <td>0.014352</td>\n",
       "      <td>-0.070801</td>\n",
       "      <td>0.048150</td>\n",
       "      <td>-0.008635</td>\n",
       "      <td>-0.063470</td>\n",
       "      <td>0.047201</td>\n",
       "      <td>0.021414</td>\n",
       "      <td>-0.000467</td>\n",
       "      <td>0.028158</td>\n",
       "      <td>-0.057286</td>\n",
       "      <td>0.098894</td>\n",
       "      <td>-0.048890</td>\n",
       "      <td>-0.098928</td>\n",
       "      <td>0.001677</td>\n",
       "      <td>0.051327</td>\n",
       "      <td>0.000741</td>\n",
       "      <td>0.017630</td>\n",
       "      <td>-0.037922</td>\n",
       "      <td>0.041874</td>\n",
       "      <td>-0.026975</td>\n",
       "      <td>0.007114</td>\n",
       "      <td>-0.065540</td>\n",
       "      <td>0.081049</td>\n",
       "      <td>0.035803</td>\n",
       "      <td>-0.071655</td>\n",
       "      <td>-0.045488</td>\n",
       "      <td>0.033470</td>\n",
       "      <td>0.048512</td>\n",
       "      <td>0.041930</td>\n",
       "      <td>-0.057332</td>\n",
       "      <td>0.017142</td>\n",
       "      <td>-0.077912</td>\n",
       "      <td>0.063516</td>\n",
       "      <td>0.037124</td>\n",
       "      <td>0.042705</td>\n",
       "      <td>0.006179</td>\n",
       "      <td>0.071375</td>\n",
       "      <td>-0.031287</td>\n",
       "      <td>0.026137</td>\n",
       "      <td>0.003060</td>\n",
       "      <td>-0.018246</td>\n",
       "      <td>-0.059071</td>\n",
       "      <td>-0.052765</td>\n",
       "      <td>-0.007959</td>\n",
       "      <td>0.046127</td>\n",
       "      <td>-0.004568</td>\n",
       "      <td>-0.037286</td>\n",
       "      <td>0.058644</td>\n",
       "      <td>0.037510</td>\n",
       "      <td>0.065247</td>\n",
       "      <td>0.086072</td>\n",
       "      <td>-0.003905</td>\n",
       "      <td>-0.006074</td>\n",
       "      <td>-0.035908</td>\n",
       "      <td>-0.020085</td>\n",
       "      <td>0.088365</td>\n",
       "      <td>0.013332</td>\n",
       "      <td>0.035841</td>\n",
       "      <td>-0.013260</td>\n",
       "      <td>0.018352</td>\n",
       "      <td>0.018212</td>\n",
       "      <td>-0.043356</td>\n",
       "      <td>-0.017639</td>\n",
       "      <td>0.051288</td>\n",
       "      <td>-0.022227</td>\n",
       "      <td>0.035003</td>\n",
       "      <td>0.015287</td>\n",
       "      <td>-0.020594</td>\n",
       "      <td>-0.022565</td>\n",
       "      <td>0.012481</td>\n",
       "      <td>-0.015975</td>\n",
       "      <td>0.008228</td>\n",
       "      <td>0.003335</td>\n",
       "      <td>-0.021644</td>\n",
       "      <td>-0.017359</td>\n",
       "      <td>-0.040477</td>\n",
       "      <td>-0.081152</td>\n",
       "      <td>0.013028</td>\n",
       "      <td>0.025296</td>\n",
       "      <td>-0.054374</td>\n",
       "      <td>-0.003910</td>\n",
       "      <td>-0.018223</td>\n",
       "      <td>0.055226</td>\n",
       "      <td>-0.009935</td>\n",
       "      <td>0.009641</td>\n",
       "      <td>-0.028406</td>\n",
       "      <td>-0.044988</td>\n",
       "      <td>0.037730</td>\n",
       "      <td>0.015193</td>\n",
       "      <td>-0.059926</td>\n",
       "      <td>0.059355</td>\n",
       "      <td>-0.005535</td>\n",
       "      <td>-0.028499</td>\n",
       "      <td>-0.100738</td>\n",
       "      <td>0.043482</td>\n",
       "      <td>0.016480</td>\n",
       "      <td>-0.085097</td>\n",
       "      <td>0.011936</td>\n",
       "      <td>-0.057385</td>\n",
       "      <td>-0.013471</td>\n",
       "      <td>-0.072598</td>\n",
       "      <td>0.006957</td>\n",
       "      <td>0.000639</td>\n",
       "      <td>-0.048620</td>\n",
       "      <td>0.068481</td>\n",
       "      <td>-0.061683</td>\n",
       "      <td>0.026060</td>\n",
       "      <td>-0.000266</td>\n",
       "      <td>0.048690</td>\n",
       "      <td>-0.014723</td>\n",
       "      <td>-0.032318</td>\n",
       "      <td>-0.043484</td>\n",
       "      <td>-0.030203</td>\n",
       "      <td>-0.002099</td>\n",
       "      <td>-0.014317</td>\n",
       "      <td>0.076883</td>\n",
       "      <td>0.020761</td>\n",
       "      <td>-0.061550</td>\n",
       "      <td>0.041207</td>\n",
       "      <td>-0.010029</td>\n",
       "      <td>-0.031426</td>\n",
       "      <td>0.016105</td>\n",
       "      <td>0.060795</td>\n",
       "      <td>-0.050861</td>\n",
       "      <td>-0.005650</td>\n",
       "      <td>0.060047</td>\n",
       "      <td>0.038969</td>\n",
       "      <td>-0.054519</td>\n",
       "      <td>-0.037378</td>\n",
       "      <td>0.084429</td>\n",
       "      <td>0.074229</td>\n",
       "      <td>0.031954</td>\n",
       "      <td>-0.031302</td>\n",
       "      <td>0.061995</td>\n",
       "      <td>0.012477</td>\n",
       "      <td>-0.063725</td>\n",
       "      <td>0.037402</td>\n",
       "      <td>0.001799</td>\n",
       "      <td>-0.057067</td>\n",
       "      <td>0.070693</td>\n",
       "      <td>-0.017125</td>\n",
       "      <td>0.012697</td>\n",
       "      <td>-0.020109</td>\n",
       "      <td>0.000914</td>\n",
       "      <td>0.003881</td>\n",
       "      <td>-0.032686</td>\n",
       "      <td>0.001576</td>\n",
       "      <td>-0.053798</td>\n",
       "      <td>-0.051374</td>\n",
       "      <td>-0.051883</td>\n",
       "      <td>-0.020765</td>\n",
       "      <td>-0.045119</td>\n",
       "      <td>-0.070013</td>\n",
       "      <td>0.014367</td>\n",
       "      <td>-0.028490</td>\n",
       "      <td>-0.035113</td>\n",
       "      <td>-0.007883</td>\n",
       "      <td>0.030466</td>\n",
       "      <td>0.029727</td>\n",
       "      <td>-0.004741</td>\n",
       "      <td>0.034916</td>\n",
       "      <td>0.050465</td>\n",
       "      <td>0.073584</td>\n",
       "      <td>-0.010026</td>\n",
       "      <td>0.028855</td>\n",
       "      <td>-0.029971</td>\n",
       "      <td>-0.024800</td>\n",
       "      <td>0.002285</td>\n",
       "      <td>0.056239</td>\n",
       "      <td>0.043059</td>\n",
       "      <td>0.040852</td>\n",
       "      <td>0.027440</td>\n",
       "      <td>0.074019</td>\n",
       "      <td>0.066492</td>\n",
       "      <td>0.047877</td>\n",
       "      <td>0.069079</td>\n",
       "      <td>-0.033940</td>\n",
       "      <td>0.051583</td>\n",
       "      <td>0.031251</td>\n",
       "      <td>0.007934</td>\n",
       "      <td>-0.004190</td>\n",
       "      <td>0.037372</td>\n",
       "      <td>0.020994</td>\n",
       "      <td>0.009656</td>\n",
       "      <td>0.017828</td>\n",
       "      <td>0.004404</td>\n",
       "      <td>-0.046655</td>\n",
       "      <td>-0.008486</td>\n",
       "      <td>0.014866</td>\n",
       "      <td>0.037617</td>\n",
       "      <td>-0.035600</td>\n",
       "      <td>-0.004711</td>\n",
       "      <td>-0.092478</td>\n",
       "      <td>0.026486</td>\n",
       "      <td>0.046783</td>\n",
       "      <td>0.029430</td>\n",
       "      <td>0.071927</td>\n",
       "      <td>-0.096384</td>\n",
       "      <td>-0.034392</td>\n",
       "      <td>0.022390</td>\n",
       "      <td>0.048505</td>\n",
       "      <td>-0.081279</td>\n",
       "      <td>0.064417</td>\n",
       "      <td>0.077251</td>\n",
       "      <td>0.004162</td>\n",
       "      <td>-0.036813</td>\n",
       "      <td>-0.048496</td>\n",
       "      <td>-0.069923</td>\n",
       "      <td>-0.020994</td>\n",
       "      <td>-0.012922</td>\n",
       "      <td>0.004662</td>\n",
       "      <td>-0.070518</td>\n",
       "      <td>0.022226</td>\n",
       "      <td>-0.024648</td>\n",
       "      <td>-0.019508</td>\n",
       "      <td>0.064945</td>\n",
       "      <td>0.044396</td>\n",
       "      <td>-0.005883</td>\n",
       "      <td>0.004745</td>\n",
       "      <td>-0.035459</td>\n",
       "      <td>0.051084</td>\n",
       "      <td>-0.011597</td>\n",
       "      <td>-0.035299</td>\n",
       "      <td>-0.027379</td>\n",
       "      <td>-0.075839</td>\n",
       "      <td>0.006203</td>\n",
       "      <td>-0.032015</td>\n",
       "      <td>-0.025780</td>\n",
       "      <td>0.007648</td>\n",
       "      <td>0.053502</td>\n",
       "      <td>0.100378</td>\n",
       "      <td>-0.031818</td>\n",
       "      <td>-0.002819</td>\n",
       "      <td>0.100586</td>\n",
       "      <td>-0.035725</td>\n",
       "      <td>-0.049472</td>\n",
       "      <td>-0.069539</td>\n",
       "      <td>0.003575</td>\n",
       "      <td>0.007195</td>\n",
       "      <td>0.030257</td>\n",
       "      <td>-0.035489</td>\n",
       "      <td>0.020287</td>\n",
       "      <td>0.049218</td>\n",
       "      <td>-0.012037</td>\n",
       "      <td>-0.006519</td>\n",
       "      <td>0.079622</td>\n",
       "      <td>0.003635</td>\n",
       "      <td>0.038311</td>\n",
       "      <td>-0.074439</td>\n",
       "      <td>-0.035789</td>\n",
       "      <td>-0.049722</td>\n",
       "      <td>0.097702</td>\n",
       "      <td>-0.100783</td>\n",
       "      <td>-0.015318</td>\n",
       "      <td>-0.005332</td>\n",
       "      <td>-0.040886</td>\n",
       "      <td>-0.022442</td>\n",
       "      <td>-0.040216</td>\n",
       "      <td>0.075266</td>\n",
       "      <td>-0.008350</td>\n",
       "      <td>-0.039790</td>\n",
       "      <td>-0.009930</td>\n",
       "      <td>0.057113</td>\n",
       "      <td>0.039237</td>\n",
       "      <td>-0.087547</td>\n",
       "      <td>0.010808</td>\n",
       "      <td>-0.011856</td>\n",
       "      <td>0.091577</td>\n",
       "      <td>-0.071830</td>\n",
       "      <td>-0.063974</td>\n",
       "      <td>-0.010768</td>\n",
       "      <td>0.033048</td>\n",
       "      <td>-0.047228</td>\n",
       "      <td>0.025019</td>\n",
       "      <td>0.054574</td>\n",
       "      <td>0.089041</td>\n",
       "      <td>0.074771</td>\n",
       "      <td>-0.006729</td>\n",
       "      <td>-0.041926</td>\n",
       "      <td>0.079909</td>\n",
       "      <td>-0.012465</td>\n",
       "      <td>0.042707</td>\n",
       "      <td>0.031484</td>\n",
       "      <td>0.068771</td>\n",
       "      <td>0.062038</td>\n",
       "      <td>-0.082406</td>\n",
       "      <td>-0.055304</td>\n",
       "      <td>-0.008403</td>\n",
       "      <td>-0.046275</td>\n",
       "      <td>-0.073741</td>\n",
       "      <td>-0.057733</td>\n",
       "      <td>-0.008102</td>\n",
       "      <td>0.030841</td>\n",
       "      <td>-0.014995</td>\n",
       "      <td>0.036079</td>\n",
       "      <td>0.017220</td>\n",
       "      <td>-0.019786</td>\n",
       "      <td>-0.038062</td>\n",
       "      <td>-0.023291</td>\n",
       "      <td>-0.032354</td>\n",
       "      <td>-0.083169</td>\n",
       "      <td>-0.011261</td>\n",
       "      <td>-0.019510</td>\n",
       "      <td>0.010261</td>\n",
       "      <td>0.005440</td>\n",
       "      <td>-0.050610</td>\n",
       "      <td>0.016131</td>\n",
       "      <td>-0.043890</td>\n",
       "      <td>0.010654</td>\n",
       "      <td>-0.044891</td>\n",
       "      <td>-0.050435</td>\n",
       "      <td>0.021113</td>\n",
       "      <td>-0.033284</td>\n",
       "      <td>0.036888</td>\n",
       "      <td>0.050781</td>\n",
       "      <td>-0.044832</td>\n",
       "      <td>-0.002877</td>\n",
       "      <td>-0.046589</td>\n",
       "      <td>-0.073294</td>\n",
       "      <td>0.010012</td>\n",
       "      <td>0.064281</td>\n",
       "      <td>0.002525</td>\n",
       "      <td>0.005992</td>\n",
       "      <td>-0.035838</td>\n",
       "      <td>0.031938</td>\n",
       "      <td>-0.024050</td>\n",
       "      <td>0.005813</td>\n",
       "      <td>-0.015585</td>\n",
       "      <td>0.037788</td>\n",
       "      <td>-0.037157</td>\n",
       "      <td>0.038411</td>\n",
       "      <td>0.002630</td>\n",
       "      <td>-0.015554</td>\n",
       "      <td>-0.050492</td>\n",
       "      <td>0.042391</td>\n",
       "      <td>0.044751</td>\n",
       "      <td>-0.020570</td>\n",
       "      <td>0.039166</td>\n",
       "      <td>-0.048313</td>\n",
       "      <td>0.035460</td>\n",
       "      <td>-0.044187</td>\n",
       "      <td>-0.036200</td>\n",
       "      <td>-0.028550</td>\n",
       "      <td>0.015374</td>\n",
       "      <td>0.022835</td>\n",
       "      <td>-0.012336</td>\n",
       "      <td>-0.011377</td>\n",
       "      <td>0.025478</td>\n",
       "      <td>0.034720</td>\n",
       "      <td>0.043319</td>\n",
       "      <td>-0.060672</td>\n",
       "      <td>-0.008116</td>\n",
       "      <td>-0.010183</td>\n",
       "      <td>-0.092518</td>\n",
       "      <td>-0.029613</td>\n",
       "      <td>-0.034366</td>\n",
       "      <td>-0.039589</td>\n",
       "      <td>-0.024305</td>\n",
       "      <td>0.004382</td>\n",
       "      <td>-0.020934</td>\n",
       "      <td>-0.047879</td>\n",
       "      <td>-0.018483</td>\n",
       "      <td>0.016533</td>\n",
       "      <td>0.080940</td>\n",
       "      <td>-0.019513</td>\n",
       "      <td>0.031710</td>\n",
       "      <td>-0.024503</td>\n",
       "      <td>0.011677</td>\n",
       "      <td>-0.060538</td>\n",
       "      <td>-0.051992</td>\n",
       "      <td>0.070915</td>\n",
       "      <td>-0.047141</td>\n",
       "      <td>0.025372</td>\n",
       "      <td>-0.039808</td>\n",
       "      <td>-0.028153</td>\n",
       "      <td>-0.045678</td>\n",
       "      <td>-0.081840</td>\n",
       "      <td>0.020064</td>\n",
       "      <td>-0.049506</td>\n",
       "      <td>-0.017800</td>\n",
       "      <td>-0.076379</td>\n",
       "      <td>0.039454</td>\n",
       "      <td>-0.078400</td>\n",
       "      <td>-0.018091</td>\n",
       "      <td>-0.061223</td>\n",
       "      <td>-0.058935</td>\n",
       "      <td>-0.016336</td>\n",
       "      <td>-0.023909</td>\n",
       "      <td>-0.019932</td>\n",
       "      <td>0.074017</td>\n",
       "      <td>-0.049200</td>\n",
       "      <td>0.002007</td>\n",
       "      <td>-0.068772</td>\n",
       "      <td>-0.004221</td>\n",
       "      <td>-0.023758</td>\n",
       "      <td>-0.034278</td>\n",
       "      <td>-0.059307</td>\n",
       "      <td>-0.065206</td>\n",
       "      <td>0.017451</td>\n",
       "      <td>-0.008543</td>\n",
       "      <td>-0.025758</td>\n",
       "      <td>-0.029586</td>\n",
       "      <td>-0.045046</td>\n",
       "      <td>0.011035</td>\n",
       "      <td>-0.002949</td>\n",
       "      <td>0.012364</td>\n",
       "      <td>0.080454</td>\n",
       "      <td>-0.050971</td>\n",
       "      <td>-0.046580</td>\n",
       "      <td>0.089115</td>\n",
       "      <td>0.017176</td>\n",
       "      <td>0.016804</td>\n",
       "      <td>-0.032324</td>\n",
       "      <td>0.004949</td>\n",
       "      <td>0.066183</td>\n",
       "      <td>-0.005415</td>\n",
       "      <td>-0.040968</td>\n",
       "      <td>0.016175</td>\n",
       "      <td>-0.032409</td>\n",
       "      <td>-0.003373</td>\n",
       "      <td>-0.045490</td>\n",
       "      <td>0.043977</td>\n",
       "      <td>-0.002709</td>\n",
       "      <td>-0.035514</td>\n",
       "      <td>0.075806</td>\n",
       "      <td>-0.000115</td>\n",
       "      <td>0.049067</td>\n",
       "      <td>-0.025091</td>\n",
       "      <td>-0.023968</td>\n",
       "      <td>-0.016429</td>\n",
       "      <td>-0.004595</td>\n",
       "      <td>0.092619</td>\n",
       "      <td>-0.063908</td>\n",
       "      <td>-0.010871</td>\n",
       "      <td>0.047848</td>\n",
       "      <td>0.054256</td>\n",
       "      <td>-0.004406</td>\n",
       "      <td>0.010212</td>\n",
       "      <td>-0.077258</td>\n",
       "      <td>0.002193</td>\n",
       "      <td>-0.080134</td>\n",
       "      <td>0.053799</td>\n",
       "      <td>0.057900</td>\n",
       "      <td>-0.014870</td>\n",
       "      <td>0.007588</td>\n",
       "      <td>0.032561</td>\n",
       "      <td>-0.013136</td>\n",
       "      <td>0.002368</td>\n",
       "      <td>0.005967</td>\n",
       "      <td>-0.010140</td>\n",
       "      <td>-0.005554</td>\n",
       "      <td>0.050540</td>\n",
       "      <td>0.048885</td>\n",
       "      <td>-0.005373</td>\n",
       "      <td>0.078077</td>\n",
       "      <td>0.034113</td>\n",
       "      <td>0.025101</td>\n",
       "      <td>0.028954</td>\n",
       "      <td>-0.048309</td>\n",
       "      <td>0.028760</td>\n",
       "      <td>0.058866</td>\n",
       "      <td>0.002107</td>\n",
       "      <td>-0.100629</td>\n",
       "      <td>-0.029300</td>\n",
       "      <td>0.061822</td>\n",
       "      <td>-0.038401</td>\n",
       "      <td>0.097580</td>\n",
       "      <td>0.002871</td>\n",
       "      <td>0.013917</td>\n",
       "      <td>0.025910</td>\n",
       "      <td>-0.072696</td>\n",
       "      <td>0.075220</td>\n",
       "      <td>0.019238</td>\n",
       "      <td>-0.025088</td>\n",
       "      <td>0.069259</td>\n",
       "      <td>0.066572</td>\n",
       "      <td>0.044215</td>\n",
       "      <td>-0.021113</td>\n",
       "      <td>-0.011015</td>\n",
       "      <td>0.038826</td>\n",
       "      <td>-0.038666</td>\n",
       "      <td>0.004937</td>\n",
       "      <td>-0.065862</td>\n",
       "      <td>-0.096661</td>\n",
       "      <td>-0.039939</td>\n",
       "      <td>-0.004821</td>\n",
       "      <td>-0.000633</td>\n",
       "      <td>-0.032066</td>\n",
       "      <td>-0.001637</td>\n",
       "      <td>-0.036178</td>\n",
       "      <td>0.032611</td>\n",
       "      <td>0.031594</td>\n",
       "      <td>-0.014061</td>\n",
       "      <td>0.024074</td>\n",
       "      <td>-0.031998</td>\n",
       "      <td>0.069268</td>\n",
       "      <td>-0.005463</td>\n",
       "      <td>0.008954</td>\n",
       "      <td>-0.024153</td>\n",
       "      <td>-0.008846</td>\n",
       "      <td>-0.099964</td>\n",
       "      <td>0.085710</td>\n",
       "      <td>0.050738</td>\n",
       "      <td>-0.021996</td>\n",
       "      <td>-0.056691</td>\n",
       "      <td>-0.066562</td>\n",
       "      <td>0.009670</td>\n",
       "      <td>-0.014906</td>\n",
       "      <td>0.037426</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>ski</th>\n",
       "      <td>-0.008974</td>\n",
       "      <td>0.020815</td>\n",
       "      <td>-0.047432</td>\n",
       "      <td>0.021539</td>\n",
       "      <td>-0.015312</td>\n",
       "      <td>-0.025122</td>\n",
       "      <td>0.034103</td>\n",
       "      <td>-0.030542</td>\n",
       "      <td>-0.026442</td>\n",
       "      <td>-0.024509</td>\n",
       "      <td>-0.017093</td>\n",
       "      <td>0.032973</td>\n",
       "      <td>-0.017679</td>\n",
       "      <td>-0.018202</td>\n",
       "      <td>0.010089</td>\n",
       "      <td>-0.037003</td>\n",
       "      <td>-0.009756</td>\n",
       "      <td>0.002367</td>\n",
       "      <td>0.056437</td>\n",
       "      <td>-0.072757</td>\n",
       "      <td>0.005942</td>\n",
       "      <td>0.012579</td>\n",
       "      <td>-0.031167</td>\n",
       "      <td>0.022203</td>\n",
       "      <td>-0.065834</td>\n",
       "      <td>-0.041644</td>\n",
       "      <td>0.011557</td>\n",
       "      <td>0.042556</td>\n",
       "      <td>0.047965</td>\n",
       "      <td>-0.008015</td>\n",
       "      <td>-0.031593</td>\n",
       "      <td>-0.013723</td>\n",
       "      <td>-0.062006</td>\n",
       "      <td>0.087361</td>\n",
       "      <td>0.043069</td>\n",
       "      <td>0.026226</td>\n",
       "      <td>0.035345</td>\n",
       "      <td>-0.019299</td>\n",
       "      <td>0.019100</td>\n",
       "      <td>0.003141</td>\n",
       "      <td>-0.080889</td>\n",
       "      <td>0.022128</td>\n",
       "      <td>0.012253</td>\n",
       "      <td>-0.054943</td>\n",
       "      <td>-0.064085</td>\n",
       "      <td>-0.003503</td>\n",
       "      <td>-0.038121</td>\n",
       "      <td>0.060335</td>\n",
       "      <td>0.028242</td>\n",
       "      <td>0.060217</td>\n",
       "      <td>0.016821</td>\n",
       "      <td>0.033607</td>\n",
       "      <td>0.086428</td>\n",
       "      <td>0.082846</td>\n",
       "      <td>-0.047861</td>\n",
       "      <td>0.030666</td>\n",
       "      <td>0.042293</td>\n",
       "      <td>-0.002252</td>\n",
       "      <td>0.077051</td>\n",
       "      <td>0.068754</td>\n",
       "      <td>-0.040835</td>\n",
       "      <td>-0.044521</td>\n",
       "      <td>-0.031198</td>\n",
       "      <td>0.062234</td>\n",
       "      <td>-0.010204</td>\n",
       "      <td>0.039655</td>\n",
       "      <td>-0.053825</td>\n",
       "      <td>-0.058385</td>\n",
       "      <td>0.079534</td>\n",
       "      <td>0.083434</td>\n",
       "      <td>-0.062137</td>\n",
       "      <td>0.007997</td>\n",
       "      <td>-0.061859</td>\n",
       "      <td>-0.036210</td>\n",
       "      <td>0.043976</td>\n",
       "      <td>0.023804</td>\n",
       "      <td>0.044386</td>\n",
       "      <td>0.006291</td>\n",
       "      <td>-0.026251</td>\n",
       "      <td>-0.014913</td>\n",
       "      <td>0.087045</td>\n",
       "      <td>-0.055562</td>\n",
       "      <td>-0.067832</td>\n",
       "      <td>-0.015189</td>\n",
       "      <td>-0.030816</td>\n",
       "      <td>0.011586</td>\n",
       "      <td>-0.026496</td>\n",
       "      <td>0.058349</td>\n",
       "      <td>-0.035090</td>\n",
       "      <td>-0.066722</td>\n",
       "      <td>-0.008994</td>\n",
       "      <td>-0.001476</td>\n",
       "      <td>0.037901</td>\n",
       "      <td>0.047203</td>\n",
       "      <td>0.012038</td>\n",
       "      <td>-0.015739</td>\n",
       "      <td>-0.015612</td>\n",
       "      <td>-0.075097</td>\n",
       "      <td>-0.020288</td>\n",
       "      <td>-0.007937</td>\n",
       "      <td>0.023888</td>\n",
       "      <td>0.016882</td>\n",
       "      <td>-0.017812</td>\n",
       "      <td>-0.081762</td>\n",
       "      <td>0.076951</td>\n",
       "      <td>-0.016334</td>\n",
       "      <td>-0.071526</td>\n",
       "      <td>-0.038650</td>\n",
       "      <td>0.019098</td>\n",
       "      <td>0.010544</td>\n",
       "      <td>-0.024124</td>\n",
       "      <td>0.039458</td>\n",
       "      <td>-0.057073</td>\n",
       "      <td>-0.027149</td>\n",
       "      <td>0.024234</td>\n",
       "      <td>0.049108</td>\n",
       "      <td>0.027225</td>\n",
       "      <td>0.009638</td>\n",
       "      <td>0.035895</td>\n",
       "      <td>-0.006994</td>\n",
       "      <td>-0.054212</td>\n",
       "      <td>0.014315</td>\n",
       "      <td>0.087423</td>\n",
       "      <td>0.021571</td>\n",
       "      <td>0.002588</td>\n",
       "      <td>0.052823</td>\n",
       "      <td>-0.005064</td>\n",
       "      <td>0.047312</td>\n",
       "      <td>-0.047794</td>\n",
       "      <td>0.026870</td>\n",
       "      <td>0.028912</td>\n",
       "      <td>0.057800</td>\n",
       "      <td>0.079127</td>\n",
       "      <td>0.063761</td>\n",
       "      <td>0.051278</td>\n",
       "      <td>0.017975</td>\n",
       "      <td>0.065606</td>\n",
       "      <td>-0.045440</td>\n",
       "      <td>0.007859</td>\n",
       "      <td>-0.054577</td>\n",
       "      <td>-0.007618</td>\n",
       "      <td>-0.003850</td>\n",
       "      <td>0.058273</td>\n",
       "      <td>-0.048575</td>\n",
       "      <td>-0.007642</td>\n",
       "      <td>0.063966</td>\n",
       "      <td>0.048158</td>\n",
       "      <td>0.071948</td>\n",
       "      <td>-0.023364</td>\n",
       "      <td>0.035273</td>\n",
       "      <td>0.031278</td>\n",
       "      <td>0.036723</td>\n",
       "      <td>-0.043487</td>\n",
       "      <td>0.006216</td>\n",
       "      <td>-0.041597</td>\n",
       "      <td>-0.053914</td>\n",
       "      <td>-0.051317</td>\n",
       "      <td>-0.026777</td>\n",
       "      <td>-0.006696</td>\n",
       "      <td>0.062884</td>\n",
       "      <td>-0.074343</td>\n",
       "      <td>-0.018255</td>\n",
       "      <td>-0.040314</td>\n",
       "      <td>0.018934</td>\n",
       "      <td>-0.037247</td>\n",
       "      <td>-0.003372</td>\n",
       "      <td>0.073950</td>\n",
       "      <td>-0.024987</td>\n",
       "      <td>0.000660</td>\n",
       "      <td>-0.054597</td>\n",
       "      <td>-0.027455</td>\n",
       "      <td>0.034847</td>\n",
       "      <td>-0.001288</td>\n",
       "      <td>-0.067958</td>\n",
       "      <td>0.065470</td>\n",
       "      <td>-0.008634</td>\n",
       "      <td>0.071443</td>\n",
       "      <td>0.006304</td>\n",
       "      <td>-0.010923</td>\n",
       "      <td>-0.025893</td>\n",
       "      <td>-0.016366</td>\n",
       "      <td>0.062647</td>\n",
       "      <td>0.001261</td>\n",
       "      <td>-0.021238</td>\n",
       "      <td>0.048750</td>\n",
       "      <td>-0.055133</td>\n",
       "      <td>0.070293</td>\n",
       "      <td>-0.053714</td>\n",
       "      <td>0.043544</td>\n",
       "      <td>0.040890</td>\n",
       "      <td>-0.028262</td>\n",
       "      <td>0.019744</td>\n",
       "      <td>0.078410</td>\n",
       "      <td>0.054656</td>\n",
       "      <td>-0.006145</td>\n",
       "      <td>-0.032923</td>\n",
       "      <td>-0.049432</td>\n",
       "      <td>-0.010649</td>\n",
       "      <td>-0.003117</td>\n",
       "      <td>0.023412</td>\n",
       "      <td>-0.015534</td>\n",
       "      <td>-0.088169</td>\n",
       "      <td>-0.045969</td>\n",
       "      <td>0.004880</td>\n",
       "      <td>-0.057624</td>\n",
       "      <td>0.050438</td>\n",
       "      <td>0.006350</td>\n",
       "      <td>-0.010420</td>\n",
       "      <td>-0.014375</td>\n",
       "      <td>-0.020847</td>\n",
       "      <td>-0.070168</td>\n",
       "      <td>-0.069010</td>\n",
       "      <td>0.044230</td>\n",
       "      <td>-0.002157</td>\n",
       "      <td>-0.049884</td>\n",
       "      <td>0.066985</td>\n",
       "      <td>0.013237</td>\n",
       "      <td>0.003601</td>\n",
       "      <td>-0.083867</td>\n",
       "      <td>0.070304</td>\n",
       "      <td>-0.011912</td>\n",
       "      <td>-0.052847</td>\n",
       "      <td>-0.070273</td>\n",
       "      <td>-0.012595</td>\n",
       "      <td>0.006501</td>\n",
       "      <td>-0.022494</td>\n",
       "      <td>-0.047114</td>\n",
       "      <td>0.011501</td>\n",
       "      <td>-0.057727</td>\n",
       "      <td>0.061519</td>\n",
       "      <td>-0.000652</td>\n",
       "      <td>-0.053517</td>\n",
       "      <td>0.025528</td>\n",
       "      <td>0.089407</td>\n",
       "      <td>-0.010306</td>\n",
       "      <td>-0.048913</td>\n",
       "      <td>-0.028896</td>\n",
       "      <td>0.041245</td>\n",
       "      <td>-0.064611</td>\n",
       "      <td>0.017081</td>\n",
       "      <td>0.009624</td>\n",
       "      <td>-0.067239</td>\n",
       "      <td>0.027983</td>\n",
       "      <td>-0.068344</td>\n",
       "      <td>0.058782</td>\n",
       "      <td>0.003981</td>\n",
       "      <td>0.059124</td>\n",
       "      <td>-0.002797</td>\n",
       "      <td>-0.046812</td>\n",
       "      <td>-0.032729</td>\n",
       "      <td>0.064521</td>\n",
       "      <td>0.016485</td>\n",
       "      <td>-0.031511</td>\n",
       "      <td>0.065694</td>\n",
       "      <td>-0.057998</td>\n",
       "      <td>-0.034471</td>\n",
       "      <td>0.027379</td>\n",
       "      <td>-0.011775</td>\n",
       "      <td>-0.013682</td>\n",
       "      <td>-0.086054</td>\n",
       "      <td>0.066720</td>\n",
       "      <td>-0.046236</td>\n",
       "      <td>0.048822</td>\n",
       "      <td>0.047340</td>\n",
       "      <td>0.055136</td>\n",
       "      <td>-0.045498</td>\n",
       "      <td>-0.032161</td>\n",
       "      <td>-0.006496</td>\n",
       "      <td>0.062494</td>\n",
       "      <td>0.010378</td>\n",
       "      <td>0.073523</td>\n",
       "      <td>-0.058110</td>\n",
       "      <td>-0.049548</td>\n",
       "      <td>-0.007543</td>\n",
       "      <td>-0.006066</td>\n",
       "      <td>0.045950</td>\n",
       "      <td>-0.011692</td>\n",
       "      <td>0.024896</td>\n",
       "      <td>0.071177</td>\n",
       "      <td>0.047250</td>\n",
       "      <td>-0.054091</td>\n",
       "      <td>-0.037249</td>\n",
       "      <td>-0.053467</td>\n",
       "      <td>-0.010726</td>\n",
       "      <td>0.078289</td>\n",
       "      <td>-0.013185</td>\n",
       "      <td>0.018943</td>\n",
       "      <td>0.056825</td>\n",
       "      <td>-0.061513</td>\n",
       "      <td>-0.049974</td>\n",
       "      <td>-0.085077</td>\n",
       "      <td>-0.034583</td>\n",
       "      <td>0.088395</td>\n",
       "      <td>-0.065070</td>\n",
       "      <td>0.052368</td>\n",
       "      <td>0.045353</td>\n",
       "      <td>0.003694</td>\n",
       "      <td>0.054088</td>\n",
       "      <td>-0.031070</td>\n",
       "      <td>-0.027797</td>\n",
       "      <td>0.046712</td>\n",
       "      <td>-0.045979</td>\n",
       "      <td>-0.040514</td>\n",
       "      <td>0.018220</td>\n",
       "      <td>0.055948</td>\n",
       "      <td>0.069598</td>\n",
       "      <td>0.058222</td>\n",
       "      <td>-0.010625</td>\n",
       "      <td>0.014938</td>\n",
       "      <td>-0.042120</td>\n",
       "      <td>-0.004039</td>\n",
       "      <td>-0.035734</td>\n",
       "      <td>0.041358</td>\n",
       "      <td>0.011643</td>\n",
       "      <td>-0.014496</td>\n",
       "      <td>-0.024022</td>\n",
       "      <td>0.004245</td>\n",
       "      <td>0.012675</td>\n",
       "      <td>-0.001701</td>\n",
       "      <td>0.017334</td>\n",
       "      <td>0.080774</td>\n",
       "      <td>0.008056</td>\n",
       "      <td>0.006145</td>\n",
       "      <td>0.051717</td>\n",
       "      <td>0.032959</td>\n",
       "      <td>0.043834</td>\n",
       "      <td>-0.037299</td>\n",
       "      <td>0.037756</td>\n",
       "      <td>0.021156</td>\n",
       "      <td>0.049670</td>\n",
       "      <td>-0.086938</td>\n",
       "      <td>-0.052023</td>\n",
       "      <td>0.009812</td>\n",
       "      <td>0.066901</td>\n",
       "      <td>-0.082460</td>\n",
       "      <td>0.031558</td>\n",
       "      <td>-0.037388</td>\n",
       "      <td>0.090036</td>\n",
       "      <td>-0.011564</td>\n",
       "      <td>0.041526</td>\n",
       "      <td>-0.000463</td>\n",
       "      <td>0.057787</td>\n",
       "      <td>-0.026298</td>\n",
       "      <td>0.001646</td>\n",
       "      <td>0.005750</td>\n",
       "      <td>0.036548</td>\n",
       "      <td>0.043405</td>\n",
       "      <td>-0.013021</td>\n",
       "      <td>-0.061553</td>\n",
       "      <td>-0.005667</td>\n",
       "      <td>0.083183</td>\n",
       "      <td>-0.009233</td>\n",
       "      <td>0.055021</td>\n",
       "      <td>-0.042728</td>\n",
       "      <td>-0.032825</td>\n",
       "      <td>-0.052098</td>\n",
       "      <td>-0.079294</td>\n",
       "      <td>-0.016389</td>\n",
       "      <td>0.025929</td>\n",
       "      <td>-0.058062</td>\n",
       "      <td>-0.003221</td>\n",
       "      <td>-0.071481</td>\n",
       "      <td>0.069081</td>\n",
       "      <td>-0.003403</td>\n",
       "      <td>0.061231</td>\n",
       "      <td>-0.049907</td>\n",
       "      <td>0.026340</td>\n",
       "      <td>0.055518</td>\n",
       "      <td>0.049522</td>\n",
       "      <td>0.006684</td>\n",
       "      <td>-0.019452</td>\n",
       "      <td>-0.014543</td>\n",
       "      <td>0.016356</td>\n",
       "      <td>-0.028724</td>\n",
       "      <td>0.007419</td>\n",
       "      <td>-0.046638</td>\n",
       "      <td>0.038896</td>\n",
       "      <td>-0.023314</td>\n",
       "      <td>0.058395</td>\n",
       "      <td>-0.071621</td>\n",
       "      <td>0.090179</td>\n",
       "      <td>-0.021649</td>\n",
       "      <td>0.026438</td>\n",
       "      <td>0.026987</td>\n",
       "      <td>0.011099</td>\n",
       "      <td>0.023020</td>\n",
       "      <td>-0.059457</td>\n",
       "      <td>-0.017660</td>\n",
       "      <td>-0.009336</td>\n",
       "      <td>0.031767</td>\n",
       "      <td>-0.045412</td>\n",
       "      <td>-0.007315</td>\n",
       "      <td>-0.012653</td>\n",
       "      <td>-0.041443</td>\n",
       "      <td>-0.003107</td>\n",
       "      <td>-0.025558</td>\n",
       "      <td>0.020126</td>\n",
       "      <td>-0.008564</td>\n",
       "      <td>-0.085617</td>\n",
       "      <td>0.037813</td>\n",
       "      <td>0.014255</td>\n",
       "      <td>0.069387</td>\n",
       "      <td>0.060026</td>\n",
       "      <td>-0.025112</td>\n",
       "      <td>-0.035866</td>\n",
       "      <td>-0.058540</td>\n",
       "      <td>0.013601</td>\n",
       "      <td>0.074204</td>\n",
       "      <td>-0.009720</td>\n",
       "      <td>-0.028407</td>\n",
       "      <td>-0.037311</td>\n",
       "      <td>0.046830</td>\n",
       "      <td>0.016659</td>\n",
       "      <td>0.060548</td>\n",
       "      <td>-0.077034</td>\n",
       "      <td>-0.073452</td>\n",
       "      <td>-0.060421</td>\n",
       "      <td>0.021793</td>\n",
       "      <td>-0.058761</td>\n",
       "      <td>0.004496</td>\n",
       "      <td>0.020455</td>\n",
       "      <td>-0.046891</td>\n",
       "      <td>-0.012097</td>\n",
       "      <td>-0.076162</td>\n",
       "      <td>0.013276</td>\n",
       "      <td>0.024276</td>\n",
       "      <td>0.008814</td>\n",
       "      <td>0.068173</td>\n",
       "      <td>0.036899</td>\n",
       "      <td>-0.027916</td>\n",
       "      <td>-0.011376</td>\n",
       "      <td>-0.081641</td>\n",
       "      <td>0.006964</td>\n",
       "      <td>-0.038498</td>\n",
       "      <td>0.053589</td>\n",
       "      <td>-0.054167</td>\n",
       "      <td>-0.007919</td>\n",
       "      <td>0.010205</td>\n",
       "      <td>-0.019242</td>\n",
       "      <td>0.047256</td>\n",
       "      <td>0.074615</td>\n",
       "      <td>0.022735</td>\n",
       "      <td>0.001066</td>\n",
       "      <td>0.023890</td>\n",
       "      <td>-0.077865</td>\n",
       "      <td>-0.025791</td>\n",
       "      <td>-0.036523</td>\n",
       "      <td>0.054939</td>\n",
       "      <td>-0.046859</td>\n",
       "      <td>-0.048878</td>\n",
       "      <td>-0.060598</td>\n",
       "      <td>0.012150</td>\n",
       "      <td>0.010922</td>\n",
       "      <td>0.023344</td>\n",
       "      <td>0.045131</td>\n",
       "      <td>0.032302</td>\n",
       "      <td>0.030158</td>\n",
       "      <td>-0.037109</td>\n",
       "      <td>-0.000478</td>\n",
       "      <td>0.070178</td>\n",
       "      <td>0.032006</td>\n",
       "      <td>0.061528</td>\n",
       "      <td>-0.004853</td>\n",
       "      <td>0.008945</td>\n",
       "      <td>-0.049595</td>\n",
       "      <td>-0.019621</td>\n",
       "      <td>-0.017550</td>\n",
       "      <td>-0.062150</td>\n",
       "      <td>0.020912</td>\n",
       "      <td>0.012919</td>\n",
       "      <td>0.041374</td>\n",
       "      <td>0.008561</td>\n",
       "      <td>0.001738</td>\n",
       "      <td>0.028067</td>\n",
       "      <td>0.082530</td>\n",
       "      <td>-0.031771</td>\n",
       "      <td>0.039285</td>\n",
       "      <td>0.069612</td>\n",
       "      <td>-0.000907</td>\n",
       "      <td>-0.044651</td>\n",
       "      <td>-0.016123</td>\n",
       "      <td>-0.066910</td>\n",
       "      <td>-0.019877</td>\n",
       "      <td>0.055000</td>\n",
       "      <td>-0.085789</td>\n",
       "      <td>-0.022484</td>\n",
       "      <td>-0.032362</td>\n",
       "      <td>0.036367</td>\n",
       "      <td>-0.004514</td>\n",
       "      <td>0.003951</td>\n",
       "      <td>-0.072870</td>\n",
       "      <td>0.084775</td>\n",
       "      <td>-0.039618</td>\n",
       "      <td>-0.032014</td>\n",
       "      <td>-0.051234</td>\n",
       "      <td>0.024639</td>\n",
       "      <td>0.053439</td>\n",
       "      <td>-0.020861</td>\n",
       "      <td>0.047370</td>\n",
       "      <td>-0.085946</td>\n",
       "      <td>0.035097</td>\n",
       "      <td>-0.043610</td>\n",
       "      <td>-0.027508</td>\n",
       "      <td>-0.088748</td>\n",
       "      <td>0.006053</td>\n",
       "      <td>0.051079</td>\n",
       "      <td>0.001197</td>\n",
       "      <td>-0.073380</td>\n",
       "      <td>-0.018774</td>\n",
       "      <td>-0.001848</td>\n",
       "      <td>0.039178</td>\n",
       "      <td>-0.057981</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>commanding</th>\n",
       "      <td>-0.047657</td>\n",
       "      <td>-0.055279</td>\n",
       "      <td>0.015045</td>\n",
       "      <td>0.037645</td>\n",
       "      <td>0.048102</td>\n",
       "      <td>0.056537</td>\n",
       "      <td>-0.018677</td>\n",
       "      <td>-0.033269</td>\n",
       "      <td>0.080872</td>\n",
       "      <td>0.004001</td>\n",
       "      <td>0.062216</td>\n",
       "      <td>-0.007820</td>\n",
       "      <td>0.000493</td>\n",
       "      <td>-0.012361</td>\n",
       "      <td>-0.034251</td>\n",
       "      <td>0.006213</td>\n",
       "      <td>0.027879</td>\n",
       "      <td>0.060953</td>\n",
       "      <td>-0.032899</td>\n",
       "      <td>0.055724</td>\n",
       "      <td>-0.022387</td>\n",
       "      <td>0.028922</td>\n",
       "      <td>-0.007465</td>\n",
       "      <td>-0.005556</td>\n",
       "      <td>0.011620</td>\n",
       "      <td>0.028660</td>\n",
       "      <td>-0.036908</td>\n",
       "      <td>-0.016999</td>\n",
       "      <td>-0.007514</td>\n",
       "      <td>0.011126</td>\n",
       "      <td>-0.000177</td>\n",
       "      <td>-0.004316</td>\n",
       "      <td>-0.014634</td>\n",
       "      <td>0.022119</td>\n",
       "      <td>0.069210</td>\n",
       "      <td>0.087923</td>\n",
       "      <td>0.042635</td>\n",
       "      <td>0.013411</td>\n",
       "      <td>0.006325</td>\n",
       "      <td>0.021378</td>\n",
       "      <td>-0.020463</td>\n",
       "      <td>0.023261</td>\n",
       "      <td>0.027898</td>\n",
       "      <td>-0.068612</td>\n",
       "      <td>-0.040640</td>\n",
       "      <td>-0.003444</td>\n",
       "      <td>-0.043978</td>\n",
       "      <td>-0.019669</td>\n",
       "      <td>0.036172</td>\n",
       "      <td>0.038745</td>\n",
       "      <td>0.055832</td>\n",
       "      <td>-0.059093</td>\n",
       "      <td>0.078977</td>\n",
       "      <td>0.060202</td>\n",
       "      <td>-0.041122</td>\n",
       "      <td>-0.027509</td>\n",
       "      <td>0.001034</td>\n",
       "      <td>-0.044381</td>\n",
       "      <td>0.014997</td>\n",
       "      <td>-0.052238</td>\n",
       "      <td>0.071739</td>\n",
       "      <td>-0.000627</td>\n",
       "      <td>-0.068639</td>\n",
       "      <td>0.047556</td>\n",
       "      <td>0.003749</td>\n",
       "      <td>0.002379</td>\n",
       "      <td>-0.071026</td>\n",
       "      <td>0.007314</td>\n",
       "      <td>0.077278</td>\n",
       "      <td>0.046022</td>\n",
       "      <td>-0.049779</td>\n",
       "      <td>0.024154</td>\n",
       "      <td>0.020324</td>\n",
       "      <td>0.036095</td>\n",
       "      <td>0.034330</td>\n",
       "      <td>-0.001989</td>\n",
       "      <td>-0.063054</td>\n",
       "      <td>0.014894</td>\n",
       "      <td>0.034280</td>\n",
       "      <td>-0.077720</td>\n",
       "      <td>-0.031121</td>\n",
       "      <td>0.050847</td>\n",
       "      <td>-0.057273</td>\n",
       "      <td>0.014884</td>\n",
       "      <td>-0.020008</td>\n",
       "      <td>0.023417</td>\n",
       "      <td>-0.002270</td>\n",
       "      <td>0.004734</td>\n",
       "      <td>0.026899</td>\n",
       "      <td>0.043444</td>\n",
       "      <td>-0.018541</td>\n",
       "      <td>-0.006400</td>\n",
       "      <td>0.013930</td>\n",
       "      <td>0.026436</td>\n",
       "      <td>-0.045634</td>\n",
       "      <td>0.021222</td>\n",
       "      <td>-0.034946</td>\n",
       "      <td>-0.043824</td>\n",
       "      <td>-0.007162</td>\n",
       "      <td>0.028520</td>\n",
       "      <td>-0.000777</td>\n",
       "      <td>-0.087618</td>\n",
       "      <td>-0.069882</td>\n",
       "      <td>-0.059919</td>\n",
       "      <td>-0.036418</td>\n",
       "      <td>-0.057232</td>\n",
       "      <td>0.029378</td>\n",
       "      <td>0.009764</td>\n",
       "      <td>0.016425</td>\n",
       "      <td>0.083470</td>\n",
       "      <td>0.010189</td>\n",
       "      <td>0.022004</td>\n",
       "      <td>-0.007916</td>\n",
       "      <td>-0.078676</td>\n",
       "      <td>-0.054208</td>\n",
       "      <td>0.017349</td>\n",
       "      <td>-0.041437</td>\n",
       "      <td>0.002205</td>\n",
       "      <td>-0.103048</td>\n",
       "      <td>-0.033330</td>\n",
       "      <td>-0.079495</td>\n",
       "      <td>-0.018680</td>\n",
       "      <td>0.046055</td>\n",
       "      <td>-0.012142</td>\n",
       "      <td>0.009600</td>\n",
       "      <td>0.067915</td>\n",
       "      <td>0.007821</td>\n",
       "      <td>0.016756</td>\n",
       "      <td>-0.030487</td>\n",
       "      <td>0.041920</td>\n",
       "      <td>-0.041492</td>\n",
       "      <td>0.041409</td>\n",
       "      <td>0.098197</td>\n",
       "      <td>0.053186</td>\n",
       "      <td>-0.028603</td>\n",
       "      <td>0.043265</td>\n",
       "      <td>0.010515</td>\n",
       "      <td>0.055092</td>\n",
       "      <td>-0.007546</td>\n",
       "      <td>0.056360</td>\n",
       "      <td>0.024749</td>\n",
       "      <td>-0.022586</td>\n",
       "      <td>0.052509</td>\n",
       "      <td>-0.017359</td>\n",
       "      <td>-0.063103</td>\n",
       "      <td>0.045872</td>\n",
       "      <td>-0.004469</td>\n",
       "      <td>-0.004066</td>\n",
       "      <td>0.006999</td>\n",
       "      <td>-0.072281</td>\n",
       "      <td>0.028378</td>\n",
       "      <td>0.026574</td>\n",
       "      <td>-0.032156</td>\n",
       "      <td>0.026939</td>\n",
       "      <td>-0.014808</td>\n",
       "      <td>-0.039717</td>\n",
       "      <td>-0.056386</td>\n",
       "      <td>-0.048155</td>\n",
       "      <td>0.077477</td>\n",
       "      <td>0.103643</td>\n",
       "      <td>0.025085</td>\n",
       "      <td>-0.037667</td>\n",
       "      <td>-0.053172</td>\n",
       "      <td>0.015718</td>\n",
       "      <td>0.007960</td>\n",
       "      <td>0.017072</td>\n",
       "      <td>-0.041063</td>\n",
       "      <td>-0.042034</td>\n",
       "      <td>0.036059</td>\n",
       "      <td>-0.049805</td>\n",
       "      <td>-0.065689</td>\n",
       "      <td>0.012487</td>\n",
       "      <td>0.057844</td>\n",
       "      <td>-0.055098</td>\n",
       "      <td>-0.078731</td>\n",
       "      <td>0.066046</td>\n",
       "      <td>-0.012303</td>\n",
       "      <td>-0.024044</td>\n",
       "      <td>-0.037923</td>\n",
       "      <td>-0.023551</td>\n",
       "      <td>-0.046415</td>\n",
       "      <td>-0.081134</td>\n",
       "      <td>0.037937</td>\n",
       "      <td>-0.019659</td>\n",
       "      <td>-0.020886</td>\n",
       "      <td>0.050087</td>\n",
       "      <td>0.050113</td>\n",
       "      <td>-0.004979</td>\n",
       "      <td>-0.033201</td>\n",
       "      <td>-0.071474</td>\n",
       "      <td>0.002224</td>\n",
       "      <td>0.038036</td>\n",
       "      <td>0.082319</td>\n",
       "      <td>0.035441</td>\n",
       "      <td>0.072397</td>\n",
       "      <td>-0.067724</td>\n",
       "      <td>0.037494</td>\n",
       "      <td>0.016616</td>\n",
       "      <td>-0.006128</td>\n",
       "      <td>0.037021</td>\n",
       "      <td>-0.071187</td>\n",
       "      <td>-0.034022</td>\n",
       "      <td>-0.029329</td>\n",
       "      <td>0.050997</td>\n",
       "      <td>0.025691</td>\n",
       "      <td>0.002951</td>\n",
       "      <td>-0.012894</td>\n",
       "      <td>-0.038917</td>\n",
       "      <td>0.002569</td>\n",
       "      <td>0.029185</td>\n",
       "      <td>-0.046088</td>\n",
       "      <td>-0.047066</td>\n",
       "      <td>0.007794</td>\n",
       "      <td>-0.016665</td>\n",
       "      <td>-0.057750</td>\n",
       "      <td>-0.014856</td>\n",
       "      <td>0.022313</td>\n",
       "      <td>0.035418</td>\n",
       "      <td>-0.054693</td>\n",
       "      <td>0.031030</td>\n",
       "      <td>-0.030233</td>\n",
       "      <td>-0.073990</td>\n",
       "      <td>0.020779</td>\n",
       "      <td>0.029783</td>\n",
       "      <td>-0.023044</td>\n",
       "      <td>-0.001528</td>\n",
       "      <td>0.025294</td>\n",
       "      <td>0.019969</td>\n",
       "      <td>-0.039750</td>\n",
       "      <td>-0.032994</td>\n",
       "      <td>-0.041686</td>\n",
       "      <td>-0.043160</td>\n",
       "      <td>-0.003726</td>\n",
       "      <td>0.113852</td>\n",
       "      <td>-0.021909</td>\n",
       "      <td>0.020274</td>\n",
       "      <td>-0.027622</td>\n",
       "      <td>-0.057428</td>\n",
       "      <td>-0.019909</td>\n",
       "      <td>-0.052428</td>\n",
       "      <td>0.031492</td>\n",
       "      <td>-0.020819</td>\n",
       "      <td>0.019262</td>\n",
       "      <td>0.042520</td>\n",
       "      <td>-0.047165</td>\n",
       "      <td>0.033633</td>\n",
       "      <td>0.014007</td>\n",
       "      <td>-0.050425</td>\n",
       "      <td>-0.053425</td>\n",
       "      <td>-0.006864</td>\n",
       "      <td>0.007938</td>\n",
       "      <td>0.034981</td>\n",
       "      <td>0.062506</td>\n",
       "      <td>-0.037280</td>\n",
       "      <td>-0.012757</td>\n",
       "      <td>-0.042682</td>\n",
       "      <td>0.044688</td>\n",
       "      <td>0.059658</td>\n",
       "      <td>0.049451</td>\n",
       "      <td>0.067256</td>\n",
       "      <td>-0.029687</td>\n",
       "      <td>0.031950</td>\n",
       "      <td>0.093893</td>\n",
       "      <td>-0.042409</td>\n",
       "      <td>0.082212</td>\n",
       "      <td>0.069638</td>\n",
       "      <td>0.031266</td>\n",
       "      <td>0.015082</td>\n",
       "      <td>0.013481</td>\n",
       "      <td>-0.038379</td>\n",
       "      <td>0.066083</td>\n",
       "      <td>-0.003622</td>\n",
       "      <td>0.032654</td>\n",
       "      <td>-0.045821</td>\n",
       "      <td>0.086081</td>\n",
       "      <td>-0.061226</td>\n",
       "      <td>-0.041345</td>\n",
       "      <td>-0.010481</td>\n",
       "      <td>0.055568</td>\n",
       "      <td>0.033546</td>\n",
       "      <td>-0.011658</td>\n",
       "      <td>0.070772</td>\n",
       "      <td>-0.030511</td>\n",
       "      <td>0.014902</td>\n",
       "      <td>-0.034981</td>\n",
       "      <td>0.040129</td>\n",
       "      <td>0.069303</td>\n",
       "      <td>0.021797</td>\n",
       "      <td>-0.068925</td>\n",
       "      <td>0.025457</td>\n",
       "      <td>0.007399</td>\n",
       "      <td>-0.007561</td>\n",
       "      <td>0.033522</td>\n",
       "      <td>-0.038232</td>\n",
       "      <td>0.050061</td>\n",
       "      <td>0.043461</td>\n",
       "      <td>-0.013283</td>\n",
       "      <td>0.020269</td>\n",
       "      <td>-0.022872</td>\n",
       "      <td>-0.091971</td>\n",
       "      <td>0.016169</td>\n",
       "      <td>-0.078690</td>\n",
       "      <td>-0.052034</td>\n",
       "      <td>-0.085122</td>\n",
       "      <td>0.020826</td>\n",
       "      <td>0.075091</td>\n",
       "      <td>0.083610</td>\n",
       "      <td>0.064273</td>\n",
       "      <td>-0.029521</td>\n",
       "      <td>-0.088203</td>\n",
       "      <td>-0.039104</td>\n",
       "      <td>0.067063</td>\n",
       "      <td>-0.026698</td>\n",
       "      <td>0.039659</td>\n",
       "      <td>-0.056743</td>\n",
       "      <td>0.006151</td>\n",
       "      <td>-0.057868</td>\n",
       "      <td>0.047792</td>\n",
       "      <td>-0.101860</td>\n",
       "      <td>-0.064901</td>\n",
       "      <td>-0.077856</td>\n",
       "      <td>-0.055927</td>\n",
       "      <td>0.069233</td>\n",
       "      <td>0.055160</td>\n",
       "      <td>-0.062843</td>\n",
       "      <td>-0.034061</td>\n",
       "      <td>0.038274</td>\n",
       "      <td>0.005391</td>\n",
       "      <td>0.003226</td>\n",
       "      <td>-0.076080</td>\n",
       "      <td>0.029647</td>\n",
       "      <td>-0.037415</td>\n",
       "      <td>0.038546</td>\n",
       "      <td>0.046572</td>\n",
       "      <td>-0.069526</td>\n",
       "      <td>0.037634</td>\n",
       "      <td>0.037299</td>\n",
       "      <td>0.027292</td>\n",
       "      <td>0.043785</td>\n",
       "      <td>-0.028656</td>\n",
       "      <td>0.022199</td>\n",
       "      <td>-0.019420</td>\n",
       "      <td>0.003542</td>\n",
       "      <td>-0.033979</td>\n",
       "      <td>0.018614</td>\n",
       "      <td>0.009059</td>\n",
       "      <td>0.004888</td>\n",
       "      <td>0.036736</td>\n",
       "      <td>0.009313</td>\n",
       "      <td>-0.031245</td>\n",
       "      <td>0.054102</td>\n",
       "      <td>0.034652</td>\n",
       "      <td>0.021098</td>\n",
       "      <td>0.035497</td>\n",
       "      <td>-0.029705</td>\n",
       "      <td>-0.056657</td>\n",
       "      <td>-0.089983</td>\n",
       "      <td>-0.055972</td>\n",
       "      <td>-0.004274</td>\n",
       "      <td>0.026076</td>\n",
       "      <td>-0.042653</td>\n",
       "      <td>0.003976</td>\n",
       "      <td>0.079068</td>\n",
       "      <td>-0.062108</td>\n",
       "      <td>0.026420</td>\n",
       "      <td>-0.045164</td>\n",
       "      <td>0.048578</td>\n",
       "      <td>0.008358</td>\n",
       "      <td>0.087004</td>\n",
       "      <td>-0.027234</td>\n",
       "      <td>-0.010025</td>\n",
       "      <td>-0.040531</td>\n",
       "      <td>0.029940</td>\n",
       "      <td>0.038127</td>\n",
       "      <td>-0.048948</td>\n",
       "      <td>0.003283</td>\n",
       "      <td>-0.000262</td>\n",
       "      <td>-0.050132</td>\n",
       "      <td>-0.052996</td>\n",
       "      <td>0.009624</td>\n",
       "      <td>-0.082127</td>\n",
       "      <td>-0.056914</td>\n",
       "      <td>-0.037447</td>\n",
       "      <td>-0.031959</td>\n",
       "      <td>0.002752</td>\n",
       "      <td>-0.002923</td>\n",
       "      <td>-0.075691</td>\n",
       "      <td>0.023673</td>\n",
       "      <td>-0.043238</td>\n",
       "      <td>0.106030</td>\n",
       "      <td>-0.035911</td>\n",
       "      <td>0.029716</td>\n",
       "      <td>-0.040331</td>\n",
       "      <td>0.015956</td>\n",
       "      <td>0.076108</td>\n",
       "      <td>-0.035181</td>\n",
       "      <td>-0.038755</td>\n",
       "      <td>-0.034328</td>\n",
       "      <td>-0.039819</td>\n",
       "      <td>-0.044165</td>\n",
       "      <td>0.035701</td>\n",
       "      <td>-0.088299</td>\n",
       "      <td>0.004164</td>\n",
       "      <td>0.033278</td>\n",
       "      <td>-0.000155</td>\n",
       "      <td>0.036771</td>\n",
       "      <td>0.024374</td>\n",
       "      <td>0.004721</td>\n",
       "      <td>0.020225</td>\n",
       "      <td>0.044062</td>\n",
       "      <td>-0.002540</td>\n",
       "      <td>-0.024182</td>\n",
       "      <td>0.015682</td>\n",
       "      <td>-0.005126</td>\n",
       "      <td>-0.059687</td>\n",
       "      <td>0.062534</td>\n",
       "      <td>-0.021763</td>\n",
       "      <td>-0.026621</td>\n",
       "      <td>-0.000471</td>\n",
       "      <td>-0.036248</td>\n",
       "      <td>0.027149</td>\n",
       "      <td>-0.035859</td>\n",
       "      <td>-0.019966</td>\n",
       "      <td>-0.050818</td>\n",
       "      <td>0.038344</td>\n",
       "      <td>-0.060490</td>\n",
       "      <td>0.052427</td>\n",
       "      <td>0.052198</td>\n",
       "      <td>-0.065401</td>\n",
       "      <td>-0.013199</td>\n",
       "      <td>0.002640</td>\n",
       "      <td>-0.007894</td>\n",
       "      <td>-0.047487</td>\n",
       "      <td>-0.038146</td>\n",
       "      <td>0.016330</td>\n",
       "      <td>-0.028192</td>\n",
       "      <td>-0.026505</td>\n",
       "      <td>-0.044466</td>\n",
       "      <td>-0.003462</td>\n",
       "      <td>0.072428</td>\n",
       "      <td>-0.018450</td>\n",
       "      <td>-0.035615</td>\n",
       "      <td>-0.035308</td>\n",
       "      <td>0.012763</td>\n",
       "      <td>0.070836</td>\n",
       "      <td>-0.052661</td>\n",
       "      <td>-0.030568</td>\n",
       "      <td>0.009623</td>\n",
       "      <td>0.035733</td>\n",
       "      <td>-0.097965</td>\n",
       "      <td>0.017374</td>\n",
       "      <td>-0.083127</td>\n",
       "      <td>-0.018958</td>\n",
       "      <td>-0.045238</td>\n",
       "      <td>0.025459</td>\n",
       "      <td>0.082861</td>\n",
       "      <td>0.042409</td>\n",
       "      <td>0.046854</td>\n",
       "      <td>0.015115</td>\n",
       "      <td>0.043093</td>\n",
       "      <td>0.010497</td>\n",
       "      <td>-0.006535</td>\n",
       "      <td>0.026580</td>\n",
       "      <td>-0.016915</td>\n",
       "      <td>0.046082</td>\n",
       "      <td>-0.057873</td>\n",
       "      <td>0.019536</td>\n",
       "      <td>-0.017568</td>\n",
       "      <td>-0.044403</td>\n",
       "      <td>0.009632</td>\n",
       "      <td>-0.044836</td>\n",
       "      <td>0.032990</td>\n",
       "      <td>-0.002223</td>\n",
       "      <td>-0.002283</td>\n",
       "      <td>0.042544</td>\n",
       "      <td>-0.031163</td>\n",
       "      <td>-0.045846</td>\n",
       "      <td>0.015638</td>\n",
       "      <td>0.040989</td>\n",
       "      <td>0.046313</td>\n",
       "      <td>-0.034952</td>\n",
       "      <td>-0.066628</td>\n",
       "      <td>0.061346</td>\n",
       "      <td>0.011170</td>\n",
       "      <td>-0.061449</td>\n",
       "      <td>-0.041902</td>\n",
       "      <td>0.022843</td>\n",
       "      <td>0.079199</td>\n",
       "      <td>0.028186</td>\n",
       "      <td>-0.044330</td>\n",
       "      <td>-0.007799</td>\n",
       "      <td>-0.000295</td>\n",
       "      <td>-0.060964</td>\n",
       "      <td>0.001387</td>\n",
       "      <td>0.048243</td>\n",
       "      <td>0.047174</td>\n",
       "      <td>0.009617</td>\n",
       "      <td>-0.085751</td>\n",
       "      <td>0.064028</td>\n",
       "      <td>-0.036543</td>\n",
       "      <td>0.066259</td>\n",
       "      <td>-0.066376</td>\n",
       "      <td>0.020403</td>\n",
       "      <td>-0.112481</td>\n",
       "      <td>0.058689</td>\n",
       "      <td>0.004924</td>\n",
       "      <td>0.037340</td>\n",
       "      <td>-0.031089</td>\n",
       "      <td>0.003641</td>\n",
       "      <td>-0.007853</td>\n",
       "      <td>0.089649</td>\n",
       "      <td>-0.050396</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>3512 rows × 512 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                    0         1         2         3         4         5         6         7         8         9         10        11        12        13        14        15        16        17        18        19        20        21        22        23        24        25        26        27        28        29        30        31        32        33        34        35        36        37        38        39        40        41        42        43        44        45        46        47        48        49        50        51        52        53        54        55        56        57        58        59        60        61        62        63        64        65        66        67        68        69        70        71        72        73        74        75        76        77        78        79        80        81        82        83        84        85        86        87        88        89        90        91        92        93        94        95        96        97        98        99        100       101       102       103       104       105       106       107       108       109       110       111       112       113       114       115       116       117       118       119       120       121       122       123       124       125       126       127       128       129       130       131       132       133       134       135       136       137       138       139       140       141       142       143       144       145       146       147       148       149       150       151       152       153       154       155       156       157       158       159       160       161       162       163       164       165       166       167       168       169       170       171       172       173       174       175       176       177       178       179       180       181       182       183       184       185       186       187       188       189       190       191       192       193       194       195       196       197  \\\n",
       "argument      -0.057584  0.000001  0.055926  0.025750 -0.052444  0.034983  0.072001 -0.062773  0.030634  0.023789 -0.030285 -0.016392  0.016137  0.018487 -0.045962 -0.081218  0.008702  0.087169 -0.019983  0.016165 -0.043445  0.015750  0.040670  0.019039 -0.030091  0.067343 -0.035152 -0.050932 -0.050478  0.007857 -0.023909 -0.036066 -0.063819  0.013660 -0.014213  0.032831  0.000128  0.022295  0.008915  0.029342 -0.039961 -0.011504 -0.005429 -0.018344  0.014334 -0.029569 -0.039182 -0.017273  0.018854 -0.044364 -0.009508 -0.038428  0.079336  0.033354 -0.017089  0.014819 -0.034245 -0.020829  0.073357  0.018781  0.017274  0.027938 -0.058285  0.033429  0.038148  0.050036 -0.042139 -0.039587  0.062817  0.018287 -0.049389 -0.006416  0.011627  0.043233 -0.030799  0.045815 -0.031661  0.017208  0.046690  0.001913  0.039871  0.023903 -0.036332 -0.020771 -0.007836  0.065198 -0.034911  0.015282 -0.012333  0.028438  0.031518 -0.019205 -0.062143 -0.015026  0.054232  0.005134  0.049093 -0.014332  0.045847 -0.008020  0.004010 -0.046933  0.040072 -0.004521  0.005188 -0.010432 -0.014471  0.024448 -0.023686  0.004382  0.004548 -0.079834 -0.032111  0.024725 -0.007961 -0.015412  0.007960  0.043120 -0.000988  0.085556 -0.087542  0.027295  0.024891 -0.052877  0.050715  0.041792 -0.029795 -0.058189 -0.063624  0.034922 -0.059336  0.023104  0.046315  0.058178  0.033994  0.054983  0.086336 -0.060357 -0.057711  0.042543 -0.040400 -0.031473  0.030364 -0.046617  0.000323 -0.003256 -0.023044 -0.019966  0.065511 -0.036345 -0.021797 -0.000303 -0.054354  0.008926 -0.061933 -0.069910 -0.071256 -0.071903 -0.064597  0.018952  0.009328 -0.000932  0.050384 -0.028000  0.038387  0.045326 -0.062275 -0.051920  0.024715  0.028655 -0.041929 -0.075593 -0.051104 -0.061172  0.004018  0.023573  0.047923 -0.012488  0.018772  0.066610 -0.045729  0.020685 -0.059596  0.059906 -0.006771 -0.010002 -0.010421  0.034321 -0.067768 -0.058326  0.077001  0.012309  0.013914  0.038823  0.062681 -0.090058  0.053686 -0.003382   \n",
       "awful         -0.076146  0.023700  0.018652  0.065849  0.031395  0.003467  0.067020 -0.051645 -0.035994  0.032914  0.019309  0.045878 -0.051007  0.016304  0.051063  0.041033 -0.020460  0.006427  0.002416 -0.065817  0.036337  0.047262 -0.011263 -0.024436 -0.038048  0.034624  0.050856  0.000241 -0.024685  0.019189  0.068494  0.011564  0.034836 -0.020168 -0.034911  0.002077  0.073511 -0.030884 -0.010971  0.042874  0.036954  0.007873 -0.039832 -0.014288  0.005792 -0.057307 -0.037924  0.042260 -0.008010  0.072971  0.068897 -0.083284  0.082698  0.083827  0.029966 -0.017867  0.056651 -0.027699  0.010190 -0.048413  0.019944 -0.030172 -0.041436  0.011228  0.030549  0.073059  0.066297 -0.078503  0.036721  0.008179 -0.029867  0.067916  0.038127 -0.062772  0.037611  0.035925 -0.052314  0.042182  0.023702  0.065532 -0.025242 -0.013086 -0.031029 -0.014235 -0.076955 -0.013248  0.044102 -0.031263  0.005484  0.005979  0.050421  0.063695  0.032133  0.016594 -0.046370  0.002882 -0.060153  0.046705 -0.047750 -0.043766 -0.006674 -0.062031  0.043071 -0.003301 -0.018319 -0.045059 -0.034041  0.012005 -0.067164  0.057331 -0.028019 -0.093891 -0.029152 -0.067930  0.044273  0.085499 -0.024145 -0.062524 -0.004840 -0.004902 -0.010149  0.052071 -0.039184 -0.003743  0.037911 -0.026832 -0.018096  0.017694  0.065242 -0.030636 -0.027106 -0.007936  0.038585  0.026767  0.075463 -0.061721  0.047792 -0.001146 -0.035391 -0.026613  0.058524  0.012451  0.009211 -0.045514  0.070426  0.016005 -0.008694  0.001654 -0.060517 -0.031460 -0.016749  0.024138 -0.059874 -0.050092 -0.041756 -0.000136  0.024711 -0.061333 -0.048236  0.020967  0.025715 -0.034998  0.080872 -0.070894  0.078175 -0.006992 -0.034740 -0.049206  0.072300 -0.058618  0.001261 -0.093785  0.035789 -0.065793 -0.046774  0.015142  0.047012 -0.000676 -0.009646 -0.088854  0.043970  0.032780 -0.067831 -0.029021  0.039883 -0.040123 -0.024022 -0.060763  0.030862  0.012750 -0.015573  0.064420  0.079513  0.032668 -0.031097 -0.087684  0.024130  0.037422   \n",
       "decorator      0.015437  0.065674  0.031793 -0.002947 -0.019215  0.056831  0.002746  0.029131  0.064465 -0.044334  0.026005  0.010356  0.065691 -0.075613  0.039199  0.059256  0.004944  0.069531  0.020214 -0.019658  0.055134  0.068949  0.020131 -0.074199  0.039254 -0.050819 -0.018112 -0.069238  0.023453 -0.009353 -0.037261  0.036599  0.019584  0.038329  0.002487 -0.039646  0.042414 -0.033027 -0.031804  0.033814  0.023004 -0.024751 -0.038306 -0.041249 -0.072157  0.052488 -0.006171  0.046659  0.076298 -0.006558  0.037305  0.004132  0.060841 -0.055396  0.038871 -0.080222 -0.020556 -0.037200  0.069074 -0.055814  0.054261  0.021472 -0.069792  0.042697  0.063749  0.029979 -0.074923 -0.034402 -0.005997  0.028390 -0.067814 -0.045679 -0.003550 -0.030747  0.016075 -0.056769 -0.065840 -0.009134  0.041753  0.026851 -0.043075  0.025882 -0.079814 -0.038577  0.073663 -0.002741 -0.005279 -0.044670  0.044587 -0.050850  0.019114 -0.043696  0.075536  0.047575 -0.060941 -0.017738 -0.038199 -0.077056 -0.021951 -0.078204  0.015555 -0.011308 -0.032753  0.040401  0.007392 -0.059576  0.024170 -0.005230 -0.025785  0.004306  0.035214  0.029278  0.018957 -0.047952  0.016966 -0.040710 -0.042458  0.028166 -0.011035  0.033820 -0.044929  0.012219  0.051090 -0.054629 -0.061649  0.072111 -0.082227  0.071163 -0.069223 -0.007345  0.024400  0.068508  0.063566  0.020393 -0.003837  0.009066  0.027641 -0.054281 -0.003913  0.058263 -0.086442  0.009382  0.045873 -0.076577  0.041925 -0.069554  0.032639 -0.030950 -0.016383 -0.048456 -0.001511  0.028026 -0.065152  0.052603 -0.055091 -0.008152  0.015010 -0.013831  0.055248  0.008181  0.028924 -0.044680  0.048349  0.011497 -0.005242  0.010580  0.034806 -0.055018  0.025380 -0.025614 -0.028189  0.020514  0.008817 -0.064089 -0.014137  0.054572  0.065426 -0.051268  0.010069 -0.005362 -0.079989  0.068649  0.034645  0.049123  0.005019  0.026484 -0.069465  0.053890  0.039131  0.069294  0.064780 -0.044164  0.064746 -0.048647 -0.041526 -0.015924  0.018057  0.041440   \n",
       "misconception -0.034154  0.056789  0.030497 -0.078713 -0.026613  0.067809 -0.016281 -0.076655 -0.037526  0.081971  0.027423  0.002251 -0.006066  0.003941 -0.001951 -0.027527  0.034397  0.047273 -0.004808 -0.053303 -0.028236  0.042453  0.009182 -0.030765  0.036382  0.043928 -0.009203 -0.008126  0.074062  0.040288  0.045802  0.002905 -0.044374  0.015171 -0.066765 -0.001422 -0.061134  0.046754 -0.013790  0.070782 -0.029035 -0.004827 -0.002575 -0.029278 -0.054538 -0.070414 -0.086206  0.051062  0.011531  0.029132  0.054043 -0.050168  0.079337 -0.017831  0.027501 -0.049313  0.044035 -0.054718  0.081314 -0.004593 -0.060233  0.005465  0.078469 -0.029092 -0.064353 -0.027618  0.031682 -0.017906  0.038917  0.079529  0.046470 -0.033299 -0.045568 -0.070669  0.023175 -0.013373  0.002263  0.041046  0.005996 -0.073019  0.025884  0.065306 -0.046600  0.002057 -0.003763  0.022454  0.074203  0.001770 -0.041545  0.046130  0.034818 -0.028614  0.026789  0.013958 -0.020135  0.057529 -0.081354  0.057267  0.011461 -0.052128  0.025705 -0.059955  0.027711 -0.013011  0.007698 -0.010456 -0.031352 -0.038039 -0.074499  0.017237  0.057172 -0.082175 -0.086558 -0.034316  0.013059 -0.004027  0.004249  0.054917  0.071742 -0.011617 -0.076143  0.044475  0.008609  0.010895  0.017432  0.036195  0.015971 -0.019197 -0.066839 -0.009980 -0.065841 -0.006096  0.058108  0.013299 -0.055490  0.041030  0.075199  0.045329 -0.002646  0.017002 -0.052113 -0.013220  0.030502 -0.046650  0.012063 -0.071703 -0.076647 -0.002664  0.042771 -0.043813  0.015949  0.042224 -0.067835  0.001561 -0.067934 -0.030295 -0.070409 -0.079169 -0.026560  0.027638  0.026364 -0.047205  0.018246 -0.020594  0.049088 -0.010302 -0.057182 -0.005701  0.009216  0.009676 -0.033836 -0.082241  0.057516 -0.017868 -0.031307  0.018975  0.068723 -0.000034  0.054391  0.017167 -0.027566  0.058595 -0.070371 -0.046508 -0.029141  0.023633 -0.014438  0.005827  0.021120 -0.061500  0.009427  0.022469 -0.024213  0.051502  0.009010 -0.050925  0.063563 -0.016744   \n",
       "crab          -0.016708  0.002911  0.007000  0.015318 -0.006244 -0.057960 -0.042535 -0.033015 -0.038489  0.019487  0.072698 -0.065903 -0.000874 -0.012989 -0.061724 -0.036529 -0.036080  0.011276  0.046366 -0.063745 -0.004769  0.045863  0.011048 -0.013588 -0.003097  0.054539  0.052981  0.056658  0.046551 -0.036492 -0.021686 -0.072016  0.061624 -0.065766 -0.059771  0.040640  0.009580  0.052817 -0.021747 -0.018621  0.008989 -0.075750  0.056454 -0.058991 -0.080451  0.044077 -0.056637 -0.022220  0.016940 -0.045077  0.079832 -0.052661  0.071158  0.047757  0.077209 -0.069621 -0.031901 -0.027668  0.039575 -0.027541 -0.000462 -0.013485 -0.004151  0.056259 -0.038508  0.055647 -0.012054 -0.003845  0.039164  0.036182 -0.046892  0.028439  0.062893 -0.051784 -0.000144 -0.005228 -0.005999  0.065260  0.030990  0.021756 -0.029967  0.070186 -0.074550  0.025069 -0.075702 -0.048358  0.007014  0.012645  0.013348 -0.061626 -0.035409  0.036397  0.024200 -0.001777  0.022490 -0.023139 -0.024822 -0.063044  0.032666  0.020477  0.024427 -0.025261 -0.060829 -0.030101  0.054820  0.004361  0.042388 -0.019440  0.025787  0.058766 -0.062082  0.040878 -0.044142  0.042879  0.054031  0.061690 -0.057573 -0.062365 -0.064791 -0.017471  0.079737 -0.060824  0.005194 -0.065564 -0.032914  0.071133 -0.051208  0.036035 -0.049490 -0.043829 -0.045300  0.078517  0.062225  0.072204 -0.014592  0.025537  0.064917 -0.006360  0.004579 -0.077026  0.007326 -0.053800  0.043089 -0.059520  0.022654  0.071411  0.038626 -0.003498 -0.063165  0.059374  0.062276  0.002944 -0.062867 -0.036747  0.033791 -0.001102  0.038814 -0.062954  0.015709 -0.010734 -0.056800 -0.066593 -0.017201 -0.000705 -0.015755 -0.033986  0.012237 -0.029840  0.020716  0.002799 -0.032390 -0.023541  0.034555 -0.022887 -0.048504 -0.033019  0.054788  0.051276 -0.021503 -0.011998  0.043651  0.015641  0.030144 -0.067091 -0.005969  0.068762 -0.077857  0.049406 -0.004070  0.038430  0.047602  0.060042  0.067295 -0.032528 -0.019642  0.035619  0.075774 -0.033484   \n",
       "...                 ...       ...       ...       ...       ...       ...       ...       ...       ...       ...       ...       ...       ...       ...       ...       ...       ...       ...       ...       ...       ...       ...       ...       ...       ...       ...       ...       ...       ...       ...       ...       ...       ...       ...       ...       ...       ...       ...       ...       ...       ...       ...       ...       ...       ...       ...       ...       ...       ...       ...       ...       ...       ...       ...       ...       ...       ...       ...       ...       ...       ...       ...       ...       ...       ...       ...       ...       ...       ...       ...       ...       ...       ...       ...       ...       ...       ...       ...       ...       ...       ...       ...       ...       ...       ...       ...       ...       ...       ...       ...       ...       ...       ...       ...       ...       ...       ...       ...       ...       ...       ...       ...       ...       ...       ...       ...       ...       ...       ...       ...       ...       ...       ...       ...       ...       ...       ...       ...       ...       ...       ...       ...       ...       ...       ...       ...       ...       ...       ...       ...       ...       ...       ...       ...       ...       ...       ...       ...       ...       ...       ...       ...       ...       ...       ...       ...       ...       ...       ...       ...       ...       ...       ...       ...       ...       ...       ...       ...       ...       ...       ...       ...       ...       ...       ...       ...       ...       ...       ...       ...       ...       ...       ...       ...       ...       ...       ...       ...       ...       ...       ...       ...       ...       ...       ...       ...       ...       ...       ...       ...       ...       ...       ...       ...       ...       ...       ...       ...   \n",
       "vaccinate      0.041393 -0.053996  0.006270 -0.031008 -0.043719  0.038320  0.027678 -0.000121 -0.043901  0.006641  0.082772  0.075204 -0.016138 -0.017668 -0.057705  0.048859 -0.082415 -0.060692  0.001952 -0.070204 -0.026326  0.022189 -0.063208  0.075920  0.053428  0.080072  0.020538 -0.025106  0.018793 -0.015768 -0.035460  0.082776 -0.014979  0.029658  0.074225  0.046354 -0.059199  0.014226 -0.033579 -0.012409  0.013193  0.036875  0.004690 -0.054965 -0.067827  0.031122  0.026535  0.033968 -0.051577  0.060606 -0.059317 -0.011460  0.076966  0.047517 -0.078353 -0.010326  0.010784 -0.056559  0.005060 -0.012383 -0.056605 -0.040747 -0.077148  0.011394  0.021280  0.037187  0.021617 -0.033748  0.009538  0.065655 -0.059822 -0.014842 -0.036477  0.060701  0.059079  0.030162  0.030193 -0.033122 -0.077544 -0.019550  0.068423 -0.030209  0.005250  0.029243 -0.059771 -0.035753 -0.003516  0.026230 -0.029676 -0.067693 -0.045914  0.036132 -0.028665  0.059655 -0.046660  0.029455  0.009680 -0.041362  0.061414  0.043852  0.030574 -0.031862  0.035690  0.034221  0.062470  0.060958  0.059123 -0.007372  0.053796 -0.045344  0.037147 -0.021527  0.000291  0.062329  0.041251 -0.003925 -0.057923 -0.015850 -0.022617  0.010191  0.004627 -0.060337  0.023784 -0.044664  0.045218  0.058547 -0.015792  0.013244 -0.023951  0.017615  0.013677  0.082859  0.069877  0.061769  0.028572  0.009356  0.034798  0.033531  0.031690  0.019452  0.005719 -0.018034 -0.036908  0.024841  0.016815  0.062610  0.001348 -0.067835  0.056583  0.043135  0.047916  0.016387 -0.063827 -0.041877  0.000339 -0.004294 -0.029280 -0.037563  0.024057  0.063139  0.029266  0.041019  0.036688 -0.019191 -0.043240 -0.005434 -0.062924 -0.016879 -0.013983 -0.029432  0.011711 -0.070706  0.030219  0.029914  0.055713  0.035761  0.007882 -0.042309 -0.046125 -0.012273 -0.064084 -0.002575  0.068239 -0.047888  0.052742 -0.057032  0.002457 -0.039006 -0.038642  0.039950 -0.044314  0.017698  0.047405 -0.004988  0.075874 -0.016020  0.050875 -0.048873   \n",
       "boastful       0.006982 -0.036990  0.001479  0.010849 -0.012228  0.074476 -0.012781 -0.065526  0.006991  0.054437 -0.008508  0.014751  0.010516 -0.001452 -0.050290 -0.088938  0.065113 -0.019750  0.016942 -0.034655  0.025398  0.068590  0.052124 -0.072730 -0.049247 -0.018123  0.027242  0.007443  0.068252  0.021166  0.033197 -0.013984 -0.037331 -0.026236  0.015629  0.088569  0.032182  0.009855 -0.005393 -0.046501 -0.036995 -0.054499  0.031506 -0.032422 -0.038621  0.033319 -0.048318 -0.012861 -0.046348  0.029922  0.021805 -0.022052  0.092005  0.070355 -0.036538 -0.035853 -0.046953 -0.059532 -0.025107  0.005947 -0.057831  0.042697 -0.059668  0.050734 -0.046341  0.050468  0.007135 -0.040002  0.033393  0.030829  0.002263  0.013901 -0.044483 -0.049996  0.086839  0.049652 -0.064902  0.010027  0.045391  0.025019  0.022550  0.074197 -0.066880 -0.046477 -0.015914 -0.017991 -0.039582  0.000153  0.047411 -0.039077  0.026213  0.004772  0.006327  0.078806 -0.038787  0.029122 -0.070279 -0.042052  0.030344  0.020169 -0.045350 -0.052541 -0.014947 -0.057364  0.061809  0.027518 -0.043141 -0.015691  0.049883  0.050189  0.018904 -0.032533 -0.035071 -0.032948 -0.033646 -0.013204 -0.012885  0.035386 -0.026496  0.014493 -0.052952  0.062394 -0.060120  0.007572 -0.054795  0.032192  0.020441 -0.069634 -0.007907 -0.022994 -0.025549 -0.035994  0.110628  0.108779  0.023031 -0.029957  0.054660  0.043424  0.010320 -0.065601 -0.087687 -0.076916  0.033915 -0.063269  0.039957 -0.057189 -0.060155  0.023702 -0.038123 -0.064914  0.054965  0.029572 -0.062713 -0.038532 -0.062120  0.030562 -0.056484 -0.005805  0.028380 -0.020050  0.023675 -0.062436 -0.014559 -0.025000  0.023563 -0.012469 -0.016942 -0.040821  0.008688 -0.073647 -0.064165  0.011827  0.004716 -0.018712  0.040118  0.048953  0.009917  0.030175  0.034624 -0.039287 -0.074250  0.031717  0.047116 -0.015456  0.022567 -0.027981  0.011419 -0.052313  0.029638 -0.024809  0.044479  0.000930  0.078747 -0.020694  0.002874 -0.062905  0.072150 -0.004347   \n",
       "phrase        -0.003138  0.030418 -0.006425  0.014352 -0.070801  0.048150 -0.008635 -0.063470  0.047201  0.021414 -0.000467  0.028158 -0.057286  0.098894 -0.048890 -0.098928  0.001677  0.051327  0.000741  0.017630 -0.037922  0.041874 -0.026975  0.007114 -0.065540  0.081049  0.035803 -0.071655 -0.045488  0.033470  0.048512  0.041930 -0.057332  0.017142 -0.077912  0.063516  0.037124  0.042705  0.006179  0.071375 -0.031287  0.026137  0.003060 -0.018246 -0.059071 -0.052765 -0.007959  0.046127 -0.004568 -0.037286  0.058644  0.037510  0.065247  0.086072 -0.003905 -0.006074 -0.035908 -0.020085  0.088365  0.013332  0.035841 -0.013260  0.018352  0.018212 -0.043356 -0.017639  0.051288 -0.022227  0.035003  0.015287 -0.020594 -0.022565  0.012481 -0.015975  0.008228  0.003335 -0.021644 -0.017359 -0.040477 -0.081152  0.013028  0.025296 -0.054374 -0.003910 -0.018223  0.055226 -0.009935  0.009641 -0.028406 -0.044988  0.037730  0.015193 -0.059926  0.059355 -0.005535 -0.028499 -0.100738  0.043482  0.016480 -0.085097  0.011936 -0.057385 -0.013471 -0.072598  0.006957  0.000639 -0.048620  0.068481 -0.061683  0.026060 -0.000266  0.048690 -0.014723 -0.032318 -0.043484 -0.030203 -0.002099 -0.014317  0.076883  0.020761 -0.061550  0.041207 -0.010029 -0.031426  0.016105  0.060795 -0.050861 -0.005650  0.060047  0.038969 -0.054519 -0.037378  0.084429  0.074229  0.031954 -0.031302  0.061995  0.012477 -0.063725  0.037402  0.001799 -0.057067  0.070693 -0.017125  0.012697 -0.020109  0.000914  0.003881 -0.032686  0.001576 -0.053798 -0.051374 -0.051883 -0.020765 -0.045119 -0.070013  0.014367 -0.028490 -0.035113 -0.007883  0.030466  0.029727 -0.004741  0.034916  0.050465  0.073584 -0.010026  0.028855 -0.029971 -0.024800  0.002285  0.056239  0.043059  0.040852  0.027440  0.074019  0.066492  0.047877  0.069079 -0.033940  0.051583  0.031251  0.007934 -0.004190  0.037372  0.020994  0.009656  0.017828  0.004404 -0.046655 -0.008486  0.014866  0.037617 -0.035600 -0.004711 -0.092478  0.026486  0.046783   \n",
       "ski           -0.008974  0.020815 -0.047432  0.021539 -0.015312 -0.025122  0.034103 -0.030542 -0.026442 -0.024509 -0.017093  0.032973 -0.017679 -0.018202  0.010089 -0.037003 -0.009756  0.002367  0.056437 -0.072757  0.005942  0.012579 -0.031167  0.022203 -0.065834 -0.041644  0.011557  0.042556  0.047965 -0.008015 -0.031593 -0.013723 -0.062006  0.087361  0.043069  0.026226  0.035345 -0.019299  0.019100  0.003141 -0.080889  0.022128  0.012253 -0.054943 -0.064085 -0.003503 -0.038121  0.060335  0.028242  0.060217  0.016821  0.033607  0.086428  0.082846 -0.047861  0.030666  0.042293 -0.002252  0.077051  0.068754 -0.040835 -0.044521 -0.031198  0.062234 -0.010204  0.039655 -0.053825 -0.058385  0.079534  0.083434 -0.062137  0.007997 -0.061859 -0.036210  0.043976  0.023804  0.044386  0.006291 -0.026251 -0.014913  0.087045 -0.055562 -0.067832 -0.015189 -0.030816  0.011586 -0.026496  0.058349 -0.035090 -0.066722 -0.008994 -0.001476  0.037901  0.047203  0.012038 -0.015739 -0.015612 -0.075097 -0.020288 -0.007937  0.023888  0.016882 -0.017812 -0.081762  0.076951 -0.016334 -0.071526 -0.038650  0.019098  0.010544 -0.024124  0.039458 -0.057073 -0.027149  0.024234  0.049108  0.027225  0.009638  0.035895 -0.006994 -0.054212  0.014315  0.087423  0.021571  0.002588  0.052823 -0.005064  0.047312 -0.047794  0.026870  0.028912  0.057800  0.079127  0.063761  0.051278  0.017975  0.065606 -0.045440  0.007859 -0.054577 -0.007618 -0.003850  0.058273 -0.048575 -0.007642  0.063966  0.048158  0.071948 -0.023364  0.035273  0.031278  0.036723 -0.043487  0.006216 -0.041597 -0.053914 -0.051317 -0.026777 -0.006696  0.062884 -0.074343 -0.018255 -0.040314  0.018934 -0.037247 -0.003372  0.073950 -0.024987  0.000660 -0.054597 -0.027455  0.034847 -0.001288 -0.067958  0.065470 -0.008634  0.071443  0.006304 -0.010923 -0.025893 -0.016366  0.062647  0.001261 -0.021238  0.048750 -0.055133  0.070293 -0.053714  0.043544  0.040890 -0.028262  0.019744  0.078410  0.054656 -0.006145 -0.032923 -0.049432 -0.010649   \n",
       "commanding    -0.047657 -0.055279  0.015045  0.037645  0.048102  0.056537 -0.018677 -0.033269  0.080872  0.004001  0.062216 -0.007820  0.000493 -0.012361 -0.034251  0.006213  0.027879  0.060953 -0.032899  0.055724 -0.022387  0.028922 -0.007465 -0.005556  0.011620  0.028660 -0.036908 -0.016999 -0.007514  0.011126 -0.000177 -0.004316 -0.014634  0.022119  0.069210  0.087923  0.042635  0.013411  0.006325  0.021378 -0.020463  0.023261  0.027898 -0.068612 -0.040640 -0.003444 -0.043978 -0.019669  0.036172  0.038745  0.055832 -0.059093  0.078977  0.060202 -0.041122 -0.027509  0.001034 -0.044381  0.014997 -0.052238  0.071739 -0.000627 -0.068639  0.047556  0.003749  0.002379 -0.071026  0.007314  0.077278  0.046022 -0.049779  0.024154  0.020324  0.036095  0.034330 -0.001989 -0.063054  0.014894  0.034280 -0.077720 -0.031121  0.050847 -0.057273  0.014884 -0.020008  0.023417 -0.002270  0.004734  0.026899  0.043444 -0.018541 -0.006400  0.013930  0.026436 -0.045634  0.021222 -0.034946 -0.043824 -0.007162  0.028520 -0.000777 -0.087618 -0.069882 -0.059919 -0.036418 -0.057232  0.029378  0.009764  0.016425  0.083470  0.010189  0.022004 -0.007916 -0.078676 -0.054208  0.017349 -0.041437  0.002205 -0.103048 -0.033330 -0.079495 -0.018680  0.046055 -0.012142  0.009600  0.067915  0.007821  0.016756 -0.030487  0.041920 -0.041492  0.041409  0.098197  0.053186 -0.028603  0.043265  0.010515  0.055092 -0.007546  0.056360  0.024749 -0.022586  0.052509 -0.017359 -0.063103  0.045872 -0.004469 -0.004066  0.006999 -0.072281  0.028378  0.026574 -0.032156  0.026939 -0.014808 -0.039717 -0.056386 -0.048155  0.077477  0.103643  0.025085 -0.037667 -0.053172  0.015718  0.007960  0.017072 -0.041063 -0.042034  0.036059 -0.049805 -0.065689  0.012487  0.057844 -0.055098 -0.078731  0.066046 -0.012303 -0.024044 -0.037923 -0.023551 -0.046415 -0.081134  0.037937 -0.019659 -0.020886  0.050087  0.050113 -0.004979 -0.033201 -0.071474  0.002224  0.038036  0.082319  0.035441  0.072397 -0.067724  0.037494  0.016616   \n",
       "\n",
       "                    198       199       200       201       202       203       204       205       206       207       208       209       210       211       212       213       214       215       216       217       218       219       220       221       222       223       224       225       226       227       228       229       230       231       232       233       234       235       236       237       238       239       240       241       242       243       244       245       246       247       248       249       250       251       252       253       254       255       256       257       258       259       260       261       262       263       264       265       266       267       268       269       270       271       272       273       274       275       276       277       278       279       280       281       282       283       284       285       286       287       288       289       290       291       292       293       294       295       296       297       298       299       300       301       302       303       304       305       306       307       308       309       310       311       312       313       314       315       316       317       318       319       320       321       322       323       324       325       326       327       328       329       330       331       332       333       334       335       336       337       338       339       340       341       342       343       344       345       346       347       348       349       350       351       352       353       354       355       356       357       358       359       360       361       362       363       364       365       366       367       368       369       370       371       372       373       374       375       376       377       378       379       380       381       382       383       384       385       386       387       388       389       390       391       392       393       394       395  \\\n",
       "argument       0.015303 -0.051602 -0.041136 -0.052476  0.023577  0.081222 -0.057633  0.024575 -0.070626 -0.058188 -0.040819 -0.009062 -0.082470 -0.013702  0.042660 -0.021233 -0.024118 -0.027460 -0.021096 -0.062413  0.007989  0.001720  0.045124 -0.059831  0.017545 -0.015112  0.010197  0.009643 -0.029225  0.043299  0.012116 -0.029709 -0.012300 -0.005173  0.044378  0.096184 -0.037672  0.044596  0.027317 -0.028190 -0.005828  0.009645  0.042948 -0.063338 -0.078461 -0.032988  0.058270  0.071955 -0.037694  0.059929  0.071091 -0.009234  0.019456 -0.029141  0.027986 -0.049155  0.010092 -0.001152  0.074553 -0.005894  0.013841 -0.020429 -0.038924  0.063172 -0.024724 -0.063695  0.022492  0.007181 -0.007627  0.002298  0.064664  0.010802  0.072767  0.026134 -0.017315  0.067959 -0.006620 -0.068015  0.014293 -0.062352  0.075880  0.007846  0.054692 -0.005531  0.050767 -0.030855 -0.013326  0.072889  0.049295  0.035683 -0.039169 -0.050700 -0.014010  0.038789 -0.071818 -0.042616  0.022737  0.056538 -0.010876 -0.051784  0.024468 -0.069797  0.028796 -0.008048 -0.090181 -0.066201  0.019653  0.079588  0.044652 -0.063610 -0.032376 -0.014145  0.030291 -0.024217 -0.022981 -0.025790 -0.022492  0.004969 -0.004608  0.036844 -0.084480 -0.032229  0.007105  0.024804 -0.016186  0.043481 -0.031115 -0.047650  0.027854  0.077159 -0.042288 -0.035300  0.013603  0.042379 -0.039202  0.076130 -0.043431 -0.029254 -0.072606 -0.052107  0.017179  0.003263  0.027721  0.014097  0.070146 -0.066576 -0.077218  0.037960  0.059802  0.019573 -0.025816  0.027238  0.079769 -0.029708 -0.006219 -0.014115 -0.030580  0.013314 -0.089792 -0.052086  0.045907 -0.069082  0.054834 -0.027700  0.084317 -0.063174  0.001991 -0.033803 -0.004145  0.018333  0.088532 -0.016738 -0.034501  0.030454  0.006947  0.046070  0.002859 -0.084782  0.067521 -0.079398 -0.051415 -0.085916 -0.061763 -0.065935 -0.090115 -0.021330  0.061905 -0.079906  0.039929  0.054689 -0.004971  0.012030  0.062726  0.046365  0.073986 -0.075979 -0.053222 -0.027387   \n",
       "awful          0.010016  0.063889 -0.063342  0.049380  0.027706  0.000200 -0.069464  0.019735 -0.055534  0.020007 -0.048360 -0.012899 -0.039110  0.003028 -0.022496 -0.026989 -0.079784  0.040686  0.039482 -0.069832 -0.004059  0.006077 -0.027229 -0.004972 -0.012753  0.039188 -0.077668  0.091806  0.031729  0.056743  0.002037 -0.026558  0.032215 -0.050665  0.017084  0.094173  0.066535  0.016531 -0.046306 -0.052921 -0.092610  0.033968  0.009872 -0.017931 -0.000709 -0.041260  0.028534  0.090091 -0.019826 -0.041295  0.022478 -0.081318  0.048759  0.085806  0.026717 -0.009958  0.012046 -0.013678  0.018020 -0.033809 -0.009707 -0.003284 -0.036073  0.023484 -0.073450  0.033753  0.035960 -0.045858  0.009989 -0.058249  0.073809  0.005028  0.082788  0.085546  0.036497  0.032613  0.041649 -0.042294 -0.049808  0.002953  0.087391  0.082777 -0.057213 -0.031808  0.011665  0.004209  0.037820  0.083069  0.056791  0.005112 -0.050093  0.047322  0.028997  0.092261 -0.079828 -0.034951  0.053753 -0.030682  0.026563  0.045139  0.066788  0.021184  0.084609 -0.035686  0.041314 -0.004158  0.034212  0.016547 -0.003487 -0.000089 -0.020263 -0.059818 -0.040742 -0.075174  0.001203  0.001711  0.047951 -0.005222  0.018197  0.064222 -0.022703  0.039388  0.021222 -0.075000  0.060255  0.006474 -0.007914  0.060536  0.004895 -0.042214 -0.039466  0.023853 -0.002251 -0.038294  0.022688  0.055275 -0.040939  0.011868 -0.008202  0.028161 -0.041423  0.018889 -0.000651  0.009558 -0.018608 -0.037636  0.042375 -0.025393  0.043634  0.002574 -0.048387 -0.004234 -0.006132 -0.066571 -0.019887  0.010871 -0.016940 -0.034968 -0.088767  0.046514 -0.026038 -0.007387  0.045358 -0.032535  0.093300  0.051422 -0.025148 -0.040950  0.003125  0.053990  0.075959 -0.031078  0.002818 -0.042940 -0.033236 -0.036373 -0.061697 -0.069972  0.031146  0.042176 -0.017943 -0.054916 -0.004784 -0.042544  0.011832 -0.007422  0.093396 -0.061648 -0.017691 -0.023652  0.010603 -0.002925  0.068141 -0.048737  0.031473  0.025743 -0.003023 -0.029445   \n",
       "decorator     -0.048777 -0.000328 -0.036432  0.066196 -0.078564 -0.014947 -0.031295  0.020629  0.035372  0.052295 -0.029933 -0.005077 -0.050017  0.081178 -0.068810 -0.061950 -0.026568 -0.065405 -0.020080  0.049664 -0.066542  0.016401 -0.001722 -0.017019 -0.075020 -0.020728  0.041361 -0.000341 -0.043822 -0.030990 -0.052294 -0.019539  0.058909  0.002056  0.018513  0.090231  0.048884  0.053404  0.007118 -0.073676 -0.060466  0.054085  0.019149 -0.075739  0.067402  0.014897  0.041953 -0.032033 -0.016655  0.047979 -0.067920  0.069193  0.006897 -0.069890  0.025095  0.051158 -0.034050  0.015111  0.034174  0.014129 -0.020497  0.030682 -0.069037 -0.000781  0.075952 -0.032652  0.043115  0.054799  0.029659  0.046451  0.068551 -0.049734  0.013996 -0.071325 -0.029056  0.008925 -0.062857 -0.065307 -0.066391 -0.052747  0.077071  0.049022  0.066229 -0.014651 -0.051110 -0.014207 -0.042188 -0.011078  0.048341  0.033682  0.000767 -0.029926 -0.030757 -0.043596  0.048538 -0.027174  0.018813  0.039662 -0.018775 -0.036942 -0.056748  0.005569  0.046507  0.019279  0.038329 -0.005456 -0.010993  0.014443 -0.059218  0.037260  0.003454 -0.013390 -0.036825  0.029628  0.054219  0.038619  0.065447  0.071682  0.044861  0.001633 -0.070580  0.054042 -0.055432  0.009267 -0.012783  0.026901 -0.049928 -0.070214  0.041941  0.042372 -0.028134  0.023039  0.009851 -0.051295  0.061595  0.071139 -0.044223  0.024194 -0.014387  0.025996  0.000863  0.025795  0.003621  0.020872  0.076698 -0.028938 -0.043122 -0.045511  0.033978 -0.042647 -0.069417  0.007300 -0.004394  0.048451 -0.057236  0.016411 -0.067716 -0.065516 -0.083176  0.082927 -0.021818 -0.075622 -0.001700  0.054432  0.023115  0.013354 -0.017812 -0.057256  0.050663 -0.007543  0.006424 -0.039833  0.031752  0.018204  0.022471  0.054691 -0.070720  0.004839  0.008086 -0.062865 -0.052130  0.008152 -0.079034 -0.017766 -0.048258 -0.034965 -0.041423 -0.060240 -0.061309  0.079033  0.042056 -0.022823 -0.050511  0.030578  0.014960 -0.033145  0.046618  0.041888   \n",
       "misconception -0.028757 -0.014449 -0.076551  0.010053 -0.037417  0.055246 -0.070086  0.027179 -0.076198 -0.006539  0.005825  0.064117 -0.063292  0.006542 -0.028721  0.025230 -0.071278  0.020629 -0.046245 -0.058321  0.000976  0.032937 -0.011987 -0.060293 -0.008482  0.039845  0.056724  0.003717 -0.008027 -0.062912 -0.022392 -0.004539 -0.053931 -0.002279  0.041470  0.091689  0.047256 -0.032948  0.027204 -0.035020 -0.034575 -0.069146  0.068564 -0.064134 -0.016592 -0.019586  0.019255  0.054613 -0.018804  0.076465  0.031736 -0.053508  0.021507  0.013832  0.004261 -0.033279 -0.007607  0.004471 -0.047477 -0.015780 -0.024102  0.034589 -0.058397 -0.060095 -0.086189 -0.022586 -0.003368  0.053644 -0.004562 -0.049887  0.041428  0.046805  0.032228 -0.042200 -0.075073 -0.025621 -0.020568 -0.070207  0.049527  0.021357  0.028856  0.075859 -0.078842 -0.045703  0.084661 -0.006199 -0.004000  0.079369 -0.017606  0.060691 -0.061375  0.019518  0.017140 -0.004531 -0.057377 -0.011032 -0.057604  0.041226 -0.003819 -0.010758  0.043441  0.000082  0.041475  0.020987 -0.046404 -0.052170  0.066946 -0.035612  0.036729 -0.014877 -0.022250 -0.006169 -0.042886  0.011059  0.015460  0.030279 -0.012471 -0.036970  0.033666  0.035401 -0.034424 -0.022334 -0.020666 -0.056259 -0.014859  0.035520 -0.068057 -0.046371  0.005644  0.008288  0.007837 -0.043657  0.021454  0.063920  0.053817  0.083319 -0.040147  0.057409 -0.042668 -0.012305  0.013440  0.068528 -0.044585  0.008306  0.019217 -0.046726 -0.041486 -0.000523  0.070361 -0.054341 -0.042647  0.028851  0.056877 -0.053782  0.032337  0.038638 -0.009494 -0.006674 -0.081569  0.059548  0.009932 -0.073700  0.011981 -0.002238  0.080643  0.006766 -0.024293 -0.004638  0.045566 -0.003386  0.029386 -0.039917 -0.030597 -0.021465  0.054359 -0.063675  0.007416 -0.047241 -0.034450 -0.051978 -0.084676 -0.034226 -0.054737  0.000705 -0.034623 -0.021208  0.091257 -0.071899 -0.009954  0.057576  0.075837 -0.017580  0.037935  0.016344 -0.002932 -0.039862 -0.034449 -0.029185   \n",
       "crab          -0.055975  0.040675  0.034260 -0.059429 -0.015919 -0.005863  0.012188 -0.054521 -0.031349  0.071273 -0.068224 -0.042330 -0.011450  0.068818 -0.039030 -0.062780 -0.006073 -0.065241  0.007722  0.019375  0.037414  0.066684 -0.037973 -0.077074 -0.046535 -0.058743 -0.039353 -0.010794 -0.023444  0.029874 -0.038092  0.031726  0.051370 -0.037315 -0.034746  0.081595  0.055963  0.063254  0.019989 -0.036706 -0.057626  0.034620  0.017276 -0.034026  0.030884  0.011983 -0.039323 -0.064660  0.080648 -0.063890 -0.041449  0.023663  0.064329  0.048727  0.007597 -0.030637 -0.033215  0.061575 -0.044511  0.047595  0.024730  0.050769 -0.045433 -0.029853 -0.001771 -0.030202  0.036024  0.039999  0.080115  0.005320  0.025570 -0.039550 -0.008781  0.014348  0.037793  0.028025  0.079354 -0.035360  0.035859 -0.034297  0.075652  0.032100  0.014291 -0.030578  0.021240  0.038162 -0.011066  0.024013 -0.014658  0.062250 -0.076272  0.005197  0.039206 -0.048179  0.080727  0.014491  0.021994  0.033844 -0.078713  0.054091 -0.057999 -0.029239  0.064402 -0.074698 -0.012669 -0.005822 -0.028326  0.067021  0.040595 -0.047290 -0.053494 -0.077079 -0.016192 -0.057975  0.054447  0.036689 -0.033355  0.044778 -0.011137  0.010875 -0.040631 -0.042331 -0.050699 -0.000015  0.058344  0.023232  0.012743  0.074750 -0.046889  0.037751 -0.020852 -0.031396 -0.021376 -0.067655  0.023899 -0.022183 -0.072389 -0.049675  0.025535  0.037350  0.020273 -0.066463 -0.046169 -0.008139 -0.076265  0.026609 -0.016196 -0.081377 -0.011060  0.047870  0.048441 -0.022399 -0.046059  0.030927 -0.078033  0.036120 -0.034290  0.022602 -0.076372 -0.008197  0.061978 -0.018166 -0.014090  0.002324  0.075844  0.055968 -0.000977 -0.059545  0.058583 -0.011686  0.002117  0.076429  0.078360  0.041819  0.041636 -0.036642 -0.015381 -0.042747  0.039568  0.000638 -0.027965 -0.028481  0.053231  0.054999  0.021657  0.036547 -0.010514 -0.014238 -0.055776 -0.055854  0.021718  0.080040 -0.020494  0.035921  0.020063 -0.045633 -0.004799 -0.018480   \n",
       "...                 ...       ...       ...       ...       ...       ...       ...       ...       ...       ...       ...       ...       ...       ...       ...       ...       ...       ...       ...       ...       ...       ...       ...       ...       ...       ...       ...       ...       ...       ...       ...       ...       ...       ...       ...       ...       ...       ...       ...       ...       ...       ...       ...       ...       ...       ...       ...       ...       ...       ...       ...       ...       ...       ...       ...       ...       ...       ...       ...       ...       ...       ...       ...       ...       ...       ...       ...       ...       ...       ...       ...       ...       ...       ...       ...       ...       ...       ...       ...       ...       ...       ...       ...       ...       ...       ...       ...       ...       ...       ...       ...       ...       ...       ...       ...       ...       ...       ...       ...       ...       ...       ...       ...       ...       ...       ...       ...       ...       ...       ...       ...       ...       ...       ...       ...       ...       ...       ...       ...       ...       ...       ...       ...       ...       ...       ...       ...       ...       ...       ...       ...       ...       ...       ...       ...       ...       ...       ...       ...       ...       ...       ...       ...       ...       ...       ...       ...       ...       ...       ...       ...       ...       ...       ...       ...       ...       ...       ...       ...       ...       ...       ...       ...       ...       ...       ...       ...       ...       ...       ...       ...       ...       ...       ...       ...       ...       ...       ...       ...       ...       ...       ...       ...       ...       ...       ...       ...       ...       ...       ...       ...       ...       ...       ...       ...       ...       ...       ...   \n",
       "vaccinate     -0.002336  0.021357 -0.034928  0.046110  0.039303  0.003042  0.001583 -0.074783 -0.015139  0.081015 -0.063060  0.064020 -0.045401 -0.006835  0.038624  0.066773 -0.060457 -0.038722  0.077861  0.050446 -0.008646  0.060800 -0.039997 -0.034271  0.070857  0.013882  0.027279 -0.055249  0.031210  0.011553 -0.046640  0.035244  0.017364 -0.010522  0.078577  0.082108  0.028731 -0.028056 -0.053339  0.018247 -0.010892 -0.029003  0.023822 -0.040213  0.017374 -0.066965 -0.043656 -0.014981  0.000252  0.021323 -0.017392 -0.055453  0.072211 -0.075839  0.004998  0.039937 -0.024987  0.005549  0.002359  0.004212  0.047502 -0.048678 -0.021398 -0.008389 -0.054579  0.042405  0.047104 -0.012104  0.042331  0.039275  0.070677  0.049368 -0.035363 -0.072097 -0.012421 -0.078613 -0.066277 -0.034878  0.048891 -0.074424  0.072630  0.077667  0.015895  0.020440 -0.036295  0.026898 -0.028113 -0.029463 -0.037961  0.050384 -0.070224 -0.007928  0.056410 -0.037946  0.078800 -0.012110  0.054140  0.068925  0.048381  0.051237 -0.070851  0.052826 -0.024953 -0.065382 -0.019471 -0.043215  0.054353  0.033137 -0.057318  0.055100 -0.015772  0.030700  0.066227  0.045186 -0.055524  0.013598  0.073974  0.012642 -0.026671  0.074775  0.048365 -0.060164  0.000294  0.028768  0.045488 -0.026078 -0.035854  0.073687  0.054658  0.070989  0.031952 -0.009804  0.063213  0.001351 -0.015314  0.076752  0.009618  0.015477  0.003546  0.058678 -0.013885  0.017009  0.035707  0.029293  0.080068  0.006524 -0.026286 -0.081493  0.039460  0.053000 -0.055508  0.051934  0.026078  0.047467  0.039581  0.065588  0.073398  0.009060 -0.070075 -0.037799 -0.011843 -0.068857  0.063748 -0.009012  0.039876  0.030824 -0.042201 -0.077752  0.021205  0.000496 -0.026340  0.072197 -0.078397  0.031649 -0.028109  0.013554  0.030434 -0.039760  0.049478 -0.017531  0.049398  0.006439  0.039892 -0.042380 -0.072161  0.063856  0.052389 -0.058272 -0.006660 -0.072475  0.014216  0.075971 -0.043970 -0.069018 -0.012582 -0.066808 -0.060694  0.026416   \n",
       "boastful       0.005578  0.078008 -0.101961  0.047172  0.016991  0.027549  0.052327 -0.074413  0.031618 -0.062335  0.031865  0.036533  0.003486 -0.013006  0.009951 -0.013115  0.068941 -0.059499 -0.052338  0.037283 -0.040879  0.044461  0.019734 -0.078896 -0.052077  0.024156 -0.014355  0.032629 -0.006387 -0.042155 -0.061048 -0.057423 -0.016177 -0.095286  0.048530  0.117524  0.025619  0.000304  0.028994 -0.012078 -0.034992 -0.040559  0.008688  0.036617  0.057016 -0.061172  0.023232 -0.034713  0.038133  0.002603 -0.021562 -0.033302  0.064885 -0.002064  0.035436 -0.057827  0.035041 -0.071123  0.052781 -0.002846 -0.007422  0.008339 -0.048439  0.015472  0.026307 -0.039965 -0.024364  0.031877  0.021205 -0.052837  0.001476 -0.014058 -0.006741 -0.087335 -0.004949 -0.002404 -0.005332  0.008617 -0.038839 -0.044578  0.043553  0.015349  0.027808  0.020776  0.060347  0.000124 -0.028743  0.075873  0.048450  0.048778 -0.044934  0.050242 -0.055424  0.040995  0.018923 -0.084035 -0.075381  0.002633 -0.014448  0.062219  0.003956 -0.043502  0.082317  0.011435 -0.043307 -0.024926  0.022903  0.042373 -0.042264  0.034753  0.040458 -0.083294 -0.028300  0.018975  0.017199  0.088574 -0.061183 -0.021605  0.023511  0.058749 -0.079480 -0.002113 -0.035712  0.018524 -0.014070  0.002299 -0.024528 -0.026118  0.002997  0.001698 -0.043691  0.014546 -0.011737  0.023094  0.076833  0.011625 -0.051375  0.030305 -0.021082 -0.000816  0.006732 -0.011991  0.042745 -0.057660  0.027031 -0.027659 -0.043771  0.012445 -0.028161  0.006314 -0.017260 -0.006904  0.020979  0.032319 -0.002480  0.045031 -0.036720 -0.053197 -0.097158 -0.034898 -0.025371 -0.019374  0.012664 -0.027051  0.067239 -0.012419  0.037818  0.028464  0.100829 -0.051197  0.065681  0.033457 -0.002663 -0.049526 -0.014274  0.071777 -0.029715 -0.067806  0.038436 -0.069939 -0.099191 -0.039537  0.001066  0.073831 -0.040274 -0.051056 -0.020469 -0.086644  0.019278 -0.000787 -0.048879  0.055948  0.042339  0.024407 -0.026598  0.044540  0.028799 -0.023075   \n",
       "phrase         0.029430  0.071927 -0.096384 -0.034392  0.022390  0.048505 -0.081279  0.064417  0.077251  0.004162 -0.036813 -0.048496 -0.069923 -0.020994 -0.012922  0.004662 -0.070518  0.022226 -0.024648 -0.019508  0.064945  0.044396 -0.005883  0.004745 -0.035459  0.051084 -0.011597 -0.035299 -0.027379 -0.075839  0.006203 -0.032015 -0.025780  0.007648  0.053502  0.100378 -0.031818 -0.002819  0.100586 -0.035725 -0.049472 -0.069539  0.003575  0.007195  0.030257 -0.035489  0.020287  0.049218 -0.012037 -0.006519  0.079622  0.003635  0.038311 -0.074439 -0.035789 -0.049722  0.097702 -0.100783 -0.015318 -0.005332 -0.040886 -0.022442 -0.040216  0.075266 -0.008350 -0.039790 -0.009930  0.057113  0.039237 -0.087547  0.010808 -0.011856  0.091577 -0.071830 -0.063974 -0.010768  0.033048 -0.047228  0.025019  0.054574  0.089041  0.074771 -0.006729 -0.041926  0.079909 -0.012465  0.042707  0.031484  0.068771  0.062038 -0.082406 -0.055304 -0.008403 -0.046275 -0.073741 -0.057733 -0.008102  0.030841 -0.014995  0.036079  0.017220 -0.019786 -0.038062 -0.023291 -0.032354 -0.083169 -0.011261 -0.019510  0.010261  0.005440 -0.050610  0.016131 -0.043890  0.010654 -0.044891 -0.050435  0.021113 -0.033284  0.036888  0.050781 -0.044832 -0.002877 -0.046589 -0.073294  0.010012  0.064281  0.002525  0.005992 -0.035838  0.031938 -0.024050  0.005813 -0.015585  0.037788 -0.037157  0.038411  0.002630 -0.015554 -0.050492  0.042391  0.044751 -0.020570  0.039166 -0.048313  0.035460 -0.044187 -0.036200 -0.028550  0.015374  0.022835 -0.012336 -0.011377  0.025478  0.034720  0.043319 -0.060672 -0.008116 -0.010183 -0.092518 -0.029613 -0.034366 -0.039589 -0.024305  0.004382 -0.020934 -0.047879 -0.018483  0.016533  0.080940 -0.019513  0.031710 -0.024503  0.011677 -0.060538 -0.051992  0.070915 -0.047141  0.025372 -0.039808 -0.028153 -0.045678 -0.081840  0.020064 -0.049506 -0.017800 -0.076379  0.039454 -0.078400 -0.018091 -0.061223 -0.058935 -0.016336 -0.023909 -0.019932  0.074017 -0.049200  0.002007 -0.068772   \n",
       "ski           -0.003117  0.023412 -0.015534 -0.088169 -0.045969  0.004880 -0.057624  0.050438  0.006350 -0.010420 -0.014375 -0.020847 -0.070168 -0.069010  0.044230 -0.002157 -0.049884  0.066985  0.013237  0.003601 -0.083867  0.070304 -0.011912 -0.052847 -0.070273 -0.012595  0.006501 -0.022494 -0.047114  0.011501 -0.057727  0.061519 -0.000652 -0.053517  0.025528  0.089407 -0.010306 -0.048913 -0.028896  0.041245 -0.064611  0.017081  0.009624 -0.067239  0.027983 -0.068344  0.058782  0.003981  0.059124 -0.002797 -0.046812 -0.032729  0.064521  0.016485 -0.031511  0.065694 -0.057998 -0.034471  0.027379 -0.011775 -0.013682 -0.086054  0.066720 -0.046236  0.048822  0.047340  0.055136 -0.045498 -0.032161 -0.006496  0.062494  0.010378  0.073523 -0.058110 -0.049548 -0.007543 -0.006066  0.045950 -0.011692  0.024896  0.071177  0.047250 -0.054091 -0.037249 -0.053467 -0.010726  0.078289 -0.013185  0.018943  0.056825 -0.061513 -0.049974 -0.085077 -0.034583  0.088395 -0.065070  0.052368  0.045353  0.003694  0.054088 -0.031070 -0.027797  0.046712 -0.045979 -0.040514  0.018220  0.055948  0.069598  0.058222 -0.010625  0.014938 -0.042120 -0.004039 -0.035734  0.041358  0.011643 -0.014496 -0.024022  0.004245  0.012675 -0.001701  0.017334  0.080774  0.008056  0.006145  0.051717  0.032959  0.043834 -0.037299  0.037756  0.021156  0.049670 -0.086938 -0.052023  0.009812  0.066901 -0.082460  0.031558 -0.037388  0.090036 -0.011564  0.041526 -0.000463  0.057787 -0.026298  0.001646  0.005750  0.036548  0.043405 -0.013021 -0.061553 -0.005667  0.083183 -0.009233  0.055021 -0.042728 -0.032825 -0.052098 -0.079294 -0.016389  0.025929 -0.058062 -0.003221 -0.071481  0.069081 -0.003403  0.061231 -0.049907  0.026340  0.055518  0.049522  0.006684 -0.019452 -0.014543  0.016356 -0.028724  0.007419 -0.046638  0.038896 -0.023314  0.058395 -0.071621  0.090179 -0.021649  0.026438  0.026987  0.011099  0.023020 -0.059457 -0.017660 -0.009336  0.031767 -0.045412 -0.007315 -0.012653 -0.041443 -0.003107 -0.025558   \n",
       "commanding    -0.006128  0.037021 -0.071187 -0.034022 -0.029329  0.050997  0.025691  0.002951 -0.012894 -0.038917  0.002569  0.029185 -0.046088 -0.047066  0.007794 -0.016665 -0.057750 -0.014856  0.022313  0.035418 -0.054693  0.031030 -0.030233 -0.073990  0.020779  0.029783 -0.023044 -0.001528  0.025294  0.019969 -0.039750 -0.032994 -0.041686 -0.043160 -0.003726  0.113852 -0.021909  0.020274 -0.027622 -0.057428 -0.019909 -0.052428  0.031492 -0.020819  0.019262  0.042520 -0.047165  0.033633  0.014007 -0.050425 -0.053425 -0.006864  0.007938  0.034981  0.062506 -0.037280 -0.012757 -0.042682  0.044688  0.059658  0.049451  0.067256 -0.029687  0.031950  0.093893 -0.042409  0.082212  0.069638  0.031266  0.015082  0.013481 -0.038379  0.066083 -0.003622  0.032654 -0.045821  0.086081 -0.061226 -0.041345 -0.010481  0.055568  0.033546 -0.011658  0.070772 -0.030511  0.014902 -0.034981  0.040129  0.069303  0.021797 -0.068925  0.025457  0.007399 -0.007561  0.033522 -0.038232  0.050061  0.043461 -0.013283  0.020269 -0.022872 -0.091971  0.016169 -0.078690 -0.052034 -0.085122  0.020826  0.075091  0.083610  0.064273 -0.029521 -0.088203 -0.039104  0.067063 -0.026698  0.039659 -0.056743  0.006151 -0.057868  0.047792 -0.101860 -0.064901 -0.077856 -0.055927  0.069233  0.055160 -0.062843 -0.034061  0.038274  0.005391  0.003226 -0.076080  0.029647 -0.037415  0.038546  0.046572 -0.069526  0.037634  0.037299  0.027292  0.043785 -0.028656  0.022199 -0.019420  0.003542 -0.033979  0.018614  0.009059  0.004888  0.036736  0.009313 -0.031245  0.054102  0.034652  0.021098  0.035497 -0.029705 -0.056657 -0.089983 -0.055972 -0.004274  0.026076 -0.042653  0.003976  0.079068 -0.062108  0.026420 -0.045164  0.048578  0.008358  0.087004 -0.027234 -0.010025 -0.040531  0.029940  0.038127 -0.048948  0.003283 -0.000262 -0.050132 -0.052996  0.009624 -0.082127 -0.056914 -0.037447 -0.031959  0.002752 -0.002923 -0.075691  0.023673 -0.043238  0.106030 -0.035911  0.029716 -0.040331  0.015956  0.076108 -0.035181   \n",
       "\n",
       "                    396       397       398       399       400       401       402       403       404       405       406       407       408       409       410       411       412       413       414       415       416       417       418       419       420       421       422       423       424       425       426       427       428       429       430       431       432       433       434       435       436       437       438       439       440       441       442       443       444       445       446       447       448       449       450       451       452       453       454       455       456       457       458       459       460       461       462       463       464       465       466       467       468       469       470       471       472       473       474       475       476       477       478       479       480       481       482       483       484       485       486       487       488       489       490       491       492       493       494       495       496       497       498       499       500       501       502       503       504       505       506       507       508       509       510       511  \n",
       "argument       0.018516 -0.071379 -0.062263 -0.064034  0.023971  0.018099  0.021977  0.061723 -0.013675 -0.035256 -0.034237  0.047126 -0.092807  0.056644  0.023457 -0.060108  0.086088 -0.059559 -0.015015 -0.035430 -0.036492 -0.031708 -0.032735 -0.040144  0.011704 -0.033966 -0.010243 -0.001060 -0.024761  0.021965  0.057084  0.078910 -0.002770  0.005780  0.005478  0.047648 -0.048318 -0.041998  0.071814  0.008772  0.011096 -0.002644  0.046806  0.080080  0.038559 -0.018047  0.045859  0.007169 -0.027391  0.007768 -0.004566  0.021875  0.085753 -0.022324  0.006330 -0.064182  0.021552  0.045393  0.060638  0.017268  0.045636 -0.006496 -0.080592  0.043981  0.027675  0.002706 -0.007369  0.039583  0.031410 -0.042385 -0.030802 -0.015994 -0.018830  0.076203  0.002100 -0.085850 -0.028647  0.000867  0.063751 -0.013786 -0.050527  0.066184 -0.001355  0.084783 -0.007728 -0.064325  0.037782 -0.066981  0.044540 -0.040387 -0.046770 -0.038879  0.042562 -0.004817 -0.062256  0.014689 -0.072014  0.001126  0.057677 -0.089631  0.071474 -0.068246  0.066406 -0.005436  0.044631  0.000822 -0.031578 -0.095102  0.047018  0.025369  0.019591  0.004513 -0.025299 -0.011916  0.010102  0.011102  \n",
       "awful         -0.036457  0.003909  0.000115 -0.037345 -0.045354  0.056018 -0.058831 -0.063883  0.008590  0.053133  0.027540  0.069558 -0.034867 -0.037002 -0.070502  0.063650 -0.040577  0.015556 -0.069908  0.053437 -0.003592 -0.032247 -0.011380 -0.018528  0.021873  0.032956  0.010046 -0.053758  0.053524  0.076512  0.062618  0.077849 -0.045502  0.028627 -0.038741  0.021568 -0.033849 -0.057351 -0.049024 -0.049809  0.002694 -0.001142 -0.017030  0.047223  0.037956 -0.066853 -0.000362 -0.000358  0.025561 -0.001205  0.081358 -0.066752 -0.062665 -0.054362 -0.009323 -0.015458  0.048016 -0.033786  0.056153  0.036444 -0.035550  0.024998 -0.027910  0.048046 -0.025117 -0.014647  0.055247 -0.044303  0.007595 -0.017439  0.001466 -0.031234  0.002822 -0.068441  0.063972 -0.035880 -0.012636  0.024031  0.048829 -0.020855 -0.068279  0.039981  0.093936  0.080030  0.004546 -0.048764  0.071922  0.001301 -0.021302  0.025027  0.053854 -0.015680  0.009696  0.034640 -0.046887 -0.015079 -0.013106 -0.057902 -0.024479  0.006301  0.081928 -0.078714  0.009384  0.011722 -0.037346 -0.003848 -0.004141 -0.084734  0.061091  0.035161  0.002606 -0.011290 -0.054125 -0.057468 -0.003473  0.002821  \n",
       "decorator     -0.014559  0.025480 -0.070325  0.028939 -0.048110 -0.036248  0.019347  0.046270  0.022403  0.009652 -0.010524 -0.010203 -0.064965 -0.053609 -0.007922 -0.087992  0.038238  0.042409 -0.015223  0.015186 -0.020519  0.013376  0.016078 -0.031622  0.045399 -0.051746 -0.049487  0.015081  0.056282 -0.059757  0.050467  0.045041 -0.023176 -0.047181  0.000647  0.057048  0.006192 -0.077530  0.032721 -0.016440  0.033782 -0.019212 -0.031443  0.058002 -0.053286 -0.004739  0.025995  0.020117  0.040129  0.060031 -0.012824 -0.009807  0.021923 -0.047796 -0.030240 -0.016530  0.074136  0.041001  0.052631  0.025878 -0.008818 -0.003311  0.029389 -0.027237 -0.005571  0.073993  0.044530  0.047586  0.062708 -0.006184 -0.029530  0.009271  0.041086  0.043945 -0.000306  0.061877 -0.041166 -0.055623 -0.056213 -0.054792 -0.049162  0.039579  0.048976 -0.016334  0.014645 -0.027659  0.052344  0.024724  0.056533 -0.030742  0.004794  0.067283  0.030255 -0.036236 -0.068788 -0.016268 -0.064952  0.029606  0.022271  0.025709  0.062627 -0.056331 -0.004910  0.046135  0.033801  0.079989  0.050995 -0.090093  0.069056  0.060365 -0.053866  0.013783 -0.011048 -0.023909  0.044623 -0.006743  \n",
       "misconception -0.047611  0.047566 -0.000416 -0.011780  0.037712  0.048843  0.024755 -0.062980  0.016671 -0.007598 -0.041735  0.059873 -0.089206 -0.029773 -0.008058 -0.076280  0.073256 -0.046136  0.040195  0.044383  0.030609 -0.070345  0.046401  0.010846 -0.030563  0.036867  0.039984  0.031604  0.055534  0.053524  0.064416  0.070160 -0.010095 -0.039146  0.020533 -0.050308 -0.086189 -0.067358  0.065705 -0.081466  0.024587 -0.027059 -0.060660 -0.049794  0.044823  0.048261  0.026147  0.048554 -0.011370  0.022707 -0.018570  0.035266  0.009890 -0.058766  0.027933 -0.023505  0.070215 -0.031561  0.029631  0.037513 -0.013175  0.048388 -0.056155  0.054404  0.053720 -0.049853  0.070728  0.064865 -0.055803 -0.072805  0.006038  0.040390 -0.004444  0.044455 -0.084494 -0.033479 -0.060080  0.002631  0.009176 -0.016510 -0.075071  0.018011  0.077930  0.066667  0.041971  0.041898 -0.050355 -0.037281  0.035519 -0.010927 -0.063864 -0.024931  0.003871  0.023619 -0.070655 -0.005272 -0.010078 -0.041793  0.026052 -0.058696  0.008776  0.018026  0.048573 -0.001823 -0.030892 -0.008985 -0.071650 -0.091038  0.068272  0.020632 -0.021735  0.002143 -0.053941  0.026607  0.025247  0.013437  \n",
       "crab          -0.062645 -0.008894 -0.016136  0.033499 -0.006930  0.015178 -0.020242 -0.021059 -0.040721  0.047935 -0.026875  0.024810  0.015256 -0.055888 -0.062866  0.035687  0.022228 -0.070362 -0.081347  0.020142  0.064630 -0.030652 -0.003897 -0.062167  0.014456  0.019430 -0.020528 -0.042739  0.031253 -0.043001 -0.003920  0.044265  0.014015 -0.047095 -0.015290  0.046872  0.048980 -0.052740 -0.007654 -0.031185 -0.025726 -0.035936 -0.011776  0.060453  0.060689 -0.007430  0.046921  0.001355  0.051434 -0.015661 -0.013057 -0.071655  0.070370  0.035660  0.068666  0.006782 -0.065619  0.032055 -0.008144  0.040906  0.071529 -0.066488 -0.067596  0.035801  0.066213  0.018814 -0.061897  0.014703 -0.041915 -0.004560 -0.032829  0.039266  0.054741  0.009650 -0.077373 -0.074589  0.081182  0.043701  0.017032 -0.011870 -0.070479 -0.031854  0.065114  0.000744  0.072337 -0.040985  0.025274  0.071217 -0.013274  0.041275 -0.019355  0.014577 -0.052955 -0.014330 -0.049519 -0.028708 -0.047589  0.020427  0.026056 -0.045594  0.064833 -0.052026 -0.029484  0.025574  0.064684 -0.041059 -0.021374 -0.078985  0.036315  0.043513 -0.002334  0.012682 -0.012753 -0.019673  0.001925 -0.033901  \n",
       "...                 ...       ...       ...       ...       ...       ...       ...       ...       ...       ...       ...       ...       ...       ...       ...       ...       ...       ...       ...       ...       ...       ...       ...       ...       ...       ...       ...       ...       ...       ...       ...       ...       ...       ...       ...       ...       ...       ...       ...       ...       ...       ...       ...       ...       ...       ...       ...       ...       ...       ...       ...       ...       ...       ...       ...       ...       ...       ...       ...       ...       ...       ...       ...       ...       ...       ...       ...       ...       ...       ...       ...       ...       ...       ...       ...       ...       ...       ...       ...       ...       ...       ...       ...       ...       ...       ...       ...       ...       ...       ...       ...       ...       ...       ...       ...       ...       ...       ...       ...       ...       ...       ...       ...       ...       ...       ...       ...       ...       ...       ...       ...       ...       ...       ...       ...       ...  \n",
       "vaccinate     -0.041792 -0.033061 -0.062973  0.010151 -0.056118 -0.050802 -0.009570  0.005110 -0.001752  0.051285 -0.017072  0.019568 -0.045850 -0.038270  0.024662  0.004760 -0.047173  0.030418 -0.005538  0.044404 -0.014341  0.050481  0.013253 -0.052401 -0.053548  0.025247  0.052459  0.005304 -0.032342 -0.057506 -0.002381  0.037012 -0.023396 -0.025962 -0.008575 -0.043294  0.044235 -0.014760  0.003989 -0.029687  0.031682 -0.001010 -0.067138 -0.016754  0.070909 -0.013612 -0.003098  0.029299 -0.025628 -0.061713  0.005865  0.067967  0.057875  0.008246  0.035280 -0.063615 -0.056653 -0.032101  0.045081  0.046592  0.022508 -0.032629 -0.043877 -0.009259 -0.077351  0.026070  0.046329  0.001020 -0.031854 -0.040619  0.024622 -0.017670 -0.032785  0.027073 -0.072396  0.012766  0.015221  0.009589 -0.029872 -0.025265 -0.057105  0.081700 -0.047924  0.063655  0.043113 -0.068742  0.038860  0.041552  0.017204 -0.058776 -0.047284  0.006175  0.065167  0.023445 -0.046691  0.036170  0.033127 -0.058263  0.040276 -0.032758  0.026630 -0.049931 -0.032048 -0.063238  0.056260 -0.064717 -0.005196 -0.079546  0.043179 -0.082887 -0.001214  0.029362  0.045764 -0.042659 -0.034748 -0.059509  \n",
       "boastful      -0.067413 -0.014196 -0.082709  0.004873  0.005131 -0.032619 -0.007272  0.038210  0.049647  0.030512 -0.028824 -0.008388 -0.032613 -0.083119 -0.073640 -0.045124  0.066278  0.034532 -0.057090  0.045727  0.004895 -0.027830  0.034169 -0.026087 -0.019225 -0.041222  0.008392 -0.032168  0.096632 -0.031408  0.059761  0.092302  0.002783  0.068226  0.040781  0.037944 -0.040092 -0.072099  0.075160 -0.019148 -0.024740 -0.012033 -0.046746  0.089455 -0.013998 -0.022423 -0.056901  0.007466 -0.007857 -0.027699  0.014038 -0.054487  0.019062  0.017756  0.028948  0.030280  0.004067 -0.001283  0.076337 -0.011588  0.012464  0.032531 -0.042552  0.049211  0.033130 -0.037410 -0.006063  0.036539 -0.059788 -0.092124  0.017384 -0.031840 -0.036138  0.069098 -0.058350  0.045435 -0.025932 -0.019635  0.053013 -0.036178 -0.020680  0.024972  0.099408  0.062121  0.021462 -0.076078  0.065728 -0.016879  0.006647 -0.023279 -0.023405  0.042554  0.074027  0.008236 -0.051422  0.010519 -0.025413  0.021815  0.043855  0.044764 -0.033540 -0.063108 -0.000659  0.024258 -0.019741 -0.028066 -0.007588 -0.116748  0.082915 -0.026700 -0.042339 -0.027726 -0.024228  0.026245  0.035487 -0.073523  \n",
       "phrase        -0.004221 -0.023758 -0.034278 -0.059307 -0.065206  0.017451 -0.008543 -0.025758 -0.029586 -0.045046  0.011035 -0.002949  0.012364  0.080454 -0.050971 -0.046580  0.089115  0.017176  0.016804 -0.032324  0.004949  0.066183 -0.005415 -0.040968  0.016175 -0.032409 -0.003373 -0.045490  0.043977 -0.002709 -0.035514  0.075806 -0.000115  0.049067 -0.025091 -0.023968 -0.016429 -0.004595  0.092619 -0.063908 -0.010871  0.047848  0.054256 -0.004406  0.010212 -0.077258  0.002193 -0.080134  0.053799  0.057900 -0.014870  0.007588  0.032561 -0.013136  0.002368  0.005967 -0.010140 -0.005554  0.050540  0.048885 -0.005373  0.078077  0.034113  0.025101  0.028954 -0.048309  0.028760  0.058866  0.002107 -0.100629 -0.029300  0.061822 -0.038401  0.097580  0.002871  0.013917  0.025910 -0.072696  0.075220  0.019238 -0.025088  0.069259  0.066572  0.044215 -0.021113 -0.011015  0.038826 -0.038666  0.004937 -0.065862 -0.096661 -0.039939 -0.004821 -0.000633 -0.032066 -0.001637 -0.036178  0.032611  0.031594 -0.014061  0.024074 -0.031998  0.069268 -0.005463  0.008954 -0.024153 -0.008846 -0.099964  0.085710  0.050738 -0.021996 -0.056691 -0.066562  0.009670 -0.014906  0.037426  \n",
       "ski            0.020126 -0.008564 -0.085617  0.037813  0.014255  0.069387  0.060026 -0.025112 -0.035866 -0.058540  0.013601  0.074204 -0.009720 -0.028407 -0.037311  0.046830  0.016659  0.060548 -0.077034 -0.073452 -0.060421  0.021793 -0.058761  0.004496  0.020455 -0.046891 -0.012097 -0.076162  0.013276  0.024276  0.008814  0.068173  0.036899 -0.027916 -0.011376 -0.081641  0.006964 -0.038498  0.053589 -0.054167 -0.007919  0.010205 -0.019242  0.047256  0.074615  0.022735  0.001066  0.023890 -0.077865 -0.025791 -0.036523  0.054939 -0.046859 -0.048878 -0.060598  0.012150  0.010922  0.023344  0.045131  0.032302  0.030158 -0.037109 -0.000478  0.070178  0.032006  0.061528 -0.004853  0.008945 -0.049595 -0.019621 -0.017550 -0.062150  0.020912  0.012919  0.041374  0.008561  0.001738  0.028067  0.082530 -0.031771  0.039285  0.069612 -0.000907 -0.044651 -0.016123 -0.066910 -0.019877  0.055000 -0.085789 -0.022484 -0.032362  0.036367 -0.004514  0.003951 -0.072870  0.084775 -0.039618 -0.032014 -0.051234  0.024639  0.053439 -0.020861  0.047370 -0.085946  0.035097 -0.043610 -0.027508 -0.088748  0.006053  0.051079  0.001197 -0.073380 -0.018774 -0.001848  0.039178 -0.057981  \n",
       "commanding    -0.038755 -0.034328 -0.039819 -0.044165  0.035701 -0.088299  0.004164  0.033278 -0.000155  0.036771  0.024374  0.004721  0.020225  0.044062 -0.002540 -0.024182  0.015682 -0.005126 -0.059687  0.062534 -0.021763 -0.026621 -0.000471 -0.036248  0.027149 -0.035859 -0.019966 -0.050818  0.038344 -0.060490  0.052427  0.052198 -0.065401 -0.013199  0.002640 -0.007894 -0.047487 -0.038146  0.016330 -0.028192 -0.026505 -0.044466 -0.003462  0.072428 -0.018450 -0.035615 -0.035308  0.012763  0.070836 -0.052661 -0.030568  0.009623  0.035733 -0.097965  0.017374 -0.083127 -0.018958 -0.045238  0.025459  0.082861  0.042409  0.046854  0.015115  0.043093  0.010497 -0.006535  0.026580 -0.016915  0.046082 -0.057873  0.019536 -0.017568 -0.044403  0.009632 -0.044836  0.032990 -0.002223 -0.002283  0.042544 -0.031163 -0.045846  0.015638  0.040989  0.046313 -0.034952 -0.066628  0.061346  0.011170 -0.061449 -0.041902  0.022843  0.079199  0.028186 -0.044330 -0.007799 -0.000295 -0.060964  0.001387  0.048243  0.047174  0.009617 -0.085751  0.064028 -0.036543  0.066259 -0.066376  0.020403 -0.112481  0.058689  0.004924  0.037340 -0.031089  0.003641 -0.007853  0.089649 -0.050396  \n",
       "\n",
       "[3512 rows x 512 columns]"
      ]
     },
     "execution_count": 48,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X_train_A"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>F_Objectivity</th>\n",
       "      <th>F_Subjectivity</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>argument</th>\n",
       "      <td>0.365722</td>\n",
       "      <td>0.647198</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>awful</th>\n",
       "      <td>0.201122</td>\n",
       "      <td>0.507171</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>decorator</th>\n",
       "      <td>0.774472</td>\n",
       "      <td>0.291107</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>misconception</th>\n",
       "      <td>0.013464</td>\n",
       "      <td>0.474901</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>crab</th>\n",
       "      <td>0.816438</td>\n",
       "      <td>0.227631</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>vaccinate</th>\n",
       "      <td>0.678125</td>\n",
       "      <td>0.443751</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>boastful</th>\n",
       "      <td>0.437815</td>\n",
       "      <td>0.575248</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>phrase</th>\n",
       "      <td>0.265757</td>\n",
       "      <td>0.144886</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>ski</th>\n",
       "      <td>0.900524</td>\n",
       "      <td>0.370531</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>commanding</th>\n",
       "      <td>0.316342</td>\n",
       "      <td>0.538581</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>3512 rows × 2 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "               F_Objectivity  F_Subjectivity\n",
       "argument            0.365722        0.647198\n",
       "awful               0.201122        0.507171\n",
       "decorator           0.774472        0.291107\n",
       "misconception       0.013464        0.474901\n",
       "crab                0.816438        0.227631\n",
       "...                      ...             ...\n",
       "vaccinate           0.678125        0.443751\n",
       "boastful            0.437815        0.575248\n",
       "phrase              0.265757        0.144886\n",
       "ski                 0.900524        0.370531\n",
       "commanding          0.316342        0.538581\n",
       "\n",
       "[3512 rows x 2 columns]"
      ]
     },
     "execution_count": 49,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "Y_train_A"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### B) Multilabel classification: splitting train and test data\n",
    "- The model predicts multiple classes for a single instance.  \n",
    "- In this case, it predicts high or low objectivity and subjectivity (i.e., four classes)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Generating data (the same process for dataset A):\n",
    "X_train_B, Y_train_B = generateData(train_df)\n",
    "X_test_B, Y_test_B = generateData(test_df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # Categorical labeling through list comprehension accordingly to SCA median criteria:\n",
    "# Y_train_B['F_Objectivity'] = ['high' if f_objectivity >= 0.565 else 'low' for f_objectivity in Y_train_B['F_Objectivity']]\n",
    "# Y_train_B['F_Subjectivity'] = ['high' if f_subjectivity >= 0.392 else 'low' for f_subjectivity in Y_train_B['F_Subjectivity']]\n",
    "\n",
    "# Y_test_B['F_Objectivity'] = ['high' if f_objectivity >= 0.565 else 'low' for f_objectivity in Y_test_B['F_Objectivity']]\n",
    "# Y_test_B['F_Subjectivity'] = ['high' if f_subjectivity >= 0.392 else 'low' for f_subjectivity in Y_test_B['F_Subjectivity']]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Categorical binary labeling through list comprehension accordingly to SCA median criteria:\n",
    "Y_train_B['F_Objectivity'] = [1 if f_objectivity >= 0.565 else 0 for f_objectivity in Y_train_B['F_Objectivity']]\n",
    "Y_train_B['F_Subjectivity'] = [1 if f_subjectivity >= 0.392 else 0 for f_subjectivity in Y_train_B['F_Subjectivity']]\n",
    "\n",
    "Y_test_B['F_Objectivity'] = [1 if f_objectivity >= 0.565 else 0 for f_objectivity in Y_test_B['F_Objectivity']]\n",
    "Y_test_B['F_Subjectivity'] = [1 if f_subjectivity >= 0.392 else 0 for f_subjectivity in Y_test_B['F_Subjectivity']]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train data dimension:\n",
      "X_train: (3512, 512)\n",
      "Y_train: (3512, 2)\n",
      "\n",
      "Test data dimension:\n",
      "X_test: (1171, 512)\n",
      "Y_test: (1171, 2)\n"
     ]
    }
   ],
   "source": [
    "## Checking the generated data dimension:\n",
    "print(\"Train data dimension:\")\n",
    "print(\"X_train:\", X_train_B.shape)\n",
    "print(\"Y_train:\", Y_train_B.shape)\n",
    "\n",
    "print(\"\\nTest data dimension:\")\n",
    "print(\"X_test:\", X_test_B.shape)\n",
    "print(\"Y_test:\", Y_test_B.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>F_Objectivity</th>\n",
       "      <th>F_Subjectivity</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>coke</th>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>accelerate</th>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>needle</th>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>fierce</th>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>lie</th>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "            F_Objectivity  F_Subjectivity\n",
       "coke                    1               0\n",
       "accelerate              0               1\n",
       "needle                  1               0\n",
       "fierce                  0               1\n",
       "lie                     0               1"
      ]
     },
     "execution_count": 54,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "Y_train_B.sample(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### C) Multiclass classification: splitting train and test data\n",
    "- The model predicts between more than two mutually exclusive classes.  \n",
    "- In this case, it will predict between its semantic content: latent, manifest, contextual, or perceptual.  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [],
   "source": [
    "## It makes use of Y_train_B and Y_test_B data.\n",
    "Y_train_C = Y_train_B.copy()\n",
    "Y_test_C = Y_test_B.copy()\n",
    "\n",
    "## It also makes a copy of X datasets:\n",
    "X_train_C = X_train_B.copy()\n",
    "X_test_C = X_test_B.copy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Defines a method to map values from columns to the respective labels according to SCA analysis.\n",
    "def map_labels(row):\n",
    "    if row['F_Objectivity'] == 0 and row['F_Subjectivity'] == 1:\n",
    "        return 'Latent'\n",
    "    elif row['F_Objectivity'] == 0 and row['F_Subjectivity'] == 0:\n",
    "        return 'Contextual'\n",
    "    elif row['F_Objectivity'] == 1 and row['F_Subjectivity'] == 0:\n",
    "        return 'Manifest'\n",
    "    elif row['F_Objectivity'] == 1 and row['F_Subjectivity'] == 1:\n",
    "        return 'Perceptual'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Maps Y_train_C, Y_test_C values into a target column:\n",
    "Y_train_C['target'] = Y_train_C.apply(map_labels, axis=1)\n",
    "Y_test_C['target'] = Y_test_C.apply(map_labels, axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Dropping unnecessariy columns:\n",
    "Y_train_C.drop(['F_Objectivity','F_Subjectivity'], axis=1, inplace=True)\n",
    "Y_test_C.drop(['F_Objectivity','F_Subjectivity'], axis=1, inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>target</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>koala</th>\n",
       "      <td>Perceptual</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>none</th>\n",
       "      <td>Contextual</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>beaker</th>\n",
       "      <td>Manifest</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>advise</th>\n",
       "      <td>Latent</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>conceal</th>\n",
       "      <td>Latent</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "             target\n",
       "koala    Perceptual\n",
       "none     Contextual\n",
       "beaker     Manifest\n",
       "advise       Latent\n",
       "conceal      Latent"
      ]
     },
     "execution_count": 59,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "Y_train_C.sample(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 60,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "## All training data, except for train_D are equals:\n",
    "X_train_B.equals(X_train_C) & X_train_A.equals(X_train_B)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### D) MAD selection data\n",
    "Since SCA uses the median as a measure of centrality instead of the mean, we will consider the median absolute deviation (MAD) as an equivalent to standard deviation. MAD is a robust measure of variability that is less sensitive to outliers compared to the standard deviation.  \n",
    "\n",
    "The purpose of using MAD is to focus on the distant data points rather than the central tendency of the data. The central data points may potentially confuse the algorithm, especially if they represent a region of overlap or ambiguity. By considering the median absolute deviation instead of the mean and standard deviation, we aim to identify data points that are far from the central measurement, thereby capturing well-separated regions of the data. This approach helps ensure that our classifier is trained on data points that are more distinct and representative of different classes or categories, leading to more effective classification performance.  \n",
    "\n",
    "$\\text{MAD} = \\text{median}(|x_i - \\text{median}(X)|)$, where $x_i$ represents each data point in the dataset X."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Median of F_Obj: 0.5649972252148387\n",
      "Median of F_Subj: 0.3925501677449325\n"
     ]
    }
   ],
   "source": [
    "med_fObj = df_factors['F_Objectivity'].median()\n",
    "med_fSubj = df_factors['F_Subjectivity'].median()\n",
    "print(f\"Median of F_Obj: {med_fObj}\")\n",
    "print(f\"Median of F_Subj: {med_fSubj}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "MAD for F_Obj: 0.24804732650935357\n",
      "MAD for F_Subj: 0.1399289370570712\n"
     ]
    }
   ],
   "source": [
    "# Calculating MAD\n",
    "mad_fObj = np.median(np.abs(df_factors['F_Objectivity'] - med_fObj))\n",
    "mad_fSubj = np.median(np.abs(df_factors['F_Subjectivity'] - med_fSubj))\n",
    "print(f\"MAD for F_Obj: {mad_fObj}\")\n",
    "print(f\"MAD for F_Subj: {mad_fSubj}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Defining thresholds as MAD:\n",
    "threshold_fObj = mad_fObj\n",
    "threshold_fSubj = mad_fSubj\n",
    "\n",
    "# ## Defining thresholds as TWICE MAD:\n",
    "# threshold_fObj = 2*mad_fObj\n",
    "# threshold_fSubj = 2*mad_fSubj"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Selecting from the original dataframe (df: SCA) only those words distant from the median (see threshold above):\n",
    "df_MAD = df[\n",
    "    (abs(df['F_Objectivity'] - med_fObj) > threshold_fObj) |\n",
    "    (abs(df['F_Subjectivity'] - med_fSubj) > threshold_fSubj)\n",
    "]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'pandas.core.frame.DataFrame'>\n",
      "Int64Index: 3484 entries, 1 to 5551\n",
      "Data columns (total 3 columns):\n",
      " #   Column          Non-Null Count  Dtype  \n",
      "---  ------          --------------  -----  \n",
      " 0   word            3484 non-null   object \n",
      " 1   F_Objectivity   3484 non-null   float64\n",
      " 2   F_Subjectivity  3484 non-null   float64\n",
      "dtypes: float64(2), object(1)\n",
      "memory usage: 108.9+ KB\n",
      "None\n",
      "\n",
      "             word  F_Objectivity  F_Subjectivity\n",
      "3815  profession       0.423083        0.574266\n",
      "2419        hope       0.259721        0.822663\n",
      "1942       flood       0.841248        0.577209\n"
     ]
    }
   ],
   "source": [
    "print(df_MAD.info())\n",
    "print('\\n',df_MAD.sample(3))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<matplotlib.collections.PathCollection at 0x211350dd510>"
      ]
     },
     "execution_count": 66,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 640x480 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "plt.scatter(df_MAD['F_Objectivity'], df_MAD['F_Subjectivity'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Repeating the splitting data in train (75%) and test (25%)\n",
    "train_MAD, test_MAD = train_test_split(df_MAD, test_size=0.25, random_state=42)\n",
    "\n",
    "## Generating data based on MAD selected points:\n",
    "X_train_D, Y_train_D = generateData(train_MAD)\n",
    "X_test_D, Y_test_D = generateData(test_MAD)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### E) MAD data for Multiclass\n",
    "This data puts together the manipulation applied for (C) and (D) datasets."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "metadata": {},
   "outputs": [],
   "source": [
    "## It makes use of Y_train_D and Y_test_D data.\n",
    "Y_train_E = Y_train_D.copy()\n",
    "Y_test_E = Y_test_D.copy()\n",
    "\n",
    "## It also makes a copy of X datasets:\n",
    "X_train_E = X_train_D.copy()\n",
    "X_test_E = X_test_D.copy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Categorical binary labeling through list comprehension accordingly to SCA median criteria:\n",
    "Y_train_E['F_Objectivity'] = [1 if f_objectivity >= 0.565 else 0 for f_objectivity in Y_train_E['F_Objectivity']]\n",
    "Y_train_E['F_Subjectivity'] = [1 if f_subjectivity >= 0.392 else 0 for f_subjectivity in Y_train_E['F_Subjectivity']]\n",
    "\n",
    "Y_test_E['F_Objectivity'] = [1 if f_objectivity >= 0.565 else 0 for f_objectivity in Y_test_E['F_Objectivity']]\n",
    "Y_test_E['F_Subjectivity'] = [1 if f_subjectivity >= 0.392 else 0 for f_subjectivity in Y_test_E['F_Subjectivity']]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Maps Y_train_C, Y_test_C values into a target column:\n",
    "Y_train_E['target'] = Y_train_E.apply(map_labels, axis=1)\n",
    "Y_test_E['target'] = Y_test_E.apply(map_labels, axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Dropping unnecessariy columns:\n",
    "Y_train_E.drop(['F_Objectivity','F_Subjectivity'], axis=1, inplace=True)\n",
    "Y_test_E.drop(['F_Objectivity','F_Subjectivity'], axis=1, inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>target</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>context</th>\n",
       "      <td>Contextual</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>fabric</th>\n",
       "      <td>Manifest</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>claws</th>\n",
       "      <td>Perceptual</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>path</th>\n",
       "      <td>Manifest</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>fortune</th>\n",
       "      <td>Latent</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "             target\n",
       "context  Contextual\n",
       "fabric     Manifest\n",
       "claws    Perceptual\n",
       "path       Manifest\n",
       "fortune      Latent"
      ]
     },
     "execution_count": 72,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "Y_train_E.sample(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "## Model 01: MLP Classifier for word semantic content\n",
    "- Multilayer perceptron with two continuous outputs between 0 and 1 (sigmoid)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from tensorflow.keras.models import Sequential\n",
    "from tensorflow.keras.layers import Dense"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Training with data (A)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:From c:\\Users\\tiago\\OneDrive - UNIVALI\\PhD\\atividades de pesquisa\\semantic_similarity\\.venv\\Lib\\site-packages\\keras\\src\\backend.py:873: The name tf.get_default_graph is deprecated. Please use tf.compat.v1.get_default_graph instead.\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:From c:\\Users\\tiago\\OneDrive - UNIVALI\\PhD\\atividades de pesquisa\\semantic_similarity\\.venv\\Lib\\site-packages\\keras\\src\\backend.py:873: The name tf.get_default_graph is deprecated. Please use tf.compat.v1.get_default_graph instead.\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:From c:\\Users\\tiago\\OneDrive - UNIVALI\\PhD\\atividades de pesquisa\\semantic_similarity\\.venv\\Lib\\site-packages\\keras\\src\\optimizers\\__init__.py:309: The name tf.train.Optimizer is deprecated. Please use tf.compat.v1.train.Optimizer instead.\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:From c:\\Users\\tiago\\OneDrive - UNIVALI\\PhD\\atividades de pesquisa\\semantic_similarity\\.venv\\Lib\\site-packages\\keras\\src\\optimizers\\__init__.py:309: The name tf.train.Optimizer is deprecated. Please use tf.compat.v1.train.Optimizer instead.\n",
      "\n"
     ]
    }
   ],
   "source": [
    "### Model 01: MLP architecture for continuous output:\n",
    "model_01_A = Sequential([\n",
    "    Dense(256, activation='relu', input_shape=(512,)),\n",
    "    Dense(128, activation='relu'),\n",
    "    Dense(64, activation='relu'),\n",
    "    Dense(32, activation='relu'),\n",
    "    # Dense(2, activation='linear')  # 2 neurons for continuous output between 0 and 1\n",
    "    Dense(2, activation='sigmoid')  # 2 neurons for continuous output between 0 and 1\n",
    "], name='Model_01_A')\n",
    "\n",
    "# Compile the model\n",
    "model_01_A.compile(optimizer='adam',\n",
    "              loss='mean_squared_error',\n",
    "              metrics=['accuracy'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"Model_01_A\"\n",
      "_________________________________________________________________\n",
      " Layer (type)                Output Shape              Param #   \n",
      "=================================================================\n",
      " dense (Dense)               (None, 256)               131328    \n",
      "                                                                 \n",
      " dense_1 (Dense)             (None, 128)               32896     \n",
      "                                                                 \n",
      " dense_2 (Dense)             (None, 64)                8256      \n",
      "                                                                 \n",
      " dense_3 (Dense)             (None, 32)                2080      \n",
      "                                                                 \n",
      " dense_4 (Dense)             (None, 2)                 66        \n",
      "                                                                 \n",
      "=================================================================\n",
      "Total params: 174626 (682.13 KB)\n",
      "Trainable params: 174626 (682.13 KB)\n",
      "Non-trainable params: 0 (0.00 Byte)\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "# Print model summary\n",
    "model_01_A.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[0.36572243 0.64719766]\n",
      " [0.20112169 0.50717064]\n",
      " [0.77447234 0.29110657]\n",
      " [0.01346386 0.47490132]\n",
      " [0.81643835 0.2276313 ]]\n"
     ]
    }
   ],
   "source": [
    "## Transforming data into numpy array:\n",
    "Y_train_A_array = Y_train_A.to_numpy()\n",
    "Y_test_A_array = Y_test_A.to_numpy()\n",
    "\n",
    "X_train_A_array = X_train_A.to_numpy()\n",
    "X_test_A_array = X_test_A.to_numpy()\n",
    "\n",
    "# Print the first few elements to verify\n",
    "print(Y_train_A_array[:5])  # Print the first 5 elements\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/50\n",
      "WARNING:tensorflow:From c:\\Users\\tiago\\OneDrive - UNIVALI\\PhD\\atividades de pesquisa\\semantic_similarity\\.venv\\Lib\\site-packages\\keras\\src\\utils\\tf_utils.py:492: The name tf.ragged.RaggedTensorValue is deprecated. Please use tf.compat.v1.ragged.RaggedTensorValue instead.\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:From c:\\Users\\tiago\\OneDrive - UNIVALI\\PhD\\atividades de pesquisa\\semantic_similarity\\.venv\\Lib\\site-packages\\keras\\src\\utils\\tf_utils.py:492: The name tf.ragged.RaggedTensorValue is deprecated. Please use tf.compat.v1.ragged.RaggedTensorValue instead.\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:From c:\\Users\\tiago\\OneDrive - UNIVALI\\PhD\\atividades de pesquisa\\semantic_similarity\\.venv\\Lib\\site-packages\\keras\\src\\engine\\base_layer_utils.py:384: The name tf.executing_eagerly_outside_functions is deprecated. Please use tf.compat.v1.executing_eagerly_outside_functions instead.\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:From c:\\Users\\tiago\\OneDrive - UNIVALI\\PhD\\atividades de pesquisa\\semantic_similarity\\.venv\\Lib\\site-packages\\keras\\src\\engine\\base_layer_utils.py:384: The name tf.executing_eagerly_outside_functions is deprecated. Please use tf.compat.v1.executing_eagerly_outside_functions instead.\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "176/176 [==============================] - 2s 4ms/step - loss: 0.0239 - accuracy: 0.8131 - val_loss: 0.0160 - val_accuracy: 0.8663\n",
      "Epoch 2/50\n",
      "176/176 [==============================] - 0s 2ms/step - loss: 0.0147 - accuracy: 0.8669 - val_loss: 0.0149 - val_accuracy: 0.8734\n",
      "Epoch 3/50\n",
      "176/176 [==============================] - 0s 2ms/step - loss: 0.0118 - accuracy: 0.8914 - val_loss: 0.0149 - val_accuracy: 0.8848\n",
      "Epoch 4/50\n",
      "176/176 [==============================] - 0s 3ms/step - loss: 0.0089 - accuracy: 0.9096 - val_loss: 0.0157 - val_accuracy: 0.8691\n",
      "Epoch 5/50\n",
      "176/176 [==============================] - 0s 3ms/step - loss: 0.0063 - accuracy: 0.9238 - val_loss: 0.0151 - val_accuracy: 0.8905\n",
      "Epoch 6/50\n",
      "176/176 [==============================] - 0s 3ms/step - loss: 0.0044 - accuracy: 0.9373 - val_loss: 0.0153 - val_accuracy: 0.8919\n",
      "Epoch 7/50\n",
      "176/176 [==============================] - 0s 2ms/step - loss: 0.0034 - accuracy: 0.9434 - val_loss: 0.0151 - val_accuracy: 0.8748\n",
      "Epoch 8/50\n",
      "176/176 [==============================] - 0s 3ms/step - loss: 0.0025 - accuracy: 0.9548 - val_loss: 0.0154 - val_accuracy: 0.8777\n",
      "Epoch 9/50\n",
      "176/176 [==============================] - 0s 3ms/step - loss: 0.0020 - accuracy: 0.9623 - val_loss: 0.0158 - val_accuracy: 0.8734\n",
      "Epoch 10/50\n",
      "176/176 [==============================] - 0s 3ms/step - loss: 0.0015 - accuracy: 0.9648 - val_loss: 0.0154 - val_accuracy: 0.8805\n",
      "Epoch 11/50\n",
      "176/176 [==============================] - 0s 2ms/step - loss: 0.0013 - accuracy: 0.9655 - val_loss: 0.0153 - val_accuracy: 0.8848\n",
      "Epoch 12/50\n",
      "176/176 [==============================] - 0s 2ms/step - loss: 0.0011 - accuracy: 0.9658 - val_loss: 0.0153 - val_accuracy: 0.8848\n",
      "Epoch 13/50\n",
      "176/176 [==============================] - 0s 2ms/step - loss: 0.0011 - accuracy: 0.9708 - val_loss: 0.0151 - val_accuracy: 0.8876\n",
      "Epoch 14/50\n",
      "176/176 [==============================] - 0s 3ms/step - loss: 0.0010 - accuracy: 0.9687 - val_loss: 0.0153 - val_accuracy: 0.8762\n",
      "Epoch 15/50\n",
      "176/176 [==============================] - 0s 2ms/step - loss: 8.5171e-04 - accuracy: 0.9761 - val_loss: 0.0152 - val_accuracy: 0.8819\n",
      "Epoch 16/50\n",
      "176/176 [==============================] - 0s 3ms/step - loss: 7.7034e-04 - accuracy: 0.9722 - val_loss: 0.0157 - val_accuracy: 0.8848\n",
      "Epoch 17/50\n",
      "176/176 [==============================] - 0s 3ms/step - loss: 7.5942e-04 - accuracy: 0.9726 - val_loss: 0.0157 - val_accuracy: 0.8819\n",
      "Epoch 18/50\n",
      "176/176 [==============================] - 0s 2ms/step - loss: 8.4335e-04 - accuracy: 0.9765 - val_loss: 0.0154 - val_accuracy: 0.8777\n",
      "Epoch 19/50\n",
      "176/176 [==============================] - 0s 2ms/step - loss: 8.6171e-04 - accuracy: 0.9676 - val_loss: 0.0154 - val_accuracy: 0.8762\n",
      "Epoch 20/50\n",
      "176/176 [==============================] - 0s 3ms/step - loss: 8.3398e-04 - accuracy: 0.9758 - val_loss: 0.0150 - val_accuracy: 0.8890\n",
      "Epoch 21/50\n",
      "176/176 [==============================] - 0s 2ms/step - loss: 7.4738e-04 - accuracy: 0.9740 - val_loss: 0.0157 - val_accuracy: 0.8834\n",
      "Epoch 22/50\n",
      "176/176 [==============================] - 0s 3ms/step - loss: 7.1572e-04 - accuracy: 0.9729 - val_loss: 0.0151 - val_accuracy: 0.8791\n",
      "Epoch 23/50\n",
      "176/176 [==============================] - 0s 2ms/step - loss: 5.8887e-04 - accuracy: 0.9808 - val_loss: 0.0151 - val_accuracy: 0.8819\n",
      "Epoch 24/50\n",
      "176/176 [==============================] - 0s 3ms/step - loss: 5.3871e-04 - accuracy: 0.9797 - val_loss: 0.0149 - val_accuracy: 0.8834\n",
      "Epoch 25/50\n",
      "176/176 [==============================] - 0s 2ms/step - loss: 5.4492e-04 - accuracy: 0.9783 - val_loss: 0.0153 - val_accuracy: 0.8777\n",
      "Epoch 26/50\n",
      "176/176 [==============================] - 0s 2ms/step - loss: 6.2739e-04 - accuracy: 0.9811 - val_loss: 0.0152 - val_accuracy: 0.8748\n",
      "Epoch 27/50\n",
      "176/176 [==============================] - 0s 3ms/step - loss: 6.5706e-04 - accuracy: 0.9722 - val_loss: 0.0150 - val_accuracy: 0.8791\n",
      "Epoch 28/50\n",
      "176/176 [==============================] - 0s 3ms/step - loss: 6.0812e-04 - accuracy: 0.9772 - val_loss: 0.0154 - val_accuracy: 0.8805\n",
      "Epoch 29/50\n",
      "176/176 [==============================] - 0s 3ms/step - loss: 5.3941e-04 - accuracy: 0.9754 - val_loss: 0.0152 - val_accuracy: 0.8862\n",
      "Epoch 30/50\n",
      "176/176 [==============================] - 0s 3ms/step - loss: 4.7227e-04 - accuracy: 0.9815 - val_loss: 0.0148 - val_accuracy: 0.8748\n",
      "Epoch 31/50\n",
      "176/176 [==============================] - 0s 3ms/step - loss: 5.0918e-04 - accuracy: 0.9840 - val_loss: 0.0153 - val_accuracy: 0.8805\n",
      "Epoch 32/50\n",
      "176/176 [==============================] - 0s 2ms/step - loss: 5.1708e-04 - accuracy: 0.9779 - val_loss: 0.0152 - val_accuracy: 0.8819\n",
      "Epoch 33/50\n",
      "176/176 [==============================] - 0s 2ms/step - loss: 5.5238e-04 - accuracy: 0.9747 - val_loss: 0.0152 - val_accuracy: 0.8819\n",
      "Epoch 34/50\n",
      "176/176 [==============================] - 0s 3ms/step - loss: 5.4166e-04 - accuracy: 0.9751 - val_loss: 0.0148 - val_accuracy: 0.8862\n",
      "Epoch 35/50\n",
      "176/176 [==============================] - 0s 3ms/step - loss: 5.3570e-04 - accuracy: 0.9794 - val_loss: 0.0151 - val_accuracy: 0.8834\n",
      "Epoch 36/50\n",
      "176/176 [==============================] - 0s 3ms/step - loss: 5.3599e-04 - accuracy: 0.9804 - val_loss: 0.0143 - val_accuracy: 0.8976\n",
      "Epoch 37/50\n",
      "176/176 [==============================] - 0s 3ms/step - loss: 5.1191e-04 - accuracy: 0.9797 - val_loss: 0.0152 - val_accuracy: 0.8762\n",
      "Epoch 38/50\n",
      "176/176 [==============================] - 0s 2ms/step - loss: 4.8353e-04 - accuracy: 0.9779 - val_loss: 0.0150 - val_accuracy: 0.8890\n",
      "Epoch 39/50\n",
      "176/176 [==============================] - 0s 3ms/step - loss: 5.6468e-04 - accuracy: 0.9758 - val_loss: 0.0149 - val_accuracy: 0.8919\n",
      "Epoch 40/50\n",
      "176/176 [==============================] - 0s 3ms/step - loss: 5.2470e-04 - accuracy: 0.9772 - val_loss: 0.0148 - val_accuracy: 0.8805\n",
      "Epoch 41/50\n",
      "176/176 [==============================] - 0s 2ms/step - loss: 4.4803e-04 - accuracy: 0.9797 - val_loss: 0.0149 - val_accuracy: 0.8791\n",
      "Epoch 42/50\n",
      "176/176 [==============================] - 0s 3ms/step - loss: 3.8284e-04 - accuracy: 0.9826 - val_loss: 0.0146 - val_accuracy: 0.8933\n",
      "Epoch 43/50\n",
      "176/176 [==============================] - 0s 3ms/step - loss: 3.3947e-04 - accuracy: 0.9858 - val_loss: 0.0150 - val_accuracy: 0.8819\n",
      "Epoch 44/50\n",
      "176/176 [==============================] - 0s 3ms/step - loss: 3.1018e-04 - accuracy: 0.9826 - val_loss: 0.0146 - val_accuracy: 0.8848\n",
      "Epoch 45/50\n",
      "176/176 [==============================] - 0s 2ms/step - loss: 3.1257e-04 - accuracy: 0.9840 - val_loss: 0.0150 - val_accuracy: 0.8791\n",
      "Epoch 46/50\n",
      "176/176 [==============================] - 0s 2ms/step - loss: 3.2035e-04 - accuracy: 0.9804 - val_loss: 0.0147 - val_accuracy: 0.8834\n",
      "Epoch 47/50\n",
      "176/176 [==============================] - 0s 2ms/step - loss: 3.4765e-04 - accuracy: 0.9843 - val_loss: 0.0147 - val_accuracy: 0.8862\n",
      "Epoch 48/50\n",
      "176/176 [==============================] - 0s 2ms/step - loss: 4.0808e-04 - accuracy: 0.9801 - val_loss: 0.0151 - val_accuracy: 0.8734\n",
      "Epoch 49/50\n",
      "176/176 [==============================] - 0s 3ms/step - loss: 5.6282e-04 - accuracy: 0.9761 - val_loss: 0.0152 - val_accuracy: 0.8834\n",
      "Epoch 50/50\n",
      "176/176 [==============================] - 0s 2ms/step - loss: 5.1410e-04 - accuracy: 0.9811 - val_loss: 0.0147 - val_accuracy: 0.8947\n"
     ]
    }
   ],
   "source": [
    "# Train the model\n",
    "history_01_A = model_01_A.fit(X_train_A_array, Y_train_A_array, epochs=50, batch_size=16, validation_split=0.2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "37/37 [==============================] - 0s 2ms/step - loss: 0.0150 - accuracy: 0.8736\n",
      "Model_01_A: Test Accuracy: 87.36%\n"
     ]
    }
   ],
   "source": [
    "# Evaluate the model\n",
    "loss_01_A, accuracy_01_A = model_01_A.evaluate(X_test_A_array, Y_test_A_array)\n",
    "print(f\"{model_01_A.name}: Test Accuracy: {accuracy_01_A * 100:.2f}%\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Applying the trained model to predict SCA for a distinct word:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 79,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Choosing an arbitrary word:\n",
    "entry = 'monster'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 80,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Getting its vector representation (from word embedding) and preparing it to the model input format:\n",
    "new_entry = nlp_getVector(entry)[1]\n",
    "new_entry = np.expand_dims(new_entry, axis=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 81,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[-0.06711828, -0.02067498, -0.04206308, -0.01115861, -0.01383582,\n",
       "         0.04510906,  0.0125503 ,  0.04529705, -0.01273363,  0.02883258,\n",
       "         0.07125575,  0.02193494,  0.03173211, -0.02368355, -0.0614245 ,\n",
       "        -0.00586793, -0.03555945, -0.04534414, -0.0763303 , -0.03360101,\n",
       "        -0.00844153, -0.00830069, -0.03626368,  0.0785375 , -0.03493651,\n",
       "        -0.01950266, -0.01525783, -0.04266157,  0.03850418,  0.02419825,\n",
       "        -0.02083688, -0.01407638,  0.02044228,  0.07852259,  0.04340887,\n",
       "         0.07285666, -0.00532492,  0.04861598,  0.03937482, -0.01199506,\n",
       "         0.04909835,  0.04842913, -0.06813003, -0.08277766, -0.03305084,\n",
       "         0.02141174, -0.05145124, -0.03189183, -0.0289927 , -0.01934705,\n",
       "         0.08942617, -0.05437987,  0.07336873,  0.07985371,  0.02580532,\n",
       "        -0.0114742 ,  0.02669582,  0.01450784,  0.0157063 ,  0.00816931,\n",
       "        -0.00712485, -0.05875389, -0.01702816,  0.0709166 , -0.02216902,\n",
       "         0.05185924, -0.0440516 ,  0.01301391,  0.07202778,  0.07053127,\n",
       "        -0.0450351 ,  0.05243647,  0.00563044,  0.00437846,  0.04061105,\n",
       "        -0.01733659,  0.03512459,  0.01851852, -0.01979814, -0.05983683,\n",
       "        -0.03962196, -0.05020931, -0.05851696,  0.02894764, -0.03248243,\n",
       "         0.01136321,  0.00171177,  0.02173644,  0.01640365,  0.01182559,\n",
       "         0.07400361,  0.05209411, -0.01864924, -0.01118034, -0.04814927,\n",
       "        -0.01120319, -0.06492821, -0.02142214, -0.03181377,  0.03392407,\n",
       "         0.0704332 , -0.0816513 , -0.03497294, -0.02129847, -0.0450225 ,\n",
       "        -0.03289407,  0.05957786, -0.06662274, -0.05521924,  0.04181825,\n",
       "        -0.01162074, -0.08622354, -0.04757201,  0.03609039, -0.04276199,\n",
       "         0.02213067, -0.05575376,  0.02922511,  0.05975431,  0.0064193 ,\n",
       "        -0.04341386, -0.00913664, -0.00333099, -0.03725883, -0.07850551,\n",
       "         0.02661724, -0.04213731, -0.02467634,  0.03402276,  0.00736821,\n",
       "        -0.05107641,  0.03774619,  0.05105195,  0.0134684 ,  0.0289786 ,\n",
       "         0.04829463,  0.07063653, -0.0565299 , -0.04557097, -0.04921497,\n",
       "         0.08018578, -0.05984532,  0.03670865, -0.03256618, -0.03221525,\n",
       "         0.03612246, -0.03250701,  0.0655218 ,  0.06859729,  0.04855406,\n",
       "         0.01850086,  0.03647652, -0.04166167,  0.04445579,  0.01376303,\n",
       "        -0.06742916, -0.05587725, -0.08983698,  0.07933   ,  0.04306247,\n",
       "         0.03947953,  0.03394263,  0.02440294,  0.04100202, -0.06200185,\n",
       "        -0.03846296, -0.01421532, -0.05031564, -0.0126809 , -0.01062615,\n",
       "        -0.01224282, -0.04231019, -0.03191055, -0.02772271, -0.07081798,\n",
       "         0.01158058, -0.01908394,  0.03807647, -0.0418224 , -0.06294686,\n",
       "         0.06703372,  0.04759116,  0.01284644,  0.07536495,  0.0275898 ,\n",
       "        -0.0287855 ,  0.03072294, -0.05281496,  0.03711602,  0.01098745,\n",
       "        -0.05107123,  0.08393139,  0.03980567,  0.02142883, -0.01182614,\n",
       "        -0.07279795,  0.06572303,  0.04171328, -0.00177111, -0.00970582,\n",
       "        -0.02698962, -0.01411453, -0.01508775,  0.0603825 , -0.08387164,\n",
       "         0.02649995, -0.04784083,  0.03101042, -0.05364978,  0.05002271,\n",
       "        -0.07133397,  0.00399393,  0.00866309, -0.02068618,  0.01690331,\n",
       "         0.01954314, -0.02973134, -0.0396944 ,  0.07391305, -0.00903901,\n",
       "        -0.01772554,  0.00448941, -0.0144108 ,  0.02446456, -0.01606818,\n",
       "        -0.01418203,  0.02937238, -0.01892703,  0.00266542, -0.01411678,\n",
       "         0.03638168, -0.06057715, -0.06850306,  0.09320423,  0.06327841,\n",
       "        -0.01958634,  0.01292341, -0.02610868, -0.02170912,  0.05568948,\n",
       "        -0.02771329, -0.03899392,  0.03426711,  0.00855309,  0.03773443,\n",
       "        -0.02668175, -0.02315066, -0.06367406,  0.01230491, -0.06491611,\n",
       "        -0.01349543, -0.00413701, -0.00877292, -0.00127252,  0.03433724,\n",
       "         0.03467945, -0.06629844,  0.04051192,  0.04290714, -0.02186215,\n",
       "        -0.07290918,  0.03449412, -0.00700217, -0.03679583,  0.05529419,\n",
       "         0.04311695,  0.00309905,  0.02440614, -0.02425452,  0.05759618,\n",
       "         0.00976556,  0.01890525,  0.02145768, -0.02347319,  0.06804965,\n",
       "         0.00413682,  0.05827197,  0.07123726,  0.05376603,  0.02766512,\n",
       "         0.02838842,  0.02616286,  0.04313504, -0.06615452,  0.07474884,\n",
       "         0.03510515, -0.00547714,  0.02548585, -0.07473349, -0.03751185,\n",
       "        -0.04806076,  0.01496168, -0.0196933 ,  0.00346894,  0.01542626,\n",
       "         0.02262024, -0.04200267,  0.07909124, -0.02064033, -0.03199983,\n",
       "         0.00663353,  0.00382567, -0.02782056, -0.07176981, -0.0177629 ,\n",
       "        -0.00121269,  0.05730446,  0.0216186 , -0.01122854, -0.04438672,\n",
       "        -0.06298556,  0.01368316,  0.07348434,  0.02874712, -0.06645232,\n",
       "         0.02186381,  0.0368508 ,  0.060555  , -0.04191736, -0.03889085,\n",
       "        -0.01733771, -0.05873377, -0.01190614,  0.0488075 , -0.04514085,\n",
       "         0.03533353, -0.05160102,  0.03367775, -0.05118763,  0.0040271 ,\n",
       "        -0.03381499,  0.02263286,  0.02632216,  0.05800126, -0.07038671,\n",
       "         0.04193743, -0.03063796, -0.08748756,  0.01874119,  0.01614196,\n",
       "        -0.01586968, -0.08812726,  0.0071868 , -0.06033091,  0.01489437,\n",
       "        -0.03956274, -0.01491776,  0.01315633, -0.01047452, -0.05228233,\n",
       "         0.05619019, -0.06973231,  0.06626201, -0.01629842, -0.05026102,\n",
       "         0.00543056, -0.08509177, -0.00927315,  0.08618828,  0.00119508,\n",
       "         0.03932803, -0.06655198,  0.09030107,  0.03011524,  0.0059446 ,\n",
       "         0.0233682 ,  0.05448265, -0.07486712,  0.03850272,  0.07128188,\n",
       "        -0.00536665,  0.03656612,  0.02544851, -0.07673572, -0.05639149,\n",
       "         0.03055414,  0.05483505, -0.02417178, -0.05963452, -0.02465089,\n",
       "        -0.00886857, -0.00781565, -0.05026549,  0.05497279,  0.03969086,\n",
       "        -0.04300754, -0.02116185, -0.07177206,  0.07994553, -0.05605719,\n",
       "         0.02339207,  0.04330398, -0.01679482,  0.04373166, -0.02128954,\n",
       "        -0.02653332, -0.02544849,  0.00195735, -0.05728388,  0.01511187,\n",
       "         0.02668349,  0.04429379, -0.02988668, -0.00741091,  0.00251764,\n",
       "        -0.04988699,  0.02241476,  0.05055011, -0.00727188, -0.04252788,\n",
       "        -0.01193236,  0.04158093, -0.05240043,  0.06266528,  0.01754327,\n",
       "         0.01860948, -0.06732004, -0.00035238,  0.03259436,  0.0452865 ,\n",
       "         0.04629664, -0.03275991,  0.0529843 , -0.03358674,  0.06650908,\n",
       "         0.00266775, -0.01370665,  0.0397798 , -0.07857961, -0.03816452,\n",
       "         0.05714469, -0.05484232,  0.06561863, -0.0363251 ,  0.02951   ,\n",
       "        -0.05087433, -0.04749605, -0.00943782,  0.01575311,  0.04170332,\n",
       "         0.04581205, -0.05343414,  0.02377679, -0.01054661, -0.07624231,\n",
       "         0.02591723,  0.01828944, -0.07874543,  0.03754847,  0.02468277,\n",
       "        -0.08914313,  0.02019696,  0.00675066, -0.0114002 ,  0.03478534,\n",
       "         0.01442098,  0.01610861,  0.00131874, -0.05969476,  0.05758169,\n",
       "         0.01291058, -0.03311213,  0.05333099,  0.04216991, -0.04757321,\n",
       "        -0.08180986, -0.08013847,  0.05534881, -0.07504999,  0.04210687,\n",
       "        -0.0763927 ,  0.00878756,  0.06910869,  0.05476296,  0.03292593,\n",
       "         0.0353524 , -0.04601804, -0.08617313,  0.04731714,  0.05877892,\n",
       "         0.07349286, -0.00991161,  0.00831567,  0.04189058,  0.00506119,\n",
       "         0.0261494 , -0.01846287, -0.02833675,  0.03666588,  0.0110497 ,\n",
       "        -0.0098734 ,  0.03833009,  0.00912972, -0.03001252,  0.07711986,\n",
       "         0.04738649,  0.05291271,  0.00492299,  0.04787276,  0.00761036,\n",
       "        -0.05896797, -0.04937851,  0.03261783, -0.09023612,  0.0205583 ,\n",
       "        -0.05908166,  0.05895694, -0.07963181, -0.0379951 , -0.07322498,\n",
       "         0.05018917, -0.07364497]], dtype=float32)"
      ]
     },
     "execution_count": 81,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "## Showing how this entry is laid out:\n",
    "new_entry"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 82,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1/1 [==============================] - 0s 100ms/step\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "array([[0.77475625, 0.7370473 ]], dtype=float32)"
      ]
     },
     "execution_count": 82,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "## Predicting the objective and subjective loads for the 'entry' word:\n",
    "result = model_01_A.predict(new_entry)\n",
    "result"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 83,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Generalizing the process above for any word through a python method:\n",
    "def wordClassifier_regression(word, model=model_01_A):\n",
    "    '''Given a word vector, shows the probability for objective and subjective semantic content, respectively.'''\n",
    "    new_entry = nlp_getVector(word)\n",
    "    if new_entry:\n",
    "        vector = nlp_getVector(word)[1]\n",
    "        vector = np.expand_dims(vector, axis=0)\n",
    "        result = model.predict(vector)\n",
    "    else:\n",
    "        print('Word not existent in database.')\n",
    "        return\n",
    "    print(f'--- {word}:\\n{result[0][0]*100:.2f} of objectivity\\n{result[0][1]*100:.2f} of subjectivity')\n",
    "    print(f'Model used: {model.name}')  # Print the name of the model\n",
    "    return\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 85,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1/1 [==============================] - 0s 25ms/step\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "--- hippopotamus:\n",
      "95.59 of objectivity\n",
      "36.98 of subjectivity\n",
      "Model used: Model_01_A\n"
     ]
    }
   ],
   "source": [
    "## Experimenting with a trial word:\n",
    "trial_word = 'hippopotamus'\n",
    "wordClassifier_regression(trial_word)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 90,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1/1 [==============================] - 0s 23ms/step\n",
      "--- I dreamed with a hippopotamus:\n",
      "76.03 of objectivity\n",
      "63.14 of subjectivity\n",
      "Model used: Model_01_A\n"
     ]
    }
   ],
   "source": [
    "## Experimenting with a trial word:\n",
    "trial_phrase = 'I dreamed with a hippopotamus'\n",
    "wordClassifier_regression(trial_phrase)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 91,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>words</th>\n",
       "      <th>F_Objectivity</th>\n",
       "      <th>F_Subjectivity</th>\n",
       "      <th>F_Context</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>2387</th>\n",
       "      <td>hippopotamus</td>\n",
       "      <td>0.973593</td>\n",
       "      <td>0.377959</td>\n",
       "      <td>0.309328</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "             words  F_Objectivity  F_Subjectivity  F_Context\n",
       "2387  hippopotamus       0.973593        0.377959   0.309328"
      ]
     },
     "execution_count": 91,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "## Checking the same trial word in the original SCA dataset (if available)\n",
    "df_factors[df_factors['words']==trial_word]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Training with data (D)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 92,
   "metadata": {},
   "outputs": [],
   "source": [
    "### Model 01: MLP architecture for continuous output:\n",
    "model_01_D = Sequential([\n",
    "    Dense(256, activation='relu', input_shape=(512,)),\n",
    "    Dense(128, activation='relu'),\n",
    "    Dense(64, activation='relu'),\n",
    "    Dense(32, activation='relu'),\n",
    "    # Dense(2, activation='linear')  # 2 neurons for continuous output between 0 and 1\n",
    "    Dense(2, activation='sigmoid')  # 2 neurons for continuous output between 0 and 1\n",
    "], name='Model_01_D')\n",
    "\n",
    "# Compile the model\n",
    "model_01_D.compile(optimizer='adam',\n",
    "              loss='mean_squared_error',\n",
    "              metrics=['accuracy'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 93,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"Model_01_D\"\n",
      "_________________________________________________________________\n",
      " Layer (type)                Output Shape              Param #   \n",
      "=================================================================\n",
      " dense_5 (Dense)             (None, 256)               131328    \n",
      "                                                                 \n",
      " dense_6 (Dense)             (None, 128)               32896     \n",
      "                                                                 \n",
      " dense_7 (Dense)             (None, 64)                8256      \n",
      "                                                                 \n",
      " dense_8 (Dense)             (None, 32)                2080      \n",
      "                                                                 \n",
      " dense_9 (Dense)             (None, 2)                 66        \n",
      "                                                                 \n",
      "=================================================================\n",
      "Total params: 174626 (682.13 KB)\n",
      "Trainable params: 174626 (682.13 KB)\n",
      "Non-trainable params: 0 (0.00 Byte)\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "# Print model summary\n",
    "model_01_D.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 94,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[0.15859087 0.58544867]\n",
      " [0.14240247 0.50945274]\n",
      " [0.95035619 0.17403995]\n",
      " [0.21539557 0.31870239]\n",
      " [0.25452163 0.66739412]]\n"
     ]
    }
   ],
   "source": [
    "## Transforming data into numpy array:\n",
    "Y_train_D_array = Y_train_D.to_numpy()\n",
    "Y_test_D_array = Y_test_D.to_numpy()\n",
    "\n",
    "X_train_D_array = X_train_D.to_numpy()\n",
    "X_test_D_array = X_test_D.to_numpy()\n",
    "\n",
    "# Print the first few elements to verify\n",
    "print(Y_train_D_array[:5])  # Print the first 5 elements\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 95,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/50\n",
      "131/131 [==============================] - 2s 4ms/step - loss: 0.0305 - accuracy: 0.8129 - val_loss: 0.0191 - val_accuracy: 0.8776\n",
      "Epoch 2/50\n",
      "131/131 [==============================] - 0s 3ms/step - loss: 0.0156 - accuracy: 0.9014 - val_loss: 0.0172 - val_accuracy: 0.8948\n",
      "Epoch 3/50\n",
      "131/131 [==============================] - 0s 2ms/step - loss: 0.0126 - accuracy: 0.9134 - val_loss: 0.0160 - val_accuracy: 0.9025\n",
      "Epoch 4/50\n",
      "131/131 [==============================] - 0s 2ms/step - loss: 0.0095 - accuracy: 0.9220 - val_loss: 0.0167 - val_accuracy: 0.8910\n",
      "Epoch 5/50\n",
      "131/131 [==============================] - 0s 2ms/step - loss: 0.0071 - accuracy: 0.9388 - val_loss: 0.0155 - val_accuracy: 0.8948\n",
      "Epoch 6/50\n",
      "131/131 [==============================] - 0s 2ms/step - loss: 0.0052 - accuracy: 0.9474 - val_loss: 0.0161 - val_accuracy: 0.8815\n",
      "Epoch 7/50\n",
      "131/131 [==============================] - 0s 3ms/step - loss: 0.0039 - accuracy: 0.9617 - val_loss: 0.0160 - val_accuracy: 0.8967\n",
      "Epoch 8/50\n",
      "131/131 [==============================] - 0s 2ms/step - loss: 0.0028 - accuracy: 0.9718 - val_loss: 0.0163 - val_accuracy: 0.8987\n",
      "Epoch 9/50\n",
      "131/131 [==============================] - 0s 2ms/step - loss: 0.0024 - accuracy: 0.9679 - val_loss: 0.0158 - val_accuracy: 0.8853\n",
      "Epoch 10/50\n",
      "131/131 [==============================] - 0s 3ms/step - loss: 0.0019 - accuracy: 0.9699 - val_loss: 0.0161 - val_accuracy: 0.8948\n",
      "Epoch 11/50\n",
      "131/131 [==============================] - 0s 3ms/step - loss: 0.0015 - accuracy: 0.9737 - val_loss: 0.0153 - val_accuracy: 0.8987\n",
      "Epoch 12/50\n",
      "131/131 [==============================] - 0s 3ms/step - loss: 0.0013 - accuracy: 0.9761 - val_loss: 0.0156 - val_accuracy: 0.9006\n",
      "Epoch 13/50\n",
      "131/131 [==============================] - 0s 2ms/step - loss: 0.0012 - accuracy: 0.9756 - val_loss: 0.0152 - val_accuracy: 0.8910\n",
      "Epoch 14/50\n",
      "131/131 [==============================] - 0s 3ms/step - loss: 0.0010 - accuracy: 0.9794 - val_loss: 0.0156 - val_accuracy: 0.8891\n",
      "Epoch 15/50\n",
      "131/131 [==============================] - 0s 2ms/step - loss: 8.9701e-04 - accuracy: 0.9761 - val_loss: 0.0162 - val_accuracy: 0.8910\n",
      "Epoch 16/50\n",
      "131/131 [==============================] - 0s 3ms/step - loss: 8.9808e-04 - accuracy: 0.9809 - val_loss: 0.0156 - val_accuracy: 0.8872\n",
      "Epoch 17/50\n",
      "131/131 [==============================] - 0s 2ms/step - loss: 9.2980e-04 - accuracy: 0.9818 - val_loss: 0.0157 - val_accuracy: 0.8987\n",
      "Epoch 18/50\n",
      "131/131 [==============================] - 0s 2ms/step - loss: 8.0996e-04 - accuracy: 0.9789 - val_loss: 0.0156 - val_accuracy: 0.8929\n",
      "Epoch 19/50\n",
      "131/131 [==============================] - 0s 2ms/step - loss: 7.9295e-04 - accuracy: 0.9823 - val_loss: 0.0157 - val_accuracy: 0.8910\n",
      "Epoch 20/50\n",
      "131/131 [==============================] - 0s 3ms/step - loss: 7.8687e-04 - accuracy: 0.9823 - val_loss: 0.0156 - val_accuracy: 0.8948\n",
      "Epoch 21/50\n",
      "131/131 [==============================] - 0s 2ms/step - loss: 6.2611e-04 - accuracy: 0.9809 - val_loss: 0.0155 - val_accuracy: 0.8948\n",
      "Epoch 22/50\n",
      "131/131 [==============================] - 0s 3ms/step - loss: 6.5340e-04 - accuracy: 0.9780 - val_loss: 0.0156 - val_accuracy: 0.8853\n",
      "Epoch 23/50\n",
      "131/131 [==============================] - 0s 3ms/step - loss: 6.9094e-04 - accuracy: 0.9813 - val_loss: 0.0156 - val_accuracy: 0.8853\n",
      "Epoch 24/50\n",
      "131/131 [==============================] - 0s 3ms/step - loss: 6.3829e-04 - accuracy: 0.9828 - val_loss: 0.0157 - val_accuracy: 0.9025\n",
      "Epoch 25/50\n",
      "131/131 [==============================] - 0s 2ms/step - loss: 5.3907e-04 - accuracy: 0.9828 - val_loss: 0.0154 - val_accuracy: 0.8948\n",
      "Epoch 26/50\n",
      "131/131 [==============================] - 0s 3ms/step - loss: 5.0125e-04 - accuracy: 0.9842 - val_loss: 0.0159 - val_accuracy: 0.8948\n",
      "Epoch 27/50\n",
      "131/131 [==============================] - 0s 2ms/step - loss: 5.2607e-04 - accuracy: 0.9828 - val_loss: 0.0148 - val_accuracy: 0.9006\n",
      "Epoch 28/50\n",
      "131/131 [==============================] - 0s 2ms/step - loss: 5.1932e-04 - accuracy: 0.9823 - val_loss: 0.0155 - val_accuracy: 0.8910\n",
      "Epoch 29/50\n",
      "131/131 [==============================] - 0s 3ms/step - loss: 4.8328e-04 - accuracy: 0.9847 - val_loss: 0.0152 - val_accuracy: 0.8967\n",
      "Epoch 30/50\n",
      "131/131 [==============================] - 0s 3ms/step - loss: 5.0561e-04 - accuracy: 0.9847 - val_loss: 0.0152 - val_accuracy: 0.8948\n",
      "Epoch 31/50\n",
      "131/131 [==============================] - 0s 3ms/step - loss: 4.8082e-04 - accuracy: 0.9871 - val_loss: 0.0155 - val_accuracy: 0.8987\n",
      "Epoch 32/50\n",
      "131/131 [==============================] - 0s 2ms/step - loss: 5.0627e-04 - accuracy: 0.9842 - val_loss: 0.0155 - val_accuracy: 0.8910\n",
      "Epoch 33/50\n",
      "131/131 [==============================] - 0s 2ms/step - loss: 5.2203e-04 - accuracy: 0.9823 - val_loss: 0.0154 - val_accuracy: 0.8910\n",
      "Epoch 34/50\n",
      "131/131 [==============================] - 0s 3ms/step - loss: 5.5407e-04 - accuracy: 0.9828 - val_loss: 0.0152 - val_accuracy: 0.8929\n",
      "Epoch 35/50\n",
      "131/131 [==============================] - 0s 2ms/step - loss: 6.2365e-04 - accuracy: 0.9842 - val_loss: 0.0158 - val_accuracy: 0.8872\n",
      "Epoch 36/50\n",
      "131/131 [==============================] - 0s 3ms/step - loss: 6.2321e-04 - accuracy: 0.9813 - val_loss: 0.0152 - val_accuracy: 0.8967\n",
      "Epoch 37/50\n",
      "131/131 [==============================] - 0s 2ms/step - loss: 5.1081e-04 - accuracy: 0.9861 - val_loss: 0.0152 - val_accuracy: 0.8948\n",
      "Epoch 38/50\n",
      "131/131 [==============================] - 0s 3ms/step - loss: 4.4132e-04 - accuracy: 0.9852 - val_loss: 0.0152 - val_accuracy: 0.8967\n",
      "Epoch 39/50\n",
      "131/131 [==============================] - 0s 3ms/step - loss: 3.8899e-04 - accuracy: 0.9861 - val_loss: 0.0150 - val_accuracy: 0.8929\n",
      "Epoch 40/50\n",
      "131/131 [==============================] - 0s 3ms/step - loss: 3.6936e-04 - accuracy: 0.9895 - val_loss: 0.0154 - val_accuracy: 0.8929\n",
      "Epoch 41/50\n",
      "131/131 [==============================] - 0s 3ms/step - loss: 3.3216e-04 - accuracy: 0.9885 - val_loss: 0.0150 - val_accuracy: 0.8987\n",
      "Epoch 42/50\n",
      "131/131 [==============================] - 0s 3ms/step - loss: 3.1816e-04 - accuracy: 0.9880 - val_loss: 0.0151 - val_accuracy: 0.9006\n",
      "Epoch 43/50\n",
      "131/131 [==============================] - 0s 3ms/step - loss: 3.4704e-04 - accuracy: 0.9871 - val_loss: 0.0153 - val_accuracy: 0.8948\n",
      "Epoch 44/50\n",
      "131/131 [==============================] - 0s 3ms/step - loss: 3.8542e-04 - accuracy: 0.9861 - val_loss: 0.0151 - val_accuracy: 0.8929\n",
      "Epoch 45/50\n",
      "131/131 [==============================] - 0s 2ms/step - loss: 3.9038e-04 - accuracy: 0.9837 - val_loss: 0.0149 - val_accuracy: 0.8948\n",
      "Epoch 46/50\n",
      "131/131 [==============================] - 0s 3ms/step - loss: 3.9379e-04 - accuracy: 0.9861 - val_loss: 0.0152 - val_accuracy: 0.9063\n",
      "Epoch 47/50\n",
      "131/131 [==============================] - 0s 3ms/step - loss: 5.4000e-04 - accuracy: 0.9804 - val_loss: 0.0152 - val_accuracy: 0.9025\n",
      "Epoch 48/50\n",
      "131/131 [==============================] - 0s 2ms/step - loss: 5.6636e-04 - accuracy: 0.9828 - val_loss: 0.0151 - val_accuracy: 0.9006\n",
      "Epoch 49/50\n",
      "131/131 [==============================] - 0s 3ms/step - loss: 5.0910e-04 - accuracy: 0.9837 - val_loss: 0.0151 - val_accuracy: 0.8987\n",
      "Epoch 50/50\n",
      "131/131 [==============================] - 0s 2ms/step - loss: 4.0114e-04 - accuracy: 0.9885 - val_loss: 0.0153 - val_accuracy: 0.9006\n"
     ]
    }
   ],
   "source": [
    "# Train the model\n",
    "history_01_D = model_01_D.fit(X_train_D_array, Y_train_D_array, epochs=50, batch_size=16, validation_split=0.2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 96,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "28/28 [==============================] - 0s 2ms/step - loss: 0.0164 - accuracy: 0.8898\n",
      "Model_01_D: Test Accuracy: 88.98%\n"
     ]
    }
   ],
   "source": [
    "# Evaluate the model\n",
    "loss_01_D, accuracy_01_D = model_01_D.evaluate(X_test_D_array, Y_test_D_array)\n",
    "print(f\"{model_01_D.name}: Test Accuracy: {accuracy_01_D * 100:.2f}%\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Applying Model_01_D to predict SCA for a distinct word:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 97,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1/1 [==============================] - 0s 21ms/step\n",
      "--- child:\n",
      "86.78 of objectivity\n",
      "37.10 of subjectivity\n",
      "Model used: Model_01_A\n",
      "1/1 [==============================] - 0s 74ms/step\n",
      "--- child:\n",
      "89.81 of objectivity\n",
      "38.68 of subjectivity\n",
      "Model used: Model_01_D\n"
     ]
    }
   ],
   "source": [
    "## Experimenting with a trial word:\n",
    "trial_word = 'child'\n",
    "wordClassifier_regression(trial_word, model=model_01_A)\n",
    "wordClassifier_regression(trial_word, model=model_01_D)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 98,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>words</th>\n",
       "      <th>F_Objectivity</th>\n",
       "      <th>F_Subjectivity</th>\n",
       "      <th>F_Context</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>859</th>\n",
       "      <td>child</td>\n",
       "      <td>0.90202</td>\n",
       "      <td>0.380464</td>\n",
       "      <td>0.233721</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "     words  F_Objectivity  F_Subjectivity  F_Context\n",
       "859  child        0.90202        0.380464   0.233721"
      ]
     },
     "execution_count": 98,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "## Checking the same trial word in the original SCA dataset (if available)\n",
    "df_factors[df_factors['words']==trial_word]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "----\n",
    "## Model 02: MLP for multilabel classification:\n",
    "- Multilayer perceptron with a single multilabel output (softmax activation) based on SCA\n",
    "- Output: Objective (0) or Subjective (1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 100,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.preprocessing import OneHotEncoder"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Training with data (C)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 101,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Preparing data:\n",
    "encoder_oneHot_C = OneHotEncoder()\n",
    "Y_train_C_encoded = encoder_oneHot_C.fit_transform(Y_train_C[['target']]).toarray()\n",
    "Y_test_C_encoded = encoder_oneHot_C.fit_transform(Y_test_C[['target']]).toarray()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 102,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Transforming data into numpy array -- Y data was transformed above:\n",
    "X_train_C_array = X_train_C.to_numpy()\n",
    "X_test_C_array = X_test_C.to_numpy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 108,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_02_C = Sequential([\n",
    "    Dense(480, activation='relu', input_shape=(X_train_C.shape[1],)), ## Equivalent to input_shape=(300,)\n",
    "    Dense(300, activation='relu'),\n",
    "    Dense(150, activation='relu'),\n",
    "    Dense(100, activation='relu'),\n",
    "    Dense(64, activation='relu'),\n",
    "    Dense(32, activation='relu'),\n",
    "    Dense(Y_train_C_encoded.shape[1], activation='softmax')  # Output layer with softmax activation for multiclass classification\n",
    "], name='Model_02_C')\n",
    "\n",
    "# Compile the model\n",
    "model_02_C.compile(optimizer='adam',\n",
    "                 loss='categorical_crossentropy', \n",
    "                 metrics=['accuracy'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 109,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"Model_02_C\"\n",
      "_________________________________________________________________\n",
      " Layer (type)                Output Shape              Param #   \n",
      "=================================================================\n",
      " dense_22 (Dense)            (None, 480)               246240    \n",
      "                                                                 \n",
      " dense_23 (Dense)            (None, 300)               144300    \n",
      "                                                                 \n",
      " dense_24 (Dense)            (None, 150)               45150     \n",
      "                                                                 \n",
      " dense_25 (Dense)            (None, 100)               15100     \n",
      "                                                                 \n",
      " dense_26 (Dense)            (None, 64)                6464      \n",
      "                                                                 \n",
      " dense_27 (Dense)            (None, 32)                2080      \n",
      "                                                                 \n",
      " dense_28 (Dense)            (None, 4)                 132       \n",
      "                                                                 \n",
      "=================================================================\n",
      "Total params: 459466 (1.75 MB)\n",
      "Trainable params: 459466 (1.75 MB)\n",
      "Non-trainable params: 0 (0.00 Byte)\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "model_02_C.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 110,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/50\n",
      "176/176 [==============================] - 2s 5ms/step - loss: 1.0135 - accuracy: 0.5803 - val_loss: 0.8517 - val_accuracy: 0.6373\n",
      "Epoch 2/50\n",
      "176/176 [==============================] - 1s 4ms/step - loss: 0.7378 - accuracy: 0.7077 - val_loss: 0.7803 - val_accuracy: 0.6686\n",
      "Epoch 3/50\n",
      "176/176 [==============================] - 1s 4ms/step - loss: 0.5924 - accuracy: 0.7558 - val_loss: 0.8785 - val_accuracy: 0.6486\n",
      "Epoch 4/50\n",
      "176/176 [==============================] - 1s 4ms/step - loss: 0.4456 - accuracy: 0.8270 - val_loss: 0.9264 - val_accuracy: 0.6401\n",
      "Epoch 5/50\n",
      "176/176 [==============================] - 1s 4ms/step - loss: 0.2833 - accuracy: 0.8936 - val_loss: 1.0602 - val_accuracy: 0.6486\n",
      "Epoch 6/50\n",
      "176/176 [==============================] - 1s 4ms/step - loss: 0.1973 - accuracy: 0.9313 - val_loss: 1.1969 - val_accuracy: 0.6273\n",
      "Epoch 7/50\n",
      "176/176 [==============================] - 1s 4ms/step - loss: 0.1220 - accuracy: 0.9598 - val_loss: 1.2918 - val_accuracy: 0.6358\n",
      "Epoch 8/50\n",
      "176/176 [==============================] - 1s 4ms/step - loss: 0.0810 - accuracy: 0.9737 - val_loss: 1.6359 - val_accuracy: 0.6387\n",
      "Epoch 9/50\n",
      "176/176 [==============================] - 1s 3ms/step - loss: 0.0559 - accuracy: 0.9794 - val_loss: 1.9899 - val_accuracy: 0.6515\n",
      "Epoch 10/50\n",
      "176/176 [==============================] - 1s 4ms/step - loss: 0.0716 - accuracy: 0.9808 - val_loss: 1.6643 - val_accuracy: 0.6785\n",
      "Epoch 11/50\n",
      "176/176 [==============================] - 1s 4ms/step - loss: 0.0775 - accuracy: 0.9754 - val_loss: 1.9824 - val_accuracy: 0.6529\n",
      "Epoch 12/50\n",
      "176/176 [==============================] - 1s 4ms/step - loss: 0.0788 - accuracy: 0.9737 - val_loss: 1.5979 - val_accuracy: 0.6287\n",
      "Epoch 13/50\n",
      "176/176 [==============================] - 1s 3ms/step - loss: 0.0528 - accuracy: 0.9811 - val_loss: 2.2227 - val_accuracy: 0.6558\n",
      "Epoch 14/50\n",
      "176/176 [==============================] - 1s 4ms/step - loss: 0.0728 - accuracy: 0.9769 - val_loss: 1.9536 - val_accuracy: 0.6543\n",
      "Epoch 15/50\n",
      "176/176 [==============================] - 1s 4ms/step - loss: 0.0040 - accuracy: 0.9996 - val_loss: 2.3833 - val_accuracy: 0.6600\n",
      "Epoch 16/50\n",
      "176/176 [==============================] - 1s 3ms/step - loss: 4.9618e-04 - accuracy: 1.0000 - val_loss: 2.5194 - val_accuracy: 0.6572\n",
      "Epoch 17/50\n",
      "176/176 [==============================] - 1s 4ms/step - loss: 2.4764e-04 - accuracy: 1.0000 - val_loss: 2.6358 - val_accuracy: 0.6600\n",
      "Epoch 18/50\n",
      "176/176 [==============================] - 1s 4ms/step - loss: 1.6312e-04 - accuracy: 1.0000 - val_loss: 2.7298 - val_accuracy: 0.6586\n",
      "Epoch 19/50\n",
      "176/176 [==============================] - 1s 4ms/step - loss: 1.1672e-04 - accuracy: 1.0000 - val_loss: 2.8099 - val_accuracy: 0.6615\n",
      "Epoch 20/50\n",
      "176/176 [==============================] - 1s 4ms/step - loss: 8.8105e-05 - accuracy: 1.0000 - val_loss: 2.8793 - val_accuracy: 0.6615\n",
      "Epoch 21/50\n",
      "176/176 [==============================] - 1s 4ms/step - loss: 6.8681e-05 - accuracy: 1.0000 - val_loss: 2.9438 - val_accuracy: 0.6615\n",
      "Epoch 22/50\n",
      "176/176 [==============================] - 1s 4ms/step - loss: 5.4886e-05 - accuracy: 1.0000 - val_loss: 3.0012 - val_accuracy: 0.6629\n",
      "Epoch 23/50\n",
      "176/176 [==============================] - 1s 4ms/step - loss: 4.4687e-05 - accuracy: 1.0000 - val_loss: 3.0547 - val_accuracy: 0.6629\n",
      "Epoch 24/50\n",
      "176/176 [==============================] - 1s 3ms/step - loss: 3.6865e-05 - accuracy: 1.0000 - val_loss: 3.1044 - val_accuracy: 0.6629\n",
      "Epoch 25/50\n",
      "176/176 [==============================] - 1s 4ms/step - loss: 3.0749e-05 - accuracy: 1.0000 - val_loss: 3.1540 - val_accuracy: 0.6643\n",
      "Epoch 26/50\n",
      "176/176 [==============================] - 1s 4ms/step - loss: 2.5755e-05 - accuracy: 1.0000 - val_loss: 3.2022 - val_accuracy: 0.6643\n",
      "Epoch 27/50\n",
      "176/176 [==============================] - 1s 4ms/step - loss: 2.1633e-05 - accuracy: 1.0000 - val_loss: 3.2515 - val_accuracy: 0.6643\n",
      "Epoch 28/50\n",
      "176/176 [==============================] - 1s 4ms/step - loss: 1.7865e-05 - accuracy: 1.0000 - val_loss: 3.3098 - val_accuracy: 0.6643\n",
      "Epoch 29/50\n",
      "176/176 [==============================] - 1s 4ms/step - loss: 1.4130e-05 - accuracy: 1.0000 - val_loss: 3.3889 - val_accuracy: 0.6643\n",
      "Epoch 30/50\n",
      "176/176 [==============================] - 1s 4ms/step - loss: 9.9676e-06 - accuracy: 1.0000 - val_loss: 3.5258 - val_accuracy: 0.6643\n",
      "Epoch 31/50\n",
      "176/176 [==============================] - 1s 4ms/step - loss: 5.9819e-06 - accuracy: 1.0000 - val_loss: 3.6972 - val_accuracy: 0.6657\n",
      "Epoch 32/50\n",
      "176/176 [==============================] - 1s 3ms/step - loss: 3.4964e-06 - accuracy: 1.0000 - val_loss: 3.8611 - val_accuracy: 0.6671\n",
      "Epoch 33/50\n",
      "176/176 [==============================] - 1s 4ms/step - loss: 2.2222e-06 - accuracy: 1.0000 - val_loss: 4.0010 - val_accuracy: 0.6671\n",
      "Epoch 34/50\n",
      "176/176 [==============================] - 1s 4ms/step - loss: 1.5509e-06 - accuracy: 1.0000 - val_loss: 4.1136 - val_accuracy: 0.6671\n",
      "Epoch 35/50\n",
      "176/176 [==============================] - 1s 3ms/step - loss: 1.1730e-06 - accuracy: 1.0000 - val_loss: 4.2075 - val_accuracy: 0.6671\n",
      "Epoch 36/50\n",
      "176/176 [==============================] - 1s 4ms/step - loss: 9.2833e-07 - accuracy: 1.0000 - val_loss: 4.2876 - val_accuracy: 0.6671\n",
      "Epoch 37/50\n",
      "176/176 [==============================] - 1s 3ms/step - loss: 7.6252e-07 - accuracy: 1.0000 - val_loss: 4.3601 - val_accuracy: 0.6671\n",
      "Epoch 38/50\n",
      "176/176 [==============================] - 1s 4ms/step - loss: 6.4238e-07 - accuracy: 1.0000 - val_loss: 4.4243 - val_accuracy: 0.6686\n",
      "Epoch 39/50\n",
      "176/176 [==============================] - 1s 4ms/step - loss: 5.5199e-07 - accuracy: 1.0000 - val_loss: 4.4839 - val_accuracy: 0.6671\n",
      "Epoch 40/50\n",
      "176/176 [==============================] - 1s 3ms/step - loss: 4.7798e-07 - accuracy: 1.0000 - val_loss: 4.5404 - val_accuracy: 0.6686\n",
      "Epoch 41/50\n",
      "176/176 [==============================] - 1s 4ms/step - loss: 4.1895e-07 - accuracy: 1.0000 - val_loss: 4.5927 - val_accuracy: 0.6686\n",
      "Epoch 42/50\n",
      "176/176 [==============================] - 1s 4ms/step - loss: 3.7006e-07 - accuracy: 1.0000 - val_loss: 4.6411 - val_accuracy: 0.6671\n",
      "Epoch 43/50\n",
      "176/176 [==============================] - 1s 4ms/step - loss: 3.2894e-07 - accuracy: 1.0000 - val_loss: 4.6902 - val_accuracy: 0.6671\n",
      "Epoch 44/50\n",
      "176/176 [==============================] - 1s 3ms/step - loss: 2.9257e-07 - accuracy: 1.0000 - val_loss: 4.7339 - val_accuracy: 0.6671\n",
      "Epoch 45/50\n",
      "176/176 [==============================] - 1s 4ms/step - loss: 2.6486e-07 - accuracy: 1.0000 - val_loss: 4.7755 - val_accuracy: 0.6671\n",
      "Epoch 46/50\n",
      "176/176 [==============================] - 1s 4ms/step - loss: 2.3939e-07 - accuracy: 1.0000 - val_loss: 4.8142 - val_accuracy: 0.6671\n",
      "Epoch 47/50\n",
      "176/176 [==============================] - 1s 4ms/step - loss: 2.1656e-07 - accuracy: 1.0000 - val_loss: 4.8546 - val_accuracy: 0.6671\n",
      "Epoch 48/50\n",
      "176/176 [==============================] - 1s 4ms/step - loss: 1.9768e-07 - accuracy: 1.0000 - val_loss: 4.8929 - val_accuracy: 0.6671\n",
      "Epoch 49/50\n",
      "176/176 [==============================] - 1s 4ms/step - loss: 1.7824e-07 - accuracy: 1.0000 - val_loss: 4.9329 - val_accuracy: 0.6686\n",
      "Epoch 50/50\n",
      "176/176 [==============================] - 1s 4ms/step - loss: 1.6241e-07 - accuracy: 1.0000 - val_loss: 4.9669 - val_accuracy: 0.6671\n"
     ]
    }
   ],
   "source": [
    "# Train the model\n",
    "history_02_C = model_02_C.fit(X_train_C_array, Y_train_C_encoded, epochs=50, batch_size=16, validation_split=0.2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 111,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " 1/37 [..............................] - ETA: 0s - loss: 7.0732 - accuracy: 0.6250"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "37/37 [==============================] - 0s 2ms/step - loss: 4.7114 - accuracy: 0.6849\n",
      "Model_02_C: Test Accuracy: 68.49%\n"
     ]
    }
   ],
   "source": [
    "# Evaluate the model\n",
    "loss_02_C, accuracy_02_C = model_02_C.evaluate(X_test_C_array, Y_test_C_encoded)\n",
    "print(f\"{model_02_C.name}: Test Accuracy: {accuracy_02_C * 100:.2f}%\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Training with data (E)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 112,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Preparing data:\n",
    "encoder_oneHot_E = OneHotEncoder()\n",
    "Y_train_E_encoded = encoder_oneHot_E.fit_transform(Y_train_E[['target']]).toarray()\n",
    "Y_test_E_encoded = encoder_oneHot_E.fit_transform(Y_test_E[['target']]).toarray()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 113,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Transforming data into numpy array -- Y data was transformed above:\n",
    "X_train_E_array = X_train_E.to_numpy()\n",
    "X_test_E_array = X_test_E.to_numpy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 114,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"Model_02_E\"\n",
      "_________________________________________________________________\n",
      " Layer (type)                Output Shape              Param #   \n",
      "=================================================================\n",
      " dense_29 (Dense)            (None, 300)               153900    \n",
      "                                                                 \n",
      " dense_30 (Dense)            (None, 150)               45150     \n",
      "                                                                 \n",
      " dense_31 (Dense)            (None, 100)               15100     \n",
      "                                                                 \n",
      " dense_32 (Dense)            (None, 64)                6464      \n",
      "                                                                 \n",
      " dense_33 (Dense)            (None, 32)                2080      \n",
      "                                                                 \n",
      " dense_34 (Dense)            (None, 4)                 132       \n",
      "                                                                 \n",
      "=================================================================\n",
      "Total params: 222826 (870.41 KB)\n",
      "Trainable params: 222826 (870.41 KB)\n",
      "Non-trainable params: 0 (0.00 Byte)\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "model_02_E = Sequential([\n",
    "    Dense(300, activation='relu', input_shape=(X_train_E.shape[1],)), ## Equivalent to input_shape=(300,)\n",
    "    Dense(150, activation='relu'),\n",
    "    Dense(100, activation='relu'),\n",
    "    Dense(64, activation='relu'),\n",
    "    Dense(32, activation='relu'),\n",
    "    Dense(Y_train_E_encoded.shape[1], activation='softmax')  # Output layer with softmax activation for multiclass classification\n",
    "], name='Model_02_E')\n",
    "\n",
    "# Compile the model\n",
    "model_02_E.compile(optimizer='adam',\n",
    "                 loss='categorical_crossentropy', \n",
    "                 metrics=['accuracy'])\n",
    "\n",
    "model_02_E.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 115,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/50\n",
      "131/131 [==============================] - 2s 5ms/step - loss: 0.9530 - accuracy: 0.6081 - val_loss: 0.7054 - val_accuracy: 0.7017\n",
      "Epoch 2/50\n",
      "131/131 [==============================] - 0s 3ms/step - loss: 0.6124 - accuracy: 0.7541 - val_loss: 0.6711 - val_accuracy: 0.7151\n",
      "Epoch 3/50\n",
      "131/131 [==============================] - 0s 3ms/step - loss: 0.4921 - accuracy: 0.7990 - val_loss: 0.7038 - val_accuracy: 0.7170\n",
      "Epoch 4/50\n",
      "131/131 [==============================] - 0s 3ms/step - loss: 0.3680 - accuracy: 0.8531 - val_loss: 0.7801 - val_accuracy: 0.7132\n",
      "Epoch 5/50\n",
      "131/131 [==============================] - 0s 3ms/step - loss: 0.2657 - accuracy: 0.9000 - val_loss: 0.9339 - val_accuracy: 0.7228\n",
      "Epoch 6/50\n",
      "131/131 [==============================] - 0s 3ms/step - loss: 0.1817 - accuracy: 0.9297 - val_loss: 0.9450 - val_accuracy: 0.7151\n",
      "Epoch 7/50\n",
      "131/131 [==============================] - 0s 3ms/step - loss: 0.1189 - accuracy: 0.9579 - val_loss: 1.0937 - val_accuracy: 0.7247\n",
      "Epoch 8/50\n",
      "131/131 [==============================] - 0s 3ms/step - loss: 0.0591 - accuracy: 0.9794 - val_loss: 1.2299 - val_accuracy: 0.7438\n",
      "Epoch 9/50\n",
      "131/131 [==============================] - 0s 3ms/step - loss: 0.0353 - accuracy: 0.9866 - val_loss: 1.3382 - val_accuracy: 0.7094\n",
      "Epoch 10/50\n",
      "131/131 [==============================] - 0s 3ms/step - loss: 0.0572 - accuracy: 0.9794 - val_loss: 1.3219 - val_accuracy: 0.7189\n",
      "Epoch 11/50\n",
      "131/131 [==============================] - 0s 3ms/step - loss: 0.0584 - accuracy: 0.9809 - val_loss: 1.3677 - val_accuracy: 0.7113\n",
      "Epoch 12/50\n",
      "131/131 [==============================] - 0s 3ms/step - loss: 0.0207 - accuracy: 0.9952 - val_loss: 1.5790 - val_accuracy: 0.7208\n",
      "Epoch 13/50\n",
      "131/131 [==============================] - 0s 3ms/step - loss: 0.0230 - accuracy: 0.9923 - val_loss: 1.7169 - val_accuracy: 0.7323\n",
      "Epoch 14/50\n",
      "131/131 [==============================] - 0s 3ms/step - loss: 0.0472 - accuracy: 0.9837 - val_loss: 1.4300 - val_accuracy: 0.7094\n",
      "Epoch 15/50\n",
      "131/131 [==============================] - 0s 3ms/step - loss: 0.0470 - accuracy: 0.9818 - val_loss: 1.4204 - val_accuracy: 0.7113\n",
      "Epoch 16/50\n",
      "131/131 [==============================] - 0s 3ms/step - loss: 0.0139 - accuracy: 0.9962 - val_loss: 1.8689 - val_accuracy: 0.7075\n",
      "Epoch 17/50\n",
      "131/131 [==============================] - 0s 3ms/step - loss: 0.0120 - accuracy: 0.9962 - val_loss: 1.8176 - val_accuracy: 0.7151\n",
      "Epoch 18/50\n",
      "131/131 [==============================] - 0s 3ms/step - loss: 0.0123 - accuracy: 0.9957 - val_loss: 1.7214 - val_accuracy: 0.7170\n",
      "Epoch 19/50\n",
      "131/131 [==============================] - 0s 3ms/step - loss: 0.0185 - accuracy: 0.9933 - val_loss: 1.9368 - val_accuracy: 0.7323\n",
      "Epoch 20/50\n",
      "131/131 [==============================] - 0s 3ms/step - loss: 0.0774 - accuracy: 0.9708 - val_loss: 1.3285 - val_accuracy: 0.7170\n",
      "Epoch 21/50\n",
      "131/131 [==============================] - 0s 3ms/step - loss: 0.0203 - accuracy: 0.9957 - val_loss: 1.5819 - val_accuracy: 0.7208\n",
      "Epoch 22/50\n",
      "131/131 [==============================] - 0s 3ms/step - loss: 0.0102 - accuracy: 0.9971 - val_loss: 1.6896 - val_accuracy: 0.7476\n",
      "Epoch 23/50\n",
      "131/131 [==============================] - 0s 3ms/step - loss: 0.0077 - accuracy: 0.9971 - val_loss: 1.8914 - val_accuracy: 0.7266\n",
      "Epoch 24/50\n",
      "131/131 [==============================] - 0s 3ms/step - loss: 0.0075 - accuracy: 0.9971 - val_loss: 2.0113 - val_accuracy: 0.7208\n",
      "Epoch 25/50\n",
      "131/131 [==============================] - 0s 3ms/step - loss: 0.0043 - accuracy: 0.9990 - val_loss: 1.9581 - val_accuracy: 0.7380\n",
      "Epoch 26/50\n",
      "131/131 [==============================] - 0s 3ms/step - loss: 0.0114 - accuracy: 0.9967 - val_loss: 1.9500 - val_accuracy: 0.7094\n",
      "Epoch 27/50\n",
      "131/131 [==============================] - 0s 3ms/step - loss: 0.0218 - accuracy: 0.9933 - val_loss: 1.9289 - val_accuracy: 0.6941\n",
      "Epoch 28/50\n",
      "131/131 [==============================] - 0s 3ms/step - loss: 0.0260 - accuracy: 0.9880 - val_loss: 1.8754 - val_accuracy: 0.7208\n",
      "Epoch 29/50\n",
      "131/131 [==============================] - 0s 3ms/step - loss: 0.0087 - accuracy: 0.9976 - val_loss: 2.0553 - val_accuracy: 0.7266\n",
      "Epoch 30/50\n",
      "131/131 [==============================] - 0s 3ms/step - loss: 0.0053 - accuracy: 0.9986 - val_loss: 2.1124 - val_accuracy: 0.7304\n",
      "Epoch 31/50\n",
      "131/131 [==============================] - 0s 3ms/step - loss: 0.0449 - accuracy: 0.9876 - val_loss: 1.5324 - val_accuracy: 0.7266\n",
      "Epoch 32/50\n",
      "131/131 [==============================] - 0s 3ms/step - loss: 0.0356 - accuracy: 0.9880 - val_loss: 1.6159 - val_accuracy: 0.7323\n",
      "Epoch 33/50\n",
      "131/131 [==============================] - 0s 3ms/step - loss: 0.0082 - accuracy: 0.9981 - val_loss: 1.7595 - val_accuracy: 0.7380\n",
      "Epoch 34/50\n",
      "131/131 [==============================] - 0s 3ms/step - loss: 0.0017 - accuracy: 0.9990 - val_loss: 1.8950 - val_accuracy: 0.7438\n",
      "Epoch 35/50\n",
      "131/131 [==============================] - 0s 3ms/step - loss: 0.0013 - accuracy: 0.9990 - val_loss: 1.9703 - val_accuracy: 0.7457\n",
      "Epoch 36/50\n",
      "131/131 [==============================] - 0s 3ms/step - loss: 8.5743e-04 - accuracy: 0.9995 - val_loss: 2.0483 - val_accuracy: 0.7457\n",
      "Epoch 37/50\n",
      "131/131 [==============================] - 0s 3ms/step - loss: 0.0011 - accuracy: 0.9990 - val_loss: 2.1018 - val_accuracy: 0.7438\n",
      "Epoch 38/50\n",
      "131/131 [==============================] - 0s 3ms/step - loss: 7.9320e-04 - accuracy: 0.9995 - val_loss: 2.1561 - val_accuracy: 0.7419\n",
      "Epoch 39/50\n",
      "131/131 [==============================] - 0s 3ms/step - loss: 9.8772e-04 - accuracy: 0.9990 - val_loss: 2.2223 - val_accuracy: 0.7438\n",
      "Epoch 40/50\n",
      "131/131 [==============================] - 0s 3ms/step - loss: 9.7017e-04 - accuracy: 0.9990 - val_loss: 2.2981 - val_accuracy: 0.7438\n",
      "Epoch 41/50\n",
      "131/131 [==============================] - 0s 3ms/step - loss: 9.8801e-04 - accuracy: 0.9990 - val_loss: 2.3592 - val_accuracy: 0.7438\n",
      "Epoch 42/50\n",
      "131/131 [==============================] - 0s 3ms/step - loss: 0.0013 - accuracy: 0.9995 - val_loss: 2.2720 - val_accuracy: 0.7419\n",
      "Epoch 43/50\n",
      "131/131 [==============================] - 0s 3ms/step - loss: 0.0014 - accuracy: 0.9995 - val_loss: 2.3204 - val_accuracy: 0.7419\n",
      "Epoch 44/50\n",
      "131/131 [==============================] - 0s 3ms/step - loss: 0.0010 - accuracy: 0.9990 - val_loss: 2.3481 - val_accuracy: 0.7400\n",
      "Epoch 45/50\n",
      "131/131 [==============================] - 0s 3ms/step - loss: 9.7029e-04 - accuracy: 0.9990 - val_loss: 2.3969 - val_accuracy: 0.7400\n",
      "Epoch 46/50\n",
      "131/131 [==============================] - 0s 3ms/step - loss: 9.4367e-04 - accuracy: 0.9990 - val_loss: 2.4430 - val_accuracy: 0.7438\n",
      "Epoch 47/50\n",
      "131/131 [==============================] - 0s 3ms/step - loss: 8.9082e-04 - accuracy: 0.9990 - val_loss: 2.4901 - val_accuracy: 0.7457\n",
      "Epoch 48/50\n",
      "131/131 [==============================] - 0s 3ms/step - loss: 6.9744e-04 - accuracy: 0.9995 - val_loss: 2.5350 - val_accuracy: 0.7457\n",
      "Epoch 49/50\n",
      "131/131 [==============================] - 0s 3ms/step - loss: 9.2526e-04 - accuracy: 0.9995 - val_loss: 2.5577 - val_accuracy: 0.7457\n",
      "Epoch 50/50\n",
      "131/131 [==============================] - 0s 3ms/step - loss: 8.7863e-04 - accuracy: 0.9995 - val_loss: 2.6059 - val_accuracy: 0.7476\n"
     ]
    }
   ],
   "source": [
    "# Train the model\n",
    "history_02_E = model_02_E.fit(X_train_E_array, Y_train_E_encoded, epochs=50, batch_size=16, validation_split=0.2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 116,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "28/28 [==============================] - 0s 2ms/step - loss: 2.5722 - accuracy: 0.7394\n",
      "Model_02_E: Test Accuracy: 73.94%\n"
     ]
    }
   ],
   "source": [
    "# Evaluate the model\n",
    "loss_02_E, accuracy_02_E = model_02_E.evaluate(X_test_E_array, Y_test_E_encoded)\n",
    "print(f\"{model_02_E.name}: Test Accuracy: {accuracy_02_E * 100:.2f}%\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Exploring Model 02 results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 117,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Generalizing the process above for any word through a python method:\n",
    "def wordClassifier_multiclass(word, model, encoder):\n",
    "    '''Given a word vector, classifies its content as perceptual, manifest, contextual, or latent, according to SCA.\n",
    "    Usage example: wordClassifier_multiclass(word='trial', model=model_02_C, encoder=encoder_oneHot_C).\n",
    "    '''\n",
    "    new_entry = nlp_getVector(word)\n",
    "    if new_entry:\n",
    "        vector = nlp_getVector(word)[1]\n",
    "        vector = np.expand_dims(vector, axis=0)\n",
    "        result = model.predict(vector)\n",
    "        decoded_result = encoder.inverse_transform(result)\n",
    "    else:\n",
    "        print('Word not existent in database.')\n",
    "        return\n",
    "    print(f'--- SCA: \"{word}\" has {decoded_result[0][0]} content.')\n",
    "    print(f'Model used: {model.name}')  # Print the name of the model\n",
    "    return\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 118,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1/1 [==============================] - 0s 97ms/step\n",
      "--- SCA: \"happyness\" has Latent content.\n",
      "Model used: Model_02_E\n",
      "1/1 [==============================] - 0s 82ms/step\n",
      "--- SCA: \"happyness\" has Perceptual content.\n",
      "Model used: Model_02_C\n"
     ]
    }
   ],
   "source": [
    "## Experiment with a given word:\n",
    "wordClassifier_multiclass('happyness',model=model_02_E, encoder=encoder_oneHot_E)\n",
    "wordClassifier_multiclass('happyness',model=model_02_C, encoder=encoder_oneHot_C)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 446,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1/1 [==============================] - 0s 31ms/step\n",
      "--- SCA: \"imagination\" has Latent content.\n",
      "Model used: Model_02_E\n",
      "1/1 [==============================] - 0s 20ms/step\n",
      "--- SCA: \"imagination\" has Latent content.\n",
      "Model used: Model_02_C\n",
      "\n",
      "-------------------------\n",
      "1/1 [==============================] - 0s 90ms/step\n",
      "--- SCA: \"care\" has Latent content.\n",
      "Model used: Model_02_E\n",
      "1/1 [==============================] - 0s 22ms/step\n",
      "--- SCA: \"care\" has Latent content.\n",
      "Model used: Model_02_C\n",
      "\n",
      "-------------------------\n",
      "1/1 [==============================] - 0s 21ms/step\n",
      "--- SCA: \"fear\" has Latent content.\n",
      "Model used: Model_02_E\n",
      "1/1 [==============================] - 0s 22ms/step\n",
      "--- SCA: \"fear\" has Latent content.\n",
      "Model used: Model_02_C\n",
      "\n",
      "-------------------------\n",
      "1/1 [==============================] - 0s 23ms/step\n",
      "--- SCA: \"play\" has Perceptual content.\n",
      "Model used: Model_02_E\n",
      "1/1 [==============================] - 0s 25ms/step\n",
      "--- SCA: \"play\" has Perceptual content.\n",
      "Model used: Model_02_C\n",
      "\n",
      "-------------------------\n",
      "1/1 [==============================] - 0s 22ms/step\n",
      "--- SCA: \"protection\" has Latent content.\n",
      "Model used: Model_02_E\n",
      "1/1 [==============================] - 0s 28ms/step\n",
      "--- SCA: \"protection\" has Latent content.\n",
      "Model used: Model_02_C\n",
      "\n",
      "-------------------------\n",
      "1/1 [==============================] - 0s 23ms/step\n",
      "--- SCA: \"monster\" has Perceptual content.\n",
      "Model used: Model_02_E\n",
      "1/1 [==============================] - 0s 25ms/step\n",
      "--- SCA: \"monster\" has Perceptual content.\n",
      "Model used: Model_02_C\n",
      "\n",
      "-------------------------\n",
      "1/1 [==============================] - 0s 23ms/step\n",
      "--- SCA: \"Mom\" has Perceptual content.\n",
      "Model used: Model_02_E\n",
      "1/1 [==============================] - 0s 26ms/step\n",
      "--- SCA: \"Mom\" has Perceptual content.\n",
      "Model used: Model_02_C\n",
      "\n",
      "-------------------------\n",
      "1/1 [==============================] - 0s 27ms/step\n",
      "--- SCA: \"Dad\" has Perceptual content.\n",
      "Model used: Model_02_E\n",
      "1/1 [==============================] - 0s 27ms/step\n",
      "--- SCA: \"Dad\" has Perceptual content.\n",
      "Model used: Model_02_C\n",
      "\n",
      "-------------------------\n",
      "1/1 [==============================] - 0s 23ms/step\n",
      "--- SCA: \"toy\" has Perceptual content.\n",
      "Model used: Model_02_E\n",
      "1/1 [==============================] - 0s 22ms/step\n",
      "--- SCA: \"toy\" has Perceptual content.\n",
      "Model used: Model_02_C\n",
      "\n",
      "-------------------------\n",
      "1/1 [==============================] - 0s 21ms/step\n",
      "--- SCA: \"child\" has Manifest content.\n",
      "Model used: Model_02_E\n",
      "1/1 [==============================] - 0s 21ms/step\n",
      "--- SCA: \"child\" has Manifest content.\n",
      "Model used: Model_02_C\n",
      "\n",
      "-------------------------\n",
      "1/1 [==============================] - 0s 19ms/step\n",
      "--- SCA: \"doll\" has Manifest content.\n",
      "Model used: Model_02_E\n",
      "1/1 [==============================] - 0s 21ms/step\n",
      "--- SCA: \"doll\" has Manifest content.\n",
      "Model used: Model_02_C\n",
      "\n",
      "-------------------------\n"
     ]
    }
   ],
   "source": [
    "## Checking words classification based on the sequence explored in the PhD Thesis.\n",
    "seq_words_A = ['imagination','care','fear','play','protection','monster','Mom','Dad','toy','child','doll']\n",
    "\n",
    "for word in seq_words_A:\n",
    "    wordClassifier_multiclass(word,model=model_02_E, encoder=encoder_oneHot_E)\n",
    "    # print('\\n')\n",
    "    wordClassifier_multiclass(word,model=model_02_C, encoder=encoder_oneHot_C)\n",
    "    print(f'\\n{25*\"-\"}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 449,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1/1 [==============================] - 0s 28ms/step\n",
      "--- SCA: \"pleasure\" has Latent content.\n",
      "Model used: Model_02_E\n",
      "1/1 [==============================] - 0s 20ms/step\n",
      "--- SCA: \"pleasure\" has Latent content.\n",
      "Model used: Model_02_C\n",
      "\n",
      "-------------------------\n",
      "1/1 [==============================] - 0s 39ms/step\n",
      "--- SCA: \"love\" has Latent content.\n",
      "Model used: Model_02_E\n",
      "1/1 [==============================] - 0s 23ms/step\n",
      "--- SCA: \"love\" has Latent content.\n",
      "Model used: Model_02_C\n",
      "\n",
      "-------------------------\n",
      "1/1 [==============================] - 0s 28ms/step\n",
      "--- SCA: \"anxiety\" has Latent content.\n",
      "Model used: Model_02_E\n",
      "1/1 [==============================] - 0s 22ms/step\n",
      "--- SCA: \"anxiety\" has Latent content.\n",
      "Model used: Model_02_C\n",
      "\n",
      "-------------------------\n",
      "1/1 [==============================] - 0s 27ms/step\n",
      "--- SCA: \"suffering\" has Latent content.\n",
      "Model used: Model_02_E\n",
      "1/1 [==============================] - 0s 24ms/step\n",
      "--- SCA: \"suffering\" has Latent content.\n",
      "Model used: Model_02_C\n",
      "\n",
      "-------------------------\n",
      "1/1 [==============================] - 0s 27ms/step\n",
      "--- SCA: \"touch\" has Latent content.\n",
      "Model used: Model_02_E\n",
      "1/1 [==============================] - 0s 29ms/step\n",
      "--- SCA: \"touch\" has Perceptual content.\n",
      "Model used: Model_02_C\n",
      "\n",
      "-------------------------\n",
      "1/1 [==============================] - 0s 28ms/step\n",
      "--- SCA: \"hug\" has Perceptual content.\n",
      "Model used: Model_02_E\n",
      "1/1 [==============================] - 0s 24ms/step\n",
      "--- SCA: \"hug\" has Perceptual content.\n",
      "Model used: Model_02_C\n",
      "\n",
      "-------------------------\n",
      "1/1 [==============================] - 0s 31ms/step\n",
      "--- SCA: \"cuddle\" has Perceptual content.\n",
      "Model used: Model_02_E\n",
      "1/1 [==============================] - 0s 32ms/step\n",
      "--- SCA: \"cuddle\" has Perceptual content.\n",
      "Model used: Model_02_C\n",
      "\n",
      "-------------------------\n",
      "1/1 [==============================] - 0s 29ms/step\n",
      "--- SCA: \"wound\" has Manifest content.\n",
      "Model used: Model_02_E\n",
      "1/1 [==============================] - 0s 26ms/step\n",
      "--- SCA: \"wound\" has Perceptual content.\n",
      "Model used: Model_02_C\n",
      "\n",
      "-------------------------\n",
      "1/1 [==============================] - 0s 20ms/step\n",
      "--- SCA: \"arms\" has Perceptual content.\n",
      "Model used: Model_02_E\n",
      "1/1 [==============================] - 0s 24ms/step\n",
      "--- SCA: \"arms\" has Perceptual content.\n",
      "Model used: Model_02_C\n",
      "\n",
      "-------------------------\n",
      "1/1 [==============================] - 0s 20ms/step\n",
      "--- SCA: \"hand\" has Manifest content.\n",
      "Model used: Model_02_E\n",
      "1/1 [==============================] - 0s 21ms/step\n",
      "--- SCA: \"hand\" has Manifest content.\n",
      "Model used: Model_02_C\n",
      "\n",
      "-------------------------\n"
     ]
    }
   ],
   "source": [
    "## Checking words classification based on the sequence explored in the PhD Thesis.\n",
    "seq_words_B = ['pleasure','love','anxiety','suffering','touch','hug','cuddle','wound','arms','hand']\n",
    "\n",
    "for word in seq_words_B:\n",
    "    wordClassifier_multiclass(word,model=model_02_E, encoder=encoder_oneHot_E)\n",
    "    # print('\\n')\n",
    "    wordClassifier_multiclass(word,model=model_02_C, encoder=encoder_oneHot_C)\n",
    "    print(f'\\n{25*\"-\"}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 125,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1/1 [==============================] - 0s 22ms/step\n",
      "--- SCA: \"A cigar is just a cigar\" has Manifest content.\n",
      "Model used: Model_02_E\n"
     ]
    }
   ],
   "source": [
    "phrase = 'A cigar is just a cigar'\n",
    "wordClassifier_multiclass(phrase,model=model_02_E, encoder=encoder_oneHot_E)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 131,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1/1 [==============================] - 0s 24ms/step\n",
      "--- SCA: \"Did you dream again?\" has Latent content.\n",
      "Model used: Model_02_E\n"
     ]
    }
   ],
   "source": [
    "phrase = 'Did you dream again?'\n",
    "wordClassifier_multiclass(phrase,model=model_02_E, encoder=encoder_oneHot_E)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### NEXT STEPS:  \n",
    "Calcular a objetividade/subjetividade para cada frase da SICK database. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "----\n",
    "## Saving and loading Keras trained models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 119,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ## Saving Keras models for posterior use:\n",
    "# model_01_A.save('../models/model_01_A.h5')\n",
    "# model_01_D.save('../models/model_01_D.h5')\n",
    "# model_02_C.save('../models/model_02_C.h5')\n",
    "# model_02_E.save('../models/model_02_E.h5')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 458,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Loading saved models:\n",
    "from keras.models import load_model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 462,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = load_model('../models/model_02_C.h5')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 467,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Saving and loading Scklearn models (e.g., OneHot Encoders)\n",
    "with open('../models/encoder_oneHot_C.pickle', 'wb') as f:\n",
    "    pickle.dump(encoder_oneHot_C, f)\n",
    "\n",
    "with open('../models/encoder_oneHot_E.pickle', 'wb') as f:\n",
    "    pickle.dump(encoder_oneHot_E, f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 469,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Loading saved encoder:\n",
    "with open('../models/encoder_oneHot_C.pickle', 'rb') as f:\n",
    "    encoder = pickle.load(f)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Trying loaded model and encoder:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 470,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1/1 [==============================] - 0s 75ms/step\n",
      "--- SCA: \"debug\" has Latent content.\n",
      "Model used: Model_02_C\n"
     ]
    }
   ],
   "source": [
    "wordClassifier_multiclass('debug',model=model, encoder=encoder)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "## Pycaret: exploring outperforming models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 302,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pycaret.classification"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 303,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>0</th>\n",
       "      <th>1</th>\n",
       "      <th>2</th>\n",
       "      <th>3</th>\n",
       "      <th>4</th>\n",
       "      <th>5</th>\n",
       "      <th>6</th>\n",
       "      <th>7</th>\n",
       "      <th>8</th>\n",
       "      <th>9</th>\n",
       "      <th>...</th>\n",
       "      <th>291</th>\n",
       "      <th>292</th>\n",
       "      <th>293</th>\n",
       "      <th>294</th>\n",
       "      <th>295</th>\n",
       "      <th>296</th>\n",
       "      <th>297</th>\n",
       "      <th>298</th>\n",
       "      <th>299</th>\n",
       "      <th>target</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>garment</th>\n",
       "      <td>-2.44960</td>\n",
       "      <td>-0.709460</td>\n",
       "      <td>-1.4459</td>\n",
       "      <td>2.7438</td>\n",
       "      <td>2.19640</td>\n",
       "      <td>0.62562</td>\n",
       "      <td>1.3016</td>\n",
       "      <td>2.0838</td>\n",
       "      <td>-2.2201</td>\n",
       "      <td>-2.2038</td>\n",
       "      <td>...</td>\n",
       "      <td>3.413000</td>\n",
       "      <td>0.53852</td>\n",
       "      <td>-0.15337</td>\n",
       "      <td>-4.65310</td>\n",
       "      <td>0.54157</td>\n",
       "      <td>1.41030</td>\n",
       "      <td>-4.592600</td>\n",
       "      <td>0.43055</td>\n",
       "      <td>0.35458</td>\n",
       "      <td>Manifest</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>oven</th>\n",
       "      <td>-2.32870</td>\n",
       "      <td>1.487700</td>\n",
       "      <td>-1.1173</td>\n",
       "      <td>4.4829</td>\n",
       "      <td>1.85580</td>\n",
       "      <td>1.05840</td>\n",
       "      <td>1.8389</td>\n",
       "      <td>-2.3667</td>\n",
       "      <td>-6.2502</td>\n",
       "      <td>3.5786</td>\n",
       "      <td>...</td>\n",
       "      <td>0.578500</td>\n",
       "      <td>-2.85220</td>\n",
       "      <td>-4.87860</td>\n",
       "      <td>-0.81042</td>\n",
       "      <td>-1.06000</td>\n",
       "      <td>1.10440</td>\n",
       "      <td>0.489910</td>\n",
       "      <td>2.73810</td>\n",
       "      <td>3.74540</td>\n",
       "      <td>Manifest</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>landscape</th>\n",
       "      <td>-0.63003</td>\n",
       "      <td>0.041465</td>\n",
       "      <td>-2.3347</td>\n",
       "      <td>-4.7172</td>\n",
       "      <td>0.10905</td>\n",
       "      <td>-1.18150</td>\n",
       "      <td>3.3926</td>\n",
       "      <td>3.1072</td>\n",
       "      <td>-2.1309</td>\n",
       "      <td>1.3538</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.072589</td>\n",
       "      <td>1.17210</td>\n",
       "      <td>0.39403</td>\n",
       "      <td>0.82975</td>\n",
       "      <td>-0.17167</td>\n",
       "      <td>-0.54614</td>\n",
       "      <td>0.039231</td>\n",
       "      <td>-4.14750</td>\n",
       "      <td>0.87988</td>\n",
       "      <td>Perceptual</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>3 rows × 301 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                 0         1       2       3        4        5       6       7       8       9  ...       291      292      293      294      295      296       297      298      299      target\n",
       "garment   -2.44960 -0.709460 -1.4459  2.7438  2.19640  0.62562  1.3016  2.0838 -2.2201 -2.2038  ...  3.413000  0.53852 -0.15337 -4.65310  0.54157  1.41030 -4.592600  0.43055  0.35458    Manifest\n",
       "oven      -2.32870  1.487700 -1.1173  4.4829  1.85580  1.05840  1.8389 -2.3667 -6.2502  3.5786  ...  0.578500 -2.85220 -4.87860 -0.81042 -1.06000  1.10440  0.489910  2.73810  3.74540    Manifest\n",
       "landscape -0.63003  0.041465 -2.3347 -4.7172  0.10905 -1.18150  3.3926  3.1072 -2.1309  1.3538  ... -0.072589  1.17210  0.39403  0.82975 -0.17167 -0.54614  0.039231 -4.14750  0.87988  Perceptual\n",
       "\n",
       "[3 rows x 301 columns]"
      ]
     },
     "execution_count": 303,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_pycaret = pd.concat([X_train_C, Y_train_C], axis=1)\n",
    "df_pycaret.sample(3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 304,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<style type=\"text/css\">\n",
       "#T_f2a58_row9_col1 {\n",
       "  background-color: lightgreen;\n",
       "}\n",
       "</style>\n",
       "<table id=\"T_f2a58\">\n",
       "  <thead>\n",
       "    <tr>\n",
       "      <th class=\"blank level0\" >&nbsp;</th>\n",
       "      <th id=\"T_f2a58_level0_col0\" class=\"col_heading level0 col0\" >Description</th>\n",
       "      <th id=\"T_f2a58_level0_col1\" class=\"col_heading level0 col1\" >Value</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th id=\"T_f2a58_level0_row0\" class=\"row_heading level0 row0\" >0</th>\n",
       "      <td id=\"T_f2a58_row0_col0\" class=\"data row0 col0\" >Session id</td>\n",
       "      <td id=\"T_f2a58_row0_col1\" class=\"data row0 col1\" >9088</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th id=\"T_f2a58_level0_row1\" class=\"row_heading level0 row1\" >1</th>\n",
       "      <td id=\"T_f2a58_row1_col0\" class=\"data row1 col0\" >Target</td>\n",
       "      <td id=\"T_f2a58_row1_col1\" class=\"data row1 col1\" >target</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th id=\"T_f2a58_level0_row2\" class=\"row_heading level0 row2\" >2</th>\n",
       "      <td id=\"T_f2a58_row2_col0\" class=\"data row2 col0\" >Target type</td>\n",
       "      <td id=\"T_f2a58_row2_col1\" class=\"data row2 col1\" >Multiclass</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th id=\"T_f2a58_level0_row3\" class=\"row_heading level0 row3\" >3</th>\n",
       "      <td id=\"T_f2a58_row3_col0\" class=\"data row3 col0\" >Target mapping</td>\n",
       "      <td id=\"T_f2a58_row3_col1\" class=\"data row3 col1\" >Contextual: 0, Latent: 1, Manifest: 2, Perceptual: 3</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th id=\"T_f2a58_level0_row4\" class=\"row_heading level0 row4\" >4</th>\n",
       "      <td id=\"T_f2a58_row4_col0\" class=\"data row4 col0\" >Original data shape</td>\n",
       "      <td id=\"T_f2a58_row4_col1\" class=\"data row4 col1\" >(3511, 301)</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th id=\"T_f2a58_level0_row5\" class=\"row_heading level0 row5\" >5</th>\n",
       "      <td id=\"T_f2a58_row5_col0\" class=\"data row5 col0\" >Transformed data shape</td>\n",
       "      <td id=\"T_f2a58_row5_col1\" class=\"data row5 col1\" >(3511, 301)</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th id=\"T_f2a58_level0_row6\" class=\"row_heading level0 row6\" >6</th>\n",
       "      <td id=\"T_f2a58_row6_col0\" class=\"data row6 col0\" >Transformed train set shape</td>\n",
       "      <td id=\"T_f2a58_row6_col1\" class=\"data row6 col1\" >(2457, 301)</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th id=\"T_f2a58_level0_row7\" class=\"row_heading level0 row7\" >7</th>\n",
       "      <td id=\"T_f2a58_row7_col0\" class=\"data row7 col0\" >Transformed test set shape</td>\n",
       "      <td id=\"T_f2a58_row7_col1\" class=\"data row7 col1\" >(1054, 301)</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th id=\"T_f2a58_level0_row8\" class=\"row_heading level0 row8\" >8</th>\n",
       "      <td id=\"T_f2a58_row8_col0\" class=\"data row8 col0\" >Numeric features</td>\n",
       "      <td id=\"T_f2a58_row8_col1\" class=\"data row8 col1\" >300</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th id=\"T_f2a58_level0_row9\" class=\"row_heading level0 row9\" >9</th>\n",
       "      <td id=\"T_f2a58_row9_col0\" class=\"data row9 col0\" >Preprocess</td>\n",
       "      <td id=\"T_f2a58_row9_col1\" class=\"data row9 col1\" >True</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th id=\"T_f2a58_level0_row10\" class=\"row_heading level0 row10\" >10</th>\n",
       "      <td id=\"T_f2a58_row10_col0\" class=\"data row10 col0\" >Imputation type</td>\n",
       "      <td id=\"T_f2a58_row10_col1\" class=\"data row10 col1\" >simple</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th id=\"T_f2a58_level0_row11\" class=\"row_heading level0 row11\" >11</th>\n",
       "      <td id=\"T_f2a58_row11_col0\" class=\"data row11 col0\" >Numeric imputation</td>\n",
       "      <td id=\"T_f2a58_row11_col1\" class=\"data row11 col1\" >mean</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th id=\"T_f2a58_level0_row12\" class=\"row_heading level0 row12\" >12</th>\n",
       "      <td id=\"T_f2a58_row12_col0\" class=\"data row12 col0\" >Categorical imputation</td>\n",
       "      <td id=\"T_f2a58_row12_col1\" class=\"data row12 col1\" >mode</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th id=\"T_f2a58_level0_row13\" class=\"row_heading level0 row13\" >13</th>\n",
       "      <td id=\"T_f2a58_row13_col0\" class=\"data row13 col0\" >Fold Generator</td>\n",
       "      <td id=\"T_f2a58_row13_col1\" class=\"data row13 col1\" >StratifiedKFold</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th id=\"T_f2a58_level0_row14\" class=\"row_heading level0 row14\" >14</th>\n",
       "      <td id=\"T_f2a58_row14_col0\" class=\"data row14 col0\" >Fold Number</td>\n",
       "      <td id=\"T_f2a58_row14_col1\" class=\"data row14 col1\" >10</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th id=\"T_f2a58_level0_row15\" class=\"row_heading level0 row15\" >15</th>\n",
       "      <td id=\"T_f2a58_row15_col0\" class=\"data row15 col0\" >CPU Jobs</td>\n",
       "      <td id=\"T_f2a58_row15_col1\" class=\"data row15 col1\" >-1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th id=\"T_f2a58_level0_row16\" class=\"row_heading level0 row16\" >16</th>\n",
       "      <td id=\"T_f2a58_row16_col0\" class=\"data row16 col0\" >Use GPU</td>\n",
       "      <td id=\"T_f2a58_row16_col1\" class=\"data row16 col1\" >False</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th id=\"T_f2a58_level0_row17\" class=\"row_heading level0 row17\" >17</th>\n",
       "      <td id=\"T_f2a58_row17_col0\" class=\"data row17 col0\" >Log Experiment</td>\n",
       "      <td id=\"T_f2a58_row17_col1\" class=\"data row17 col1\" >False</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th id=\"T_f2a58_level0_row18\" class=\"row_heading level0 row18\" >18</th>\n",
       "      <td id=\"T_f2a58_row18_col0\" class=\"data row18 col0\" >Experiment Name</td>\n",
       "      <td id=\"T_f2a58_row18_col1\" class=\"data row18 col1\" >clf-default-name</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th id=\"T_f2a58_level0_row19\" class=\"row_heading level0 row19\" >19</th>\n",
       "      <td id=\"T_f2a58_row19_col0\" class=\"data row19 col0\" >USI</td>\n",
       "      <td id=\"T_f2a58_row19_col1\" class=\"data row19 col1\" >ae3f</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n"
      ],
      "text/plain": [
       "<pandas.io.formats.style.Styler at 0x20e4a392a90>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "## Fazendo um experimento:\n",
    "exp_class = pycaret.classification.setup(df_pycaret, target='target', session_id=9088)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 305,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>Initiated</th>\n",
       "      <td>. . . . . . . . . . . . . . . . . .</td>\n",
       "      <td>21:55:05</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Status</th>\n",
       "      <td>. . . . . . . . . . . . . . . . . .</td>\n",
       "      <td>Fitting 10 Folds</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Estimator</th>\n",
       "      <td>. . . . . . . . . . . . . . . . . .</td>\n",
       "      <td>Logistic Regression</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                                   \n",
       "                                                                   \n",
       "Initiated  . . . . . . . . . . . . . . . . . .             21:55:05\n",
       "Status     . . . . . . . . . . . . . . . . . .     Fitting 10 Folds\n",
       "Estimator  . . . . . . . . . . . . . . . . . .  Logistic Regression"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<style type=\"text/css\">\n",
       "#T_e8219 th {\n",
       "  text-align: left;\n",
       "}\n",
       "#T_e8219_row0_col0, #T_e8219_row0_col2, #T_e8219_row0_col4, #T_e8219_row0_col5, #T_e8219_row0_col6, #T_e8219_row0_col7, #T_e8219_row1_col0, #T_e8219_row1_col1, #T_e8219_row1_col2, #T_e8219_row1_col3, #T_e8219_row2_col0, #T_e8219_row2_col1, #T_e8219_row2_col3, #T_e8219_row2_col4, #T_e8219_row2_col5, #T_e8219_row2_col6, #T_e8219_row2_col7, #T_e8219_row3_col0, #T_e8219_row3_col1, #T_e8219_row3_col2, #T_e8219_row3_col3, #T_e8219_row3_col4, #T_e8219_row3_col5, #T_e8219_row3_col6, #T_e8219_row3_col7, #T_e8219_row4_col0, #T_e8219_row4_col1, #T_e8219_row4_col2, #T_e8219_row4_col3, #T_e8219_row4_col4, #T_e8219_row4_col5, #T_e8219_row4_col6, #T_e8219_row4_col7, #T_e8219_row5_col0, #T_e8219_row5_col1, #T_e8219_row5_col2, #T_e8219_row5_col3, #T_e8219_row5_col4, #T_e8219_row5_col5, #T_e8219_row5_col6, #T_e8219_row5_col7, #T_e8219_row6_col0, #T_e8219_row6_col1, #T_e8219_row6_col2, #T_e8219_row6_col3, #T_e8219_row6_col4, #T_e8219_row6_col5, #T_e8219_row6_col6, #T_e8219_row6_col7, #T_e8219_row7_col0, #T_e8219_row7_col1, #T_e8219_row7_col2, #T_e8219_row7_col3, #T_e8219_row7_col4, #T_e8219_row7_col5, #T_e8219_row7_col6, #T_e8219_row7_col7, #T_e8219_row8_col0, #T_e8219_row8_col1, #T_e8219_row8_col2, #T_e8219_row8_col3, #T_e8219_row8_col4, #T_e8219_row8_col5, #T_e8219_row8_col6, #T_e8219_row8_col7, #T_e8219_row9_col0, #T_e8219_row9_col1, #T_e8219_row9_col2, #T_e8219_row9_col3, #T_e8219_row9_col4, #T_e8219_row9_col5, #T_e8219_row9_col6, #T_e8219_row9_col7, #T_e8219_row10_col0, #T_e8219_row10_col1, #T_e8219_row10_col2, #T_e8219_row10_col3, #T_e8219_row10_col4, #T_e8219_row10_col5, #T_e8219_row10_col6, #T_e8219_row10_col7, #T_e8219_row11_col0, #T_e8219_row11_col1, #T_e8219_row11_col2, #T_e8219_row11_col3, #T_e8219_row11_col4, #T_e8219_row11_col5, #T_e8219_row11_col6, #T_e8219_row11_col7, #T_e8219_row12_col0, #T_e8219_row12_col1, #T_e8219_row12_col2, #T_e8219_row12_col3, #T_e8219_row12_col4, #T_e8219_row12_col5, #T_e8219_row12_col6, #T_e8219_row12_col7, #T_e8219_row13_col0, #T_e8219_row13_col1, #T_e8219_row13_col2, #T_e8219_row13_col3, #T_e8219_row13_col4, #T_e8219_row13_col5, #T_e8219_row13_col6, #T_e8219_row13_col7 {\n",
       "  text-align: left;\n",
       "}\n",
       "#T_e8219_row0_col1, #T_e8219_row0_col3, #T_e8219_row1_col4, #T_e8219_row1_col5, #T_e8219_row1_col6, #T_e8219_row1_col7, #T_e8219_row2_col2 {\n",
       "  text-align: left;\n",
       "  background-color: yellow;\n",
       "}\n",
       "#T_e8219_row0_col8, #T_e8219_row13_col8 {\n",
       "  text-align: left;\n",
       "  background-color: yellow;\n",
       "  background-color: lightgrey;\n",
       "}\n",
       "#T_e8219_row1_col8, #T_e8219_row2_col8, #T_e8219_row3_col8, #T_e8219_row4_col8, #T_e8219_row5_col8, #T_e8219_row6_col8, #T_e8219_row7_col8, #T_e8219_row8_col8, #T_e8219_row9_col8, #T_e8219_row10_col8, #T_e8219_row11_col8, #T_e8219_row12_col8 {\n",
       "  text-align: left;\n",
       "  background-color: lightgrey;\n",
       "}\n",
       "</style>\n",
       "<table id=\"T_e8219\">\n",
       "  <thead>\n",
       "    <tr>\n",
       "      <th class=\"blank level0\" >&nbsp;</th>\n",
       "      <th id=\"T_e8219_level0_col0\" class=\"col_heading level0 col0\" >Model</th>\n",
       "      <th id=\"T_e8219_level0_col1\" class=\"col_heading level0 col1\" >Accuracy</th>\n",
       "      <th id=\"T_e8219_level0_col2\" class=\"col_heading level0 col2\" >AUC</th>\n",
       "      <th id=\"T_e8219_level0_col3\" class=\"col_heading level0 col3\" >Recall</th>\n",
       "      <th id=\"T_e8219_level0_col4\" class=\"col_heading level0 col4\" >Prec.</th>\n",
       "      <th id=\"T_e8219_level0_col5\" class=\"col_heading level0 col5\" >F1</th>\n",
       "      <th id=\"T_e8219_level0_col6\" class=\"col_heading level0 col6\" >Kappa</th>\n",
       "      <th id=\"T_e8219_level0_col7\" class=\"col_heading level0 col7\" >MCC</th>\n",
       "      <th id=\"T_e8219_level0_col8\" class=\"col_heading level0 col8\" >TT (Sec)</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th id=\"T_e8219_level0_row0\" class=\"row_heading level0 row0\" >ridge</th>\n",
       "      <td id=\"T_e8219_row0_col0\" class=\"data row0 col0\" >Ridge Classifier</td>\n",
       "      <td id=\"T_e8219_row0_col1\" class=\"data row0 col1\" >0.6789</td>\n",
       "      <td id=\"T_e8219_row0_col2\" class=\"data row0 col2\" >0.0000</td>\n",
       "      <td id=\"T_e8219_row0_col3\" class=\"data row0 col3\" >0.6789</td>\n",
       "      <td id=\"T_e8219_row0_col4\" class=\"data row0 col4\" >0.6689</td>\n",
       "      <td id=\"T_e8219_row0_col5\" class=\"data row0 col5\" >0.6675</td>\n",
       "      <td id=\"T_e8219_row0_col6\" class=\"data row0 col6\" >0.5516</td>\n",
       "      <td id=\"T_e8219_row0_col7\" class=\"data row0 col7\" >0.5552</td>\n",
       "      <td id=\"T_e8219_row0_col8\" class=\"data row0 col8\" >0.0220</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th id=\"T_e8219_level0_row1\" class=\"row_heading level0 row1\" >lda</th>\n",
       "      <td id=\"T_e8219_row1_col0\" class=\"data row1 col0\" >Linear Discriminant Analysis</td>\n",
       "      <td id=\"T_e8219_row1_col1\" class=\"data row1 col1\" >0.6777</td>\n",
       "      <td id=\"T_e8219_row1_col2\" class=\"data row1 col2\" >0.8846</td>\n",
       "      <td id=\"T_e8219_row1_col3\" class=\"data row1 col3\" >0.6777</td>\n",
       "      <td id=\"T_e8219_row1_col4\" class=\"data row1 col4\" >0.6770</td>\n",
       "      <td id=\"T_e8219_row1_col5\" class=\"data row1 col5\" >0.6758</td>\n",
       "      <td id=\"T_e8219_row1_col6\" class=\"data row1 col6\" >0.5564</td>\n",
       "      <td id=\"T_e8219_row1_col7\" class=\"data row1 col7\" >0.5573</td>\n",
       "      <td id=\"T_e8219_row1_col8\" class=\"data row1 col8\" >0.0560</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th id=\"T_e8219_level0_row2\" class=\"row_heading level0 row2\" >lightgbm</th>\n",
       "      <td id=\"T_e8219_row2_col0\" class=\"data row2 col0\" >Light Gradient Boosting Machine</td>\n",
       "      <td id=\"T_e8219_row2_col1\" class=\"data row2 col1\" >0.6712</td>\n",
       "      <td id=\"T_e8219_row2_col2\" class=\"data row2 col2\" >0.8857</td>\n",
       "      <td id=\"T_e8219_row2_col3\" class=\"data row2 col3\" >0.6712</td>\n",
       "      <td id=\"T_e8219_row2_col4\" class=\"data row2 col4\" >0.6594</td>\n",
       "      <td id=\"T_e8219_row2_col5\" class=\"data row2 col5\" >0.6541</td>\n",
       "      <td id=\"T_e8219_row2_col6\" class=\"data row2 col6\" >0.5381</td>\n",
       "      <td id=\"T_e8219_row2_col7\" class=\"data row2 col7\" >0.5441</td>\n",
       "      <td id=\"T_e8219_row2_col8\" class=\"data row2 col8\" >2.0130</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th id=\"T_e8219_level0_row3\" class=\"row_heading level0 row3\" >gbc</th>\n",
       "      <td id=\"T_e8219_row3_col0\" class=\"data row3 col0\" >Gradient Boosting Classifier</td>\n",
       "      <td id=\"T_e8219_row3_col1\" class=\"data row3 col1\" >0.6590</td>\n",
       "      <td id=\"T_e8219_row3_col2\" class=\"data row3 col2\" >0.8830</td>\n",
       "      <td id=\"T_e8219_row3_col3\" class=\"data row3 col3\" >0.6590</td>\n",
       "      <td id=\"T_e8219_row3_col4\" class=\"data row3 col4\" >0.6475</td>\n",
       "      <td id=\"T_e8219_row3_col5\" class=\"data row3 col5\" >0.6451</td>\n",
       "      <td id=\"T_e8219_row3_col6\" class=\"data row3 col6\" >0.5227</td>\n",
       "      <td id=\"T_e8219_row3_col7\" class=\"data row3 col7\" >0.5270</td>\n",
       "      <td id=\"T_e8219_row3_col8\" class=\"data row3 col8\" >13.9600</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th id=\"T_e8219_level0_row4\" class=\"row_heading level0 row4\" >lr</th>\n",
       "      <td id=\"T_e8219_row4_col0\" class=\"data row4 col0\" >Logistic Regression</td>\n",
       "      <td id=\"T_e8219_row4_col1\" class=\"data row4 col1\" >0.6443</td>\n",
       "      <td id=\"T_e8219_row4_col2\" class=\"data row4 col2\" >0.8568</td>\n",
       "      <td id=\"T_e8219_row4_col3\" class=\"data row4 col3\" >0.6443</td>\n",
       "      <td id=\"T_e8219_row4_col4\" class=\"data row4 col4\" >0.6458</td>\n",
       "      <td id=\"T_e8219_row4_col5\" class=\"data row4 col5\" >0.6440</td>\n",
       "      <td id=\"T_e8219_row4_col6\" class=\"data row4 col6\" >0.5121</td>\n",
       "      <td id=\"T_e8219_row4_col7\" class=\"data row4 col7\" >0.5128</td>\n",
       "      <td id=\"T_e8219_row4_col8\" class=\"data row4 col8\" >0.6520</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th id=\"T_e8219_level0_row5\" class=\"row_heading level0 row5\" >svm</th>\n",
       "      <td id=\"T_e8219_row5_col0\" class=\"data row5 col0\" >SVM - Linear Kernel</td>\n",
       "      <td id=\"T_e8219_row5_col1\" class=\"data row5 col1\" >0.6325</td>\n",
       "      <td id=\"T_e8219_row5_col2\" class=\"data row5 col2\" >0.0000</td>\n",
       "      <td id=\"T_e8219_row5_col3\" class=\"data row5 col3\" >0.6325</td>\n",
       "      <td id=\"T_e8219_row5_col4\" class=\"data row5 col4\" >0.6294</td>\n",
       "      <td id=\"T_e8219_row5_col5\" class=\"data row5 col5\" >0.6254</td>\n",
       "      <td id=\"T_e8219_row5_col6\" class=\"data row5 col6\" >0.4924</td>\n",
       "      <td id=\"T_e8219_row5_col7\" class=\"data row5 col7\" >0.4956</td>\n",
       "      <td id=\"T_e8219_row5_col8\" class=\"data row5 col8\" >0.0560</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th id=\"T_e8219_level0_row6\" class=\"row_heading level0 row6\" >rf</th>\n",
       "      <td id=\"T_e8219_row6_col0\" class=\"data row6 col0\" >Random Forest Classifier</td>\n",
       "      <td id=\"T_e8219_row6_col1\" class=\"data row6 col1\" >0.6305</td>\n",
       "      <td id=\"T_e8219_row6_col2\" class=\"data row6 col2\" >0.8571</td>\n",
       "      <td id=\"T_e8219_row6_col3\" class=\"data row6 col3\" >0.6305</td>\n",
       "      <td id=\"T_e8219_row6_col4\" class=\"data row6 col4\" >0.6097</td>\n",
       "      <td id=\"T_e8219_row6_col5\" class=\"data row6 col5\" >0.5810</td>\n",
       "      <td id=\"T_e8219_row6_col6\" class=\"data row6 col6\" >0.4691</td>\n",
       "      <td id=\"T_e8219_row6_col7\" class=\"data row6 col7\" >0.4888</td>\n",
       "      <td id=\"T_e8219_row6_col8\" class=\"data row6 col8\" >0.3810</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th id=\"T_e8219_level0_row7\" class=\"row_heading level0 row7\" >et</th>\n",
       "      <td id=\"T_e8219_row7_col0\" class=\"data row7 col0\" >Extra Trees Classifier</td>\n",
       "      <td id=\"T_e8219_row7_col1\" class=\"data row7 col1\" >0.6260</td>\n",
       "      <td id=\"T_e8219_row7_col2\" class=\"data row7 col2\" >0.8647</td>\n",
       "      <td id=\"T_e8219_row7_col3\" class=\"data row7 col3\" >0.6260</td>\n",
       "      <td id=\"T_e8219_row7_col4\" class=\"data row7 col4\" >0.6127</td>\n",
       "      <td id=\"T_e8219_row7_col5\" class=\"data row7 col5\" >0.5636</td>\n",
       "      <td id=\"T_e8219_row7_col6\" class=\"data row7 col6\" >0.4588</td>\n",
       "      <td id=\"T_e8219_row7_col7\" class=\"data row7 col7\" >0.4857</td>\n",
       "      <td id=\"T_e8219_row7_col8\" class=\"data row7 col8\" >0.1060</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th id=\"T_e8219_level0_row8\" class=\"row_heading level0 row8\" >knn</th>\n",
       "      <td id=\"T_e8219_row8_col0\" class=\"data row8 col0\" >K Neighbors Classifier</td>\n",
       "      <td id=\"T_e8219_row8_col1\" class=\"data row8 col1\" >0.6215</td>\n",
       "      <td id=\"T_e8219_row8_col2\" class=\"data row8 col2\" >0.8314</td>\n",
       "      <td id=\"T_e8219_row8_col3\" class=\"data row8 col3\" >0.6215</td>\n",
       "      <td id=\"T_e8219_row8_col4\" class=\"data row8 col4\" >0.6134</td>\n",
       "      <td id=\"T_e8219_row8_col5\" class=\"data row8 col5\" >0.6043</td>\n",
       "      <td id=\"T_e8219_row8_col6\" class=\"data row8 col6\" >0.4684</td>\n",
       "      <td id=\"T_e8219_row8_col7\" class=\"data row8 col7\" >0.4747</td>\n",
       "      <td id=\"T_e8219_row8_col8\" class=\"data row8 col8\" >0.2140</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th id=\"T_e8219_level0_row9\" class=\"row_heading level0 row9\" >qda</th>\n",
       "      <td id=\"T_e8219_row9_col0\" class=\"data row9 col0\" >Quadratic Discriminant Analysis</td>\n",
       "      <td id=\"T_e8219_row9_col1\" class=\"data row9 col1\" >0.6125</td>\n",
       "      <td id=\"T_e8219_row9_col2\" class=\"data row9 col2\" >0.7806</td>\n",
       "      <td id=\"T_e8219_row9_col3\" class=\"data row9 col3\" >0.6125</td>\n",
       "      <td id=\"T_e8219_row9_col4\" class=\"data row9 col4\" >0.5969</td>\n",
       "      <td id=\"T_e8219_row9_col5\" class=\"data row9 col5\" >0.4932</td>\n",
       "      <td id=\"T_e8219_row9_col6\" class=\"data row9 col6\" >0.4298</td>\n",
       "      <td id=\"T_e8219_row9_col7\" class=\"data row9 col7\" >0.4790</td>\n",
       "      <td id=\"T_e8219_row9_col8\" class=\"data row9 col8\" >0.0360</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th id=\"T_e8219_level0_row10\" class=\"row_heading level0 row10\" >ada</th>\n",
       "      <td id=\"T_e8219_row10_col0\" class=\"data row10 col0\" >Ada Boost Classifier</td>\n",
       "      <td id=\"T_e8219_row10_col1\" class=\"data row10 col1\" >0.5967</td>\n",
       "      <td id=\"T_e8219_row10_col2\" class=\"data row10 col2\" >0.8112</td>\n",
       "      <td id=\"T_e8219_row10_col3\" class=\"data row10 col3\" >0.5967</td>\n",
       "      <td id=\"T_e8219_row10_col4\" class=\"data row10 col4\" >0.5852</td>\n",
       "      <td id=\"T_e8219_row10_col5\" class=\"data row10 col5\" >0.5890</td>\n",
       "      <td id=\"T_e8219_row10_col6\" class=\"data row10 col6\" >0.4413</td>\n",
       "      <td id=\"T_e8219_row10_col7\" class=\"data row10 col7\" >0.4425</td>\n",
       "      <td id=\"T_e8219_row10_col8\" class=\"data row10 col8\" >0.6600</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th id=\"T_e8219_level0_row11\" class=\"row_heading level0 row11\" >nb</th>\n",
       "      <td id=\"T_e8219_row11_col0\" class=\"data row11 col0\" >Naive Bayes</td>\n",
       "      <td id=\"T_e8219_row11_col1\" class=\"data row11 col1\" >0.5934</td>\n",
       "      <td id=\"T_e8219_row11_col2\" class=\"data row11 col2\" >0.8249</td>\n",
       "      <td id=\"T_e8219_row11_col3\" class=\"data row11 col3\" >0.5934</td>\n",
       "      <td id=\"T_e8219_row11_col4\" class=\"data row11 col4\" >0.5949</td>\n",
       "      <td id=\"T_e8219_row11_col5\" class=\"data row11 col5\" >0.5933</td>\n",
       "      <td id=\"T_e8219_row11_col6\" class=\"data row11 col6\" >0.4423</td>\n",
       "      <td id=\"T_e8219_row11_col7\" class=\"data row11 col7\" >0.4428</td>\n",
       "      <td id=\"T_e8219_row11_col8\" class=\"data row11 col8\" >0.0270</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th id=\"T_e8219_level0_row12\" class=\"row_heading level0 row12\" >dt</th>\n",
       "      <td id=\"T_e8219_row12_col0\" class=\"data row12 col0\" >Decision Tree Classifier</td>\n",
       "      <td id=\"T_e8219_row12_col1\" class=\"data row12 col1\" >0.4587</td>\n",
       "      <td id=\"T_e8219_row12_col2\" class=\"data row12 col2\" >0.6353</td>\n",
       "      <td id=\"T_e8219_row12_col3\" class=\"data row12 col3\" >0.4587</td>\n",
       "      <td id=\"T_e8219_row12_col4\" class=\"data row12 col4\" >0.4639</td>\n",
       "      <td id=\"T_e8219_row12_col5\" class=\"data row12 col5\" >0.4602</td>\n",
       "      <td id=\"T_e8219_row12_col6\" class=\"data row12 col6\" >0.2590</td>\n",
       "      <td id=\"T_e8219_row12_col7\" class=\"data row12 col7\" >0.2595</td>\n",
       "      <td id=\"T_e8219_row12_col8\" class=\"data row12 col8\" >0.1000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th id=\"T_e8219_level0_row13\" class=\"row_heading level0 row13\" >dummy</th>\n",
       "      <td id=\"T_e8219_row13_col0\" class=\"data row13 col0\" >Dummy Classifier</td>\n",
       "      <td id=\"T_e8219_row13_col1\" class=\"data row13 col1\" >0.3407</td>\n",
       "      <td id=\"T_e8219_row13_col2\" class=\"data row13 col2\" >0.5000</td>\n",
       "      <td id=\"T_e8219_row13_col3\" class=\"data row13 col3\" >0.3407</td>\n",
       "      <td id=\"T_e8219_row13_col4\" class=\"data row13 col4\" >0.1161</td>\n",
       "      <td id=\"T_e8219_row13_col5\" class=\"data row13 col5\" >0.1731</td>\n",
       "      <td id=\"T_e8219_row13_col6\" class=\"data row13 col6\" >0.0000</td>\n",
       "      <td id=\"T_e8219_row13_col7\" class=\"data row13 col7\" >0.0000</td>\n",
       "      <td id=\"T_e8219_row13_col8\" class=\"data row13 col8\" >0.0220</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n"
      ],
      "text/plain": [
       "<pandas.io.formats.style.Styler at 0x20e3fb57b10>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<style>#sk-container-id-1 {color: black;background-color: white;}#sk-container-id-1 pre{padding: 0;}#sk-container-id-1 div.sk-toggleable {background-color: white;}#sk-container-id-1 label.sk-toggleable__label {cursor: pointer;display: block;width: 100%;margin-bottom: 0;padding: 0.3em;box-sizing: border-box;text-align: center;}#sk-container-id-1 label.sk-toggleable__label-arrow:before {content: \"▸\";float: left;margin-right: 0.25em;color: #696969;}#sk-container-id-1 label.sk-toggleable__label-arrow:hover:before {color: black;}#sk-container-id-1 div.sk-estimator:hover label.sk-toggleable__label-arrow:before {color: black;}#sk-container-id-1 div.sk-toggleable__content {max-height: 0;max-width: 0;overflow: hidden;text-align: left;background-color: #f0f8ff;}#sk-container-id-1 div.sk-toggleable__content pre {margin: 0.2em;color: black;border-radius: 0.25em;background-color: #f0f8ff;}#sk-container-id-1 input.sk-toggleable__control:checked~div.sk-toggleable__content {max-height: 200px;max-width: 100%;overflow: auto;}#sk-container-id-1 input.sk-toggleable__control:checked~label.sk-toggleable__label-arrow:before {content: \"▾\";}#sk-container-id-1 div.sk-estimator input.sk-toggleable__control:checked~label.sk-toggleable__label {background-color: #d4ebff;}#sk-container-id-1 div.sk-label input.sk-toggleable__control:checked~label.sk-toggleable__label {background-color: #d4ebff;}#sk-container-id-1 input.sk-hidden--visually {border: 0;clip: rect(1px 1px 1px 1px);clip: rect(1px, 1px, 1px, 1px);height: 1px;margin: -1px;overflow: hidden;padding: 0;position: absolute;width: 1px;}#sk-container-id-1 div.sk-estimator {font-family: monospace;background-color: #f0f8ff;border: 1px dotted black;border-radius: 0.25em;box-sizing: border-box;margin-bottom: 0.5em;}#sk-container-id-1 div.sk-estimator:hover {background-color: #d4ebff;}#sk-container-id-1 div.sk-parallel-item::after {content: \"\";width: 100%;border-bottom: 1px solid gray;flex-grow: 1;}#sk-container-id-1 div.sk-label:hover label.sk-toggleable__label {background-color: #d4ebff;}#sk-container-id-1 div.sk-serial::before {content: \"\";position: absolute;border-left: 1px solid gray;box-sizing: border-box;top: 0;bottom: 0;left: 50%;z-index: 0;}#sk-container-id-1 div.sk-serial {display: flex;flex-direction: column;align-items: center;background-color: white;padding-right: 0.2em;padding-left: 0.2em;position: relative;}#sk-container-id-1 div.sk-item {position: relative;z-index: 1;}#sk-container-id-1 div.sk-parallel {display: flex;align-items: stretch;justify-content: center;background-color: white;position: relative;}#sk-container-id-1 div.sk-item::before, #sk-container-id-1 div.sk-parallel-item::before {content: \"\";position: absolute;border-left: 1px solid gray;box-sizing: border-box;top: 0;bottom: 0;left: 50%;z-index: -1;}#sk-container-id-1 div.sk-parallel-item {display: flex;flex-direction: column;z-index: 1;position: relative;background-color: white;}#sk-container-id-1 div.sk-parallel-item:first-child::after {align-self: flex-end;width: 50%;}#sk-container-id-1 div.sk-parallel-item:last-child::after {align-self: flex-start;width: 50%;}#sk-container-id-1 div.sk-parallel-item:only-child::after {width: 0;}#sk-container-id-1 div.sk-dashed-wrapped {border: 1px dashed gray;margin: 0 0.4em 0.5em 0.4em;box-sizing: border-box;padding-bottom: 0.4em;background-color: white;}#sk-container-id-1 div.sk-label label {font-family: monospace;font-weight: bold;display: inline-block;line-height: 1.2em;}#sk-container-id-1 div.sk-label-container {text-align: center;}#sk-container-id-1 div.sk-container {/* jupyter's `normalize.less` sets `[hidden] { display: none; }` but bootstrap.min.css set `[hidden] { display: none !important; }` so we also need the `!important` here to be able to override the default hidden behavior on the sphinx rendered scikit-learn.org. See: https://github.com/scikit-learn/scikit-learn/issues/21755 */display: inline-block !important;position: relative;}#sk-container-id-1 div.sk-text-repr-fallback {display: none;}</style><div id=\"sk-container-id-1\" class=\"sk-top-container\"><div class=\"sk-text-repr-fallback\"><pre>RidgeClassifier(alpha=1.0, class_weight=None, copy_X=True, fit_intercept=True,\n",
       "                max_iter=None, positive=False, random_state=9088, solver=&#x27;auto&#x27;,\n",
       "                tol=0.0001)</pre><b>In a Jupyter environment, please rerun this cell to show the HTML representation or trust the notebook. <br />On GitHub, the HTML representation is unable to render, please try loading this page with nbviewer.org.</b></div><div class=\"sk-container\" hidden><div class=\"sk-item\"><div class=\"sk-estimator sk-toggleable\"><input class=\"sk-toggleable__control sk-hidden--visually\" id=\"sk-estimator-id-1\" type=\"checkbox\" checked><label for=\"sk-estimator-id-1\" class=\"sk-toggleable__label sk-toggleable__label-arrow\">RidgeClassifier</label><div class=\"sk-toggleable__content\"><pre>RidgeClassifier(alpha=1.0, class_weight=None, copy_X=True, fit_intercept=True,\n",
       "                max_iter=None, positive=False, random_state=9088, solver=&#x27;auto&#x27;,\n",
       "                tol=0.0001)</pre></div></div></div></div></div>"
      ],
      "text/plain": [
       "RidgeClassifier(alpha=1.0, class_weight=None, copy_X=True, fit_intercept=True,\n",
       "                max_iter=None, positive=False, random_state=9088, solver='auto',\n",
       "                tol=0.0001)"
      ]
     },
     "execution_count": 305,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "exp_class.compare_models()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "hide_input": false,
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.7"
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {},
   "number_sections": true,
   "sideBar": true,
   "skip_h1_title": true,
   "title_cell": "Summary",
   "title_sidebar": "Contents",
   "toc_cell": true,
   "toc_position": {
    "height": "calc(100% - 180px)",
    "left": "10px",
    "top": "150px",
    "width": "300.15px"
   },
   "toc_section_display": true,
   "toc_window_display": false
  },
  "varInspector": {
   "cols": {
    "lenName": 16,
    "lenType": 16,
    "lenVar": 40
   },
   "kernels_config": {
    "python": {
     "delete_cmd_postfix": "",
     "delete_cmd_prefix": "del ",
     "library": "var_list.py",
     "varRefreshCmd": "print(var_dic_list())"
    },
    "r": {
     "delete_cmd_postfix": ") ",
     "delete_cmd_prefix": "rm(",
     "library": "var_list.r",
     "varRefreshCmd": "cat(var_dic_list()) "
    }
   },
   "position": {
    "height": "498.85px",
    "left": "651.8px",
    "right": "20px",
    "top": "56px",
    "width": "715px"
   },
   "types_to_exclude": [
    "module",
    "function",
    "builtin_function_or_method",
    "instance",
    "_Feature"
   ],
   "window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
